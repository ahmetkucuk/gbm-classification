2017-06-21 04:40:42.406826: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-21 04:40:42.406882: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-21 04:40:42.406891: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-21 04:40:42.406897: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-21 04:40:42.406903: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-06-21 04:40:42.876163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.3285
pciBusID 0000:82:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2017-06-21 04:40:42.876238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-06-21 04:40:42.876255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-06-21 04:40:42.876306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
Number of Patches: 307374
Global Iter: 100 training loss: 1.99067
Global Iter: 100 training acc: 0.15625
Global Iter: 200 training loss: 1.97028
Global Iter: 200 training acc: 0.125
Global Iter: 300 training loss: 2.05674
Global Iter: 300 training acc: 0.21875
Global Iter: 400 training loss: 2.04707
Global Iter: 400 training acc: 0.0625
Global Iter: 500 training loss: 2.11171
Global Iter: 500 training acc: 0.125
Global Iter: 600 training loss: 2.1148
Global Iter: 600 training acc: 0.125
Global Iter: 700 training loss: 2.00808
Global Iter: 700 training acc: 0.25
Global Iter: 800 training loss: 1.98119
Global Iter: 800 training acc: 0.1875
Global Iter: 900 training loss: 1.9091
Global Iter: 900 training acc: 0.125
Global Iter: 1000 training loss: 2.17836
Global Iter: 1000 training acc: 0.1875
Global Iter: 1100 training loss: 1.94894
Global Iter: 1100 training acc: 0.15625
Global Iter: 1200 training loss: 1.98083
Global Iter: 1200 training acc: 0.09375
Global Iter: 1300 training loss: 1.94924
Global Iter: 1300 training acc: 0.21875
Global Iter: 1400 training loss: 2.02782
Global Iter: 1400 training acc: 0.15625
Global Iter: 1500 training loss: 2.04245
Global Iter: 1500 training acc: 0.09375
Global Iter: 1600 training loss: 2.10957
Global Iter: 1600 training acc: 0.125
Global Iter: 1700 training loss: 2.02575
Global Iter: 1700 training acc: 0.21875
Global Iter: 1800 training loss: 1.94489
Global Iter: 1800 training acc: 0.28125
Global Iter: 1900 training loss: 1.97833
Global Iter: 1900 training acc: 0.21875
Global Iter: 2000 training loss: 2.09679
Global Iter: 2000 training acc: 0.125
Global Iter: 2100 training loss: 1.99586
Global Iter: 2100 training acc: 0.21875
Global Iter: 2200 training loss: 1.98987
Global Iter: 2200 training acc: 0.15625
Global Iter: 2300 training loss: 1.97432
Global Iter: 2300 training acc: 0.1875
Global Iter: 2400 training loss: 1.93347
Global Iter: 2400 training acc: 0.375
Global Iter: 2500 training loss: 1.99012
Global Iter: 2500 training acc: 0.125
Global Iter: 2600 training loss: 1.98619
Global Iter: 2600 training acc: 0.15625
Global Iter: 2700 training loss: 1.99923
Global Iter: 2700 training acc: 0.125
Global Iter: 2800 training loss: 1.91836
Global Iter: 2800 training acc: 0.21875
Global Iter: 2900 training loss: 2.03307
Global Iter: 2900 training acc: 0.125
Global Iter: 3000 training loss: 1.94284
Global Iter: 3000 training acc: 0.28125
Global Iter: 3100 training loss: 2.14829
Global Iter: 3100 training acc: 0.15625
Global Iter: 3200 training loss: 2.0034
Global Iter: 3200 training acc: 0.3125
Global Iter: 3300 training loss: 2.05927
Global Iter: 3300 training acc: 0.0625
Global Iter: 3400 training loss: 1.97064
Global Iter: 3400 training acc: 0.21875
Global Iter: 3500 training loss: 2.07188
Global Iter: 3500 training acc: 0.25
Global Iter: 3600 training loss: 2.00979
Global Iter: 3600 training acc: 0.1875
Global Iter: 3700 training loss: 1.9871
Global Iter: 3700 training acc: 0.1875
Global Iter: 3800 training loss: 1.88835
Global Iter: 3800 training acc: 0.21875
Global Iter: 3900 training loss: 1.94812
Global Iter: 3900 training acc: 0.28125
Global Iter: 4000 training loss: 2.03571
Global Iter: 4000 training acc: 0.25
Global Iter: 4100 training loss: 1.98848
Global Iter: 4100 training acc: 0.125
Global Iter: 4200 training loss: 1.95342
Global Iter: 4200 training acc: 0.1875
Global Iter: 4300 training loss: 1.97484
Global Iter: 4300 training acc: 0.28125
Global Iter: 4400 training loss: 2.03052
Global Iter: 4400 training acc: 0.15625
Global Iter: 4500 training loss: 1.94308
Global Iter: 4500 training acc: 0.1875
Global Iter: 4600 training loss: 1.99527
Global Iter: 4600 training acc: 0.1875
Global Iter: 4700 training loss: 1.97906
Global Iter: 4700 training acc: 0.125
Global Iter: 4800 training loss: 2.02414
Global Iter: 4800 training acc: 0.15625
Global Iter: 4900 training loss: 2.04271
Global Iter: 4900 training acc: 0.1875
Global Iter: 5000 training loss: 1.91215
Global Iter: 5000 training acc: 0.21875
Global Iter: 5100 training loss: 2.00871
Global Iter: 5100 training acc: 0.125
Global Iter: 5200 training loss: 1.98407
Global Iter: 5200 training acc: 0.09375
Global Iter: 5300 training loss: 2.00492
Global Iter: 5300 training acc: 0.125
Global Iter: 5400 training loss: 2.00278
Global Iter: 5400 training acc: 0.1875
Global Iter: 5500 training loss: 1.9922
Global Iter: 5500 training acc: 0.125
Global Iter: 5600 training loss: 2.01375
Global Iter: 5600 training acc: 0.09375
Global Iter: 5700 training loss: 2.06593
Global Iter: 5700 training acc: 0.21875
Global Iter: 5800 training loss: 2.04258
Global Iter: 5800 training acc: 0.25
Global Iter: 5900 training loss: 2.02321
Global Iter: 5900 training acc: 0.15625
Global Iter: 6000 training loss: 1.99445
Global Iter: 6000 training acc: 0.0625
Global Iter: 6100 training loss: 2.00035
Global Iter: 6100 training acc: 0.25
Global Iter: 6200 training loss: 1.9993
Global Iter: 6200 training acc: 0.15625
Global Iter: 6300 training loss: 2.1238
Global Iter: 6300 training acc: 0.21875
Global Iter: 6400 training loss: 1.98542
Global Iter: 6400 training acc: 0.15625
Global Iter: 6500 training loss: 1.93715
Global Iter: 6500 training acc: 0.28125
Global Iter: 6600 training loss: 2.08861
Global Iter: 6600 training acc: 0.15625
Global Iter: 6700 training loss: 1.95658
Global Iter: 6700 training acc: 0.21875
Global Iter: 6800 training loss: 2.06223
Global Iter: 6800 training acc: 0.1875
Global Iter: 6900 training loss: 1.97453
Global Iter: 6900 training acc: 0.15625
Global Iter: 7000 training loss: 2.02219
Global Iter: 7000 training acc: 0.09375
Global Iter: 7100 training loss: 1.98684
Global Iter: 7100 training acc: 0.15625
Global Iter: 7200 training loss: 1.98557
Global Iter: 7200 training acc: 0.21875
Global Iter: 7300 training loss: 2.03633
Global Iter: 7300 training acc: 0.21875
Global Iter: 7400 training loss: 2.11858
Global Iter: 7400 training acc: 0.09375
Global Iter: 7500 training loss: 1.96572
Global Iter: 7500 training acc: 0.15625
Global Iter: 7600 training loss: 1.92563
Global Iter: 7600 training acc: 0.15625
Global Iter: 7700 training loss: 1.909
Global Iter: 7700 training acc: 0.25
Global Iter: 7800 training loss: 2.02026
Global Iter: 7800 training acc: 0.125
Global Iter: 7900 training loss: 2.04403
Global Iter: 7900 training acc: 0.15625
Global Iter: 8000 training loss: 1.97864
Global Iter: 8000 training acc: 0.1875
Global Iter: 8100 training loss: 1.94065
Global Iter: 8100 training acc: 0.34375
Global Iter: 8200 training loss: 1.97109
Global Iter: 8200 training acc: 0.25
Global Iter: 8300 training loss: 1.9181
Global Iter: 8300 training acc: 0.34375
Global Iter: 8400 training loss: 2.10571
Global Iter: 8400 training acc: 0.15625
Global Iter: 8500 training loss: 1.94595
Global Iter: 8500 training acc: 0.1875
Global Iter: 8600 training loss: 1.9675
Global Iter: 8600 training acc: 0.09375
Global Iter: 8700 training loss: 1.98942
Global Iter: 8700 training acc: 0.1875
Global Iter: 8800 training loss: 2.10133
Global Iter: 8800 training acc: 0.21875
Global Iter: 8900 training loss: 1.97445
Global Iter: 8900 training acc: 0.15625
Global Iter: 9000 training loss: 1.94583
Global Iter: 9000 training acc: 0.21875
Global Iter: 9100 training loss: 2.03221
Global Iter: 9100 training acc: 0.15625
Global Iter: 9200 training loss: 2.18995
Global Iter: 9200 training acc: 0.125
Global Iter: 9300 training loss: 2.10229
Global Iter: 9300 training acc: 0.125
Global Iter: 9400 training loss: 2.05183
Global Iter: 9400 training acc: 0.15625
Global Iter: 9500 training loss: 2.02048
Global Iter: 9500 training acc: 0.1875
Global Iter: 9600 training loss: 1.91073
Global Iter: 9600 training acc: 0.25
Global Iter: 9700 training loss: 2.04091
Global Iter: 9700 training acc: 0.125
Global Iter: 9800 training loss: 1.94718
Global Iter: 9800 training acc: 0.15625
Global Iter: 9900 training loss: 2.01511
Global Iter: 9900 training acc: 0.125
Global Iter: 10000 training loss: 2.0246
Global Iter: 10000 training acc: 0.125
Global Iter: 10100 training loss: 2.00277
Global Iter: 10100 training acc: 0.09375
Global Iter: 10200 training loss: 2.08279
Global Iter: 10200 training acc: 0.15625
Global Iter: 10300 training loss: 1.88611
Global Iter: 10300 training acc: 0.25
Global Iter: 10400 training loss: 2.04181
Global Iter: 10400 training acc: 0.1875
Global Iter: 10500 training loss: 2.00928
Global Iter: 10500 training acc: 0.3125
Global Iter: 10600 training loss: 1.96647
Global Iter: 10600 training acc: 0.1875
Global Iter: 10700 training loss: 1.90111
Global Iter: 10700 training acc: 0.15625
Global Iter: 10800 training loss: 1.95961
Global Iter: 10800 training acc: 0.1875
Global Iter: 10900 training loss: 2.01251
Global Iter: 10900 training acc: 0.15625
Global Iter: 11000 training loss: 2.02571
Global Iter: 11000 training acc: 0.15625
Global Iter: 11100 training loss: 2.15524
Global Iter: 11100 training acc: 0.21875
Global Iter: 11200 training loss: 1.99862
Global Iter: 11200 training acc: 0.125
Global Iter: 11300 training loss: 1.97437
Global Iter: 11300 training acc: 0.15625
Global Iter: 11400 training loss: 1.94826
Global Iter: 11400 training acc: 0.1875
Global Iter: 11500 training loss: 2.02476
Global Iter: 11500 training acc: 0.25
Global Iter: 11600 training loss: 1.99349
Global Iter: 11600 training acc: 0.125
Global Iter: 11700 training loss: 2.01138
Global Iter: 11700 training acc: 0.09375
Global Iter: 11800 training loss: 1.99279
Global Iter: 11800 training acc: 0.125
Global Iter: 11900 training loss: 1.97739
Global Iter: 11900 training acc: 0.1875
Global Iter: 12000 training loss: 1.96973
Global Iter: 12000 training acc: 0.1875
Global Iter: 12100 training loss: 2.05356
Global Iter: 12100 training acc: 0.125
Global Iter: 12200 training loss: 1.96195
Global Iter: 12200 training acc: 0.21875
Global Iter: 12300 training loss: 1.93061
Global Iter: 12300 training acc: 0.1875
Global Iter: 12400 training loss: 2.02153
Global Iter: 12400 training acc: 0.1875
Global Iter: 12500 training loss: 1.93245
Global Iter: 12500 training acc: 0.21875
Global Iter: 12600 training loss: 2.05729
Global Iter: 12600 training acc: 0.21875
Global Iter: 12700 training loss: 1.9777
Global Iter: 12700 training acc: 0.1875
Global Iter: 12800 training loss: 1.88974
Global Iter: 12800 training acc: 0.15625
Global Iter: 12900 training loss: 1.97196
Global Iter: 12900 training acc: 0.1875
Global Iter: 13000 training loss: 1.98554
Global Iter: 13000 training acc: 0.15625
Global Iter: 13100 training loss: 1.98076
Global Iter: 13100 training acc: 0.21875
Global Iter: 13200 training loss: 2.02029
Global Iter: 13200 training acc: 0.125
Global Iter: 13300 training loss: 2.02047
Global Iter: 13300 training acc: 0.125
Global Iter: 13400 training loss: 1.99365
Global Iter: 13400 training acc: 0.0625
Global Iter: 13500 training loss: 1.98277
Global Iter: 13500 training acc: 0.1875
Global Iter: 13600 training loss: 2.02543
Global Iter: 13600 training acc: 0.15625
Global Iter: 13700 training loss: 1.9706
Global Iter: 13700 training acc: 0.15625
Global Iter: 13800 training loss: 1.98964
Global Iter: 13800 training acc: 0.1875
Global Iter: 13900 training loss: 1.92375
Global Iter: 13900 training acc: 0.25
Global Iter: 14000 training loss: 2.08439
Global Iter: 14000 training acc: 0.1875
Global Iter: 14100 training loss: 1.94814
Global Iter: 14100 training acc: 0.28125
Global Iter: 14200 training loss: 1.99908
Global Iter: 14200 training acc: 0.1875
Global Iter: 14300 training loss: 2.03713
Global Iter: 14300 training acc: 0.15625
Global Iter: 14400 training loss: 2.01048
Global Iter: 14400 training acc: 0.21875
Global Iter: 14500 training loss: 1.97363
Global Iter: 14500 training acc: 0.09375
Global Iter: 14600 training loss: 1.96674
Global Iter: 14600 training acc: 0.1875
Global Iter: 14700 training loss: 1.98879
Global Iter: 14700 training acc: 0.03125
Global Iter: 14800 training loss: 2.08964
Global Iter: 14800 training acc: 0.09375
Global Iter: 14900 training loss: 1.97979
Global Iter: 14900 training acc: 0.1875
Global Iter: 15000 training loss: 2.07815
Global Iter: 15000 training acc: 0.15625
Global Iter: 15100 training loss: 2.08103
Global Iter: 15100 training acc: 0.125
Global Iter: 15200 training loss: 2.01372
Global Iter: 15200 training acc: 0.1875
Glo2017-06-21 05:13:11.335204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-19211
bal Iter: 15300 training loss: 1.99101
Global Iter: 15300 training acc: 0.15625
Global Iter: 15400 training loss: 2.04421
Global Iter: 15400 training acc: 0.15625
Global Iter: 15500 training loss: 2.16605
Global Iter: 15500 training acc: 0.21875
Global Iter: 15600 training loss: 2.11521
Global Iter: 15600 training acc: 0.125
Global Iter: 15700 training loss: 2.02196
Global Iter: 15700 training acc: 0.125
Global Iter: 15800 training loss: 1.98023
Global Iter: 15800 training acc: 0.28125
Global Iter: 15900 training loss: 2.00869
Global Iter: 15900 training acc: 0.1875
Global Iter: 16000 training loss: 1.96385
Global Iter: 16000 training acc: 0.125
Global Iter: 16100 training loss: 1.92552
Global Iter: 16100 training acc: 0.21875
Global Iter: 16200 training loss: 1.97091
Global Iter: 16200 training acc: 0.25
Global Iter: 16300 training loss: 2.00976
Global Iter: 16300 training acc: 0.15625
Global Iter: 16400 training loss: 2.02469
Global Iter: 16400 training acc: 0.09375
Global Iter: 16500 training loss: 1.94615
Global Iter: 16500 training acc: 0.21875
Global Iter: 16600 training loss: 2.0741
Global Iter: 16600 training acc: 0.0625
Global Iter: 16700 training loss: 2.06283
Global Iter: 16700 training acc: 0.15625
Global Iter: 16800 training loss: 2.08856
Global Iter: 16800 training acc: 0.15625
Global Iter: 16900 training loss: 2.13861
Global Iter: 16900 training acc: 0.125
Global Iter: 17000 training loss: 2.0149
Global Iter: 17000 training acc: 0.21875
Global Iter: 17100 training loss: 1.94929
Global Iter: 17100 training acc: 0.15625
Global Iter: 17200 training loss: 1.93134
Global Iter: 17200 training acc: 0.28125
Global Iter: 17300 training loss: 2.1381
Global Iter: 17300 training acc: 0.09375
Global Iter: 17400 training loss: 2.04427
Global Iter: 17400 training acc: 0.1875
Global Iter: 17500 training loss: 2.02975
Global Iter: 17500 training acc: 0.0625
Global Iter: 17600 training loss: 2.04575
Global Iter: 17600 training acc: 0.25
Global Iter: 17700 training loss: 1.9596
Global Iter: 17700 training acc: 0.1875
Global Iter: 17800 training loss: 1.9815
Global Iter: 17800 training acc: 0.21875
Global Iter: 17900 training loss: 1.89847
Global Iter: 17900 training acc: 0.375
Global Iter: 18000 training loss: 1.90889
Global Iter: 18000 training acc: 0.125
Global Iter: 18100 training loss: 2.00604
Global Iter: 18100 training acc: 0.1875
Global Iter: 18200 training loss: 1.93383
Global Iter: 18200 training acc: 0.25
Global Iter: 18300 training loss: 1.98316
Global Iter: 18300 training acc: 0.15625
Global Iter: 18400 training loss: 2.00792
Global Iter: 18400 training acc: 0.15625
Global Iter: 18500 training loss: 2.11006
Global Iter: 18500 training acc: 0.125
Global Iter: 18600 training loss: 2.17705
Global Iter: 18600 training acc: 0.125
Global Iter: 18700 training loss: 1.9593
Global Iter: 18700 training acc: 0.125
Global Iter: 18800 training loss: 1.99396
Global Iter: 18800 training acc: 0.15625
Global Iter: 18900 training loss: 2.12522
Global Iter: 18900 training acc: 0.1875
Global Iter: 19000 training loss: 1.98054
Global Iter: 19000 training acc: 0.1875
Global Iter: 19100 training loss: 2.07121
Global Iter: 19100 training acc: 0.09375
Global Iter: 19200 training loss: 1.91968
Global Iter: 19200 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-19211
Number of Patches: 304301
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-19211
Global Iter: 19300 training loss: 2.04203
Global Iter: 19300 training acc: 0.125
Global Iter: 19400 training loss: 2.08177
Global Iter: 19400 training acc: 0.28125
Global Iter: 19500 training loss: 1.94735
Global Iter: 19500 training acc: 0.28125
Global Iter: 19600 training loss: 1.94489
Global Iter: 19600 training acc: 0.34375
Global Iter: 19700 training loss: 2.02712
Global Iter: 19700 training acc: 0.125
Global Iter: 19800 training loss: 1.99909
Global Iter: 19800 training acc: 0.21875
Global Iter: 19900 training loss: 1.9929
Global Iter: 19900 training acc: 0.1875
Global Iter: 20000 training loss: 2.11225
Global Iter: 20000 training acc: 0.1875
Global Iter: 20100 training loss: 2.07926
Global Iter: 20100 training acc: 0.1875
Global Iter: 20200 training loss: 1.96839
Global Iter: 20200 training acc: 0.21875
Global Iter: 20300 training loss: 1.96353
Global Iter: 20300 training acc: 0.15625
Global Iter: 20400 training loss: 1.96177
Global Iter: 20400 training acc: 0.15625
Global Iter: 20500 training loss: 1.94791
Global Iter: 20500 training acc: 0.1875
Global Iter: 20600 training loss: 2.11677
Global Iter: 20600 training acc: 0.03125
Global Iter: 20700 training loss: 2.01731
Global Iter: 20700 training acc: 0.15625
Global Iter: 20800 training loss: 1.92938
Global Iter: 20800 training acc: 0.25
Global Iter: 20900 training loss: 2.03923
Global Iter: 20900 training acc: 0.28125
Global Iter: 21000 training loss: 1.99885
Global Iter: 21000 training acc: 0.25
Global Iter: 21100 training loss: 2.06868
Global Iter: 21100 training acc: 0.15625
Global Iter: 21200 training loss: 1.94767
Global Iter: 21200 training acc: 0.1875
Global Iter: 21300 training loss: 2.06208
Global Iter: 21300 training acc: 0.21875
Global Iter: 21400 training loss: 2.12486
Global Iter: 21400 training acc: 0.0625
Global Iter: 21500 training loss: 1.96955
Global Iter: 21500 training acc: 0.15625
Global Iter: 21600 training loss: 1.96671
Global Iter: 21600 training acc: 0.28125
Global Iter: 21700 training loss: 1.93488
Global Iter: 21700 training acc: 0.1875
Global Iter: 21800 training loss: 2.10492
Global Iter: 21800 training acc: 0.15625
Global Iter: 21900 training loss: 2.01495
Global Iter: 21900 training acc: 0.25
Global Iter: 22000 training loss: 2.15829
Global Iter: 22000 training acc: 0.125
Global Iter: 22100 training loss: 2.05245
Global Iter: 22100 training acc: 0.25
Global Iter: 22200 training loss: 2.02436
Global Iter: 22200 training acc: 0.125
Global Iter: 22300 training loss: 1.95757
Global Iter: 22300 training acc: 0.125
Global Iter: 22400 training loss: 1.98774
Global Iter: 22400 training acc: 0.15625
Global Iter: 22500 training loss: 1.96766
Global Iter: 22500 training acc: 0.125
Global Iter: 22600 training loss: 1.85711
Global Iter: 22600 training acc: 0.34375
Global Iter: 22700 training loss: 1.90078
Global Iter: 22700 training acc: 0.125
Global Iter: 22800 training loss: 2.00658
Global Iter: 22800 training acc: 0.25
Global Iter: 22900 training loss: 1.91304
Global Iter: 22900 training acc: 0.25
Global Iter: 23000 training loss: 1.94625
Global Iter: 23000 training acc: 0.15625
Global Iter: 23100 training loss: 1.91165
Global Iter: 23100 training acc: 0.28125
Global Iter: 23200 training loss: 1.98222
Global Iter: 23200 training acc: 0.125
Global Iter: 23300 training loss: 1.97289
Global Iter: 23300 training acc: 0.25
Global Iter: 23400 training loss: 2.0611
Global Iter: 23400 training acc: 0.09375
Global Iter: 23500 training loss: 1.932
Global Iter: 23500 training acc: 0.15625
Global Iter: 23600 training loss: 2.1443
Global Iter: 23600 training acc: 0.09375
Global Iter: 23700 training loss: 1.96691
Global Iter: 23700 training acc: 0.125
Global Iter: 23800 training loss: 1.98205
Global Iter: 23800 training acc: 0.28125
Global Iter: 23900 training loss: 1.95449
Global Iter: 23900 training acc: 0.1875
Global Iter: 24000 training loss: 1.91565
Global Iter: 24000 training acc: 0.21875
Global Iter: 24100 training loss: 1.95841
Global Iter: 24100 training acc: 0.1875
Global Iter: 24200 training loss: 2.02555
Global Iter: 24200 training acc: 0.125
Global Iter: 24300 training loss: 2.05889
Global Iter: 24300 training acc: 0.1875
Global Iter: 24400 training loss: 1.89853
Global Iter: 24400 training acc: 0.28125
Global Iter: 24500 training loss: 2.04724
Global Iter: 24500 training acc: 0.1875
Global Iter: 24600 training loss: 1.97834
Global Iter: 24600 training acc: 0.25
Global Iter: 24700 training loss: 2.00772
Global Iter: 24700 training acc: 0.125
Global Iter: 24800 training loss: 1.99847
Global Iter: 24800 training acc: 0.1875
Global Iter: 24900 training loss: 2.08692
Global Iter: 24900 training acc: 0.0625
Global Iter: 25000 training loss: 1.98477
Global Iter: 25000 training acc: 0.1875
Global Iter: 25100 training loss: 2.03803
Global Iter: 25100 training acc: 0.125
Global Iter: 25200 training loss: 2.04731
Global Iter: 25200 training acc: 0.21875
Global Iter: 25300 training loss: 1.96107
Global Iter: 25300 training acc: 0.21875
Global Iter: 25400 training loss: 2.01263
Global Iter: 25400 training acc: 0.1875
Global Iter: 25500 training loss: 1.99886
Global Iter: 25500 training acc: 0.1875
Global Iter: 25600 training loss: 1.96038
Global Iter: 25600 training acc: 0.15625
Global Iter: 25700 training loss: 1.90911
Global Iter: 25700 training acc: 0.21875
Global Iter: 25800 training loss: 1.97429
Global Iter: 25800 training acc: 0.28125
Global Iter: 25900 training loss: 1.96134
Global Iter: 25900 training acc: 0.1875
Global Iter: 26000 training loss: 1.99081
Global Iter: 26000 training acc: 0.125
Global Iter: 26100 training loss: 1.97969
Global Iter: 26100 training acc: 0.1875
Global Iter: 26200 training loss: 1.94004
Global Iter: 26200 training acc: 0.25
Global Iter: 26300 training loss: 2.00317
Global Iter: 26300 training acc: 0.15625
Global Iter: 26400 training loss: 1.981
Global Iter: 26400 training acc: 0.125
Global Iter: 26500 training loss: 2.15444
Global Iter: 26500 training acc: 0.15625
Global Iter: 26600 training loss: 2.06237
Global Iter: 26600 training acc: 0.1875
Global Iter: 26700 training loss: 2.01898
Global Iter: 26700 training acc: 0.09375
Global Iter: 26800 training loss: 1.85814
Global Iter: 26800 training acc: 0.25
Global Iter: 26900 training loss: 1.96309
Global Iter: 26900 training acc: 0.25
Global Iter: 27000 training loss: 1.92967
Global Iter: 27000 training acc: 0.25
Global Iter: 27100 training loss: 2.00407
Global Iter: 27100 training acc: 0.0625
Global Iter: 27200 training loss: 2.10607
Global Iter: 27200 training acc: 0.09375
Global Iter: 27300 training loss: 2.09209
Global Iter: 27300 training acc: 0.15625
Global Iter: 27400 training loss: 1.93217
Global Iter: 27400 training acc: 0.3125
Global Iter: 27500 training loss: 2.00992
Global Iter: 27500 training acc: 0.125
Global Iter: 27600 training loss: 1.98336
Global Iter: 27600 training acc: 0.15625
Global Iter: 27700 training loss: 1.90927
Global Iter: 27700 training acc: 0.09375
Global Iter: 27800 training loss: 2.04513
Global Iter: 27800 training acc: 0.1875
Global Iter: 27900 training loss: 1.98837
Global Iter: 27900 training acc: 0.34375
Global Iter: 28000 training loss: 1.9442
Global Iter: 28000 training acc: 0.28125
Global Iter: 28100 training loss: 1.91131
Global Iter: 28100 training acc: 0.34375
Global Iter: 28200 training loss: 1.93417
Global Iter: 28200 training acc: 0.25
Global Iter: 28300 training loss: 1.9975
Global Iter: 28300 training acc: 0.1875
Global Iter: 28400 training loss: 2.13399
Global Iter: 28400 training acc: 0.15625
Global Iter: 28500 training loss: 1.92045
Global Iter: 28500 training acc: 0.1875
Global Iter: 28600 training loss: 2.00448
Global Iter: 28600 training acc: 0.125
Global Iter: 28700 training loss: 1.9274
Global Iter: 28700 training acc: 0.1875
Global Iter: 28800 training loss: 1.97712
Global Iter: 28800 training acc: 0.125
Global Iter: 28900 training loss: 2.0333
Global Iter: 28900 training acc: 0.125
Global Iter: 29000 training loss: 2.07396
Global Iter: 29000 training acc: 0.15625
Global Iter: 29100 training loss: 1.9741
Global Iter: 29100 training acc: 0.125
Global Iter: 29200 training loss: 2.11424
Global Iter: 29200 training acc: 0.125
Global Iter: 29300 training loss: 2.06463
Global Iter: 29300 training acc: 0.125
Global Iter: 29400 training loss: 1.9643
Global Iter: 29400 training acc: 0.21875
Global Iter: 29500 training loss: 2.00095
Global Iter: 29500 training acc: 0.15625
Global Iter: 29600 training loss: 2.02022
Global Iter: 29600 training acc: 0.125
Global Iter: 29700 training loss: 2.0083
Global Iter: 29700 training acc: 0.09375
Global Iter: 29800 training loss: 1.97164
Global Iter: 29800 training acc: 0.25
Global Iter: 29900 training loss: 1.97196
Global Iter: 29900 training acc: 0.21875
Global Iter: 30000 training loss: 2.0439
Global Iter: 30000 training acc: 0.09375
Global Iter: 30100 training loss: 1.91603
Global Iter: 30100 training acc: 0.21875
Global Iter: 30200 training loss: 2.15591
Global Iter: 30200 training acc: 0.09375
Global Iter: 30300 training loss: 2.01142
Global Iter: 30300 training acc: 0.15625
Global Iter: 30400 training loss: 2.05204
Global Iter: 30400 training acc: 0.25
Global Iter: 30500 training loss: 2.0409
Global Iter: 30500 training acc: 0.125
Global Iter: 30600 training loss: 2.04077
Global Iter: 30600 training acc: 0.15625
Global Iter: 30700 training loss: 2.06472
Global Iter: 30700 training acc: 0.1875
Global Iter: 30800 training loss: 1.9993
Global Iter: 30800 training acc: 0.28125
Global Iter: 30900 training loss: 1.97369
Global Iter: 30900 training acc: 0.15625
Global Iter: 31000 training loss: 2.00581
Global Iter: 31000 training acc: 0.125
Global Iter: 31100 training loss: 1.99915
Global Iter: 31100 training acc: 0.125
Global Iter: 31200 training loss: 2.06813
Global Iter: 31200 training acc: 0.09375
Global Iter: 31300 training loss: 2.02822
Global Iter: 31300 training acc: 0.125
Global Iter: 31400 training loss: 1.93898
Global Iter: 31400 training acc: 0.25
Global Iter: 31500 training loss: 1.92497
Global Iter: 31500 training acc: 0.1875
Global Iter: 31600 training loss: 2.03628
Global Iter: 31600 training acc: 0.25
Global Iter: 31700 training loss: 1.93835
Global Iter: 31700 training acc: 0.15625
Global Iter: 31800 training loss: 1.93762
Global Iter: 31800 training acc: 0.21875
Global Iter: 31900 training loss: 1.88056
Global Iter: 31900 training acc: 0.3125
Global Iter: 32000 training loss: 2.00672
Global Iter: 32000 training acc: 0.28125
Global Iter: 32100 training loss: 1.90405
Global Iter: 32100 training acc: 0.1875
Global Iter: 32200 training loss: 1.92659
Global Iter: 32200 training acc: 0.25
Global Iter: 32300 training loss: 1.98981
Global Iter: 32300 training acc: 0.21875
Global Iter: 32400 training loss: 2.0892
Global Iter: 32400 training acc: 0.15625
Global Iter: 32500 training loss: 1.95562
Global Iter: 32500 training acc: 0.15625
Global Iter: 32600 training loss: 1.88692
Global Iter: 32600 training acc: 0.28125
Global Iter: 32700 training loss: 1.97953
Global Iter: 32700 training acc: 0.1875
Global Iter: 32800 training loss: 1.9891
Global Iter: 32800 training acc: 0.1875
Global Iter: 32900 training loss: 1.95421
Global Iter: 32900 training acc: 0.28125
Global Iter: 33000 training loss: 1.97874
Global Iter: 33000 training acc: 0.1875
Global Iter: 33100 training loss: 1.98118
Global Iter: 33100 training acc: 0.15625
Global Iter: 33200 training loss: 1.97191
Global Iter: 33200 training acc: 0.1875
Global Iter: 33300 training loss: 1.99932
Global Iter: 33300 training acc: 0.125
Global Iter: 33400 training loss: 2.09463
Global Iter: 33400 training acc: 0.1875
Global Iter: 33500 training loss: 2.00861
Global Iter: 33500 training acc: 0.1875
Global Iter: 33600 training loss: 1.94284
Global Iter: 33600 training acc: 0.3125
Global Iter: 33700 training loss: 1.89348
Global Iter: 33700 training acc: 0.34375
Global Iter: 33800 training loss: 2.06024
Global Iter: 33800 training acc: 0.125
Global Iter: 33900 training loss: 2.119
Global Iter: 33900 training acc: 0.1875
Global Iter: 34000 training loss: 1.96627
Global Iter: 34000 training acc: 0.125
Global Iter: 34100 training loss: 2.01985
Global Iter: 34100 training acc: 0.09375
Global Iter: 34200 training loss: 1.87099
Global Iter: 34200 training acc: 0.21875
Global Iter: 34300 training loss: 1.90927
Global Iter: 34300 training acc: 0.21875
Global Iter: 34400 training loss: 2.00479
Global Iter: 34400 training acc: 0.15625
Global Iter: 34500 training loss: 1.90291
Global Iter: 34500 training acc: 0.21875
Global Iter: 34600 training loss: 2.05467
Global Iter: 34600 training acc: 0.1875
Global Iter: 34700 training loss: 1.88491
Global Iter: 34700 training acc: 0.25
Global Iter: 34800 training loss: 2.02943
Global Iter: 34800 training acc: 0.21875
Global Iter: 34900 training loss: 1.97884
Global Iter: 34900 training acc: 0.1875
Global Iter: 35000 training loss: 2017-06-21 05:44:29.037849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-38230
1.93469
Global Iter: 35000 training acc: 0.1875
Global Iter: 35100 training loss: 2.05132
Global Iter: 35100 training acc: 0.1875
Global Iter: 35200 training loss: 1.84541
Global Iter: 35200 training acc: 0.375
Global Iter: 35300 training loss: 1.94491
Global Iter: 35300 training acc: 0.125
Global Iter: 35400 training loss: 2.00795
Global Iter: 35400 training acc: 0.21875
Global Iter: 35500 training loss: 2.01705
Global Iter: 35500 training acc: 0.125
Global Iter: 35600 training loss: 1.99732
Global Iter: 35600 training acc: 0.15625
Global Iter: 35700 training loss: 2.04061
Global Iter: 35700 training acc: 0.125
Global Iter: 35800 training loss: 1.99945
Global Iter: 35800 training acc: 0.21875
Global Iter: 35900 training loss: 2.01592
Global Iter: 35900 training acc: 0.25
Global Iter: 36000 training loss: 1.94729
Global Iter: 36000 training acc: 0.1875
Global Iter: 36100 training loss: 1.97109
Global Iter: 36100 training acc: 0.28125
Global Iter: 36200 training loss: 1.87483
Global Iter: 36200 training acc: 0.28125
Global Iter: 36300 training loss: 2.03289
Global Iter: 36300 training acc: 0.21875
Global Iter: 36400 training loss: 2.05235
Global Iter: 36400 training acc: 0.09375
Global Iter: 36500 training loss: 2.05906
Global Iter: 36500 training acc: 0.1875
Global Iter: 36600 training loss: 1.94148
Global Iter: 36600 training acc: 0.21875
Global Iter: 36700 training loss: 2.07598
Global Iter: 36700 training acc: 0.3125
Global Iter: 36800 training loss: 1.90471
Global Iter: 36800 training acc: 0.3125
Global Iter: 36900 training loss: 2.06957
Global Iter: 36900 training acc: 0.15625
Global Iter: 37000 training loss: 2.00188
Global Iter: 37000 training acc: 0.1875
Global Iter: 37100 training loss: 2.02005
Global Iter: 37100 training acc: 0.09375
Global Iter: 37200 training loss: 1.93906
Global Iter: 37200 training acc: 0.1875
Global Iter: 37300 training loss: 1.99739
Global Iter: 37300 training acc: 0.21875
Global Iter: 37400 training loss: 1.90354
Global Iter: 37400 training acc: 0.21875
Global Iter: 37500 training loss: 2.05458
Global Iter: 37500 training acc: 0.25
Global Iter: 37600 training loss: 1.89417
Global Iter: 37600 training acc: 0.25
Global Iter: 37700 training loss: 2.01965
Global Iter: 37700 training acc: 0.1875
Global Iter: 37800 training loss: 1.86442
Global Iter: 37800 training acc: 0.375
Global Iter: 37900 training loss: 1.92934
Global Iter: 37900 training acc: 0.1875
Global Iter: 38000 training loss: 1.90467
Global Iter: 38000 training acc: 0.15625
Global Iter: 38100 training loss: 2.03503
Global Iter: 38100 training acc: 0.15625
Global Iter: 38200 training loss: 1.94799
Global Iter: 38200 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-38230
Number of Patches: 301258
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-38230
Global Iter: 38300 training loss: 2.01324
Global Iter: 38300 training acc: 0.09375
Global Iter: 38400 training loss: 2.05395
Global Iter: 38400 training acc: 0.15625
Global Iter: 38500 training loss: 1.97854
Global Iter: 38500 training acc: 0.1875
Global Iter: 38600 training loss: 2.02154
Global Iter: 38600 training acc: 0.125
Global Iter: 38700 training loss: 2.00445
Global Iter: 38700 training acc: 0.28125
Global Iter: 38800 training loss: 2.08905
Global Iter: 38800 training acc: 0.25
Global Iter: 38900 training loss: 1.90445
Global Iter: 38900 training acc: 0.34375
Global Iter: 39000 training loss: 1.97331
Global Iter: 39000 training acc: 0.28125
Global Iter: 39100 training loss: 1.92473
Global Iter: 39100 training acc: 0.21875
Global Iter: 39200 training loss: 1.99079
Global Iter: 39200 training acc: 0.21875
Global Iter: 39300 training loss: 1.97211
Global Iter: 39300 training acc: 0.1875
Global Iter: 39400 training loss: 1.95113
Global Iter: 39400 training acc: 0.15625
Global Iter: 39500 training loss: 1.92441
Global Iter: 39500 training acc: 0.09375
Global Iter: 39600 training loss: 2.00438
Global Iter: 39600 training acc: 0.125
Global Iter: 39700 training loss: 1.96504
Global Iter: 39700 training acc: 0.1875
Global Iter: 39800 training loss: 1.99748
Global Iter: 39800 training acc: 0.15625
Global Iter: 39900 training loss: 2.02683
Global Iter: 39900 training acc: 0.21875
Global Iter: 40000 training loss: 1.96681
Global Iter: 40000 training acc: 0.09375
Global Iter: 40100 training loss: 1.98088
Global Iter: 40100 training acc: 0.1875
Global Iter: 40200 training loss: 2.01651
Global Iter: 40200 training acc: 0.21875
Global Iter: 40300 training loss: 2.00998
Global Iter: 40300 training acc: 0.21875
Global Iter: 40400 training loss: 2.00161
Global Iter: 40400 training acc: 0.15625
Global Iter: 40500 training loss: 1.98685
Global Iter: 40500 training acc: 0.09375
Global Iter: 40600 training loss: 1.95743
Global Iter: 40600 training acc: 0.34375
Global Iter: 40700 training loss: 1.96614
Global Iter: 40700 training acc: 0.15625
Global Iter: 40800 training loss: 1.94515
Global Iter: 40800 training acc: 0.21875
Global Iter: 40900 training loss: 1.88251
Global Iter: 40900 training acc: 0.3125
Global Iter: 41000 training loss: 2.03534
Global Iter: 41000 training acc: 0.0625
Global Iter: 41100 training loss: 1.95171
Global Iter: 41100 training acc: 0.28125
Global Iter: 41200 training loss: 1.9668
Global Iter: 41200 training acc: 0.28125
Global Iter: 41300 training loss: 1.99014
Global Iter: 41300 training acc: 0.09375
Global Iter: 41400 training loss: 2.06087
Global Iter: 41400 training acc: 0.125
Global Iter: 41500 training loss: 1.99991
Global Iter: 41500 training acc: 0.375
Global Iter: 41600 training loss: 2.03624
Global Iter: 41600 training acc: 0.0625
Global Iter: 41700 training loss: 2.05008
Global Iter: 41700 training acc: 0.15625
Global Iter: 41800 training loss: 1.88495
Global Iter: 41800 training acc: 0.28125
Global Iter: 41900 training loss: 2.07195
Global Iter: 41900 training acc: 0.125
Global Iter: 42000 training loss: 2.08595
Global Iter: 42000 training acc: 0.125
Global Iter: 42100 training loss: 1.97905
Global Iter: 42100 training acc: 0.125
Global Iter: 42200 training loss: 2.11926
Global Iter: 42200 training acc: 0.09375
Global Iter: 42300 training loss: 2.06152
Global Iter: 42300 training acc: 0.15625
Global Iter: 42400 training loss: 2.07632
Global Iter: 42400 training acc: 0.15625
Global Iter: 42500 training loss: 2.06384
Global Iter: 42500 training acc: 0.1875
Global Iter: 42600 training loss: 1.99797
Global Iter: 42600 training acc: 0.25
Global Iter: 42700 training loss: 1.95385
Global Iter: 42700 training acc: 0.15625
Global Iter: 42800 training loss: 1.89455
Global Iter: 42800 training acc: 0.21875
Global Iter: 42900 training loss: 1.92146
Global Iter: 42900 training acc: 0.28125
Global Iter: 43000 training loss: 2.07467
Global Iter: 43000 training acc: 0.25
Global Iter: 43100 training loss: 1.92822
Global Iter: 43100 training acc: 0.3125
Global Iter: 43200 training loss: 1.93058
Global Iter: 43200 training acc: 0.15625
Global Iter: 43300 training loss: 1.97618
Global Iter: 43300 training acc: 0.25
Global Iter: 43400 training loss: 2.02319
Global Iter: 43400 training acc: 0.15625
Global Iter: 43500 training loss: 2.02943
Global Iter: 43500 training acc: 0.09375
Global Iter: 43600 training loss: 2.0231
Global Iter: 43600 training acc: 0.15625
Global Iter: 43700 training loss: 2.09346
Global Iter: 43700 training acc: 0.09375
Global Iter: 43800 training loss: 2.08198
Global Iter: 43800 training acc: 0.125
Global Iter: 43900 training loss: 2.11143
Global Iter: 43900 training acc: 0.15625
Global Iter: 44000 training loss: 1.9092
Global Iter: 44000 training acc: 0.28125
Global Iter: 44100 training loss: 1.94071
Global Iter: 44100 training acc: 0.25
Global Iter: 44200 training loss: 2.03865
Global Iter: 44200 training acc: 0.15625
Global Iter: 44300 training loss: 1.95517
Global Iter: 44300 training acc: 0.25
Global Iter: 44400 training loss: 1.96614
Global Iter: 44400 training acc: 0.21875
Global Iter: 44500 training loss: 1.99221
Global Iter: 44500 training acc: 0.1875
Global Iter: 44600 training loss: 1.92889
Global Iter: 44600 training acc: 0.34375
Global Iter: 44700 training loss: 2.02667
Global Iter: 44700 training acc: 0.15625
Global Iter: 44800 training loss: 2.04348
Global Iter: 44800 training acc: 0.1875
Global Iter: 44900 training loss: 1.99442
Global Iter: 44900 training acc: 0.1875
Global Iter: 45000 training loss: 1.86211
Global Iter: 45000 training acc: 0.28125
Global Iter: 45100 training loss: 2.01067
Global Iter: 45100 training acc: 0.1875
Global Iter: 45200 training loss: 2.05383
Global Iter: 45200 training acc: 0.15625
Global Iter: 45300 training loss: 1.9628
Global Iter: 45300 training acc: 0.0625
Global Iter: 45400 training loss: 1.93401
Global Iter: 45400 training acc: 0.1875
Global Iter: 45500 training loss: 2.00327
Global Iter: 45500 training acc: 0.125
Global Iter: 45600 training loss: 1.85956
Global Iter: 45600 training acc: 0.21875
Global Iter: 45700 training loss: 2.01187
Global Iter: 45700 training acc: 0.28125
Global Iter: 45800 training loss: 2.02562
Global Iter: 45800 training acc: 0.09375
Global Iter: 45900 training loss: 1.95527
Global Iter: 45900 training acc: 0.125
Global Iter: 46000 training loss: 1.93117
Global Iter: 46000 training acc: 0.1875
Global Iter: 46100 training loss: 1.95357
Global Iter: 46100 training acc: 0.21875
Global Iter: 46200 training loss: 1.95833
Global Iter: 46200 training acc: 0.1875
Global Iter: 46300 training loss: 2.00445
Global Iter: 46300 training acc: 0.21875
Global Iter: 46400 training loss: 1.95443
Global Iter: 46400 training acc: 0.1875
Global Iter: 46500 training loss: 2.00299
Global Iter: 46500 training acc: 0.21875
Global Iter: 46600 training loss: 2.07764
Global Iter: 46600 training acc: 0.125
Global Iter: 46700 training loss: 2.04821
Global Iter: 46700 training acc: 0.125
Global Iter: 46800 training loss: 1.96348
Global Iter: 46800 training acc: 0.125
Global Iter: 46900 training loss: 2.00811
Global Iter: 46900 training acc: 0.21875
Global Iter: 47000 training loss: 1.89506
Global Iter: 47000 training acc: 0.28125
Global Iter: 47100 training loss: 1.9846
Global Iter: 47100 training acc: 0.21875
Global Iter: 47200 training loss: 2.17008
Global Iter: 47200 training acc: 0.15625
Global Iter: 47300 training loss: 2.097
Global Iter: 47300 training acc: 0.15625
Global Iter: 47400 training loss: 1.97035
Global Iter: 47400 training acc: 0.1875
Global Iter: 47500 training loss: 1.95211
Global Iter: 47500 training acc: 0.34375
Global Iter: 47600 training loss: 2.01749
Global Iter: 47600 training acc: 0.09375
Global Iter: 47700 training loss: 1.8701
Global Iter: 47700 training acc: 0.3125
Global Iter: 47800 training loss: 1.85391
Global Iter: 47800 training acc: 0.5
Global Iter: 47900 training loss: 2.08067
Global Iter: 47900 training acc: 0.0625
Global Iter: 48000 training loss: 1.94402
Global Iter: 48000 training acc: 0.1875
Global Iter: 48100 training loss: 2.0123
Global Iter: 48100 training acc: 0.21875
Global Iter: 48200 training loss: 1.89771
Global Iter: 48200 training acc: 0.28125
Global Iter: 48300 training loss: 1.94115
Global Iter: 48300 training acc: 0.28125
Global Iter: 48400 training loss: 1.96977
Global Iter: 48400 training acc: 0.15625
Global Iter: 48500 training loss: 2.01497
Global Iter: 48500 training acc: 0.3125
Global Iter: 48600 training loss: 1.96524
Global Iter: 48600 training acc: 0.28125
Global Iter: 48700 training loss: 1.95649
Global Iter: 48700 training acc: 0.34375
Global Iter: 48800 training loss: 1.96912
Global Iter: 48800 training acc: 0.125
Global Iter: 48900 training loss: 1.94405
Global Iter: 48900 training acc: 0.34375
Global Iter: 49000 training loss: 1.92382
Global Iter: 49000 training acc: 0.125
Global Iter: 49100 training loss: 2.13471
Global Iter: 49100 training acc: 0.25
Global Iter: 49200 training loss: 2.02159
Global Iter: 49200 training acc: 0.125
Global Iter: 49300 training loss: 2.0069
Global Iter: 49300 training acc: 0.0625
Global Iter: 49400 training loss: 1.97756
Global Iter: 49400 training acc: 0.1875
Global Iter: 49500 training loss: 1.94747
Global Iter: 49500 training acc: 0.25
Global Iter: 49600 training loss: 2.01707
Global Iter: 49600 training acc: 0.09375
Global Iter: 49700 training loss: 2.09097
Global Iter: 49700 training acc: 0.1875
Global Iter: 49800 training loss: 2.0073
Global Iter: 49800 training acc: 0.09375
Global Iter: 49900 training loss: 2.04992
Global Iter: 49900 training acc: 0.15625
Global Iter: 50000 training loss: 1.88461
Global Iter: 50000 training acc: 0.375
Global Iter: 50100 training loss: 1.91477
Global Iter: 50100 training acc: 0.15625
Global Iter: 50200 training loss: 1.99747
Global Iter: 50200 training acc: 0.25
Global Iter: 50300 training loss: 1.98724
Global Iter: 50300 training acc: 0.25
Global Iter: 50400 training loss: 1.97639
Global Iter: 50400 training acc: 0.15625
Global Iter: 50500 training loss: 1.91968
Global Iter: 50500 training acc: 0.25
Global Iter: 50600 training loss: 1.88491
Global Iter: 50600 training acc: 0.25
Global Iter: 50700 training loss: 1.97423
Global Iter: 50700 training acc: 0.09375
Global Iter: 50800 training loss: 1.97616
Global Iter: 50800 training acc: 0.1875
Global Iter: 50900 training loss: 1.9619
Global Iter: 50900 training acc: 0.125
Global Iter: 51000 training loss: 2.12183
Global Iter: 51000 training acc: 0.0625
Global Iter: 51100 training loss: 1.89482
Global Iter: 51100 training acc: 0.3125
Global Iter: 51200 training loss: 2.04503
Global Iter: 51200 training acc: 0.15625
Global Iter: 51300 training loss: 1.95055
Global Iter: 51300 training acc: 0.1875
Global Iter: 51400 training loss: 1.95465
Global Iter: 51400 training acc: 0.15625
Global Iter: 51500 training loss: 1.98259
Global Iter: 51500 training acc: 0.1875
Global Iter: 51600 training loss: 2.12616
Global Iter: 51600 training acc: 0.125
Global Iter: 51700 training loss: 1.92151
Global Iter: 51700 training acc: 0.1875
Global Iter: 51800 training loss: 1.98968
Global Iter: 51800 training acc: 0.1875
Global Iter: 51900 training loss: 2.14409
Global Iter: 51900 training acc: 0.1875
Global Iter: 52000 training loss: 2.03219
Global Iter: 52000 training acc: 0.25
Global Iter: 52100 training loss: 2.14046
Global Iter: 52100 training acc: 0.125
Global Iter: 52200 training loss: 1.92776
Global Iter: 52200 training acc: 0.21875
Global Iter: 52300 training loss: 2.08823
Global Iter: 52300 training acc: 0.125
Global Iter: 52400 training loss: 2.02327
Global Iter: 52400 training acc: 0.28125
Global Iter: 52500 training loss: 1.98651
Global Iter: 52500 training acc: 0.25
Global Iter: 52600 training loss: 1.99466
Global Iter: 52600 training acc: 0.21875
Global Iter: 52700 training loss: 2.04639
Global Iter: 52700 training acc: 0.09375
Global Iter: 52800 training loss: 1.93235
Global Iter: 52800 training acc: 0.21875
Global Iter: 52900 training loss: 1.94475
Global Iter: 52900 training acc: 0.1875
Global Iter: 53000 training loss: 1.96467
Global Iter: 53000 training acc: 0.1875
Global Iter: 53100 training loss: 1.88693
Global Iter: 53100 training acc: 0.3125
Global Iter: 53200 training loss: 1.9896
Global Iter: 53200 training acc: 0.25
Global Iter: 53300 training loss: 1.85164
Global Iter: 53300 training acc: 0.375
Global Iter: 53400 training loss: 1.9188
Global Iter: 53400 training acc: 0.25
Global Iter: 53500 training loss: 1.97892
Global Iter: 53500 training acc: 0.1875
Global Iter: 53600 training loss: 1.92656
Global Iter: 53600 training acc: 0.125
Global Iter: 53700 training loss: 1.87235
Global Iter: 53700 training acc: 0.25
Global Iter: 53800 training loss: 1.93007
Global Iter: 53800 training acc: 0.25
Global Iter: 53900 training loss: 1.94715
Global Iter: 53900 training acc: 0.09375
Global Iter: 54000 training loss: 2.00364
Global Iter: 54000 training acc: 0.125
Global Iter: 54100 training loss: 2.06316
Global Iter: 54100 training acc: 0.1875
Global Iter: 54200 training loss: 1.96853
Global Iter: 54200 training acc: 0.34375
Global Iter: 54300 training loss: 1.9893
Global Iter: 54300 training acc: 0.1875
Global Iter: 54400 training loss: 1.99711
Global Iter: 54400 training acc: 0.09375
Global Iter: 54500 training loss: 1.94157
Global Iter: 54500 training acc: 0.21875
Global Iter: 54600 training loss: 2.04094
Global Iter: 54600 training acc: 0.21875
Global Iter: 54700 training loss: 2.0452017-06-21 06:15:41.178626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-57059
83
Global Iter: 54700 training acc: 0.125
Global Iter: 54800 training loss: 2.1429
Global Iter: 54800 training acc: 0.09375
Global Iter: 54900 training loss: 1.99535
Global Iter: 54900 training acc: 0.15625
Global Iter: 55000 training loss: 2.02148
Global Iter: 55000 training acc: 0.15625
Global Iter: 55100 training loss: 2.1216
Global Iter: 55100 training acc: 0.1875
Global Iter: 55200 training loss: 1.88994
Global Iter: 55200 training acc: 0.375
Global Iter: 55300 training loss: 1.91863
Global Iter: 55300 training acc: 0.125
Global Iter: 55400 training loss: 1.96272
Global Iter: 55400 training acc: 0.34375
Global Iter: 55500 training loss: 1.91945
Global Iter: 55500 training acc: 0.21875
Global Iter: 55600 training loss: 1.93978
Global Iter: 55600 training acc: 0.15625
Global Iter: 55700 training loss: 2.01684
Global Iter: 55700 training acc: 0.28125
Global Iter: 55800 training loss: 1.99995
Global Iter: 55800 training acc: 0.15625
Global Iter: 55900 training loss: 2.08643
Global Iter: 55900 training acc: 0.09375
Global Iter: 56000 training loss: 2.00253
Global Iter: 56000 training acc: 0.1875
Global Iter: 56100 training loss: 1.84654
Global Iter: 56100 training acc: 0.46875
Global Iter: 56200 training loss: 2.02048
Global Iter: 56200 training acc: 0.09375
Global Iter: 56300 training loss: 2.04665
Global Iter: 56300 training acc: 0.0625
Global Iter: 56400 training loss: 1.89717
Global Iter: 56400 training acc: 0.21875
Global Iter: 56500 training loss: 1.96558
Global Iter: 56500 training acc: 0.21875
Global Iter: 56600 training loss: 1.91836
Global Iter: 56600 training acc: 0.28125
Global Iter: 56700 training loss: 1.99519
Global Iter: 56700 training acc: 0.15625
Global Iter: 56800 training loss: 2.02525
Global Iter: 56800 training acc: 0.09375
Global Iter: 56900 training loss: 1.96531
Global Iter: 56900 training acc: 0.09375
Global Iter: 57000 training loss: 2.03756
Global Iter: 57000 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-57059
Number of Patches: 298246
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-57059
Global Iter: 57100 training loss: 2.07548
Global Iter: 57100 training acc: 0.09375
Global Iter: 57200 training loss: 2.00226
Global Iter: 57200 training acc: 0.25
Global Iter: 57300 training loss: 1.95183
Global Iter: 57300 training acc: 0.25
Global Iter: 57400 training loss: 1.9499
Global Iter: 57400 training acc: 0.15625
Global Iter: 57500 training loss: 1.93738
Global Iter: 57500 training acc: 0.21875
Global Iter: 57600 training loss: 1.99723
Global Iter: 57600 training acc: 0.25
Global Iter: 57700 training loss: 1.97041
Global Iter: 57700 training acc: 0.1875
Global Iter: 57800 training loss: 2.17864
Global Iter: 57800 training acc: 0.15625
Global Iter: 57900 training loss: 2.04391
Global Iter: 57900 training acc: 0.21875
Global Iter: 58000 training loss: 2.04323
Global Iter: 58000 training acc: 0.1875
Global Iter: 58100 training loss: 1.8885
Global Iter: 58100 training acc: 0.34375
Global Iter: 58200 training loss: 2.00128
Global Iter: 58200 training acc: 0.1875
Global Iter: 58300 training loss: 1.92317
Global Iter: 58300 training acc: 0.15625
Global Iter: 58400 training loss: 1.88896
Global Iter: 58400 training acc: 0.15625
Global Iter: 58500 training loss: 2.04566
Global Iter: 58500 training acc: 0.125
Global Iter: 58600 training loss: 2.02622
Global Iter: 58600 training acc: 0.125
Global Iter: 58700 training loss: 2.1245
Global Iter: 58700 training acc: 0.1875
Global Iter: 58800 training loss: 1.97155
Global Iter: 58800 training acc: 0.1875
Global Iter: 58900 training loss: 1.95327
Global Iter: 58900 training acc: 0.1875
Global Iter: 59000 training loss: 1.96217
Global Iter: 59000 training acc: 0.21875
Global Iter: 59100 training loss: 2.00978
Global Iter: 59100 training acc: 0.125
Global Iter: 59200 training loss: 2.01721
Global Iter: 59200 training acc: 0.15625
Global Iter: 59300 training loss: 1.93942
Global Iter: 59300 training acc: 0.1875
Global Iter: 59400 training loss: 1.92435
Global Iter: 59400 training acc: 0.28125
Global Iter: 59500 training loss: 1.95001
Global Iter: 59500 training acc: 0.34375
Global Iter: 59600 training loss: 1.91686
Global Iter: 59600 training acc: 0.15625
Global Iter: 59700 training loss: 1.93118
Global Iter: 59700 training acc: 0.15625
Global Iter: 59800 training loss: 1.93962
Global Iter: 59800 training acc: 0.3125
Global Iter: 59900 training loss: 1.97875
Global Iter: 59900 training acc: 0.28125
Global Iter: 60000 training loss: 2.11356
Global Iter: 60000 training acc: 0.03125
Global Iter: 60100 training loss: 1.98664
Global Iter: 60100 training acc: 0.0
Global Iter: 60200 training loss: 2.05091
Global Iter: 60200 training acc: 0.15625
Global Iter: 60300 training loss: 2.03805
Global Iter: 60300 training acc: 0.1875
Global Iter: 60400 training loss: 1.98457
Global Iter: 60400 training acc: 0.125
Global Iter: 60500 training loss: 1.89232
Global Iter: 60500 training acc: 0.1875
Global Iter: 60600 training loss: 2.0952
Global Iter: 60600 training acc: 0.125
Global Iter: 60700 training loss: 2.00073
Global Iter: 60700 training acc: 0.15625
Global Iter: 60800 training loss: 2.02361
Global Iter: 60800 training acc: 0.03125
Global Iter: 60900 training loss: 1.94778
Global Iter: 60900 training acc: 0.125
Global Iter: 61000 training loss: 2.04558
Global Iter: 61000 training acc: 0.09375
Global Iter: 61100 training loss: 1.98635
Global Iter: 61100 training acc: 0.1875
Global Iter: 61200 training loss: 1.92903
Global Iter: 61200 training acc: 0.1875
Global Iter: 61300 training loss: 1.92393
Global Iter: 61300 training acc: 0.28125
Global Iter: 61400 training loss: 1.98646
Global Iter: 61400 training acc: 0.1875
Global Iter: 61500 training loss: 2.1925
Global Iter: 61500 training acc: 0.09375
Global Iter: 61600 training loss: 2.09918
Global Iter: 61600 training acc: 0.0625
Global Iter: 61700 training loss: 1.91256
Global Iter: 61700 training acc: 0.21875
Global Iter: 61800 training loss: 1.94395
Global Iter: 61800 training acc: 0.28125
Global Iter: 61900 training loss: 2.13976
Global Iter: 61900 training acc: 0.125
Global Iter: 62000 training loss: 1.93167
Global Iter: 62000 training acc: 0.1875
Global Iter: 62100 training loss: 2.06981
Global Iter: 62100 training acc: 0.1875
Global Iter: 62200 training loss: 2.05259
Global Iter: 62200 training acc: 0.21875
Global Iter: 62300 training loss: 1.94999
Global Iter: 62300 training acc: 0.125
Global Iter: 62400 training loss: 1.98939
Global Iter: 62400 training acc: 0.15625
Global Iter: 62500 training loss: 2.02392
Global Iter: 62500 training acc: 0.1875
Global Iter: 62600 training loss: 1.94024
Global Iter: 62600 training acc: 0.3125
Global Iter: 62700 training loss: 2.05658
Global Iter: 62700 training acc: 0.15625
Global Iter: 62800 training loss: 2.0402
Global Iter: 62800 training acc: 0.21875
Global Iter: 62900 training loss: 1.94914
Global Iter: 62900 training acc: 0.125
Global Iter: 63000 training loss: 1.95782
Global Iter: 63000 training acc: 0.0625
Global Iter: 63100 training loss: 2.00092
Global Iter: 63100 training acc: 0.1875
Global Iter: 63200 training loss: 1.97675
Global Iter: 63200 training acc: 0.15625
Global Iter: 63300 training loss: 1.91771
Global Iter: 63300 training acc: 0.25
Global Iter: 63400 training loss: 1.9303
Global Iter: 63400 training acc: 0.1875
Global Iter: 63500 training loss: 1.96411
Global Iter: 63500 training acc: 0.15625
Global Iter: 63600 training loss: 1.91866
Global Iter: 63600 training acc: 0.1875
Global Iter: 63700 training loss: 1.97669
Global Iter: 63700 training acc: 0.09375
Global Iter: 63800 training loss: 2.00583
Global Iter: 63800 training acc: 0.15625
Global Iter: 63900 training loss: 1.98633
Global Iter: 63900 training acc: 0.21875
Global Iter: 64000 training loss: 2.05363
Global Iter: 64000 training acc: 0.0625
Global Iter: 64100 training loss: 2.02983
Global Iter: 64100 training acc: 0.09375
Global Iter: 64200 training loss: 1.97208
Global Iter: 64200 training acc: 0.21875
Global Iter: 64300 training loss: 1.94286
Global Iter: 64300 training acc: 0.34375
Global Iter: 64400 training loss: 2.00551
Global Iter: 64400 training acc: 0.125
Global Iter: 64500 training loss: 1.99912
Global Iter: 64500 training acc: 0.09375
Global Iter: 64600 training loss: 1.92364
Global Iter: 64600 training acc: 0.1875
Global Iter: 64700 training loss: 2.03295
Global Iter: 64700 training acc: 0.125
Global Iter: 64800 training loss: 1.99542
Global Iter: 64800 training acc: 0.21875
Global Iter: 64900 training loss: 1.97569
Global Iter: 64900 training acc: 0.21875
Global Iter: 65000 training loss: 2.03711
Global Iter: 65000 training acc: 0.15625
Global Iter: 65100 training loss: 2.06648
Global Iter: 65100 training acc: 0.1875
Global Iter: 65200 training loss: 2.03477
Global Iter: 65200 training acc: 0.3125
Global Iter: 65300 training loss: 2.08651
Global Iter: 65300 training acc: 0.1875
Global Iter: 65400 training loss: 2.06722
Global Iter: 65400 training acc: 0.09375
Global Iter: 65500 training loss: 2.10826
Global Iter: 65500 training acc: 0.125
Global Iter: 65600 training loss: 2.04947
Global Iter: 65600 training acc: 0.21875
Global Iter: 65700 training loss: 2.04231
Global Iter: 65700 training acc: 0.125
Global Iter: 65800 training loss: 1.93673
Global Iter: 65800 training acc: 0.125
Global Iter: 65900 training loss: 1.95938
Global Iter: 65900 training acc: 0.25
Global Iter: 66000 training loss: 2.0072
Global Iter: 66000 training acc: 0.21875
Global Iter: 66100 training loss: 2.01849
Global Iter: 66100 training acc: 0.25
Global Iter: 66200 training loss: 1.99184
Global Iter: 66200 training acc: 0.25
Global Iter: 66300 training loss: 2.00579
Global Iter: 66300 training acc: 0.0625
Global Iter: 66400 training loss: 1.89572
Global Iter: 66400 training acc: 0.21875
Global Iter: 66500 training loss: 2.07711
Global Iter: 66500 training acc: 0.0625
Global Iter: 66600 training loss: 1.95552
Global Iter: 66600 training acc: 0.25
Global Iter: 66700 training loss: 1.93467
Global Iter: 66700 training acc: 0.25
Global Iter: 66800 training loss: 1.95475
Global Iter: 66800 training acc: 0.15625
Global Iter: 66900 training loss: 1.96941
Global Iter: 66900 training acc: 0.1875
Global Iter: 67000 training loss: 2.01329
Global Iter: 67000 training acc: 0.15625
Global Iter: 67100 training loss: 1.9541
Global Iter: 67100 training acc: 0.25
Global Iter: 67200 training loss: 1.95279
Global Iter: 67200 training acc: 0.15625
Global Iter: 67300 training loss: 2.03409
Global Iter: 67300 training acc: 0.125
Global Iter: 67400 training loss: 1.95101
Global Iter: 67400 training acc: 0.21875
Global Iter: 67500 training loss: 2.00074
Global Iter: 67500 training acc: 0.25
Global Iter: 67600 training loss: 1.88726
Global Iter: 67600 training acc: 0.1875
Global Iter: 67700 training loss: 2.0283
Global Iter: 67700 training acc: 0.125
Global Iter: 67800 training loss: 2.02691
Global Iter: 67800 training acc: 0.125
Global Iter: 67900 training loss: 1.85888
Global Iter: 67900 training acc: 0.28125
Global Iter: 68000 training loss: 1.94023
Global Iter: 68000 training acc: 0.125
Global Iter: 68100 training loss: 1.97832
Global Iter: 68100 training acc: 0.25
Global Iter: 68200 training loss: 1.90628
Global Iter: 68200 training acc: 0.25
Global Iter: 68300 training loss: 2.04274
Global Iter: 68300 training acc: 0.0625
Global Iter: 68400 training loss: 2.0005
Global Iter: 68400 training acc: 0.28125
Global Iter: 68500 training loss: 2.01789
Global Iter: 68500 training acc: 0.1875
Global Iter: 68600 training loss: 1.98537
Global Iter: 68600 training acc: 0.15625
Global Iter: 68700 training loss: 2.00464
Global Iter: 68700 training acc: 0.3125
Global Iter: 68800 training loss: 1.87812
Global Iter: 68800 training acc: 0.28125
Global Iter: 68900 training loss: 1.96359
Global Iter: 68900 training acc: 0.21875
Global Iter: 69000 training loss: 2.0269
Global Iter: 69000 training acc: 0.125
Global Iter: 69100 training loss: 1.90166
Global Iter: 69100 training acc: 0.28125
Global Iter: 69200 training loss: 1.91211
Global Iter: 69200 training acc: 0.25
Global Iter: 69300 training loss: 1.92872
Global Iter: 69300 training acc: 0.21875
Global Iter: 69400 training loss: 2.16274
Global Iter: 69400 training acc: 0.25
Global Iter: 69500 training loss: 1.95598
Global Iter: 69500 training acc: 0.21875
Global Iter: 69600 training loss: 1.9453
Global Iter: 69600 training acc: 0.25
Global Iter: 69700 training loss: 2.02654
Global Iter: 69700 training acc: 0.15625
Global Iter: 69800 training loss: 1.95169
Global Iter: 69800 training acc: 0.25
Global Iter: 69900 training loss: 1.93011
Global Iter: 69900 training acc: 0.1875
Global Iter: 70000 training loss: 2.06828
Global Iter: 70000 training acc: 0.125
Global Iter: 70100 training loss: 1.99677
Global Iter: 70100 training acc: 0.09375
Global Iter: 70200 training loss: 1.95445
Global Iter: 70200 training acc: 0.09375
Global Iter: 70300 training loss: 2.12855
Global Iter: 70300 training acc: 0.3125
Global Iter: 70400 training loss: 1.96049
Global Iter: 70400 training acc: 0.1875
Global Iter: 70500 training loss: 1.98982
Global Iter: 70500 training acc: 0.09375
Global Iter: 70600 training loss: 2.04794
Global Iter: 70600 training acc: 0.28125
Global Iter: 70700 training loss: 1.94723
Global Iter: 70700 training acc: 0.1875
Global Iter: 70800 training loss: 2.06698
Global Iter: 70800 training acc: 0.125
Global Iter: 70900 training loss: 1.97453
Global Iter: 70900 training acc: 0.15625
Global Iter: 71000 training loss: 2.05656
Global Iter: 71000 training acc: 0.125
Global Iter: 71100 training loss: 1.9853
Global Iter: 71100 training acc: 0.09375
Global Iter: 71200 training loss: 1.93621
Global Iter: 71200 training acc: 0.15625
Global Iter: 71300 training loss: 1.98492
Global Iter: 71300 training acc: 0.1875
Global Iter: 71400 training loss: 2.00971
Global Iter: 71400 training acc: 0.15625
Global Iter: 71500 training loss: 2.06064
Global Iter: 71500 training acc: 0.25
Global Iter: 71600 training loss: 1.91206
Global Iter: 71600 training acc: 0.4375
Global Iter: 71700 training loss: 1.96026
Global Iter: 71700 training acc: 0.15625
Global Iter: 71800 training loss: 1.86467
Global Iter: 71800 training acc: 0.28125
Global Iter: 71900 training loss: 1.95949
Global Iter: 71900 training acc: 0.1875
Global Iter: 72000 training loss: 1.91398
Global Iter: 72000 training acc: 0.21875
Global Iter: 72100 training loss: 2.03824
Global Iter: 72100 training acc: 0.09375
Global Iter: 72200 training loss: 1.95768
Global Iter: 72200 training acc: 0.25
Global Iter: 72300 training loss: 2.08138
Global Iter: 72300 training acc: 0.15625
Global Iter: 72400 training loss: 2.1248
Global Iter: 72400 training acc: 0.0625
Global Iter: 72500 training loss: 2.0136
Global Iter: 72500 training acc: 0.1875
Global Iter: 72600 training loss: 2.00784
Global Iter: 72600 training acc: 0.25
Global Iter: 72700 training loss: 1.95853
Global Iter: 72700 training acc: 0.125
Global Iter: 72800 training loss: 2.23554
Global Iter: 72800 training acc: 0.1875
Global Iter: 72900 training loss: 1.96919
Global Iter: 72900 training acc: 0.21875
Global Iter: 73000 training loss: 1.89372
Global Iter: 73000 training acc: 0.3125
Global Iter: 73100 training loss: 1.96429
Global Iter: 73100 training acc: 0.1875
Global Iter: 73200 training loss: 2.03359
Global Iter: 73200 training acc: 0.15625
Global Iter: 73300 training loss: 2.05179
Global Iter: 73300 training acc: 0.1875
Global Iter: 73400 training loss: 2.04363
Global Iter: 73400 training acc: 0.25
Global Iter: 73500 training loss: 1.99025
Global Iter: 73500 training acc: 0.1875
Global Iter: 73600 training loss: 1.96394
Global Iter: 73600 training acc: 0.1875
Global Iter: 73700 training loss: 1.98278
Global Iter: 73700 training acc: 0.15625
Global Iter: 73800 training loss: 2.07859
Global Iter: 73800 training acc: 0.125
Global Iter: 73900 training loss: 1.97261
Global Iter: 73900 training acc: 0.25
Global Iter: 74000 training loss: 2.09249
Global Iter: 74000 training acc: 0.21875
Global Iter: 74100 training loss: 1.95678
Global Iter: 74100 training acc: 0.125
Global Iter: 74200 training loss: 1.86391
Global Iter: 74200 training acc: 0.28125
Global Iter: 74300 training loss: 1.93396
Global Iter: 74300 training acc: 0.25
Global Iter: 74400 training loss: 1.93383
Global I2017-06-21 06:46:28.076902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-75700
ter: 74400 training acc: 0.15625
Global Iter: 74500 training loss: 1.99416
Global Iter: 74500 training acc: 0.28125
Global Iter: 74600 training loss: 1.97313
Global Iter: 74600 training acc: 0.25
Global Iter: 74700 training loss: 1.93406
Global Iter: 74700 training acc: 0.25
Global Iter: 74800 training loss: 2.09665
Global Iter: 74800 training acc: 0.125
Global Iter: 74900 training loss: 1.89927
Global Iter: 74900 training acc: 0.25
Global Iter: 75000 training loss: 2.01981
Global Iter: 75000 training acc: 0.1875
Global Iter: 75100 training loss: 1.97505
Global Iter: 75100 training acc: 0.1875
Global Iter: 75200 training loss: 2.02618
Global Iter: 75200 training acc: 0.125
Global Iter: 75300 training loss: 2.1561
Global Iter: 75300 training acc: 0.09375
Global Iter: 75400 training loss: 1.99972
Global Iter: 75400 training acc: 0.21875
Global Iter: 75500 training loss: 2.00263
Global Iter: 75500 training acc: 0.15625
Global Iter: 75600 training loss: 2.02458
Global Iter: 75600 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-75700
Number of Patches: 295264
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-75700
Global Iter: 75800 training loss: 2.00284
Global Iter: 75800 training acc: 0.03125
Global Iter: 75900 training loss: 1.9956
Global Iter: 75900 training acc: 0.21875
Global Iter: 76000 training loss: 1.91183
Global Iter: 76000 training acc: 0.34375
Global Iter: 76100 training loss: 1.98218
Global Iter: 76100 training acc: 0.09375
Global Iter: 76200 training loss: 2.05088
Global Iter: 76200 training acc: 0.09375
Global Iter: 76300 training loss: 1.92769
Global Iter: 76300 training acc: 0.21875
Global Iter: 76400 training loss: 1.96526
Global Iter: 76400 training acc: 0.21875
Global Iter: 76500 training loss: 2.08468
Global Iter: 76500 training acc: 0.125
Global Iter: 76600 training loss: 2.04038
Global Iter: 76600 training acc: 0.125
Global Iter: 76700 training loss: 1.93647
Global Iter: 76700 training acc: 0.1875
Global Iter: 76800 training loss: 1.9506
Global Iter: 76800 training acc: 0.21875
Global Iter: 76900 training loss: 1.92697
Global Iter: 76900 training acc: 0.21875
Global Iter: 77000 training loss: 1.99897
Global Iter: 77000 training acc: 0.15625
Global Iter: 77100 training loss: 2.03697
Global Iter: 77100 training acc: 0.03125
Global Iter: 77200 training loss: 1.93211
Global Iter: 77200 training acc: 0.21875
Global Iter: 77300 training loss: 1.89138
Global Iter: 77300 training acc: 0.25
Global Iter: 77400 training loss: 2.09887
Global Iter: 77400 training acc: 0.21875
Global Iter: 77500 training loss: 2.04026
Global Iter: 77500 training acc: 0.15625
Global Iter: 77600 training loss: 2.08846
Global Iter: 77600 training acc: 0.125
Global Iter: 77700 training loss: 1.96309
Global Iter: 77700 training acc: 0.21875
Global Iter: 77800 training loss: 1.92405
Global Iter: 77800 training acc: 0.21875
Global Iter: 77900 training loss: 1.97949
Global Iter: 77900 training acc: 0.25
Global Iter: 78000 training loss: 1.9078
Global Iter: 78000 training acc: 0.21875
Global Iter: 78100 training loss: 2.02732
Global Iter: 78100 training acc: 0.21875
Global Iter: 78200 training loss: 2.0222
Global Iter: 78200 training acc: 0.21875
Global Iter: 78300 training loss: 2.10935
Global Iter: 78300 training acc: 0.09375
Global Iter: 78400 training loss: 2.0467
Global Iter: 78400 training acc: 0.25
Global Iter: 78500 training loss: 1.97703
Global Iter: 78500 training acc: 0.0625
Global Iter: 78600 training loss: 2.0214
Global Iter: 78600 training acc: 0.125
Global Iter: 78700 training loss: 1.97401
Global Iter: 78700 training acc: 0.25
Global Iter: 78800 training loss: 2.11878
Global Iter: 78800 training acc: 0.125
Global Iter: 78900 training loss: 1.99416
Global Iter: 78900 training acc: 0.125
Global Iter: 79000 training loss: 2.04611
Global Iter: 79000 training acc: 0.28125
Global Iter: 79100 training loss: 2.00318
Global Iter: 79100 training acc: 0.0625
Global Iter: 79200 training loss: 2.02223
Global Iter: 79200 training acc: 0.125
Global Iter: 79300 training loss: 2.20254
Global Iter: 79300 training acc: 0.0625
Global Iter: 79400 training loss: 2.0656
Global Iter: 79400 training acc: 0.15625
Global Iter: 79500 training loss: 1.97316
Global Iter: 79500 training acc: 0.21875
Global Iter: 79600 training loss: 1.96646
Global Iter: 79600 training acc: 0.21875
Global Iter: 79700 training loss: 2.08711
Global Iter: 79700 training acc: 0.1875
Global Iter: 79800 training loss: 2.03809
Global Iter: 79800 training acc: 0.15625
Global Iter: 79900 training loss: 1.95048
Global Iter: 79900 training acc: 0.1875
Global Iter: 80000 training loss: 2.05038
Global Iter: 80000 training acc: 0.1875
Global Iter: 80100 training loss: 1.93618
Global Iter: 80100 training acc: 0.125
Global Iter: 80200 training loss: 1.98097
Global Iter: 80200 training acc: 0.125
Global Iter: 80300 training loss: 1.8717
Global Iter: 80300 training acc: 0.28125
Global Iter: 80400 training loss: 2.02667
Global Iter: 80400 training acc: 0.15625
Global Iter: 80500 training loss: 1.98003
Global Iter: 80500 training acc: 0.25
Global Iter: 80600 training loss: 1.98456
Global Iter: 80600 training acc: 0.34375
Global Iter: 80700 training loss: 1.96978
Global Iter: 80700 training acc: 0.25
Global Iter: 80800 training loss: 1.94171
Global Iter: 80800 training acc: 0.1875
Global Iter: 80900 training loss: 2.06769
Global Iter: 80900 training acc: 0.1875
Global Iter: 81000 training loss: 1.95615
Global Iter: 81000 training acc: 0.3125
Global Iter: 81100 training loss: 2.06093
Global Iter: 81100 training acc: 0.125
Global Iter: 81200 training loss: 1.91568
Global Iter: 81200 training acc: 0.3125
Global Iter: 81300 training loss: 2.00477
Global Iter: 81300 training acc: 0.1875
Global Iter: 81400 training loss: 1.99904
Global Iter: 81400 training acc: 0.125
Global Iter: 81500 training loss: 1.89484
Global Iter: 81500 training acc: 0.28125
Global Iter: 81600 training loss: 2.04631
Global Iter: 81600 training acc: 0.125
Global Iter: 81700 training loss: 1.99202
Global Iter: 81700 training acc: 0.15625
Global Iter: 81800 training loss: 1.84779
Global Iter: 81800 training acc: 0.21875
Global Iter: 81900 training loss: 2.0038
Global Iter: 81900 training acc: 0.1875
Global Iter: 82000 training loss: 2.05385
Global Iter: 82000 training acc: 0.125
Global Iter: 82100 training loss: 1.93413
Global Iter: 82100 training acc: 0.1875
Global Iter: 82200 training loss: 1.98623
Global Iter: 82200 training acc: 0.125
Global Iter: 82300 training loss: 2.08314
Global Iter: 82300 training acc: 0.0
Global Iter: 82400 training loss: 1.92824
Global Iter: 82400 training acc: 0.3125
Global Iter: 82500 training loss: 1.9789
Global Iter: 82500 training acc: 0.09375
Global Iter: 82600 training loss: 1.99086
Global Iter: 82600 training acc: 0.21875
Global Iter: 82700 training loss: 1.9183
Global Iter: 82700 training acc: 0.15625
Global Iter: 82800 training loss: 2.0361
Global Iter: 82800 training acc: 0.15625
Global Iter: 82900 training loss: 2.0085
Global Iter: 82900 training acc: 0.1875
Global Iter: 83000 training loss: 1.98778
Global Iter: 83000 training acc: 0.21875
Global Iter: 83100 training loss: 2.09233
Global Iter: 83100 training acc: 0.1875
Global Iter: 83200 training loss: 2.02922
Global Iter: 83200 training acc: 0.25
Global Iter: 83300 training loss: 2.02628
Global Iter: 83300 training acc: 0.15625
Global Iter: 83400 training loss: 2.04341
Global Iter: 83400 training acc: 0.1875
Global Iter: 83500 training loss: 2.06968
Global Iter: 83500 training acc: 0.15625
Global Iter: 83600 training loss: 2.04252
Global Iter: 83600 training acc: 0.125
Global Iter: 83700 training loss: 1.93546
Global Iter: 83700 training acc: 0.25
Global Iter: 83800 training loss: 2.0464
Global Iter: 83800 training acc: 0.21875
Global Iter: 83900 training loss: 2.02489
Global Iter: 83900 training acc: 0.25
Global Iter: 84000 training loss: 1.99526
Global Iter: 84000 training acc: 0.21875
Global Iter: 84100 training loss: 1.96293
Global Iter: 84100 training acc: 0.1875
Global Iter: 84200 training loss: 1.89786
Global Iter: 84200 training acc: 0.28125
Global Iter: 84300 training loss: 2.08219
Global Iter: 84300 training acc: 0.1875
Global Iter: 84400 training loss: 2.00784
Global Iter: 84400 training acc: 0.09375
Global Iter: 84500 training loss: 2.15582
Global Iter: 84500 training acc: 0.09375
Global Iter: 84600 training loss: 2.03691
Global Iter: 84600 training acc: 0.25
Global Iter: 84700 training loss: 1.95696
Global Iter: 84700 training acc: 0.1875
Global Iter: 84800 training loss: 1.94295
Global Iter: 84800 training acc: 0.28125
Global Iter: 84900 training loss: 2.03238
Global Iter: 84900 training acc: 0.15625
Global Iter: 85000 training loss: 1.95024
Global Iter: 85000 training acc: 0.21875
Global Iter: 85100 training loss: 2.04011
Global Iter: 85100 training acc: 0.1875
Global Iter: 85200 training loss: 2.01821
Global Iter: 85200 training acc: 0.09375
Global Iter: 85300 training loss: 1.97474
Global Iter: 85300 training acc: 0.21875
Global Iter: 85400 training loss: 1.97213
Global Iter: 85400 training acc: 0.1875
Global Iter: 85500 training loss: 2.01001
Global Iter: 85500 training acc: 0.21875
Global Iter: 85600 training loss: 1.91389
Global Iter: 85600 training acc: 0.1875
Global Iter: 85700 training loss: 1.98807
Global Iter: 85700 training acc: 0.09375
Global Iter: 85800 training loss: 2.0131
Global Iter: 85800 training acc: 0.25
Global Iter: 85900 training loss: 2.00612
Global Iter: 85900 training acc: 0.15625
Global Iter: 86000 training loss: 1.9642
Global Iter: 86000 training acc: 0.25
Global Iter: 86100 training loss: 1.94415
Global Iter: 86100 training acc: 0.28125
Global Iter: 86200 training loss: 1.98141
Global Iter: 86200 training acc: 0.28125
Global Iter: 86300 training loss: 1.98044
Global Iter: 86300 training acc: 0.28125
Global Iter: 86400 training loss: 1.95125
Global Iter: 86400 training acc: 0.09375
Global Iter: 86500 training loss: 1.94769
Global Iter: 86500 training acc: 0.21875
Global Iter: 86600 training loss: 2.10303
Global Iter: 86600 training acc: 0.125
Global Iter: 86700 training loss: 2.01282
Global Iter: 86700 training acc: 0.1875
Global Iter: 86800 training loss: 1.98886
Global Iter: 86800 training acc: 0.125
Global Iter: 86900 training loss: 2.05294
Global Iter: 86900 training acc: 0.1875
Global Iter: 87000 training loss: 1.9695
Global Iter: 87000 training acc: 0.15625
Global Iter: 87100 training loss: 1.90106
Global Iter: 87100 training acc: 0.25
Global Iter: 87200 training loss: 1.98105
Global Iter: 87200 training acc: 0.125
Global Iter: 87300 training loss: 1.90729
Global Iter: 87300 training acc: 0.21875
Global Iter: 87400 training loss: 1.96726
Global Iter: 87400 training acc: 0.03125
Global Iter: 87500 training loss: 2.03701
Global Iter: 87500 training acc: 0.28125
Global Iter: 87600 training loss: 1.98397
Global Iter: 87600 training acc: 0.125
Global Iter: 87700 training loss: 1.95121
Global Iter: 87700 training acc: 0.15625
Global Iter: 87800 training loss: 1.91973
Global Iter: 87800 training acc: 0.1875
Global Iter: 87900 training loss: 1.89313
Global Iter: 87900 training acc: 0.25
Global Iter: 88000 training loss: 1.97041
Global Iter: 88000 training acc: 0.1875
Global Iter: 88100 training loss: 1.97217
Global Iter: 88100 training acc: 0.125
Global Iter: 88200 training loss: 2.00315
Global Iter: 88200 training acc: 0.09375
Global Iter: 88300 training loss: 1.9362
Global Iter: 88300 training acc: 0.09375
Global Iter: 88400 training loss: 2.04859
Global Iter: 88400 training acc: 0.1875
Global Iter: 88500 training loss: 2.03631
Global Iter: 88500 training acc: 0.15625
Global Iter: 88600 training loss: 2.00541
Global Iter: 88600 training acc: 0.15625
Global Iter: 88700 training loss: 1.89613
Global Iter: 88700 training acc: 0.15625
Global Iter: 88800 training loss: 2.04909
Global Iter: 88800 training acc: 0.1875
Global Iter: 88900 training loss: 1.91219
Global Iter: 88900 training acc: 0.1875
Global Iter: 89000 training loss: 1.96732
Global Iter: 89000 training acc: 0.15625
Global Iter: 89100 training loss: 2.01272
Global Iter: 89100 training acc: 0.3125
Global Iter: 89200 training loss: 1.96481
Global Iter: 89200 training acc: 0.21875
Global Iter: 89300 training loss: 2.02947
Global Iter: 89300 training acc: 0.15625
Global Iter: 89400 training loss: 1.92846
Global Iter: 89400 training acc: 0.21875
Global Iter: 89500 training loss: 1.9861
Global Iter: 89500 training acc: 0.125
Global Iter: 89600 training loss: 1.93189
Global Iter: 89600 training acc: 0.21875
Global Iter: 89700 training loss: 1.96784
Global Iter: 89700 training acc: 0.21875
Global Iter: 89800 training loss: 1.88552
Global Iter: 89800 training acc: 0.34375
Global Iter: 89900 training loss: 2.08247
Global Iter: 89900 training acc: 0.125
Global Iter: 90000 training loss: 1.99571
Global Iter: 90000 training acc: 0.21875
Global Iter: 90100 training loss: 2.01977
Global Iter: 90100 training acc: 0.25
Global Iter: 90200 training loss: 1.9573
Global Iter: 90200 training acc: 0.25
Global Iter: 90300 training loss: 1.96374
Global Iter: 90300 training acc: 0.0625
Global Iter: 90400 training loss: 1.92749
Global Iter: 90400 training acc: 0.125
Global Iter: 90500 training loss: 1.95075
Global Iter: 90500 training acc: 0.21875
Global Iter: 90600 training loss: 1.92039
Global Iter: 90600 training acc: 0.28125
Global Iter: 90700 training loss: 1.98975
Global Iter: 90700 training acc: 0.125
Global Iter: 90800 training loss: 1.99852
Global Iter: 90800 training acc: 0.1875
Global Iter: 90900 training loss: 2.02904
Global Iter: 90900 training acc: 0.0625
Global Iter: 91000 training loss: 1.87343
Global Iter: 91000 training acc: 0.21875
Global Iter: 91100 training loss: 1.86805
Global Iter: 91100 training acc: 0.21875
Global Iter: 91200 training loss: 2.18606
Global Iter: 91200 training acc: 0.15625
Global Iter: 91300 training loss: 1.96346
Global Iter: 91300 training acc: 0.125
Global Iter: 91400 training loss: 1.90173
Global Iter: 91400 training acc: 0.375
Global Iter: 91500 training loss: 1.98563
Global Iter: 91500 training acc: 0.28125
Global Iter: 91600 training loss: 2.04662
Global Iter: 91600 training acc: 0.15625
Global Iter: 91700 training loss: 2.01164
Global Iter: 91700 training acc: 0.21875
Global Iter: 91800 training loss: 2.00112
Global Iter: 91800 training acc: 0.125
Global Iter: 91900 training loss: 1.91352
Global Iter: 91900 training acc: 0.1875
Global Iter: 92000 training loss: 2.11101
Global Iter: 92000 training acc: 0.09375
Global Iter: 92100 training loss: 2.01085
Global Iter: 92100 training acc: 0.15625
Global Iter: 92200 training loss: 1.9194
Global Iter: 92200 training acc: 0.3125
Global Iter: 92300 training loss: 1.92801
Global Iter: 92300 training acc: 0.1875
Global Iter: 92400 training loss: 1.97061
Global Iter: 92400 training acc: 0.3125
Global Iter: 92500 training loss: 2.01601
Global Iter: 92500 training acc: 0.1875
Global Iter: 92600 training loss: 1.98238
Global Iter: 92600 training acc: 0.09375
Global Iter: 92700 training loss: 2.09195
Global Iter: 92700 training acc: 0.1875
Global Iter: 92800 training loss: 1.99576
Global Iter: 92800 training acc: 0.125
Global Iter: 92900 training loss: 2.08559
Global Iter: 92900 training acc: 0.15625
Global Iter: 93000 training loss: 1.96521
Global Iter: 93000 training acc: 0.21875
Global Iter: 93100 training loss: 2.06482
Global Iter: 93100 training acc: 0.1875
Global Iter: 93200 training loss: 1.93738
Global Iter: 93200 training acc: 0.09375
Global Iter: 93300 training loss: 1.95624
Global Iter: 93300 training acc: 0.21875
Global Iter: 93400 training loss: 1.88223
Global Iter: 93400 training acc: 0.25
Global Iter: 93500 training loss: 1.93788
Global Iter: 93500 training acc: 0.25
Global Iter: 93600 training loss: 1.90757
Global Iter: 93600 training acc: 0.21875
Global Iter: 93700 training loss: 1.91313
Global Iter: 93700 training acc: 0.1875
Global Iter: 93800 training loss: 2.01261
Global Iter: 93800 training acc: 0.15625
Global Iter: 93900 training loss: 1.91481
Global Iter: 93900 training acc: 0.21875
Global Iter: 94000 training loss: 2.06504
Global Iter: 94000 training acc: 0.21875
Global Iter: 94100 training loss: 1.98757
Global Iter: 94100 training acc: 0.375
Model saved in file: /home/ahmet/workspace/tensorboard/tissu2017-06-21 07:17:28.689638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-94154
e_alexnet_b256_lr0005/model.ckpt-94154
Number of Patches: 292312
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-94154
Global Iter: 94200 training loss: 2.08602
Global Iter: 94200 training acc: 0.21875
Global Iter: 94300 training loss: 2.03075
Global Iter: 94300 training acc: 0.125
Global Iter: 94400 training loss: 2.03305
Global Iter: 94400 training acc: 0.125
Global Iter: 94500 training loss: 1.95629
Global Iter: 94500 training acc: 0.125
Global Iter: 94600 training loss: 1.90552
Global Iter: 94600 training acc: 0.1875
Global Iter: 94700 training loss: 2.00192
Global Iter: 94700 training acc: 0.15625
Global Iter: 94800 training loss: 2.05991
Global Iter: 94800 training acc: 0.09375
Global Iter: 94900 training loss: 1.88153
Global Iter: 94900 training acc: 0.21875
Global Iter: 95000 training loss: 2.04697
Global Iter: 95000 training acc: 0.15625
Global Iter: 95100 training loss: 1.96404
Global Iter: 95100 training acc: 0.34375
Global Iter: 95200 training loss: 2.13057
Global Iter: 95200 training acc: 0.125
Global Iter: 95300 training loss: 1.99765
Global Iter: 95300 training acc: 0.21875
Global Iter: 95400 training loss: 1.96832
Global Iter: 95400 training acc: 0.1875
Global Iter: 95500 training loss: 1.99455
Global Iter: 95500 training acc: 0.21875
Global Iter: 95600 training loss: 2.01599
Global Iter: 95600 training acc: 0.125
Global Iter: 95700 training loss: 2.09171
Global Iter: 95700 training acc: 0.15625
Global Iter: 95800 training loss: 2.0555
Global Iter: 95800 training acc: 0.09375
Global Iter: 95900 training loss: 2.03872
Global Iter: 95900 training acc: 0.1875
Global Iter: 96000 training loss: 1.97155
Global Iter: 96000 training acc: 0.1875
Global Iter: 96100 training loss: 1.96252
Global Iter: 96100 training acc: 0.3125
Global Iter: 96200 training loss: 2.06351
Global Iter: 96200 training acc: 0.15625
Global Iter: 96300 training loss: 2.01556
Global Iter: 96300 training acc: 0.15625
Global Iter: 96400 training loss: 2.02865
Global Iter: 96400 training acc: 0.21875
Global Iter: 96500 training loss: 2.0339
Global Iter: 96500 training acc: 0.21875
Global Iter: 96600 training loss: 2.03397
Global Iter: 96600 training acc: 0.1875
Global Iter: 96700 training loss: 1.97715
Global Iter: 96700 training acc: 0.21875
Global Iter: 96800 training loss: 1.88523
Global Iter: 96800 training acc: 0.25
Global Iter: 96900 training loss: 1.99982
Global Iter: 96900 training acc: 0.09375
Global Iter: 97000 training loss: 1.93054
Global Iter: 97000 training acc: 0.125
Global Iter: 97100 training loss: 2.08023
Global Iter: 97100 training acc: 0.09375
Global Iter: 97200 training loss: 2.00714
Global Iter: 97200 training acc: 0.09375
Global Iter: 97300 training loss: 1.98676
Global Iter: 97300 training acc: 0.28125
Global Iter: 97400 training loss: 2.05588
Global Iter: 97400 training acc: 0.09375
Global Iter: 97500 training loss: 2.02229
Global Iter: 97500 training acc: 0.09375
Global Iter: 97600 training loss: 1.93208
Global Iter: 97600 training acc: 0.28125
Global Iter: 97700 training loss: 2.00705
Global Iter: 97700 training acc: 0.1875
Global Iter: 97800 training loss: 1.98025
Global Iter: 97800 training acc: 0.15625
Global Iter: 97900 training loss: 2.06201
Global Iter: 97900 training acc: 0.125
Global Iter: 98000 training loss: 2.04267
Global Iter: 98000 training acc: 0.125
Global Iter: 98100 training loss: 1.95377
Global Iter: 98100 training acc: 0.21875
Global Iter: 98200 training loss: 2.03478
Global Iter: 98200 training acc: 0.21875
Global Iter: 98300 training loss: 2.12862
Global Iter: 98300 training acc: 0.28125
Global Iter: 98400 training loss: 1.97965
Global Iter: 98400 training acc: 0.21875
Global Iter: 98500 training loss: 2.04804
Global Iter: 98500 training acc: 0.1875
Global Iter: 98600 training loss: 1.96761
Global Iter: 98600 training acc: 0.25
Global Iter: 98700 training loss: 1.96243
Global Iter: 98700 training acc: 0.28125
Global Iter: 98800 training loss: 1.95343
Global Iter: 98800 training acc: 0.28125
Global Iter: 98900 training loss: 1.93585
Global Iter: 98900 training acc: 0.1875
Global Iter: 99000 training loss: 1.89661
Global Iter: 99000 training acc: 0.25
Global Iter: 99100 training loss: 2.22757
Global Iter: 99100 training acc: 0.15625
Global Iter: 99200 training loss: 1.99918
Global Iter: 99200 training acc: 0.125
Global Iter: 99300 training loss: 1.98931
Global Iter: 99300 training acc: 0.25
Global Iter: 99400 training loss: 1.95972
Global Iter: 99400 training acc: 0.1875
Global Iter: 99500 training loss: 2.06158
Global Iter: 99500 training acc: 0.125
Global Iter: 99600 training loss: 2.01565
Global Iter: 99600 training acc: 0.125
Global Iter: 99700 training loss: 2.12386
Global Iter: 99700 training acc: 0.125
Global Iter: 99800 training loss: 1.95979
Global Iter: 99800 training acc: 0.25
Global Iter: 99900 training loss: 1.97119
Global Iter: 99900 training acc: 0.125
Global Iter: 100000 training loss: 2.18734
Global Iter: 100000 training acc: 0.125
Global Iter: 100100 training loss: 2.0105
Global Iter: 100100 training acc: 0.125
Global Iter: 100200 training loss: 2.03335
Global Iter: 100200 training acc: 0.1875
Global Iter: 100300 training loss: 2.04512
Global Iter: 100300 training acc: 0.125
Global Iter: 100400 training loss: 2.05577
Global Iter: 100400 training acc: 0.21875
Global Iter: 100500 training loss: 2.16994
Global Iter: 100500 training acc: 0.1875
Global Iter: 100600 training loss: 1.97495
Global Iter: 100600 training acc: 0.1875
Global Iter: 100700 training loss: 2.03562
Global Iter: 100700 training acc: 0.1875
Global Iter: 100800 training loss: 1.9517
Global Iter: 100800 training acc: 0.15625
Global Iter: 100900 training loss: 2.03638
Global Iter: 100900 training acc: 0.15625
Global Iter: 101000 training loss: 2.06316
Global Iter: 101000 training acc: 0.09375
Global Iter: 101100 training loss: 2.09982
Global Iter: 101100 training acc: 0.1875
Global Iter: 101200 training loss: 2.05112
Global Iter: 101200 training acc: 0.28125
Global Iter: 101300 training loss: 2.03137
Global Iter: 101300 training acc: 0.1875
Global Iter: 101400 training loss: 2.03939
Global Iter: 101400 training acc: 0.15625
Global Iter: 101500 training loss: 2.01926
Global Iter: 101500 training acc: 0.21875
Global Iter: 101600 training loss: 2.08808
Global Iter: 101600 training acc: 0.1875
Global Iter: 101700 training loss: 1.95184
Global Iter: 101700 training acc: 0.15625
Global Iter: 101800 training loss: 1.92009
Global Iter: 101800 training acc: 0.15625
Global Iter: 101900 training loss: 2.01285
Global Iter: 101900 training acc: 0.1875
Global Iter: 102000 training loss: 1.97731
Global Iter: 102000 training acc: 0.1875
Global Iter: 102100 training loss: 2.04283
Global Iter: 102100 training acc: 0.125
Global Iter: 102200 training loss: 1.87564
Global Iter: 102200 training acc: 0.3125
Global Iter: 102300 training loss: 1.97786
Global Iter: 102300 training acc: 0.1875
Global Iter: 102400 training loss: 2.06586
Global Iter: 102400 training acc: 0.1875
Global Iter: 102500 training loss: 2.10928
Global Iter: 102500 training acc: 0.15625
Global Iter: 102600 training loss: 2.00526
Global Iter: 102600 training acc: 0.21875
Global Iter: 102700 training loss: 2.01214
Global Iter: 102700 training acc: 0.09375
Global Iter: 102800 training loss: 2.01983
Global Iter: 102800 training acc: 0.15625
Global Iter: 102900 training loss: 1.89239
Global Iter: 102900 training acc: 0.125
Global Iter: 103000 training loss: 2.02081
Global Iter: 103000 training acc: 0.25
Global Iter: 103100 training loss: 2.07755
Global Iter: 103100 training acc: 0.15625
Global Iter: 103200 training loss: 1.89949
Global Iter: 103200 training acc: 0.34375
Global Iter: 103300 training loss: 2.01836
Global Iter: 103300 training acc: 0.1875
Global Iter: 103400 training loss: 1.98479
Global Iter: 103400 training acc: 0.3125
Global Iter: 103500 training loss: 1.92589
Global Iter: 103500 training acc: 0.21875
Global Iter: 103600 training loss: 1.96993
Global Iter: 103600 training acc: 0.15625
Global Iter: 103700 training loss: 1.97325
Global Iter: 103700 training acc: 0.28125
Global Iter: 103800 training loss: 2.03788
Global Iter: 103800 training acc: 0.125
Global Iter: 103900 training loss: 1.99806
Global Iter: 103900 training acc: 0.15625
Global Iter: 104000 training loss: 2.06755
Global Iter: 104000 training acc: 0.21875
Global Iter: 104100 training loss: 1.94157
Global Iter: 104100 training acc: 0.1875
Global Iter: 104200 training loss: 1.92589
Global Iter: 104200 training acc: 0.125
Global Iter: 104300 training loss: 1.9805
Global Iter: 104300 training acc: 0.125
Global Iter: 104400 training loss: 1.96487
Global Iter: 104400 training acc: 0.15625
Global Iter: 104500 training loss: 2.01911
Global Iter: 104500 training acc: 0.125
Global Iter: 104600 training loss: 2.00577
Global Iter: 104600 training acc: 0.21875
Global Iter: 104700 training loss: 2.00265
Global Iter: 104700 training acc: 0.125
Global Iter: 104800 training loss: 1.93426
Global Iter: 104800 training acc: 0.21875
Global Iter: 104900 training loss: 2.06893
Global Iter: 104900 training acc: 0.21875
Global Iter: 105000 training loss: 1.99914
Global Iter: 105000 training acc: 0.21875
Global Iter: 105100 training loss: 1.90353
Global Iter: 105100 training acc: 0.28125
Global Iter: 105200 training loss: 2.00096
Global Iter: 105200 training acc: 0.0
Global Iter: 105300 training loss: 2.00556
Global Iter: 105300 training acc: 0.09375
Global Iter: 105400 training loss: 2.02151
Global Iter: 105400 training acc: 0.125
Global Iter: 105500 training loss: 1.95902
Global Iter: 105500 training acc: 0.1875
Global Iter: 105600 training loss: 2.02845
Global Iter: 105600 training acc: 0.15625
Global Iter: 105700 training loss: 2.11511
Global Iter: 105700 training acc: 0.09375
Global Iter: 105800 training loss: 1.99137
Global Iter: 105800 training acc: 0.25
Global Iter: 105900 training loss: 2.04209
Global Iter: 105900 training acc: 0.125
Global Iter: 106000 training loss: 2.03421
Global Iter: 106000 training acc: 0.15625
Global Iter: 106100 training loss: 1.93118
Global Iter: 106100 training acc: 0.40625
Global Iter: 106200 training loss: 1.9884
Global Iter: 106200 training acc: 0.25
Global Iter: 106300 training loss: 1.92642
Global Iter: 106300 training acc: 0.28125
Global Iter: 106400 training loss: 2.12198
Global Iter: 106400 training acc: 0.15625
Global Iter: 106500 training loss: 2.00212
Global Iter: 106500 training acc: 0.21875
Global Iter: 106600 training loss: 2.00264
Global Iter: 106600 training acc: 0.1875
Global Iter: 106700 training loss: 1.95909
Global Iter: 106700 training acc: 0.21875
Global Iter: 106800 training loss: 2.04705
Global Iter: 106800 training acc: 0.09375
Global Iter: 106900 training loss: 2.01789
Global Iter: 106900 training acc: 0.21875
Global Iter: 107000 training loss: 1.93336
Global Iter: 107000 training acc: 0.09375
Global Iter: 107100 training loss: 2.05265
Global Iter: 107100 training acc: 0.15625
Global Iter: 107200 training loss: 2.06728
Global Iter: 107200 training acc: 0.125
Global Iter: 107300 training loss: 2.00988
Global Iter: 107300 training acc: 0.125
Global Iter: 107400 training loss: 1.98231
Global Iter: 107400 training acc: 0.125
Global Iter: 107500 training loss: 2.0406
Global Iter: 107500 training acc: 0.21875
Global Iter: 107600 training loss: 1.94064
Global Iter: 107600 training acc: 0.3125
Global Iter: 107700 training loss: 1.94319
Global Iter: 107700 training acc: 0.15625
Global Iter: 107800 training loss: 2.02397
Global Iter: 107800 training acc: 0.21875
Global Iter: 107900 training loss: 2.05027
Global Iter: 107900 training acc: 0.125
Global Iter: 108000 training loss: 1.97953
Global Iter: 108000 training acc: 0.15625
Global Iter: 108100 training loss: 1.97987
Global Iter: 108100 training acc: 0.25
Global Iter: 108200 training loss: 1.94713
Global Iter: 108200 training acc: 0.25
Global Iter: 108300 training loss: 1.93551
Global Iter: 108300 training acc: 0.21875
Global Iter: 108400 training loss: 2.0089
Global Iter: 108400 training acc: 0.15625
Global Iter: 108500 training loss: 1.92781
Global Iter: 108500 training acc: 0.15625
Global Iter: 108600 training loss: 1.92578
Global Iter: 108600 training acc: 0.15625
Global Iter: 108700 training loss: 2.01559
Glo2017-06-21 07:48:55.993260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-112424
bal Iter: 108700 training acc: 0.125
Global Iter: 108800 training loss: 1.95988
Global Iter: 108800 training acc: 0.125
Global Iter: 108900 training loss: 1.89943
Global Iter: 108900 training acc: 0.34375
Global Iter: 109000 training loss: 1.9537
Global Iter: 109000 training acc: 0.09375
Global Iter: 109100 training loss: 1.98302
Global Iter: 109100 training acc: 0.28125
Global Iter: 109200 training loss: 1.93565
Global Iter: 109200 training acc: 0.1875
Global Iter: 109300 training loss: 1.91953
Global Iter: 109300 training acc: 0.25
Global Iter: 109400 training loss: 1.97465
Global Iter: 109400 training acc: 0.125
Global Iter: 109500 training loss: 1.94847
Global Iter: 109500 training acc: 0.28125
Global Iter: 109600 training loss: 2.07802
Global Iter: 109600 training acc: 0.1875
Global Iter: 109700 training loss: 2.04458
Global Iter: 109700 training acc: 0.125
Global Iter: 109800 training loss: 1.96065
Global Iter: 109800 training acc: 0.1875
Global Iter: 109900 training loss: 1.94874
Global Iter: 109900 training acc: 0.125
Global Iter: 110000 training loss: 1.94148
Global Iter: 110000 training acc: 0.21875
Global Iter: 110100 training loss: 2.01471
Global Iter: 110100 training acc: 0.09375
Global Iter: 110200 training loss: 1.95418
Global Iter: 110200 training acc: 0.25
Global Iter: 110300 training loss: 2.00532
Global Iter: 110300 training acc: 0.125
Global Iter: 110400 training loss: 1.94617
Global Iter: 110400 training acc: 0.0625
Global Iter: 110500 training loss: 2.10171
Global Iter: 110500 training acc: 0.125
Global Iter: 110600 training loss: 2.00957
Global Iter: 110600 training acc: 0.21875
Global Iter: 110700 training loss: 2.03096
Global Iter: 110700 training acc: 0.1875
Global Iter: 110800 training loss: 1.97755
Global Iter: 110800 training acc: 0.28125
Global Iter: 110900 training loss: 2.00495
Global Iter: 110900 training acc: 0.21875
Global Iter: 111000 training loss: 1.94857
Global Iter: 111000 training acc: 0.15625
Global Iter: 111100 training loss: 1.98688
Global Iter: 111100 training acc: 0.1875
Global Iter: 111200 training loss: 2.02102
Global Iter: 111200 training acc: 0.15625
Global Iter: 111300 training loss: 2.17203
Global Iter: 111300 training acc: 0.15625
Global Iter: 111400 training loss: 2.0615
Global Iter: 111400 training acc: 0.0625
Global Iter: 111500 training loss: 2.01582
Global Iter: 111500 training acc: 0.125
Global Iter: 111600 training loss: 2.05191
Global Iter: 111600 training acc: 0.125
Global Iter: 111700 training loss: 1.9663
Global Iter: 111700 training acc: 0.125
Global Iter: 111800 training loss: 1.95965
Global Iter: 111800 training acc: 0.28125
Global Iter: 111900 training loss: 2.03029
Global Iter: 111900 training acc: 0.03125
Global Iter: 112000 training loss: 1.97343
Global Iter: 112000 training acc: 0.125
Global Iter: 112100 training loss: 1.9677
Global Iter: 112100 training acc: 0.28125
Global Iter: 112200 training loss: 2.02749
Global Iter: 112200 training acc: 0.1875
Global Iter: 112300 training loss: 1.95525
Global Iter: 112300 training acc: 0.21875
Global Iter: 112400 training loss: 2.01691
Global Iter: 112400 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-112424
Number of Patches: 289389
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-112424
Global Iter: 112500 training loss: 1.99099
Global Iter: 112500 training acc: 0.34375
Global Iter: 112600 training loss: 2.02033
Global Iter: 112600 training acc: 0.0625
Global Iter: 112700 training loss: 2.05606
Global Iter: 112700 training acc: 0.21875
Global Iter: 112800 training loss: 1.91118
Global Iter: 112800 training acc: 0.1875
Global Iter: 112900 training loss: 1.9909
Global Iter: 112900 training acc: 0.1875
Global Iter: 113000 training loss: 2.04
Global Iter: 113000 training acc: 0.15625
Global Iter: 113100 training loss: 1.93076
Global Iter: 113100 training acc: 0.125
Global Iter: 113200 training loss: 2.01943
Global Iter: 113200 training acc: 0.09375
Global Iter: 113300 training loss: 1.97662
Global Iter: 113300 training acc: 0.1875
Global Iter: 113400 training loss: 2.00918
Global Iter: 113400 training acc: 0.0625
Global Iter: 113500 training loss: 1.98815
Global Iter: 113500 training acc: 0.1875
Global Iter: 113600 training loss: 1.97772
Global Iter: 113600 training acc: 0.15625
Global Iter: 113700 training loss: 1.98095
Global Iter: 113700 training acc: 0.25
Global Iter: 113800 training loss: 2.07203
Global Iter: 113800 training acc: 0.15625
Global Iter: 113900 training loss: 2.04533
Global Iter: 113900 training acc: 0.09375
Global Iter: 114000 training loss: 2.04681
Global Iter: 114000 training acc: 0.25
Global Iter: 114100 training loss: 1.92126
Global Iter: 114100 training acc: 0.25
Global Iter: 114200 training loss: 2.0353
Global Iter: 114200 training acc: 0.0625
Global Iter: 114300 training loss: 1.95425
Global Iter: 114300 training acc: 0.21875
Global Iter: 114400 training loss: 1.95615
Global Iter: 114400 training acc: 0.15625
Global Iter: 114500 training loss: 1.85012
Global Iter: 114500 training acc: 0.4375
Global Iter: 114600 training loss: 1.91203
Global Iter: 114600 training acc: 0.125
Global Iter: 114700 training loss: 1.98234
Global Iter: 114700 training acc: 0.125
Global Iter: 114800 training loss: 2.03597
Global Iter: 114800 training acc: 0.0625
Global Iter: 114900 training loss: 1.93634
Global Iter: 114900 training acc: 0.21875
Global Iter: 115000 training loss: 1.95318
Global Iter: 115000 training acc: 0.1875
Global Iter: 115100 training loss: 1.91504
Global Iter: 115100 training acc: 0.15625
Global Iter: 115200 training loss: 2.05097
Global Iter: 115200 training acc: 0.15625
Global Iter: 115300 training loss: 1.96311
Global Iter: 115300 training acc: 0.21875
Global Iter: 115400 training loss: 1.91475
Global Iter: 115400 training acc: 0.1875
Global Iter: 115500 training loss: 1.94626
Global Iter: 115500 training acc: 0.25
Global Iter: 115600 training loss: 2.04298
Global Iter: 115600 training acc: 0.1875
Global Iter: 115700 training loss: 1.9343
Global Iter: 115700 training acc: 0.21875
Global Iter: 115800 training loss: 1.91007
Global Iter: 115800 training acc: 0.40625
Global Iter: 115900 training loss: 1.95289
Global Iter: 115900 training acc: 0.1875
Global Iter: 116000 training loss: 1.97457
Global Iter: 116000 training acc: 0.1875
Global Iter: 116100 training loss: 2.15568
Global Iter: 116100 training acc: 0.15625
Global Iter: 116200 training loss: 1.9107
Global Iter: 116200 training acc: 0.375
Global Iter: 116300 training loss: 1.93778
Global Iter: 116300 training acc: 0.21875
Global Iter: 116400 training loss: 2.06498
Global Iter: 116400 training acc: 0.1875
Global Iter: 116500 training loss: 2.0169
Global Iter: 116500 training acc: 0.09375
Global Iter: 116600 training loss: 1.85267
Global Iter: 116600 training acc: 0.21875
Global Iter: 116700 training loss: 2.07516
Global Iter: 116700 training acc: 0.09375
Global Iter: 116800 training loss: 2.11289
Global Iter: 116800 training acc: 0.1875
Global Iter: 116900 training loss: 2.02609
Global Iter: 116900 training acc: 0.125
Global Iter: 117000 training loss: 1.95619
Global Iter: 117000 training acc: 0.21875
Global Iter: 117100 training loss: 1.93247
Global Iter: 117100 training acc: 0.15625
Global Iter: 117200 training loss: 2.03775
Global Iter: 117200 training acc: 0.21875
Global Iter: 117300 training loss: 2.04763
Global Iter: 117300 training acc: 0.1875
Global Iter: 117400 training loss: 2.01508
Global Iter: 117400 training acc: 0.15625
Global Iter: 117500 training loss: 2.00484
Global Iter: 117500 training acc: 0.125
Global Iter: 117600 training loss: 1.94224
Global Iter: 117600 training acc: 0.1875
Global Iter: 117700 training loss: 1.86651
Global Iter: 117700 training acc: 0.25
Global Iter: 117800 training loss: 1.96435
Global Iter: 117800 training acc: 0.15625
Global Iter: 117900 training loss: 1.92999
Global Iter: 117900 training acc: 0.28125
Global Iter: 118000 training loss: 2.1027
Global Iter: 118000 training acc: 0.09375
Global Iter: 118100 training loss: 1.97938
Global Iter: 118100 training acc: 0.21875
Global Iter: 118200 training loss: 2.09522
Global Iter: 118200 training acc: 0.1875
Global Iter: 118300 training loss: 1.9988
Global Iter: 118300 training acc: 0.125
Global Iter: 118400 training loss: 2.00447
Global Iter: 118400 training acc: 0.0625
Global Iter: 118500 training loss: 1.86638
Global Iter: 118500 training acc: 0.28125
Global Iter: 118600 training loss: 1.90408
Global Iter: 118600 training acc: 0.28125
Global Iter: 118700 training loss: 1.94806
Global Iter: 118700 training acc: 0.1875
Global Iter: 118800 training loss: 1.97993
Global Iter: 118800 training acc: 0.21875
Global Iter: 118900 training loss: 1.9562
Global Iter: 118900 training acc: 0.15625
Global Iter: 119000 training loss: 1.96317
Global Iter: 119000 training acc: 0.25
Global Iter: 119100 training loss: 2.02715
Global Iter: 119100 training acc: 0.0625
Global Iter: 119200 training loss: 2.07386
Global Iter: 119200 training acc: 0.3125
Global Iter: 119300 training loss: 2.00999
Global Iter: 119300 training acc: 0.0625
Global Iter: 119400 training loss: 1.92183
Global Iter: 119400 training acc: 0.09375
Global Iter: 119500 training loss: 2.0843
Global Iter: 119500 training acc: 0.125
Global Iter: 119600 training loss: 1.99915
Global Iter: 119600 training acc: 0.125
Global Iter: 119700 training loss: 2.0703
Global Iter: 119700 training acc: 0.15625
Global Iter: 119800 training loss: 2.09745
Global Iter: 119800 training acc: 0.15625
Global Iter: 119900 training loss: 1.93675
Global Iter: 119900 training acc: 0.125
Global Iter: 120000 training loss: 2.11813
Global Iter: 120000 training acc: 0.15625
Global Iter: 120100 training loss: 1.9751
Global Iter: 120100 training acc: 0.25
Global Iter: 120200 training loss: 1.93268
Global Iter: 120200 training acc: 0.15625
Global Iter: 120300 training loss: 2.00799
Global Iter: 120300 training acc: 0.03125
Global Iter: 120400 training loss: 1.94358
Global Iter: 120400 training acc: 0.25
Global Iter: 120500 training loss: 2.00294
Global Iter: 120500 training acc: 0.25
Global Iter: 120600 training loss: 2.03861
Global Iter: 120600 training acc: 0.09375
Global Iter: 120700 training loss: 2.02128
Global Iter: 120700 training acc: 0.15625
Global Iter: 120800 training loss: 1.93099
Global Iter: 120800 training acc: 0.1875
Global Iter: 120900 training loss: 2.02335
Global Iter: 120900 training acc: 0.1875
Global Iter: 121000 training loss: 2.00389
Global Iter: 121000 training acc: 0.125
Global Iter: 121100 training loss: 1.95424
Global Iter: 121100 training acc: 0.25
Global Iter: 121200 training loss: 2.00461
Global Iter: 121200 training acc: 0.09375
Global Iter: 121300 training loss: 2.02838
Global Iter: 121300 training acc: 0.15625
Global Iter: 121400 training loss: 2.02613
Global Iter: 121400 training acc: 0.125
Global Iter: 121500 training loss: 2.10146
Global Iter: 121500 training acc: 0.0625
Global Iter: 121600 training loss: 1.98955
Global Iter: 121600 training acc: 0.125
Global Iter: 121700 training loss: 1.96149
Global Iter: 121700 training acc: 0.15625
Global Iter: 121800 training loss: 1.87864
Global Iter: 121800 training acc: 0.3125
Global Iter: 121900 training loss: 1.97749
Global Iter: 121900 training acc: 0.125
Global Iter: 122000 training loss: 1.96118
Global Iter: 122000 training acc: 0.25
Global Iter: 122100 training loss: 1.99412
Global Iter: 122100 training acc: 0.3125
Global Iter: 122200 training loss: 2.05001
Global Iter: 122200 training acc: 0.1875
Global Iter: 122300 training loss: 2.08535
Global Iter: 122300 training acc: 0.21875
Global Iter: 122400 training loss: 1.95014
Global Iter: 122400 training acc: 0.15625
Global Iter: 122500 training loss: 2.10734
Global Iter: 122500 training acc: 0.125
Global Iter: 122600 training loss: 2.18667
Global Iter: 122600 training acc: 0.21875
Global Iter: 122700 training loss: 2.00301
Global Iter: 122700 training acc: 0.15625
Global Iter: 122800 training loss: 1.98864
Global Iter: 122800 training acc: 0.21875
Global Iter: 122900 training loss: 1.95854
Global Iter: 122900 training acc: 0.15625
Global Iter: 123000 training loss: 2.00397
Global Iter: 123000 training acc: 0.15625
Global Iter: 123100 training loss: 2.01163
Global Iter: 123100 training acc: 0.15625
Global Iter: 123200 training loss: 1.93777
Global Iter: 123200 training acc: 0.1875
Global Iter: 123300 training loss: 2.04456
Global Iter: 123300 training acc: 0.125
Global Iter: 123400 training loss: 2.04954
Global Iter: 123400 training acc: 0.21875
Global Iter: 123500 training loss: 1.95337
Global Iter: 123500 training acc: 0.21875
Global Iter: 123600 training loss: 2.05591
Global Iter: 123600 training acc: 0.1875
Global Iter: 123700 training loss: 1.93648
Global Iter: 123700 training acc: 0.25
Global Iter: 123800 training loss: 1.96493
Global Iter: 123800 training acc: 0.25
Global Iter: 123900 training loss: 1.95429
Global Iter: 123900 training acc: 0.09375
Global Iter: 124000 training loss: 1.9541
Global Iter: 124000 training acc: 0.3125
Global Iter: 124100 training loss: 1.98451
Global Iter: 124100 training acc: 0.09375
Global Iter: 124200 training loss: 1.90186
Global Iter: 124200 training acc: 0.25
Global Iter: 124300 training loss: 1.98093
Global Iter: 124300 training acc: 0.28125
Global Iter: 124400 training loss: 2.0031
Global Iter: 124400 training acc: 0.125
Global Iter: 124500 training loss: 2.00782
Global Iter: 124500 training acc: 0.15625
Global Iter: 124600 training loss: 1.92388
Global Iter: 124600 training acc: 0.21875
Global Iter: 124700 training loss: 2.01973
Global Iter: 124700 training acc: 0.25
Global Iter: 124800 training loss: 2.01496
Global Iter: 124800 training acc: 0.28125
Global Iter: 124900 training loss: 1.94713
Global Iter: 124900 training acc: 0.25
Global Iter: 125000 training loss: 2.11228
Global Iter: 125000 training acc: 0.15625
Global Iter: 125100 training loss: 2.00311
Global Iter: 125100 training acc: 0.1875
Global Iter: 125200 training loss: 1.92466
Global Iter: 125200 training acc: 0.25
Global Iter: 125300 training loss: 1.94361
Global Iter: 125300 training acc: 0.25
Global Iter: 125400 training loss: 1.9723
Global Iter: 125400 training acc: 0.15625
Global Iter: 125500 training loss: 1.97155
Global Iter: 125500 training acc: 0.125
Global Iter: 125600 training loss: 2.06161
Global Iter: 125600 training acc: 0.0625
Global Iter: 125700 training loss: 2.02007
Global Iter: 125700 training acc: 0.34375
Global Iter: 125800 training loss: 1.9666
Global Iter: 125800 training acc: 0.25
Global Iter: 125900 training loss: 1.89005
Global Iter: 125900 training acc: 0.21875
Global Iter: 126000 training loss: 2.01726
Global Iter: 126000 training acc: 0.15625
Global Iter: 126100 training loss: 1.87697
Global Iter: 126100 training acc: 0.3125
Global Iter: 126200 training loss: 2.06723
Global Iter: 126200 training acc: 0.15625
Global Iter: 126300 training loss: 2.00986
Global Iter: 126300 training acc: 0.1875
Global Iter: 126400 training loss: 1.92535
Global Iter: 126400 training acc: 0.25
Global Iter: 126500 training loss: 2.07277
Global Iter: 126500 training acc: 0.0625
Global Iter: 126600 training loss: 2.00058
Global Iter: 126600 training acc: 0.15625
Global Iter: 126700 training loss: 1.94388
Global Iter: 126700 training acc: 0.15625
Global Iter: 126800 training loss: 2.10151
Global Iter: 126800 training acc: 0.15625
Global Iter: 126900 training loss: 2.06423
Global Iter: 126900 training acc: 0.25
Global Iter: 127000 training loss: 1.96043
Global Iter: 127000 training acc: 0.15625
Global Iter: 127100 training loss: 2.06816
Global Iter: 127100 training acc: 0.09375
Global Iter: 127200 training loss: 2.00708
Global Iter: 127200 training acc: 0.21875
Global Iter: 127300 training loss: 2.04344
Global Iter: 127300 training acc: 0.15625
Global Iter: 127400 training loss: 1.96879
Global Iter: 127400 training acc: 0.125
Global Iter: 127500 training loss: 2.02542
Global Iter: 127500 training acc: 0.15625
Global Iter: 127600 training loss: 1.95856
Global Iter: 127600 training acc: 0.125
Global Iter: 127700 training loss: 1.97625
Global Iter: 127700 training acc: 0.125
Global Iter: 127800 training loss: 2.01329
Global Iter: 127800 training acc: 0.21875
Global Iter: 127900 training loss: 2.05182
Global Iter: 127900 training acc: 0.15625
Glo2017-06-21 08:20:48.031392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-130511
bal Iter: 128000 training loss: 2.02571
Global Iter: 128000 training acc: 0.15625
Global Iter: 128100 training loss: 2.06464
Global Iter: 128100 training acc: 0.1875
Global Iter: 128200 training loss: 1.90744
Global Iter: 128200 training acc: 0.15625
Global Iter: 128300 training loss: 2.12721
Global Iter: 128300 training acc: 0.09375
Global Iter: 128400 training loss: 2.09856
Global Iter: 128400 training acc: 0.25
Global Iter: 128500 training loss: 1.97554
Global Iter: 128500 training acc: 0.1875
Global Iter: 128600 training loss: 2.03361
Global Iter: 128600 training acc: 0.21875
Global Iter: 128700 training loss: 2.00564
Global Iter: 128700 training acc: 0.1875
Global Iter: 128800 training loss: 2.08046
Global Iter: 128800 training acc: 0.09375
Global Iter: 128900 training loss: 2.04346
Global Iter: 128900 training acc: 0.1875
Global Iter: 129000 training loss: 2.04111
Global Iter: 129000 training acc: 0.1875
Global Iter: 129100 training loss: 2.03018
Global Iter: 129100 training acc: 0.15625
Global Iter: 129200 training loss: 2.00531
Global Iter: 129200 training acc: 0.125
Global Iter: 129300 training loss: 1.94208
Global Iter: 129300 training acc: 0.34375
Global Iter: 129400 training loss: 1.99101
Global Iter: 129400 training acc: 0.15625
Global Iter: 129500 training loss: 2.05181
Global Iter: 129500 training acc: 0.125
Global Iter: 129600 training loss: 1.9163
Global Iter: 129600 training acc: 0.1875
Global Iter: 129700 training loss: 1.96635
Global Iter: 129700 training acc: 0.125
Global Iter: 129800 training loss: 2.0026
Global Iter: 129800 training acc: 0.09375
Global Iter: 129900 training loss: 2.04971
Global Iter: 129900 training acc: 0.1875
Global Iter: 130000 training loss: 2.00353
Global Iter: 130000 training acc: 0.1875
Global Iter: 130100 training loss: 1.95905
Global Iter: 130100 training acc: 0.1875
Global Iter: 130200 training loss: 1.99806
Global Iter: 130200 training acc: 0.21875
Global Iter: 130300 training loss: 2.11728
Global Iter: 130300 training acc: 0.03125
Global Iter: 130400 training loss: 1.91523
Global Iter: 130400 training acc: 0.21875
Global Iter: 130500 training loss: 2.03913
Global Iter: 130500 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-130511
Number of Patches: 286496
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-130511
Global Iter: 130600 training loss: 1.99639
Global Iter: 130600 training acc: 0.125
Global Iter: 130700 training loss: 1.98248
Global Iter: 130700 training acc: 0.25
Global Iter: 130800 training loss: 1.99163
Global Iter: 130800 training acc: 0.21875
Global Iter: 130900 training loss: 1.92662
Global Iter: 130900 training acc: 0.125
Global Iter: 131000 training loss: 2.07658
Global Iter: 131000 training acc: 0.21875
Global Iter: 131100 training loss: 1.93845
Global Iter: 131100 training acc: 0.25
Global Iter: 131200 training loss: 2.0254
Global Iter: 131200 training acc: 0.125
Global Iter: 131300 training loss: 1.91949
Global Iter: 131300 training acc: 0.15625
Global Iter: 131400 training loss: 1.93043
Global Iter: 131400 training acc: 0.15625
Global Iter: 131500 training loss: 1.94441
Global Iter: 131500 training acc: 0.1875
Global Iter: 131600 training loss: 1.93596
Global Iter: 131600 training acc: 0.15625
Global Iter: 131700 training loss: 2.0064
Global Iter: 131700 training acc: 0.09375
Global Iter: 131800 training loss: 1.97087
Global Iter: 131800 training acc: 0.125
Global Iter: 131900 training loss: 2.0461
Global Iter: 131900 training acc: 0.1875
Global Iter: 132000 training loss: 2.091
Global Iter: 132000 training acc: 0.15625
Global Iter: 132100 training loss: 1.94155
Global Iter: 132100 training acc: 0.25
Global Iter: 132200 training loss: 1.87461
Global Iter: 132200 training acc: 0.25
Global Iter: 132300 training loss: 2.12478
Global Iter: 132300 training acc: 0.21875
Global Iter: 132400 training loss: 1.99006
Global Iter: 132400 training acc: 0.125
Global Iter: 132500 training loss: 1.9175
Global Iter: 132500 training acc: 0.1875
Global Iter: 132600 training loss: 2.0285
Global Iter: 132600 training acc: 0.21875
Global Iter: 132700 training loss: 1.85178
Global Iter: 132700 training acc: 0.40625
Global Iter: 132800 training loss: 1.98475
Global Iter: 132800 training acc: 0.25
Global Iter: 132900 training loss: 2.08204
Global Iter: 132900 training acc: 0.1875
Global Iter: 133000 training loss: 1.97795
Global Iter: 133000 training acc: 0.28125
Global Iter: 133100 training loss: 2.11049
Global Iter: 133100 training acc: 0.15625
Global Iter: 133200 training loss: 2.00503
Global Iter: 133200 training acc: 0.0625
Global Iter: 133300 training loss: 2.18135
Global Iter: 133300 training acc: 0.15625
Global Iter: 133400 training loss: 2.07933
Global Iter: 133400 training acc: 0.0625
Global Iter: 133500 training loss: 1.91782
Global Iter: 133500 training acc: 0.21875
Global Iter: 133600 training loss: 1.99406
Global Iter: 133600 training acc: 0.125
Global Iter: 133700 training loss: 2.07922
Global Iter: 133700 training acc: 0.1875
Global Iter: 133800 training loss: 1.95177
Global Iter: 133800 training acc: 0.375
Global Iter: 133900 training loss: 2.02616
Global Iter: 133900 training acc: 0.125
Global Iter: 134000 training loss: 2.12677
Global Iter: 134000 training acc: 0.09375
Global Iter: 134100 training loss: 1.98598
Global Iter: 134100 training acc: 0.15625
Global Iter: 134200 training loss: 2.04221
Global Iter: 134200 training acc: 0.0625
Global Iter: 134300 training loss: 2.01293
Global Iter: 134300 training acc: 0.25
Global Iter: 134400 training loss: 1.94479
Global Iter: 134400 training acc: 0.25
Global Iter: 134500 training loss: 1.95223
Global Iter: 134500 training acc: 0.15625
Global Iter: 134600 training loss: 1.98859
Global Iter: 134600 training acc: 0.1875
Global Iter: 134700 training loss: 2.02777
Global Iter: 134700 training acc: 0.1875
Global Iter: 134800 training loss: 1.94666
Global Iter: 134800 training acc: 0.125
Global Iter: 134900 training loss: 2.00016
Global Iter: 134900 training acc: 0.15625
Global Iter: 135000 training loss: 1.95224
Global Iter: 135000 training acc: 0.34375
Global Iter: 135100 training loss: 1.97804
Global Iter: 135100 training acc: 0.21875
Global Iter: 135200 training loss: 1.96057
Global Iter: 135200 training acc: 0.21875
Global Iter: 135300 training loss: 1.98525
Global Iter: 135300 training acc: 0.15625
Global Iter: 135400 training loss: 1.96218
Global Iter: 135400 training acc: 0.21875
Global Iter: 135500 training loss: 2.01991
Global Iter: 135500 training acc: 0.15625
Global Iter: 135600 training loss: 2.00138
Global Iter: 135600 training acc: 0.1875
Global Iter: 135700 training loss: 1.95177
Global Iter: 135700 training acc: 0.0625
Global Iter: 135800 training loss: 2.00188
Global Iter: 135800 training acc: 0.0625
Global Iter: 135900 training loss: 1.98667
Global Iter: 135900 training acc: 0.03125
Global Iter: 136000 training loss: 1.90062
Global Iter: 136000 training acc: 0.21875
Global Iter: 136100 training loss: 1.95261
Global Iter: 136100 training acc: 0.21875
Global Iter: 136200 training loss: 1.94808
Global Iter: 136200 training acc: 0.15625
Global Iter: 136300 training loss: 1.89834
Global Iter: 136300 training acc: 0.28125
Global Iter: 136400 training loss: 1.92905
Global Iter: 136400 training acc: 0.28125
Global Iter: 136500 training loss: 1.94476
Global Iter: 136500 training acc: 0.15625
Global Iter: 136600 training loss: 1.94065
Global Iter: 136600 training acc: 0.25
Global Iter: 136700 training loss: 2.18029
Global Iter: 136700 training acc: 0.1875
Global Iter: 136800 training loss: 2.08344
Global Iter: 136800 training acc: 0.15625
Global Iter: 136900 training loss: 2.03991
Global Iter: 136900 training acc: 0.1875
Global Iter: 137000 training loss: 2.0522
Global Iter: 137000 training acc: 0.15625
Global Iter: 137100 training loss: 2.06618
Global Iter: 137100 training acc: 0.15625
Global Iter: 137200 training loss: 2.08477
Global Iter: 137200 training acc: 0.15625
Global Iter: 137300 training loss: 1.93052
Global Iter: 137300 training acc: 0.25
Global Iter: 137400 training loss: 1.85019
Global Iter: 137400 training acc: 0.09375
Global Iter: 137500 training loss: 2.00545
Global Iter: 137500 training acc: 0.21875
Global Iter: 137600 training loss: 1.97481
Global Iter: 137600 training acc: 0.125
Global Iter: 137700 training loss: 1.99062
Global Iter: 137700 training acc: 0.0625
Global Iter: 137800 training loss: 1.97314
Global Iter: 137800 training acc: 0.1875
Global Iter: 137900 training loss: 1.95656
Global Iter: 137900 training acc: 0.25
Global Iter: 138000 training loss: 1.96297
Global Iter: 138000 training acc: 0.15625
Global Iter: 138100 training loss: 2.0052
Global Iter: 138100 training acc: 0.1875
Global Iter: 138200 training loss: 2.07118
Global Iter: 138200 training acc: 0.0625
Global Iter: 138300 training loss: 2.03609
Global Iter: 138300 training acc: 0.1875
Global Iter: 138400 training loss: 2.0478
Global Iter: 138400 training acc: 0.15625
Global Iter: 138500 training loss: 2.01593
Global Iter: 138500 training acc: 0.21875
Global Iter: 138600 training loss: 1.98751
Global Iter: 138600 training acc: 0.15625
Global Iter: 138700 training loss: 1.94563
Global Iter: 138700 training acc: 0.1875
Global Iter: 138800 training loss: 1.98788
Global Iter: 138800 training acc: 0.28125
Global Iter: 138900 training loss: 1.95241
Global Iter: 138900 training acc: 0.1875
Global Iter: 139000 training loss: 1.96217
Global Iter: 139000 training acc: 0.0625
Global Iter: 139100 training loss: 2.06247
Global Iter: 139100 training acc: 0.15625
Global Iter: 139200 training loss: 1.92433
Global Iter: 139200 training acc: 0.3125
Global Iter: 139300 training loss: 1.93434
Global Iter: 139300 training acc: 0.09375
Global Iter: 139400 training loss: 1.97906
Global Iter: 139400 training acc: 0.1875
Global Iter: 139500 training loss: 1.98817
Global Iter: 139500 training acc: 0.15625
Global Iter: 139600 training loss: 1.96447
Global Iter: 139600 training acc: 0.28125
Global Iter: 139700 training loss: 2.0986
Global Iter: 139700 training acc: 0.21875
Global Iter: 139800 training loss: 1.92885
Global Iter: 139800 training acc: 0.25
Global Iter: 139900 training loss: 1.96012
Global Iter: 139900 training acc: 0.28125
Global Iter: 140000 training loss: 1.92878
Global Iter: 140000 training acc: 0.28125
Global Iter: 140100 training loss: 1.93651
Global Iter: 140100 training acc: 0.1875
Global Iter: 140200 training loss: 1.90786
Global Iter: 140200 training acc: 0.3125
Global Iter: 140300 training loss: 1.936
Global Iter: 140300 training acc: 0.15625
Global Iter: 140400 training loss: 2.00668
Global Iter: 140400 training acc: 0.25
Global Iter: 140500 training loss: 1.9529
Global Iter: 140500 training acc: 0.25
Global Iter: 140600 training loss: 1.95756
Global Iter: 140600 training acc: 0.25
Global Iter: 140700 training loss: 2.00574
Global Iter: 140700 training acc: 0.25
Global Iter: 140800 training loss: 2.0707
Global Iter: 140800 training acc: 0.1875
Global Iter: 140900 training loss: 2.04979
Global Iter: 140900 training acc: 0.1875
Global Iter: 141000 training loss: 2.06637
Global Iter: 141000 training acc: 0.125
Global Iter: 141100 training loss: 1.91509
Global Iter: 141100 training acc: 0.25
Global Iter: 141200 training loss: 1.98333
Global Iter: 141200 training acc: 0.21875
Global Iter: 141300 training loss: 1.99414
Global Iter: 141300 training acc: 0.125
Global Iter: 141400 training loss: 2.00398
Global Iter: 141400 training acc: 0.125
Global Iter: 141500 training loss: 2.05892
Global Iter: 141500 training acc: 0.1875
Global Iter: 141600 training loss: 1.95478
Global Iter: 141600 training acc: 0.21875
Global Iter: 141700 training loss: 1.99527
Global Iter: 141700 training acc: 0.0625
Global Iter: 141800 training loss: 1.98816
Global Iter: 141800 training acc: 0.15625
Global Iter: 141900 training loss: 1.87399
Global Iter: 141900 training acc: 0.28125
Global Iter: 142000 training loss: 2.05004
Global Iter: 142000 training acc: 0.09375
Global Iter: 142100 training loss: 2.04724
Global Iter: 142100 training acc: 0.1875
Global Iter: 142200 training loss: 1.96483
Global Iter: 142200 training acc: 0.34375
Global Iter: 142300 training loss: 2.00129
Global Iter: 142300 training acc: 0.09375
Global Iter: 142400 training loss: 2.06883
Global Iter: 142400 training acc: 0.125
Global Iter: 142500 training loss: 1.9553
Global Iter: 142500 training acc: 0.25
Global Iter: 142600 training loss: 2.07789
Global Iter: 142600 training acc: 0.15625
Global Iter: 142700 training loss: 1.99661
Global Iter: 142700 training acc: 0.21875
Global Iter: 142800 training loss: 2.07262
Global Iter: 142800 training acc: 0.15625
Global Iter: 142900 training loss: 2.04837
Global Iter: 142900 training acc: 0.1875
Global Iter: 143000 training loss: 2.00163
Global Iter: 143000 training acc: 0.125
Global Iter: 143100 training loss: 1.98144
Global Iter: 143100 training acc: 0.25
Global Iter: 143200 training loss: 2.0847
Global Iter: 143200 training acc: 0.125
Global Iter: 143300 training loss: 2.1548
Global Iter: 143300 training acc: 0.1875
Global Iter: 143400 training loss: 1.89859
Global Iter: 143400 training acc: 0.125
Global Iter: 143500 training loss: 1.91533
Global Iter: 143500 training acc: 0.25
Global Iter: 143600 training loss: 2.0362
Global Iter: 143600 training acc: 0.125
Global Iter: 143700 training loss: 1.9233
Global Iter: 143700 training acc: 0.125
Global Iter: 143800 training loss: 1.99448
Global Iter: 143800 training acc: 0.1875
Global Iter: 143900 training loss: 1.96395
Global Iter: 143900 training acc: 0.21875
Global Iter: 144000 training loss: 2.05662
Global Iter: 144000 training acc: 0.09375
Global Iter: 144100 training loss: 1.94147
Global Iter: 144100 training acc: 0.28125
Global Iter: 144200 training loss: 2.01774
Global Iter: 144200 training acc: 0.15625
Global Iter: 144300 training loss: 1.98704
Global Iter: 144300 training acc: 0.25
Global Iter: 144400 training loss: 1.95766
Global Iter: 144400 training acc: 0.21875
Global Iter: 144500 training loss: 2.02791
Global Iter: 144500 training acc: 0.15625
Global Iter: 144600 training loss: 2.00195
Global Iter: 144600 training acc: 0.125
Global Iter: 144700 training loss: 1.99358
Global Iter: 144700 training acc: 0.09375
Global Iter: 144800 training loss: 1.98118
Global Iter: 144800 training acc: 0.21875
Global Iter: 144900 training loss: 1.93917
Global Iter: 144900 training acc: 0.21875
Global Iter: 145000 training loss: 1.9316
Global Iter: 145000 training acc: 0.21875
Global Iter: 145100 training loss: 1.97
Global Iter: 145100 training acc: 0.15625
Global Iter: 145200 training loss: 1.93494
Global Iter: 145200 training acc: 0.25
Global Iter: 145300 training loss: 2.0511
Global Iter: 145300 training acc: 0.1875
Global Iter: 145400 training loss: 1.99494
Global Iter: 145400 training acc: 0.1875
Global Iter: 145500 training loss: 2.12847
Global Iter: 145500 training acc: 0.125
Global Iter: 145600 training loss: 2.02594
Global Iter: 145600 training acc: 0.21875
Global Iter: 145700 training loss: 2.00248
Global Iter: 145700 training acc: 0.125
Global Iter: 145800 training loss: 1.9601
Global Iter: 145800 training acc: 0.15625
Global Iter: 145900 training loss: 1.95667
Global Iter: 145900 training acc: 0.15625
Global Iter: 146000 training loss: 1.97325
Global Iter: 146000 training acc: 0.25
Global Iter: 146100 training loss: 1.95381
Global Iter: 146100 training acc: 0.21875
Global Iter: 146200 training loss: 1.97608
Global Iter: 146200 training acc: 0.125
Global Iter: 146300 training loss: 2.11567
Global Iter: 146300 training acc: 0.1875
Global Iter: 146400 training loss: 1.91486
Global Iter: 146400 training acc: 0.1875
Global Iter: 146500 training loss: 1.97911
Global Iter: 146500 training acc: 0.1875
Global Iter: 146600 training loss: 1.96482
Global Iter: 146600 training acc: 0.125
Global Iter: 146700 training loss: 2.09201
Global Iter: 146700 training acc: 0.125
Global Iter: 146800 training loss: 1.90576
Global Iter: 146800 training acc: 0.3125
Global Iter: 146900 training loss: 2.05928
Global Iter: 146900 training acc: 0.21875
Global Iter: 147000 training loss: 2.08649
Global Iter: 147000 training acc: 0.21875
Global Iter: 147100 training loss: 2.01716
Global Iter: 147100 training acc: 0.21875
Global Iter: 147200 training loss: 1.92558
Global 2017-06-21 08:51:54.588617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-148417
Iter: 147200 training acc: 0.15625
Global Iter: 147300 training loss: 1.90012
Global Iter: 147300 training acc: 0.21875
Global Iter: 147400 training loss: 1.93195
Global Iter: 147400 training acc: 0.1875
Global Iter: 147500 training loss: 2.06578
Global Iter: 147500 training acc: 0.21875
Global Iter: 147600 training loss: 1.90077
Global Iter: 147600 training acc: 0.3125
Global Iter: 147700 training loss: 2.01172
Global Iter: 147700 training acc: 0.1875
Global Iter: 147800 training loss: 1.94464
Global Iter: 147800 training acc: 0.1875
Global Iter: 147900 training loss: 2.02598
Global Iter: 147900 training acc: 0.03125
Global Iter: 148000 training loss: 1.99644
Global Iter: 148000 training acc: 0.15625
Global Iter: 148100 training loss: 2.04739
Global Iter: 148100 training acc: 0.1875
Global Iter: 148200 training loss: 1.92158
Global Iter: 148200 training acc: 0.1875
Global Iter: 148300 training loss: 1.97682
Global Iter: 148300 training acc: 0.21875
Global Iter: 148400 training loss: 2.13068
Global Iter: 148400 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-148417
Number of Patches: 283632
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-148417
Global Iter: 148500 training loss: 2.007
Global Iter: 148500 training acc: 0.125
Global Iter: 148600 training loss: 1.9722
Global Iter: 148600 training acc: 0.1875
Global Iter: 148700 training loss: 1.95463
Global Iter: 148700 training acc: 0.15625
Global Iter: 148800 training loss: 1.92565
Global Iter: 148800 training acc: 0.25
Global Iter: 148900 training loss: 1.95139
Global Iter: 148900 training acc: 0.03125
Global Iter: 149000 training loss: 1.99578
Global Iter: 149000 training acc: 0.125
Global Iter: 149100 training loss: 2.02122
Global Iter: 149100 training acc: 0.1875
Global Iter: 149200 training loss: 1.97551
Global Iter: 149200 training acc: 0.21875
Global Iter: 149300 training loss: 1.94868
Global Iter: 149300 training acc: 0.25
Global Iter: 149400 training loss: 2.02716
Global Iter: 149400 training acc: 0.15625
Global Iter: 149500 training loss: 2.03037
Global Iter: 149500 training acc: 0.1875
Global Iter: 149600 training loss: 1.96148
Global Iter: 149600 training acc: 0.21875
Global Iter: 149700 training loss: 1.92689
Global Iter: 149700 training acc: 0.3125
Global Iter: 149800 training loss: 2.0182
Global Iter: 149800 training acc: 0.0625
Global Iter: 149900 training loss: 1.88297
Global Iter: 149900 training acc: 0.3125
Global Iter: 150000 training loss: 2.04712
Global Iter: 150000 training acc: 0.09375
Global Iter: 150100 training loss: 1.98187
Global Iter: 150100 training acc: 0.1875
Global Iter: 150200 training loss: 2.14325
Global Iter: 150200 training acc: 0.21875
Global Iter: 150300 training loss: 1.93566
Global Iter: 150300 training acc: 0.15625
Global Iter: 150400 training loss: 1.89122
Global Iter: 150400 training acc: 0.25
Global Iter: 150500 training loss: 2.07833
Global Iter: 150500 training acc: 0.15625
Global Iter: 150600 training loss: 2.08352
Global Iter: 150600 training acc: 0.28125
Global Iter: 150700 training loss: 2.08187
Global Iter: 150700 training acc: 0.1875
Global Iter: 150800 training loss: 2.1142
Global Iter: 150800 training acc: 0.125
Global Iter: 150900 training loss: 1.93547
Global Iter: 150900 training acc: 0.25
Global Iter: 151000 training loss: 1.94725
Global Iter: 151000 training acc: 0.21875
Global Iter: 151100 training loss: 1.96745
Global Iter: 151100 training acc: 0.15625
Global Iter: 151200 training loss: 1.98804
Global Iter: 151200 training acc: 0.09375
Global Iter: 151300 training loss: 2.07948
Global Iter: 151300 training acc: 0.15625
Global Iter: 151400 training loss: 2.02518
Global Iter: 151400 training acc: 0.21875
Global Iter: 151500 training loss: 2.01685
Global Iter: 151500 training acc: 0.1875
Global Iter: 151600 training loss: 1.96845
Global Iter: 151600 training acc: 0.15625
Global Iter: 151700 training loss: 1.9453
Global Iter: 151700 training acc: 0.1875
Global Iter: 151800 training loss: 2.00145
Global Iter: 151800 training acc: 0.125
Global Iter: 151900 training loss: 2.07263
Global Iter: 151900 training acc: 0.15625
Global Iter: 152000 training loss: 1.9591
Global Iter: 152000 training acc: 0.15625
Global Iter: 152100 training loss: 2.09414
Global Iter: 152100 training acc: 0.1875
Global Iter: 152200 training loss: 1.92665
Global Iter: 152200 training acc: 0.28125
Global Iter: 152300 training loss: 1.95251
Global Iter: 152300 training acc: 0.21875
Global Iter: 152400 training loss: 2.00013
Global Iter: 152400 training acc: 0.09375
Global Iter: 152500 training loss: 2.00661
Global Iter: 152500 training acc: 0.09375
Global Iter: 152600 training loss: 2.00147
Global Iter: 152600 training acc: 0.09375
Global Iter: 152700 training loss: 1.94053
Global Iter: 152700 training acc: 0.21875
Global Iter: 152800 training loss: 2.0021
Global Iter: 152800 training acc: 0.25
Global Iter: 152900 training loss: 2.09649
Global Iter: 152900 training acc: 0.21875
Global Iter: 153000 training loss: 1.9167
Global Iter: 153000 training acc: 0.1875
Global Iter: 153100 training loss: 1.92198
Global Iter: 153100 training acc: 0.25
Global Iter: 153200 training loss: 2.04086
Global Iter: 153200 training acc: 0.15625
Global Iter: 153300 training loss: 1.97068
Global Iter: 153300 training acc: 0.25
Global Iter: 153400 training loss: 2.10154
Global Iter: 153400 training acc: 0.125
Global Iter: 153500 training loss: 1.97074
Global Iter: 153500 training acc: 0.28125
Global Iter: 153600 training loss: 1.99641
Global Iter: 153600 training acc: 0.15625
Global Iter: 153700 training loss: 2.05029
Global Iter: 153700 training acc: 0.09375
Global Iter: 153800 training loss: 2.16771
Global Iter: 153800 training acc: 0.15625
Global Iter: 153900 training loss: 2.01123
Global Iter: 153900 training acc: 0.1875
Global Iter: 154000 training loss: 1.93905
Global Iter: 154000 training acc: 0.25
Global Iter: 154100 training loss: 1.95615
Global Iter: 154100 training acc: 0.21875
Global Iter: 154200 training loss: 1.92074
Global Iter: 154200 training acc: 0.1875
Global Iter: 154300 training loss: 1.92619
Global Iter: 154300 training acc: 0.15625
Global Iter: 154400 training loss: 2.12412
Global Iter: 154400 training acc: 0.03125
Global Iter: 154500 training loss: 2.02932
Global Iter: 154500 training acc: 0.15625
Global Iter: 154600 training loss: 2.00917
Global Iter: 154600 training acc: 0.28125
Global Iter: 154700 training loss: 1.9427
Global Iter: 154700 training acc: 0.25
Global Iter: 154800 training loss: 1.9357
Global Iter: 154800 training acc: 0.1875
Global Iter: 154900 training loss: 1.99946
Global Iter: 154900 training acc: 0.1875
Global Iter: 155000 training loss: 1.95307
Global Iter: 155000 training acc: 0.25
Global Iter: 155100 training loss: 2.02052
Global Iter: 155100 training acc: 0.25
Global Iter: 155200 training loss: 2.13168
Global Iter: 155200 training acc: 0.15625
Global Iter: 155300 training loss: 2.01028
Global Iter: 155300 training acc: 0.21875
Global Iter: 155400 training loss: 1.97309
Global Iter: 155400 training acc: 0.1875
Global Iter: 155500 training loss: 1.98986
Global Iter: 155500 training acc: 0.1875
Global Iter: 155600 training loss: 2.05845
Global Iter: 155600 training acc: 0.09375
Global Iter: 155700 training loss: 1.99696
Global Iter: 155700 training acc: 0.15625
Global Iter: 155800 training loss: 1.96259
Global Iter: 155800 training acc: 0.15625
Global Iter: 155900 training loss: 2.05849
Global Iter: 155900 training acc: 0.15625
Global Iter: 156000 training loss: 1.94285
Global Iter: 156000 training acc: 0.28125
Global Iter: 156100 training loss: 1.96524
Global Iter: 156100 training acc: 0.28125
Global Iter: 156200 training loss: 1.99417
Global Iter: 156200 training acc: 0.1875
Global Iter: 156300 training loss: 1.9771
Global Iter: 156300 training acc: 0.25
Global Iter: 156400 training loss: 1.93525
Global Iter: 156400 training acc: 0.21875
Global Iter: 156500 training loss: 1.95253
Global Iter: 156500 training acc: 0.1875
Global Iter: 156600 training loss: 1.98848
Global Iter: 156600 training acc: 0.1875
Global Iter: 156700 training loss: 1.89545
Global Iter: 156700 training acc: 0.21875
Global Iter: 156800 training loss: 1.90887
Global Iter: 156800 training acc: 0.1875
Global Iter: 156900 training loss: 2.01803
Global Iter: 156900 training acc: 0.25
Global Iter: 157000 training loss: 1.92205
Global Iter: 157000 training acc: 0.1875
Global Iter: 157100 training loss: 2.01445
Global Iter: 157100 training acc: 0.21875
Global Iter: 157200 training loss: 1.966
Global Iter: 157200 training acc: 0.125
Global Iter: 157300 training loss: 2.07838
Global Iter: 157300 training acc: 0.09375
Global Iter: 157400 training loss: 1.99557
Global Iter: 157400 training acc: 0.1875
Global Iter: 157500 training loss: 1.86104
Global Iter: 157500 training acc: 0.21875
Global Iter: 157600 training loss: 1.8771
Global Iter: 157600 training acc: 0.28125
Global Iter: 157700 training loss: 2.18958
Global Iter: 157700 training acc: 0.21875
Global Iter: 157800 training loss: 2.08601
Global Iter: 157800 training acc: 0.15625
Global Iter: 157900 training loss: 2.00834
Global Iter: 157900 training acc: 0.21875
Global Iter: 158000 training loss: 2.07445
Global Iter: 158000 training acc: 0.125
Global Iter: 158100 training loss: 2.09288
Global Iter: 158100 training acc: 0.15625
Global Iter: 158200 training loss: 1.98832
Global Iter: 158200 training acc: 0.09375
Global Iter: 158300 training loss: 2.043
Global Iter: 158300 training acc: 0.15625
Global Iter: 158400 training loss: 2.08212
Global Iter: 158400 training acc: 0.21875
Global Iter: 158500 training loss: 1.95435
Global Iter: 158500 training acc: 0.25
Global Iter: 158600 training loss: 1.88312
Global Iter: 158600 training acc: 0.28125
Global Iter: 158700 training loss: 2.03319
Global Iter: 158700 training acc: 0.1875
Global Iter: 158800 training loss: 1.9864
Global Iter: 158800 training acc: 0.3125
Global Iter: 158900 training loss: 2.03753
Global Iter: 158900 training acc: 0.25
Global Iter: 159000 training loss: 1.91911
Global Iter: 159000 training acc: 0.21875
Global Iter: 159100 training loss: 1.92701
Global Iter: 159100 training acc: 0.15625
Global Iter: 159200 training loss: 2.0878
Global Iter: 159200 training acc: 0.09375
Global Iter: 159300 training loss: 2.00777
Global Iter: 159300 training acc: 0.125
Global Iter: 159400 training loss: 2.08105
Global Iter: 159400 training acc: 0.28125
Global Iter: 159500 training loss: 1.96604
Global Iter: 159500 training acc: 0.1875
Global Iter: 159600 training loss: 1.93624
Global Iter: 159600 training acc: 0.09375
Global Iter: 159700 training loss: 2.08581
Global Iter: 159700 training acc: 0.125
Global Iter: 159800 training loss: 1.97535
Global Iter: 159800 training acc: 0.1875
Global Iter: 159900 training loss: 2.06902
Global Iter: 159900 training acc: 0.15625
Global Iter: 160000 training loss: 1.84175
Global Iter: 160000 training acc: 0.21875
Global Iter: 160100 training loss: 2.05763
Global Iter: 160100 training acc: 0.15625
Global Iter: 160200 training loss: 2.05935
Global Iter: 160200 training acc: 0.0625
Global Iter: 160300 training loss: 2.08077
Global Iter: 160300 training acc: 0.21875
Global Iter: 160400 training loss: 2.06597
Global Iter: 160400 training acc: 0.1875
Global Iter: 160500 training loss: 1.98744
Global Iter: 160500 training acc: 0.09375
Global Iter: 160600 training loss: 1.88794
Global Iter: 160600 training acc: 0.28125
Global Iter: 160700 training loss: 1.96226
Global Iter: 160700 training acc: 0.125
Global Iter: 160800 training loss: 1.98342
Global Iter: 160800 training acc: 0.15625
Global Iter: 160900 training loss: 1.96101
Global Iter: 160900 training acc: 0.25
Global Iter: 161000 training loss: 1.91554
Global Iter: 161000 training acc: 0.25
Global Iter: 161100 training loss: 1.88429
Global Iter: 161100 training acc: 0.21875
Global Iter: 161200 training loss: 2.05752
Global Iter: 161200 training acc: 0.0625
Global Iter: 161300 training loss: 1.97755
Global Iter: 161300 training acc: 0.25
Global Iter: 161400 training loss: 1.99535
Global Iter: 161400 training acc: 0.15625
Global Iter: 161500 training loss: 2.06199
Global Iter: 161500 training acc: 0.1875
Global 2017-06-21 09:22:59.856632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-166144
Iter: 161600 training loss: 2.05088
Global Iter: 161600 training acc: 0.1875
Global Iter: 161700 training loss: 1.96336
Global Iter: 161700 training acc: 0.25
Global Iter: 161800 training loss: 2.02128
Global Iter: 161800 training acc: 0.125
Global Iter: 161900 training loss: 2.01879
Global Iter: 161900 training acc: 0.125
Global Iter: 162000 training loss: 1.98069
Global Iter: 162000 training acc: 0.25
Global Iter: 162100 training loss: 1.98285
Global Iter: 162100 training acc: 0.25
Global Iter: 162200 training loss: 2.07681
Global Iter: 162200 training acc: 0.125
Global Iter: 162300 training loss: 2.03828
Global Iter: 162300 training acc: 0.1875
Global Iter: 162400 training loss: 2.00553
Global Iter: 162400 training acc: 0.1875
Global Iter: 162500 training loss: 2.02972
Global Iter: 162500 training acc: 0.1875
Global Iter: 162600 training loss: 2.11002
Global Iter: 162600 training acc: 0.15625
Global Iter: 162700 training loss: 2.09592
Global Iter: 162700 training acc: 0.0625
Global Iter: 162800 training loss: 2.13423
Global Iter: 162800 training acc: 0.125
Global Iter: 162900 training loss: 2.19152
Global Iter: 162900 training acc: 0.09375
Global Iter: 163000 training loss: 1.96915
Global Iter: 163000 training acc: 0.15625
Global Iter: 163100 training loss: 1.93361
Global Iter: 163100 training acc: 0.125
Global Iter: 163200 training loss: 1.94776
Global Iter: 163200 training acc: 0.125
Global Iter: 163300 training loss: 1.93581
Global Iter: 163300 training acc: 0.28125
Global Iter: 163400 training loss: 2.0185
Global Iter: 163400 training acc: 0.25
Global Iter: 163500 training loss: 2.00218
Global Iter: 163500 training acc: 0.15625
Global Iter: 163600 training loss: 1.9199
Global Iter: 163600 training acc: 0.28125
Global Iter: 163700 training loss: 1.95714
Global Iter: 163700 training acc: 0.21875
Global Iter: 163800 training loss: 1.95644
Global Iter: 163800 training acc: 0.21875
Global Iter: 163900 training loss: 1.89757
Global Iter: 163900 training acc: 0.3125
Global Iter: 164000 training loss: 2.02302
Global Iter: 164000 training acc: 0.09375
Global Iter: 164100 training loss: 1.91226
Global Iter: 164100 training acc: 0.1875
Global Iter: 164200 training loss: 2.00771
Global Iter: 164200 training acc: 0.15625
Global Iter: 164300 training loss: 2.05107
Global Iter: 164300 training acc: 0.1875
Global Iter: 164400 training loss: 1.93971
Global Iter: 164400 training acc: 0.25
Global Iter: 164500 training loss: 2.02123
Global Iter: 164500 training acc: 0.21875
Global Iter: 164600 training loss: 2.03746
Global Iter: 164600 training acc: 0.1875
Global Iter: 164700 training loss: 1.96938
Global Iter: 164700 training acc: 0.1875
Global Iter: 164800 training loss: 2.01729
Global Iter: 164800 training acc: 0.21875
Global Iter: 164900 training loss: 1.96122
Global Iter: 164900 training acc: 0.21875
Global Iter: 165000 training loss: 1.93594
Global Iter: 165000 training acc: 0.3125
Global Iter: 165100 training loss: 1.90898
Global Iter: 165100 training acc: 0.125
Global Iter: 165200 training loss: 1.94888
Global Iter: 165200 training acc: 0.15625
Global Iter: 165300 training loss: 1.95321
Global Iter: 165300 training acc: 0.21875
Global Iter: 165400 training loss: 1.9118
Global Iter: 165400 training acc: 0.25
Global Iter: 165500 training loss: 1.92474
Global Iter: 165500 training acc: 0.1875
Global Iter: 165600 training loss: 1.95578
Global Iter: 165600 training acc: 0.21875
Global Iter: 165700 training loss: 1.92069
Global Iter: 165700 training acc: 0.1875
Global Iter: 165800 training loss: 2.03648
Global Iter: 165800 training acc: 0.15625
Global Iter: 165900 training loss: 2.0288
Global Iter: 165900 training acc: 0.15625
Global Iter: 166000 training loss: 1.90252
Global Iter: 166000 training acc: 0.15625
Global Iter: 166100 training loss: 2.05613
Global Iter: 166100 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-166144
Number of Patches: 280796
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-166144
Global Iter: 166200 training loss: 1.93337
Global Iter: 166200 training acc: 0.21875
Global Iter: 166300 training loss: 2.11858
Global Iter: 166300 training acc: 0.15625
Global Iter: 166400 training loss: 1.92881
Global Iter: 166400 training acc: 0.21875
Global Iter: 166500 training loss: 2.08963
Global Iter: 166500 training acc: 0.09375
Global Iter: 166600 training loss: 1.91977
Global Iter: 166600 training acc: 0.09375
Global Iter: 166700 training loss: 1.93596
Global Iter: 166700 training acc: 0.25
Global Iter: 166800 training loss: 1.96687
Global Iter: 166800 training acc: 0.125
Global Iter: 166900 training loss: 1.99682
Global Iter: 166900 training acc: 0.09375
Global Iter: 167000 training loss: 1.9399
Global Iter: 167000 training acc: 0.25
Global Iter: 167100 training loss: 2.07147
Global Iter: 167100 training acc: 0.21875
Global Iter: 167200 training loss: 2.17604
Global Iter: 167200 training acc: 0.15625
Global Iter: 167300 training loss: 1.96246
Global Iter: 167300 training acc: 0.21875
Global Iter: 167400 training loss: 1.95266
Global Iter: 167400 training acc: 0.09375
Global Iter: 167500 training loss: 2.05449
Global Iter: 167500 training acc: 0.0625
Global Iter: 167600 training loss: 1.97029
Global Iter: 167600 training acc: 0.09375
Global Iter: 167700 training loss: 1.88198
Global Iter: 167700 training acc: 0.375
Global Iter: 167800 training loss: 1.96696
Global Iter: 167800 training acc: 0.3125
Global Iter: 167900 training loss: 2.12577
Global Iter: 167900 training acc: 0.1875
Global Iter: 168000 training loss: 1.97014
Global Iter: 168000 training acc: 0.125
Global Iter: 168100 training loss: 1.98637
Global Iter: 168100 training acc: 0.1875
Global Iter: 168200 training loss: 1.9729
Global Iter: 168200 training acc: 0.15625
Global Iter: 168300 training loss: 1.89554
Global Iter: 168300 training acc: 0.1875
Global Iter: 168400 training loss: 1.92164
Global Iter: 168400 training acc: 0.25
Global Iter: 168500 training loss: 1.98183
Global Iter: 168500 training acc: 0.1875
Global Iter: 168600 training loss: 2.01062
Global Iter: 168600 training acc: 0.21875
Global Iter: 168700 training loss: 2.09797
Global Iter: 168700 training acc: 0.1875
Global Iter: 168800 training loss: 1.91073
Global Iter: 168800 training acc: 0.1875
Global Iter: 168900 training loss: 2.03729
Global Iter: 168900 training acc: 0.09375
Global Iter: 169000 training loss: 1.89754
Global Iter: 169000 training acc: 0.28125
Global Iter: 169100 training loss: 1.93434
Global Iter: 169100 training acc: 0.15625
Global Iter: 169200 training loss: 2.114
Global Iter: 169200 training acc: 0.21875
Global Iter: 169300 training loss: 1.97398
Global Iter: 169300 training acc: 0.28125
Global Iter: 169400 training loss: 1.98187
Global Iter: 169400 training acc: 0.15625
Global Iter: 169500 training loss: 1.99148
Global Iter: 169500 training acc: 0.15625
Global Iter: 169600 training loss: 2.11112
Global Iter: 169600 training acc: 0.0625
Global Iter: 169700 training loss: 2.05743
Global Iter: 169700 training acc: 0.125
Global Iter: 169800 training loss: 1.95747
Global Iter: 169800 training acc: 0.1875
Global Iter: 169900 training loss: 2.06357
Global Iter: 169900 training acc: 0.25
Global Iter: 170000 training loss: 1.99098
Global Iter: 170000 training acc: 0.25
Global Iter: 170100 training loss: 1.95536
Global Iter: 170100 training acc: 0.25
Global Iter: 170200 training loss: 2.01363
Global Iter: 170200 training acc: 0.15625
Global Iter: 170300 training loss: 1.98298
Global Iter: 170300 training acc: 0.25
Global Iter: 170400 training loss: 2.1207
Global Iter: 170400 training acc: 0.09375
Global Iter: 170500 training loss: 1.88946
Global Iter: 170500 training acc: 0.3125
Global Iter: 170600 training loss: 1.99374
Global Iter: 170600 training acc: 0.21875
Global Iter: 170700 training loss: 1.875
Global Iter: 170700 training acc: 0.21875
Global Iter: 170800 training loss: 2.0935
Global Iter: 170800 training acc: 0.125
Global Iter: 170900 training loss: 1.97012
Global Iter: 170900 training acc: 0.28125
Global Iter: 171000 training loss: 1.95981
Global Iter: 171000 training acc: 0.28125
Global Iter: 171100 training loss: 1.99134
Global Iter: 171100 training acc: 0.09375
Global Iter: 171200 training loss: 1.94302
Global Iter: 171200 training acc: 0.15625
Global Iter: 171300 training loss: 1.94764
Global Iter: 171300 training acc: 0.1875
Global Iter: 171400 training loss: 2.0027
Global Iter: 171400 training acc: 0.0625
Global Iter: 171500 training loss: 1.89577
Global Iter: 171500 training acc: 0.15625
Global Iter: 171600 training loss: 2.11917
Global Iter: 171600 training acc: 0.1875
Global Iter: 171700 training loss: 1.9516
Global Iter: 171700 training acc: 0.28125
Global Iter: 171800 training loss: 2.00795
Global Iter: 171800 training acc: 0.1875
Global Iter: 171900 training loss: 2.11066
Global Iter: 171900 training acc: 0.125
Global Iter: 172000 training loss: 2.04613
Global Iter: 172000 training acc: 0.25
Global Iter: 172100 training loss: 2.00887
Global Iter: 172100 training acc: 0.09375
Global Iter: 172200 training loss: 1.93075
Global Iter: 172200 training acc: 0.21875
Global Iter: 172300 training loss: 1.96075
Global Iter: 172300 training acc: 0.125
Global Iter: 172400 training loss: 1.98775
Global Iter: 172400 training acc: 0.25
Global Iter: 172500 training loss: 1.92959
Global Iter: 172500 training acc: 0.21875
Global Iter: 172600 training loss: 1.97419
Global Iter: 172600 training acc: 0.28125
Global Iter: 172700 training loss: 1.97362
Global Iter: 172700 training acc: 0.1875
Global Iter: 172800 training loss: 2.05179
Global Iter: 172800 training acc: 0.25
Global Iter: 172900 training loss: 1.95394
Global Iter: 172900 training acc: 0.21875
Global Iter: 173000 training loss: 2.07813
Global Iter: 173000 training acc: 0.125
Global Iter: 173100 training loss: 1.92344
Global Iter: 173100 training acc: 0.25
Global Iter: 173200 training loss: 2.03932
Global Iter: 173200 training acc: 0.1875
Global Iter: 173300 training loss: 2.07247
Global Iter: 173300 training acc: 0.09375
Global Iter: 173400 training loss: 1.9592
Global Iter: 173400 training acc: 0.25
Global Iter: 173500 training loss: 1.9423
Global Iter: 173500 training acc: 0.21875
Global Iter: 173600 training loss: 2.16053
Global Iter: 173600 training acc: 0.15625
Global Iter: 173700 training loss: 1.92706
Global Iter: 173700 training acc: 0.3125
Global Iter: 173800 training loss: 1.97745
Global Iter: 173800 training acc: 0.21875
Global Iter: 173900 training loss: 1.9373
Global Iter: 173900 training acc: 0.21875
Global Iter: 174000 training loss: 1.89361
Global Iter: 174000 training acc: 0.15625
Global Iter: 174100 training loss: 2.0171
Global Iter: 174100 training acc: 0.28125
Global Iter: 174200 training loss: 2.02226
Global Iter: 174200 training acc: 0.15625
Global Iter: 174300 training loss: 1.91394
Global Iter: 174300 training acc: 0.21875
Global Iter: 174400 training loss: 1.94669
Global Iter: 174400 training acc: 0.1875
Global Iter: 174500 training loss: 1.87452
Global Iter: 174500 training acc: 0.3125
Global Iter: 174600 training loss: 2.03648
Global Iter: 174600 training acc: 0.15625
Global Iter: 174700 training loss: 1.90302
Global Iter: 174700 training acc: 0.25
Global Iter: 174800 training loss: 1.91284
Global Iter: 174800 training acc: 0.15625
Global Iter: 174900 training loss: 1.95505
Global Iter: 174900 training acc: 0.28125
Global Iter: 175000 training loss: 1.88621
Global Iter: 175000 training acc: 0.21875
Global Iter: 175100 training loss: 2.01457
Global Iter: 175100 training acc: 0.1875
Global Iter: 175200 training loss: 1.98841
Global Iter: 175200 training acc: 0.09375
Global Iter: 175300 training loss: 2.05241
Global Iter: 175300 training acc: 0.21875
Global Iter: 175400 training loss: 2.00514
Global Iter: 175400 training acc: 0.1875
Global Iter: 175500 training loss: 2.0572
Global Iter: 175500 training acc: 0.25
Global Iter: 175600 training loss: 2.147
Global Iter: 175600 training acc: 0.0625
Global Iter: 175700 training loss: 1.92207
Global Iter: 175700 training acc: 0.34375
Global Iter: 175800 training loss: 2.09774
Global Iter: 175800 training acc: 0.15625
Global Iter: 175900 training loss: 1.88202
Global Iter: 175900 training acc: 0.375
Global Iter: 176000 training loss: 1.95021
Global Iter: 176000 training acc: 0.3125
Global Iter: 176100 training loss: 1.98307
Global Iter: 176100 training acc: 0.21875
Global Iter: 176200 training loss: 1.92498
Global Iter: 176200 training acc: 0.15625
Global Iter: 176300 training loss: 1.90485
Global Iter: 176300 training acc: 0.1875
Global Iter: 176400 training loss: 1.88025
Global Iter: 176400 training acc: 0.25
Global Iter: 176500 training loss: 1.91567
Global Iter: 176500 training acc: 0.1875
Global Iter: 176600 training loss: 2.01127
Global Iter: 176600 training acc: 0.25
Global Iter: 176700 training loss: 1.96736
Global Iter: 176700 training acc: 0.09375
Global Iter: 176800 training loss: 1.91775
Global Iter: 176800 training acc: 0.3125
Global Iter: 176900 training loss: 2.04732
Global Iter: 176900 training acc: 0.28125
Global Iter: 177000 training loss: 1.99668
Global Iter: 177000 training acc: 0.125
Global Iter: 177100 training loss: 2.0015
Global Iter: 177100 training acc: 0.125
Global Iter: 177200 training loss: 1.93289
Global Iter: 177200 training acc: 0.1875
Global Iter: 177300 training loss: 1.88542
Global Iter: 177300 training acc: 0.34375
Global Iter: 177400 training loss: 2.1585
Global Iter: 177400 training acc: 0.15625
Global Iter: 177500 training loss: 1.97265
Global Iter: 177500 training acc: 0.25
Global Iter: 177600 training loss: 2.01138
Global Iter: 177600 training acc: 0.125
Global Iter: 177700 training loss: 1.95248
Global Iter: 177700 training acc: 0.28125
Global Iter: 177800 training loss: 2.04174
Global Iter: 177800 training acc: 0.15625
Global Iter: 177900 training loss: 1.91705
Global Iter: 177900 training acc: 0.15625
Global Iter: 178000 training loss: 2.05836
Global Iter: 178000 training acc: 0.15625
Global Iter: 178100 training loss: 2.04119
Global Iter: 178100 training acc: 0.1875
Global Iter: 178200 training loss: 2.02061
Global Iter: 178200 training acc: 0.1875
Global Iter: 178300 training loss: 1.90936
Global Iter: 178300 training acc: 0.25
Global Iter: 178400 training loss: 1.9826
Global Iter: 178400 training acc: 0.25
Global Iter: 178500 training loss: 1.95962
Global Iter: 178500 training acc: 0.25
Global Iter: 178600 training loss: 2.01134
Global Iter: 178600 training acc: 0.21875
Global Iter: 178700 training loss: 1.96301
Global Iter: 178700 training acc: 0.3125
Global Iter: 178800 training loss: 1.967
Global Iter: 178800 training acc: 0.21875
Global Iter: 178900 training loss: 1.88329
Global Iter: 178900 training acc: 0.25
Global Iter: 179000 training loss: 1.97275
Global Iter: 179000 training acc: 0.1875
Global Iter: 179100 training loss: 1.99295
Global Iter: 179100 training acc: 0.3125
Global Iter: 179200 training loss: 1.99494
Global Iter: 179200 training acc: 0.15625
Global Iter: 179300 training loss: 2.00385
Global Iter: 179300 training acc: 0.21875
Global Iter: 179400 training loss: 1.9832
Global Iter: 179400 training acc: 0.25
Global Iter: 179500 training loss: 1.9739
Global Iter: 179500 training acc: 0.21875
Global Iter: 179600 training loss: 2.03808
Global Iter: 179600 training acc: 0.09375
Global Iter: 179700 training loss: 2.08417
Global Iter: 179700 training acc: 0.125
Global Iter: 179800 training loss: 2.04454
Global Iter: 179800 training acc: 0.1875
Global Iter: 179900 training loss: 2.00224
Global Iter: 179900 training acc: 0.15625
Global Iter: 180000 training loss: 2.07734
Global Iter: 180000 training acc: 0.09375
Global Iter: 180100 training loss: 1.94528
Global Iter: 180100 training acc: 0.09375
Global Iter: 180200 training loss: 2.07888
Global Iter: 180200 training acc: 0.25
Global Iter: 180300 training loss: 1.98102
Global Iter: 180300 training acc: 0.1875
Global Iter: 180400 training loss: 2.03925
Global Iter: 180400 training acc: 0.28125
Global Iter: 180500 training loss: 1.93578
Global Iter: 180500 training acc: 0.21875
Global Iter: 180600 training loss: 1.9921
Global Iter: 180600 training acc: 0.15625
Global Iter: 180700 training loss: 2.18439
Global Iter: 180700 training acc: 0.125
Global Iter: 180800 training loss: 1.99443
Global Iter:2017-06-21 09:54:37.656656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-183694
 180800 training acc: 0.25
Global Iter: 180900 training loss: 1.92016
Global Iter: 180900 training acc: 0.15625
Global Iter: 181000 training loss: 1.98133
Global Iter: 181000 training acc: 0.28125
Global Iter: 181100 training loss: 1.96263
Global Iter: 181100 training acc: 0.1875
Global Iter: 181200 training loss: 1.87582
Global Iter: 181200 training acc: 0.28125
Global Iter: 181300 training loss: 1.92459
Global Iter: 181300 training acc: 0.25
Global Iter: 181400 training loss: 1.98628
Global Iter: 181400 training acc: 0.125
Global Iter: 181500 training loss: 1.9415
Global Iter: 181500 training acc: 0.125
Global Iter: 181600 training loss: 1.99026
Global Iter: 181600 training acc: 0.21875
Global Iter: 181700 training loss: 2.0571
Global Iter: 181700 training acc: 0.1875
Global Iter: 181800 training loss: 2.01489
Global Iter: 181800 training acc: 0.0625
Global Iter: 181900 training loss: 2.00158
Global Iter: 181900 training acc: 0.15625
Global Iter: 182000 training loss: 2.18645
Global Iter: 182000 training acc: 0.15625
Global Iter: 182100 training loss: 1.91307
Global Iter: 182100 training acc: 0.40625
Global Iter: 182200 training loss: 2.08082
Global Iter: 182200 training acc: 0.25
Global Iter: 182300 training loss: 1.92075
Global Iter: 182300 training acc: 0.25
Global Iter: 182400 training loss: 1.97601
Global Iter: 182400 training acc: 0.15625
Global Iter: 182500 training loss: 1.9814
Global Iter: 182500 training acc: 0.21875
Global Iter: 182600 training loss: 1.87294
Global Iter: 182600 training acc: 0.15625
Global Iter: 182700 training loss: 1.98768
Global Iter: 182700 training acc: 0.09375
Global Iter: 182800 training loss: 2.08159
Global Iter: 182800 training acc: 0.125
Global Iter: 182900 training loss: 1.93261
Global Iter: 182900 training acc: 0.1875
Global Iter: 183000 training loss: 2.01281
Global Iter: 183000 training acc: 0.34375
Global Iter: 183100 training loss: 1.93524
Global Iter: 183100 training acc: 0.1875
Global Iter: 183200 training loss: 1.94506
Global Iter: 183200 training acc: 0.34375
Global Iter: 183300 training loss: 1.94849
Global Iter: 183300 training acc: 0.34375
Global Iter: 183400 training loss: 2.06586
Global Iter: 183400 training acc: 0.21875
Global Iter: 183500 training loss: 1.97682
Global Iter: 183500 training acc: 0.15625
Global Iter: 183600 training loss: 2.03879
Global Iter: 183600 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-183694
Number of Patches: 277989
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-183694
Global Iter: 183700 training loss: 2.03357
Global Iter: 183700 training acc: 0.21875
Global Iter: 183800 training loss: 2.01521
Global Iter: 183800 training acc: 0.0625
Global Iter: 183900 training loss: 1.94013
Global Iter: 183900 training acc: 0.21875
Global Iter: 184000 training loss: 2.08621
Global Iter: 184000 training acc: 0.1875
Global Iter: 184100 training loss: 2.03955
Global Iter: 184100 training acc: 0.125
Global Iter: 184200 training loss: 2.01536
Global Iter: 184200 training acc: 0.09375
Global Iter: 184300 training loss: 1.95188
Global Iter: 184300 training acc: 0.1875
Global Iter: 184400 training loss: 1.96506
Global Iter: 184400 training acc: 0.28125
Global Iter: 184500 training loss: 1.99457
Global Iter: 184500 training acc: 0.09375
Global Iter: 184600 training loss: 1.89846
Global Iter: 184600 training acc: 0.1875
Global Iter: 184700 training loss: 1.8823
Global Iter: 184700 training acc: 0.3125
Global Iter: 184800 training loss: 1.9745
Global Iter: 184800 training acc: 0.21875
Global Iter: 184900 training loss: 2.01602
Global Iter: 184900 training acc: 0.125
Global Iter: 185000 training loss: 2.00991
Global Iter: 185000 training acc: 0.125
Global Iter: 185100 training loss: 1.97642
Global Iter: 185100 training acc: 0.15625
Global Iter: 185200 training loss: 2.00818
Global Iter: 185200 training acc: 0.25
Global Iter: 185300 training loss: 1.97243
Global Iter: 185300 training acc: 0.28125
Global Iter: 185400 training loss: 2.07163
Global Iter: 185400 training acc: 0.09375
Global Iter: 185500 training loss: 1.92874
Global Iter: 185500 training acc: 0.1875
Global Iter: 185600 training loss: 1.98422
Global Iter: 185600 training acc: 0.25
Global Iter: 185700 training loss: 2.09627
Global Iter: 185700 training acc: 0.1875
Global Iter: 185800 training loss: 1.99852
Global Iter: 185800 training acc: 0.09375
Global Iter: 185900 training loss: 2.09907
Global Iter: 185900 training acc: 0.125
Global Iter: 186000 training loss: 2.02378
Global Iter: 186000 training acc: 0.1875
Global Iter: 186100 training loss: 1.99447
Global Iter: 186100 training acc: 0.15625
Global Iter: 186200 training loss: 1.92439
Global Iter: 186200 training acc: 0.28125
Global Iter: 186300 training loss: 2.05711
Global Iter: 186300 training acc: 0.1875
Global Iter: 186400 training loss: 2.11723
Global Iter: 186400 training acc: 0.09375
Global Iter: 186500 training loss: 2.00495
Global Iter: 186500 training acc: 0.1875
Global Iter: 186600 training loss: 1.92172
Global Iter: 186600 training acc: 0.15625
Global Iter: 186700 training loss: 2.07407
Global Iter: 186700 training acc: 0.125
Global Iter: 186800 training loss: 2.06181
Global Iter: 186800 training acc: 0.03125
Global Iter: 186900 training loss: 2.08075
Global Iter: 186900 training acc: 0.1875
Global Iter: 187000 training loss: 2.01461
Global Iter: 187000 training acc: 0.15625
Global Iter: 187100 training loss: 1.94257
Global Iter: 187100 training acc: 0.15625
Global Iter: 187200 training loss: 2.05675
Global Iter: 187200 training acc: 0.0625
Global Iter: 187300 training loss: 1.92021
Global Iter: 187300 training acc: 0.21875
Global Iter: 187400 training loss: 1.91053
Global Iter: 187400 training acc: 0.21875
Global Iter: 187500 training loss: 1.96899
Global Iter: 187500 training acc: 0.1875
Global Iter: 187600 training loss: 1.97495
Global Iter: 187600 training acc: 0.1875
Global Iter: 187700 training loss: 1.94382
Global Iter: 187700 training acc: 0.25
Global Iter: 187800 training loss: 2.01463
Global Iter: 187800 training acc: 0.21875
Global Iter: 187900 training loss: 1.90116
Global Iter: 187900 training acc: 0.1875
Global Iter: 188000 training loss: 1.98523
Global Iter: 188000 training acc: 0.1875
Global Iter: 188100 training loss: 2.13961
Global Iter: 188100 training acc: 0.125
Global Iter: 188200 training loss: 1.92994
Global Iter: 188200 training acc: 0.21875
Global Iter: 188300 training loss: 2.02063
Global Iter: 188300 training acc: 0.15625
Global Iter: 188400 training loss: 1.96277
Global Iter: 188400 training acc: 0.1875
Global Iter: 188500 training loss: 2.01766
Global Iter: 188500 training acc: 0.1875
Global Iter: 188600 training loss: 2.07985
Global Iter: 188600 training acc: 0.15625
Global Iter: 188700 training loss: 1.95101
Global Iter: 188700 training acc: 0.21875
Global Iter: 188800 training loss: 2.06763
Global Iter: 188800 training acc: 0.28125
Global Iter: 188900 training loss: 1.90834
Global Iter: 188900 training acc: 0.21875
Global Iter: 189000 training loss: 1.95073
Global Iter: 189000 training acc: 0.15625
Global Iter: 189100 training loss: 1.90915
Global Iter: 189100 training acc: 0.28125
Global Iter: 189200 training loss: 1.92558
Global Iter: 189200 training acc: 0.25
Global Iter: 189300 training loss: 1.90075
Global Iter: 189300 training acc: 0.25
Global Iter: 189400 training loss: 1.99009
Global Iter: 189400 training acc: 0.125
Global Iter: 189500 training loss: 2.01964
Global Iter: 189500 training acc: 0.15625
Global Iter: 189600 training loss: 2.02927
Global Iter: 189600 training acc: 0.15625
Global Iter: 189700 training loss: 1.96364
Global Iter: 189700 training acc: 0.15625
Global Iter: 189800 training loss: 1.9714
Global Iter: 189800 training acc: 0.1875
Global Iter: 189900 training loss: 1.93043
Global Iter: 189900 training acc: 0.125
Global Iter: 190000 training loss: 2.01341
Global Iter: 190000 training acc: 0.15625
Global Iter: 190100 training loss: 1.92161
Global Iter: 190100 training acc: 0.3125
Global Iter: 190200 training loss: 2.00984
Global Iter: 190200 training acc: 0.3125
Global Iter: 190300 training loss: 2.01301
Global Iter: 190300 training acc: 0.09375
Global Iter: 190400 training loss: 2.00519
Global Iter: 190400 training acc: 0.1875
Global Iter: 190500 training loss: 1.99491
Global Iter: 190500 training acc: 0.21875
Global Iter: 190600 training loss: 1.91404
Global Iter: 190600 training acc: 0.25
Global Iter: 190700 training loss: 1.99221
Global Iter: 190700 training acc: 0.21875
Global Iter: 190800 training loss: 1.9558
Global Iter: 190800 training acc: 0.125
Global Iter: 190900 training loss: 1.86568
Global Iter: 190900 training acc: 0.40625
Global Iter: 191000 training loss: 1.88874
Global Iter: 191000 training acc: 0.375
Global Iter: 191100 training loss: 1.90324
Global Iter: 191100 training acc: 0.3125
Global Iter: 191200 training loss: 1.97922
Global Iter: 191200 training acc: 0.125
Global Iter: 191300 training loss: 2.00026
Global Iter: 191300 training acc: 0.1875
Global Iter: 191400 training loss: 2.01457
Global Iter: 191400 training acc: 0.1875
Global Iter: 191500 training loss: 2.11527
Global Iter: 191500 training acc: 0.1875
Global Iter: 191600 training loss: 2.00981
Global Iter: 191600 training acc: 0.21875
Global Iter: 191700 training loss: 2.02403
Global Iter: 191700 training acc: 0.25
Global Iter: 191800 training loss: 1.89888
Global Iter: 191800 training acc: 0.09375
Global Iter: 191900 training loss: 1.87015
Global Iter: 191900 training acc: 0.375
Global Iter: 192000 training loss: 1.95514
Global Iter: 192000 training acc: 0.15625
Global Iter: 192100 training loss: 2.03927
Global Iter: 192100 training acc: 0.21875
Global Iter: 192200 training loss: 2.0718
Global Iter: 192200 training acc: 0.125
Global Iter: 192300 training loss: 1.90514
Global Iter: 192300 training acc: 0.34375
Global Iter: 192400 training loss: 2.01859
Global Iter: 192400 training acc: 0.15625
Global Iter: 192500 training loss: 2.01727
Global Iter: 192500 training acc: 0.28125
Global Iter: 192600 training loss: 1.97962
Global Iter: 192600 training acc: 0.1875
Global Iter: 192700 training loss: 1.94391
Global Iter: 192700 training acc: 0.28125
Global Iter: 192800 training loss: 1.98183
Global Iter: 192800 training acc: 0.15625
Global Iter: 192900 training loss: 2.00365
Global Iter: 192900 training acc: 0.09375
Global Iter: 193000 training loss: 2.06147
Global Iter: 193000 training acc: 0.09375
Global Iter: 193100 training loss: 2.11493
Global Iter: 193100 training acc: 0.21875
Global Iter: 193200 training loss: 2.00615
Global Iter: 193200 training acc: 0.21875
Global Iter: 193300 training loss: 1.96882
Global Iter: 193300 training acc: 0.1875
Global Iter: 193400 training loss: 1.91281
Global Iter: 193400 training acc: 0.21875
Global Iter: 193500 training loss: 2.14158
Global Iter: 193500 training acc: 0.09375
Global Iter: 193600 training loss: 1.96395
Global Iter: 193600 training acc: 0.21875
Global Iter: 193700 training loss: 1.94721
Global Iter: 193700 training acc: 0.21875
Global Iter: 193800 training loss: 2.0712
Global Iter: 193800 training acc: 0.15625
Global Iter: 193900 training loss: 1.98274
Global Iter: 193900 training acc: 0.25
Global Iter: 194000 training loss: 1.88671
Global Iter: 194000 training acc: 0.21875
Global Iter: 194100 training loss: 2.12945
Global Iter: 194100 training acc: 0.125
Global Iter: 194200 training loss: 1.97869
Global Iter: 194200 training acc: 0.15625
Global Iter: 194300 training loss: 1.85395
Global Iter: 194300 training acc: 0.21875
Global Iter: 194400 training loss: 1.89524
Global Iter: 194400 training acc: 0.21875
Global Iter: 194500 training loss: 2.00053
Global Iter: 194500 training acc: 0.1875
Global Iter: 194600 training loss: 1.95899
Global Iter: 194600 training acc: 0.125
Global Iter: 194700 training loss: 1.94962
Global Iter: 194700 training acc: 0.25
Global Iter: 194800 training loss: 2.08527
Global Iter: 194800 training acc: 0.09375
Global Iter: 194900 training loss: 1.8566
Global Iter: 194900 training acc: 0.34375
Global Iter: 195000 training loss: 1.9818
Global Iter: 195000 training acc: 0.1875
Global Iter: 195100 training loss: 1.9332
Global Iter: 195100 training acc: 0.15625
Global Iter: 195200 training loss: 1.96338
Global Iter: 195200 training acc: 0.3125
Global Iter: 195300 training loss: 1.97522
Global Iter: 195300 training acc: 0.09375
Global Iter: 195400 training loss: 2.04617
Global Iter: 195400 training acc: 0.0625
Global Iter: 195500 training loss: 2.13668
Global Iter: 195500 training acc: 0.125
Global Iter: 195600 training loss: 2.05225
Global Iter: 195600 training acc: 0.1875
Global Iter: 195700 training loss: 1.97998
Global Iter: 195700 training acc: 0.21875
Global Iter: 195800 training loss: 2.06722
Global Iter: 195800 training acc: 0.15625
Global Iter: 195900 training loss: 2.06674
Global Iter: 195900 training acc: 0.1875
Global Iter: 196000 training loss: 1.88221
Global Iter: 196000 training acc: 0.3125
Global Iter: 196100 training loss: 2.05246
Global Iter: 196100 training acc: 0.21875
Global Iter: 196200 training loss: 2.00844
Global Iter: 196200 training acc: 0.1875
Global Iter: 196300 training loss: 1.91707
Global Iter: 196300 training acc: 0.125
Global Iter: 196400 training loss: 1.96813
Global Iter: 196400 training acc: 0.25
Global Iter: 196500 training loss: 2.01663
Global Iter: 196500 training acc: 0.25
Global Iter: 196600 training loss: 2.00691
Global Iter: 196600 training acc: 0.125
Global Iter: 196700 training loss: 2.06789
Global Iter: 196700 training acc: 0.28125
Global Iter: 196800 training loss: 1.95591
Global Iter: 196800 training acc: 0.21875
Global Iter: 196900 training loss: 1.94147
Global Iter: 196900 training acc: 0.1875
Global Iter: 197000 training loss: 1.90338
Global Iter: 197000 training acc: 0.1875
Global Iter: 197100 training loss: 2.00265
Global Iter: 197100 training acc: 0.125
Global Iter: 197200 training loss: 2.14383
Global Iter: 197200 training acc: 0.125
Global Iter: 197300 training loss: 2.13108
Global Iter: 197300 training acc: 0.0625
Global Iter: 197400 training loss: 1.95535
Global Iter: 197400 training acc: 0.1875
Global Iter: 197500 training loss: 2.00222
Global Iter: 197500 training acc: 0.21875
Global Iter: 197600 training loss: 2.00267
Global Iter: 197600 training acc: 0.15625
Global Iter: 197700 training loss: 1.93655
Global Iter: 197700 training acc: 0.1875
Global Iter: 197800 training loss: 1.89837
Global Iter: 197800 training acc: 0.3125
Global Iter: 197900 training loss: 1.87361
Global Iter: 197900 training acc: 0.34375
Global Iter: 198000 training loss: 1.88982
Global Iter: 198000 training acc: 0.21875
Global Iter: 198100 training loss: 1.9506
Global Iter: 198100 training acc: 0.21875
Global Iter: 198200 training loss: 2.01999
Global Iter: 198200 training acc: 0.125
Global Iter: 198300 training loss: 2.03027
Global Iter: 198300 training acc: 0.09375
Global Iter: 198400 training loss: 1.95814
Global Iter: 198400 training acc: 0.21875
Global Iter: 198500 training loss: 1.96837
Global Iter: 198500 training acc: 0.21875
Global Iter: 198600 training loss: 1.97261
Global Iter: 198600 training acc: 0.1875
Global Iter: 198700 training loss: 2.07577
Global Iter: 198700 training acc: 0.09375
Global Iter: 198800 training loss: 2.07559
Global Iter: 198800 training acc: 0.1875
Global Iter: 198900 training loss: 1.99908
Global Iter: 198900 training acc: 0.1875
Global Iter: 199000 training loss: 1.88628
Global Iter: 199000 training acc: 0.21875
Global Iter: 199100 training loss: 1.92188
Global Iter: 199100 training acc: 0.3125
Global Iter: 199200 training loss: 1.97647
Global Iter: 199200 training acc: 0.15625
Global Iter: 199300 training loss: 1.93291
Global Iter: 199300 training acc: 0.3125
Global Iter: 199400 training loss: 2.03405
Global Iter: 199400 training acc: 0.09375
Global Iter: 199500 training loss: 1.93783
Global Iter: 199500 training acc: 0.28125
Global Iter: 199600 training loss: 1.92038
Global Iter: 199600 training acc: 0.28125
Global Iter: 199700 training loss: 2.02292
Global Iter: 199700 training acc: 0.15625
Global Iter: 199800 training loss: 2.02222
Global Iter: 199800 training acc: 0.15625
Global Iter: 199900 training loss: 2.03699
Global Iter: 199900 training acc: 0.125
Global Iter: 200000 training loss: 1.96649
Global Iter: 202017-06-21 10:24:35.981717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-201069
0000 training acc: 0.15625
Global Iter: 200100 training loss: 2.0214
Global Iter: 200100 training acc: 0.1875
Global Iter: 200200 training loss: 2.00639
Global Iter: 200200 training acc: 0.0625
Global Iter: 200300 training loss: 1.97381
Global Iter: 200300 training acc: 0.21875
Global Iter: 200400 training loss: 2.11634
Global Iter: 200400 training acc: 0.1875
Global Iter: 200500 training loss: 1.98296
Global Iter: 200500 training acc: 0.34375
Global Iter: 200600 training loss: 1.93341
Global Iter: 200600 training acc: 0.1875
Global Iter: 200700 training loss: 2.01783
Global Iter: 200700 training acc: 0.1875
Global Iter: 200800 training loss: 1.92969
Global Iter: 200800 training acc: 0.15625
Global Iter: 200900 training loss: 1.91072
Global Iter: 200900 training acc: 0.28125
Global Iter: 201000 training loss: 1.96826
Global Iter: 201000 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-201069
Number of Patches: 275210
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-201069
Global Iter: 201100 training loss: 1.98833
Global Iter: 201100 training acc: 0.28125
Global Iter: 201200 training loss: 2.10113
Global Iter: 201200 training acc: 0.1875
Global Iter: 201300 training loss: 2.06352
Global Iter: 201300 training acc: 0.0625
Global Iter: 201400 training loss: 2.01868
Global Iter: 201400 training acc: 0.25
Global Iter: 201500 training loss: 1.97887
Global Iter: 201500 training acc: 0.125
Global Iter: 201600 training loss: 2.12249
Global Iter: 201600 training acc: 0.09375
Global Iter: 201700 training loss: 1.96545
Global Iter: 201700 training acc: 0.1875
Global Iter: 201800 training loss: 1.94396
Global Iter: 201800 training acc: 0.21875
Global Iter: 201900 training loss: 2.04239
Global Iter: 201900 training acc: 0.125
Global Iter: 202000 training loss: 1.90206
Global Iter: 202000 training acc: 0.1875
Global Iter: 202100 training loss: 2.00829
Global Iter: 202100 training acc: 0.28125
Global Iter: 202200 training loss: 1.92506
Global Iter: 202200 training acc: 0.28125
Global Iter: 202300 training loss: 2.0186
Global Iter: 202300 training acc: 0.1875
Global Iter: 202400 training loss: 2.05071
Global Iter: 202400 training acc: 0.21875
Global Iter: 202500 training loss: 1.97771
Global Iter: 202500 training acc: 0.21875
Global Iter: 202600 training loss: 1.97889
Global Iter: 202600 training acc: 0.15625
Global Iter: 202700 training loss: 2.07906
Global Iter: 202700 training acc: 0.15625
Global Iter: 202800 training loss: 1.98531
Global Iter: 202800 training acc: 0.15625
Global Iter: 202900 training loss: 2.013
Global Iter: 202900 training acc: 0.125
Global Iter: 203000 training loss: 1.95772
Global Iter: 203000 training acc: 0.0625
Global Iter: 203100 training loss: 1.98014
Global Iter: 203100 training acc: 0.15625
Global Iter: 203200 training loss: 2.01051
Global Iter: 203200 training acc: 0.15625
Global Iter: 203300 training loss: 2.06599
Global Iter: 203300 training acc: 0.15625
Global Iter: 203400 training loss: 1.97487
Global Iter: 203400 training acc: 0.1875
Global Iter: 203500 training loss: 2.05638
Global Iter: 203500 training acc: 0.09375
Global Iter: 203600 training loss: 1.99009
Global Iter: 203600 training acc: 0.21875
Global Iter: 203700 training loss: 1.93635
Global Iter: 203700 training acc: 0.125
Global Iter: 203800 training loss: 1.92016
Global Iter: 203800 training acc: 0.3125
Global Iter: 203900 training loss: 1.92407
Global Iter: 203900 training acc: 0.15625
Global Iter: 204000 training loss: 2.00639
Global Iter: 204000 training acc: 0.28125
Global Iter: 204100 training loss: 1.97534
Global Iter: 204100 training acc: 0.28125
Global Iter: 204200 training loss: 2.06446
Global Iter: 204200 training acc: 0.15625
Global Iter: 204300 training loss: 1.91781
Global Iter: 204300 training acc: 0.125
Global Iter: 204400 training loss: 1.94073
Global Iter: 204400 training acc: 0.21875
Global Iter: 204500 training loss: 2.0989
Global Iter: 204500 training acc: 0.15625
Global Iter: 204600 training loss: 2.01815
Global Iter: 204600 training acc: 0.3125
Global Iter: 204700 training loss: 2.07404
Global Iter: 204700 training acc: 0.15625
Global Iter: 204800 training loss: 1.94534
Global Iter: 204800 training acc: 0.21875
Global Iter: 204900 training loss: 1.99938
Global Iter: 204900 training acc: 0.1875
Global Iter: 205000 training loss: 1.9519
Global Iter: 205000 training acc: 0.28125
Global Iter: 205100 training loss: 2.04475
Global Iter: 205100 training acc: 0.09375
Global Iter: 205200 training loss: 1.91702
Global Iter: 205200 training acc: 0.15625
Global Iter: 205300 training loss: 1.99579
Global Iter: 205300 training acc: 0.1875
Global Iter: 205400 training loss: 2.03309
Global Iter: 205400 training acc: 0.28125
Global Iter: 205500 training loss: 2.04258
Global Iter: 205500 training acc: 0.125
Global Iter: 205600 training loss: 1.94727
Global Iter: 205600 training acc: 0.28125
Global Iter: 205700 training loss: 2.09372
Global Iter: 205700 training acc: 0.21875
Global Iter: 205800 training loss: 1.92979
Global Iter: 205800 training acc: 0.28125
Global Iter: 205900 training loss: 1.95545
Global Iter: 205900 training acc: 0.21875
Global Iter: 206000 training loss: 2.05784
Global Iter: 206000 training acc: 0.1875
Global Iter: 206100 training loss: 1.93627
Global Iter: 206100 training acc: 0.25
Global Iter: 206200 training loss: 1.96099
Global Iter: 206200 training acc: 0.25
Global Iter: 206300 training loss: 1.9125
Global Iter: 206300 training acc: 0.125
Global Iter: 206400 training loss: 2.04357
Global Iter: 206400 training acc: 0.15625
Global Iter: 206500 training loss: 1.99594
Global Iter: 206500 training acc: 0.15625
Global Iter: 206600 training loss: 1.93977
Global Iter: 206600 training acc: 0.28125
Global Iter: 206700 training loss: 1.94981
Global Iter: 206700 training acc: 0.21875
Global Iter: 206800 training loss: 2.03072
Global Iter: 206800 training acc: 0.15625
Global Iter: 206900 training loss: 2.09105
Global Iter: 206900 training acc: 0.15625
Global Iter: 207000 training loss: 1.97268
Global Iter: 207000 training acc: 0.125
Global Iter: 207100 training loss: 2.04052
Global Iter: 207100 training acc: 0.1875
Global Iter: 207200 training loss: 1.95588
Global Iter: 207200 training acc: 0.15625
Global Iter: 207300 training loss: 2.06191
Global Iter: 207300 training acc: 0.15625
Global Iter: 207400 training loss: 1.96392
Global Iter: 207400 training acc: 0.1875
Global Iter: 207500 training loss: 1.95848
Global Iter: 207500 training acc: 0.21875
Global Iter: 207600 training loss: 1.90532
Global Iter: 207600 training acc: 0.3125
Global Iter: 207700 training loss: 2.05669
Global Iter: 207700 training acc: 0.15625
Global Iter: 207800 training loss: 2.11268
Global Iter: 207800 training acc: 0.125
Global Iter: 207900 training loss: 1.92119
Global Iter: 207900 training acc: 0.1875
Global Iter: 208000 training loss: 2.04867
Global Iter: 208000 training acc: 0.21875
Global Iter: 208100 training loss: 2.1166
Global Iter: 208100 training acc: 0.15625
Global Iter: 208200 training loss: 2.04615
Global Iter: 208200 training acc: 0.15625
Global Iter: 208300 training loss: 1.89378
Global Iter: 208300 training acc: 0.21875
Global Iter: 208400 training loss: 1.94359
Global Iter: 208400 training acc: 0.1875
Global Iter: 208500 training loss: 1.88617
Global Iter: 208500 training acc: 0.28125
Global Iter: 208600 training loss: 1.9619
Global Iter: 208600 training acc: 0.28125
Global Iter: 208700 training loss: 2.07628
Global Iter: 208700 training acc: 0.21875
Global Iter: 208800 training loss: 1.9861
Global Iter: 208800 training acc: 0.09375
Global Iter: 208900 training loss: 1.9489
Global Iter: 208900 training acc: 0.15625
Global Iter: 209000 training loss: 1.97608
Global Iter: 209000 training acc: 0.125
Global Iter: 209100 training loss: 1.93239
Global Iter: 209100 training acc: 0.25
Global Iter: 209200 training loss: 2.03547
Global Iter: 209200 training acc: 0.09375
Global Iter: 209300 training loss: 1.97389
Global Iter: 209300 training acc: 0.125
Global Iter: 209400 training loss: 1.92358
Global Iter: 209400 training acc: 0.21875
Global Iter: 209500 training loss: 1.87361
Global Iter: 209500 training acc: 0.40625
Global Iter: 209600 training loss: 2.13381
Global Iter: 209600 training acc: 0.28125
Global Iter: 209700 training loss: 1.96786
Global Iter: 209700 training acc: 0.1875
Global Iter: 209800 training loss: 1.99547
Global Iter: 209800 training acc: 0.0625
Global Iter: 209900 training loss: 1.93351
Global Iter: 209900 training acc: 0.21875
Global Iter: 210000 training loss: 1.98707
Global Iter: 210000 training acc: 0.125
Global Iter: 210100 training loss: 2.00719
Global Iter: 210100 training acc: 0.15625
Global Iter: 210200 training loss: 1.98057
Global Iter: 210200 training acc: 0.25
Global Iter: 210300 training loss: 1.87457
Global Iter: 210300 training acc: 0.3125
Global Iter: 210400 training loss: 1.94513
Global Iter: 210400 training acc: 0.15625
Global Iter: 210500 training loss: 1.9998
Global Iter: 210500 training acc: 0.09375
Global Iter: 210600 training loss: 1.962
Global Iter: 210600 training acc: 0.28125
Global Iter: 210700 training loss: 1.96739
Global Iter: 210700 training acc: 0.09375
Global Iter: 210800 training loss: 1.94531
Global Iter: 210800 training acc: 0.15625
Global Iter: 210900 training loss: 2.09122
Global Iter: 210900 training acc: 0.21875
Global Iter: 211000 training loss: 2.01789
Global Iter: 211000 training acc: 0.1875
Global Iter: 211100 training loss: 1.9178
Global Iter: 211100 training acc: 0.28125
Global Iter: 211200 training loss: 1.90512
Global Iter: 211200 training acc: 0.21875
Global Iter: 211300 training loss: 1.93462
Global Iter: 211300 training acc: 0.15625
Global Iter: 211400 training loss: 2.06632
Global Iter: 211400 training acc: 0.125
Global Iter: 211500 training loss: 1.98182
Global Iter: 211500 training acc: 0.15625
Global Iter: 211600 training loss: 2.02169
Global Iter: 211600 training acc: 0.1875
Global Iter: 211700 training loss: 1.9872
Global Iter: 211700 training acc: 0.125
Global Iter: 211800 training loss: 1.91106
Global Iter: 211800 training acc: 0.25
Global Iter: 211900 training loss: 2.0327
Global Iter: 211900 training acc: 0.125
Global Iter: 212000 training loss: 1.89472
Global Iter: 212000 training acc: 0.15625
Global Iter: 212100 training loss: 1.95007
Global Iter: 212100 training acc: 0.1875
Global Iter: 212200 training loss: 2.03991
Global Iter: 212200 training acc: 0.0625
Global Iter: 212300 training loss: 1.96189
Global Iter: 212300 training acc: 0.1875
Global Iter: 212400 training loss: 2.0207
Global Iter: 212400 training acc: 0.15625
Global Iter: 212500 training loss: 2.02271
Global Iter: 212500 training acc: 0.15625
Global Iter: 212600 training loss: 1.97613
Global Iter: 212600 training acc: 0.28125
Global Iter: 212700 training loss: 1.98018
Global Iter: 212700 training acc: 0.1875
Global Iter: 212800 training loss: 2.00796
Global Iter: 212800 training acc: 0.15625
Global Iter: 212900 training loss: 2.09264
Global Iter: 212900 training acc: 0.21875
Global Iter: 213000 training loss: 2.05461
Global Iter: 213000 training acc: 0.15625
Global Iter: 213100 training loss: 1.9493
Global Iter: 213100 training acc: 0.25
Global Iter: 213200 training loss: 2.05744
Global Iter: 213200 training acc: 0.1875
Global Iter: 213300 training loss: 1.99242
Global Iter: 213300 training acc: 0.15625
Global Iter: 213400 training loss: 1.96802
Global Iter: 213400 training acc: 0.25
Global Iter: 213500 training loss: 1.96139
Global Iter: 213500 training acc: 0.15625
Global Iter: 213600 training loss: 1.99379
Global Iter: 213600 training acc: 0.21875
Global Iter: 213700 training loss: 2.05722
Global Iter: 213700 training acc: 0.21875
Global Iter: 213800 training loss: 1.93894
Global Iter: 213800 training acc: 0.1875
Global Iter: 213900 training loss: 2.03927
Global Iter: 213900 training acc: 0.09375
Global Iter: 214000 training loss: 2.07069
Global Iter: 214000 training acc: 0.125
Global Iter: 214100 training loss: 1.97779
Global Iter: 214100 training acc: 0.15625
Global Iter: 214200 training loss: 1.9185
Global Iter: 214200 training acc: 0.21875
Global Iter: 214300 training loss: 1.92868
Global Iter: 214300 training acc: 0.2017-06-21 10:54:34.036781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-218270
15625
Global Iter: 214400 training loss: 1.93824
Global Iter: 214400 training acc: 0.3125
Global Iter: 214500 training loss: 1.96074
Global Iter: 214500 training acc: 0.28125
Global Iter: 214600 training loss: 1.90176
Global Iter: 214600 training acc: 0.25
Global Iter: 214700 training loss: 1.95287
Global Iter: 214700 training acc: 0.28125
Global Iter: 214800 training loss: 2.0899
Global Iter: 214800 training acc: 0.125
Global Iter: 214900 training loss: 2.05392
Global Iter: 214900 training acc: 0.1875
Global Iter: 215000 training loss: 1.92753
Global Iter: 215000 training acc: 0.28125
Global Iter: 215100 training loss: 1.93675
Global Iter: 215100 training acc: 0.1875
Global Iter: 215200 training loss: 1.92293
Global Iter: 215200 training acc: 0.125
Global Iter: 215300 training loss: 2.11703
Global Iter: 215300 training acc: 0.09375
Global Iter: 215400 training loss: 2.0062
Global Iter: 215400 training acc: 0.1875
Global Iter: 215500 training loss: 1.93683
Global Iter: 215500 training acc: 0.25
Global Iter: 215600 training loss: 2.09626
Global Iter: 215600 training acc: 0.1875
Global Iter: 215700 training loss: 2.06736
Global Iter: 215700 training acc: 0.15625
Global Iter: 215800 training loss: 1.87895
Global Iter: 215800 training acc: 0.40625
Global Iter: 215900 training loss: 2.02268
Global Iter: 215900 training acc: 0.125
Global Iter: 216000 training loss: 2.09565
Global Iter: 216000 training acc: 0.1875
Global Iter: 216100 training loss: 2.05964
Global Iter: 216100 training acc: 0.125
Global Iter: 216200 training loss: 1.96628
Global Iter: 216200 training acc: 0.1875
Global Iter: 216300 training loss: 2.09095
Global Iter: 216300 training acc: 0.15625
Global Iter: 216400 training loss: 2.0588
Global Iter: 216400 training acc: 0.3125
Global Iter: 216500 training loss: 1.9851
Global Iter: 216500 training acc: 0.1875
Global Iter: 216600 training loss: 1.98175
Global Iter: 216600 training acc: 0.15625
Global Iter: 216700 training loss: 2.01404
Global Iter: 216700 training acc: 0.1875
Global Iter: 216800 training loss: 1.97583
Global Iter: 216800 training acc: 0.09375
Global Iter: 216900 training loss: 2.02338
Global Iter: 216900 training acc: 0.25
Global Iter: 217000 training loss: 2.0018
Global Iter: 217000 training acc: 0.1875
Global Iter: 217100 training loss: 1.98472
Global Iter: 217100 training acc: 0.15625
Global Iter: 217200 training loss: 2.1495
Global Iter: 217200 training acc: 0.09375
Global Iter: 217300 training loss: 2.06394
Global Iter: 217300 training acc: 0.09375
Global Iter: 217400 training loss: 2.04946
Global Iter: 217400 training acc: 0.15625
Global Iter: 217500 training loss: 2.00206
Global Iter: 217500 training acc: 0.125
Global Iter: 217600 training loss: 1.95232
Global Iter: 217600 training acc: 0.25
Global Iter: 217700 training loss: 1.93008
Global Iter: 217700 training acc: 0.3125
Global Iter: 217800 training loss: 1.96624
Global Iter: 217800 training acc: 0.15625
Global Iter: 217900 training loss: 2.02052
Global Iter: 217900 training acc: 0.15625
Global Iter: 218000 training loss: 1.95801
Global Iter: 218000 training acc: 0.15625
Global Iter: 218100 training loss: 1.95696
Global Iter: 218100 training acc: 0.125
Global Iter: 218200 training loss: 1.98646
Global Iter: 218200 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-218270
Number of Patches: 272458
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-218270
Global Iter: 218300 training loss: 1.96967
Global Iter: 218300 training acc: 0.375
Global Iter: 218400 training loss: 1.92891
Global Iter: 218400 training acc: 0.09375
Global Iter: 218500 training loss: 2.0245
Global Iter: 218500 training acc: 0.21875
Global Iter: 218600 training loss: 1.94157
Global Iter: 218600 training acc: 0.15625
Global Iter: 218700 training loss: 2.02737
Global Iter: 218700 training acc: 0.125
Global Iter: 218800 training loss: 2.15343
Global Iter: 218800 training acc: 0.09375
Global Iter: 218900 training loss: 1.91458
Global Iter: 218900 training acc: 0.125
Global Iter: 219000 training loss: 1.93929
Global Iter: 219000 training acc: 0.21875
Global Iter: 219100 training loss: 1.97782
Global Iter: 219100 training acc: 0.25
Global Iter: 219200 training loss: 1.95799
Global Iter: 219200 training acc: 0.1875
Global Iter: 219300 training loss: 1.97587
Global Iter: 219300 training acc: 0.21875
Global Iter: 219400 training loss: 2.1347
Global Iter: 219400 training acc: 0.09375
Global Iter: 219500 training loss: 1.94319
Global Iter: 219500 training acc: 0.1875
Global Iter: 219600 training loss: 2.01473
Global Iter: 219600 training acc: 0.1875
Global Iter: 219700 training loss: 2.08535
Global Iter: 219700 training acc: 0.25
Global Iter: 219800 training loss: 2.03267
Global Iter: 219800 training acc: 0.21875
Global Iter: 219900 training loss: 1.93068
Global Iter: 219900 training acc: 0.15625
Global Iter: 220000 training loss: 1.88583
Global Iter: 220000 training acc: 0.21875
Global Iter: 220100 training loss: 2.02198
Global Iter: 220100 training acc: 0.125
Global Iter: 220200 training loss: 2.02139
Global Iter: 220200 training acc: 0.21875
Global Iter: 220300 training loss: 1.91742
Global Iter: 220300 training acc: 0.25
Global Iter: 220400 training loss: 2.04448
Global Iter: 220400 training acc: 0.125
Global Iter: 220500 training loss: 1.93186
Global Iter: 220500 training acc: 0.15625
Global Iter: 220600 training loss: 2.00427
Global Iter: 220600 training acc: 0.15625
Global Iter: 220700 training loss: 2.00218
Global Iter: 220700 training acc: 0.15625
Global Iter: 220800 training loss: 2.10703
Global Iter: 220800 training acc: 0.1875
Global Iter: 220900 training loss: 2.02199
Global Iter: 220900 training acc: 0.21875
Global Iter: 221000 training loss: 1.97846
Global Iter: 221000 training acc: 0.0625
Global Iter: 221100 training loss: 1.95803
Global Iter: 221100 training acc: 0.15625
Global Iter: 221200 training loss: 2.01996
Global Iter: 221200 training acc: 0.25
Global Iter: 221300 training loss: 2.02916
Global Iter: 221300 training acc: 0.125
Global Iter: 221400 training loss: 1.96221
Global Iter: 221400 training acc: 0.21875
Global Iter: 221500 training loss: 1.9238
Global Iter: 221500 training acc: 0.3125
Global Iter: 221600 training loss: 2.02191
Global Iter: 221600 training acc: 0.21875
Global Iter: 221700 training loss: 1.97684
Global Iter: 221700 training acc: 0.1875
Global Iter: 221800 training loss: 2.0792
Global Iter: 221800 training acc: 0.125
Global Iter: 221900 training loss: 1.90958
Global Iter: 221900 training acc: 0.3125
Global Iter: 222000 training loss: 1.99885
Global Iter: 222000 training acc: 0.125
Global Iter: 222100 training loss: 1.93621
Global Iter: 222100 training acc: 0.25
Global Iter: 222200 training loss: 2.01395
Global Iter: 222200 training acc: 0.1875
Global Iter: 222300 training loss: 2.03131
Global Iter: 222300 training acc: 0.15625
Global Iter: 222400 training loss: 1.92707
Global Iter: 222400 training acc: 0.28125
Global Iter: 222500 training loss: 1.98739
Global Iter: 222500 training acc: 0.09375
Global Iter: 222600 training loss: 1.90649
Global Iter: 222600 training acc: 0.21875
Global Iter: 222700 training loss: 1.89035
Global Iter: 222700 training acc: 0.15625
Global Iter: 222800 training loss: 1.97647
Global Iter: 222800 training acc: 0.1875
Global Iter: 222900 training loss: 1.96083
Global Iter: 222900 training acc: 0.125
Global Iter: 223000 training loss: 2.15202
Global Iter: 223000 training acc: 0.15625
Global Iter: 223100 training loss: 1.88714
Global Iter: 223100 training acc: 0.125
Global Iter: 223200 training loss: 1.99069
Global Iter: 223200 training acc: 0.1875
Global Iter: 223300 training loss: 1.93162
Global Iter: 223300 training acc: 0.1875
Global Iter: 223400 training loss: 2.10241
Global Iter: 223400 training acc: 0.09375
Global Iter: 223500 training loss: 1.952
Global Iter: 223500 training acc: 0.21875
Global Iter: 223600 training loss: 1.91859
Global Iter: 223600 training acc: 0.34375
Global Iter: 223700 training loss: 1.9681
Global Iter: 223700 training acc: 0.09375
Global Iter: 223800 training loss: 1.9748
Global Iter: 223800 training acc: 0.1875
Global Iter: 223900 training loss: 1.9735
Global Iter: 223900 training acc: 0.3125
Global Iter: 224000 training loss: 1.97551
Global Iter: 224000 training acc: 0.09375
Global Iter: 224100 training loss: 1.82429
Global Iter: 224100 training acc: 0.3125
Global Iter: 224200 training loss: 2.00896
Global Iter: 224200 training acc: 0.1875
Global Iter: 224300 training loss: 2.08593
Global Iter: 224300 training acc: 0.15625
Global Iter: 224400 training loss: 1.97969
Global Iter: 224400 training acc: 0.25
Global Iter: 224500 training loss: 1.96193
Global Iter: 224500 training acc: 0.15625
Global Iter: 224600 training loss: 2.01647
Global Iter: 224600 training acc: 0.1875
Global Iter: 224700 training loss: 1.92165
Global Iter: 224700 training acc: 0.25
Global Iter: 224800 training loss: 1.92107
Global Iter: 224800 training acc: 0.3125
Global Iter: 224900 training loss: 1.88841
Global Iter: 224900 training acc: 0.21875
Global Iter: 225000 training loss: 1.92034
Global Iter: 225000 training acc: 0.25
Global Iter: 225100 training loss: 2.03098
Global Iter: 225100 training acc: 0.15625
Global Iter: 225200 training loss: 2.00033
Global Iter: 225200 training acc: 0.09375
Global Iter: 225300 training loss: 1.86395
Global Iter: 225300 training acc: 0.125
Global Iter: 225400 training loss: 1.99268
Global Iter: 225400 training acc: 0.15625
Global Iter: 225500 training loss: 1.98525
Global Iter: 225500 training acc: 0.15625
Global Iter: 225600 training loss: 2.11149
Global Iter: 225600 training acc: 0.03125
Global Iter: 225700 training loss: 2.10245
Global Iter: 225700 training acc: 0.0625
Global Iter: 225800 training loss: 1.9505
Global Iter: 225800 training acc: 0.1875
Global Iter: 225900 training loss: 2.07557
Global Iter: 225900 training acc: 0.28125
Global Iter: 226000 training loss: 1.95355
Global Iter: 226000 training acc: 0.25
Global Iter: 226100 training loss: 1.95318
Global Iter: 226100 training acc: 0.1875
Global Iter: 226200 training loss: 2.0583
Global Iter: 226200 training acc: 0.15625
Global Iter: 226300 training loss: 2.04419
Global Iter: 226300 training acc: 0.21875
Global Iter: 226400 training loss: 1.92202
Global Iter: 226400 training acc: 0.25
Global Iter: 226500 training loss: 1.99242
Global Iter: 226500 training acc: 0.1875
Global Iter: 226600 training loss: 2.04205
Global Iter: 226600 training acc: 0.125
Global Iter: 226700 training loss: 1.99759
Global Iter: 226700 training acc: 0.28125
Global Iter: 226800 training loss: 1.98564
Global Iter: 226800 training acc: 0.1875
Global Iter: 226900 training loss: 2.05266
Global Iter: 226900 training acc: 0.15625
Global Iter: 227000 training loss: 2.07196
Global Iter: 227000 training acc: 0.1875
Global Iter: 227100 training loss: 1.91399
Global Iter: 227100 training acc: 0.25
Global Iter: 227200 training loss: 1.9393
Global Iter: 227200 training acc: 0.28125
Global Iter: 227300 training loss: 2.07522
Global Iter: 227300 training acc: 0.21875
Global Iter: 227400 training loss: 1.90731
Global Iter: 227400 training acc: 0.34375
Global Iter: 227500 training loss: 2.00061
Global Iter: 227500 training acc: 0.25
Global Iter: 227600 training loss: 1.9482
Global Iter: 227600 training acc: 0.21875
Global Iter: 227700 training loss: 1.90037
Global Iter: 227700 training acc: 0.1875
Global Iter: 227800 training loss: 1.96424
Global Iter: 227800 training acc: 0.1875
Global Iter: 227900 training loss: 1.91302
Global Iter: 227900 training acc: 0.21875
Global Iter: 228000 training loss: 2.00345
Global Iter: 228000 training acc: 0.1875
Global Iter: 228100 training loss: 2.08613
Global Iter: 228100 training acc: 0.1875
Global Iter: 228200 training loss: 2.08777
Global Iter: 228200 training acc: 0.21875
Global Iter: 228300 training loss: 1.94537
Global Iter: 228300 training acc: 0.25
Global Iter: 228400 training loss: 2.04077
Global Iter: 228400 training acc: 0.15625
Global Iter: 228500 training loss: 2.00716
Global Iter: 228500 training acc: 0.125
Global Iter: 228600 training loss: 1.99183
Global Iter: 228600 training acc: 0.1875
Global Iter: 228700 training loss: 2.04373
Global Iter: 228700 training acc: 0.1875
Global Iter: 228800 training loss: 2.04945
Global Iter: 228800 training acc: 0.0625
Global Iter: 228900 training loss: 1.98648
Global Iter: 228900 training acc: 0.21875
Global Iter: 229000 training loss: 1.99678
Global Iter: 229000 training acc: 0.125
Global Iter: 229100 training loss: 1.98362
Global Iter: 229100 training acc: 0.1875
Global Iter: 229200 training loss: 1.99935
Global Iter: 229200 training acc: 0.34375
Global Iter: 229300 training loss: 2.0208
Global Iter: 229300 training acc: 0.21875
Global Iter: 229400 training loss: 1.94966
Global Iter: 229400 training acc: 0.3125
Global Iter: 229500 training loss: 1.94412
Global Iter: 229500 training acc: 0.09375
Global Iter: 229600 training loss: 2.09473
Global Iter: 229600 training acc: 0.125
Global Iter: 229700 training loss: 1.92243
Global Iter: 229700 training acc: 0.25
Global Iter: 229800 training loss: 1.98383
Global Iter: 229800 training acc: 0.21875
Global Iter: 229900 training loss: 1.93267
Global Iter: 229900 training acc: 0.21875
Global Iter: 230000 training loss: 1.98705
Global Iter: 230000 training acc: 0.1875
Global Iter: 230100 training loss: 2.05291
Global Iter: 230100 training acc: 0.1875
Global Iter: 230200 training loss: 1.92606
Global Iter: 230200 training acc: 0.15625
Global Iter: 230300 training loss: 1.94621
Global Iter: 230300 training acc: 0.15625
Global Iter: 230400 training loss: 1.86738
Global Iter: 230400 training acc: 0.1875
Global Iter: 230500 training loss: 1.99329
Global Iter: 230500 training acc: 0.125
Global Iter: 230600 training loss: 2.02633
Global Iter: 230600 training acc: 0.09375
Global Iter: 230700 training loss: 1.9656
Global Iter: 230700 training acc: 0.1875
Global Iter: 230800 training loss: 1.94296
Global Iter: 230800 training acc: 0.125
Global Iter: 230900 training loss: 2.01385
Global Iter: 230900 training acc: 0.21875
Global Iter: 231000 training loss: 1.97316
Global Iter: 231000 training acc: 0.34375
Global Iter: 231100 training loss: 1.95088
Global Iter: 231100 training acc: 0.15625
Global Iter: 231200 training loss: 1.98148
Global Iter: 231200 training acc: 0.21875
Global Iter: 231300 training loss: 1.90845
Global Iter: 231300 training acc: 0.25
Global Iter: 231400 training loss: 1.96302
Global Iter: 231400 training acc: 0.125
Global Iter: 231500 training loss: 1.94935
Global Iter: 231500 training acc: 0.1875
Global Iter: 231600 training loss: 2.07202
Global Iter: 231600 training acc: 0.03125
Global Iter: 231700 training loss: 1.96998
Global Iter: 231700 training acc: 0.1875
Global Iter: 231800 training loss: 1.95331
Global Iter: 231800 training acc: 0.21875
Global Iter: 231900 training loss: 2.07851
Global Iter: 231900 training acc: 0.25
Global Iter: 232000 training loss: 1.92443
Global Iter: 232000 training acc: 0.28125
Global Iter: 232100 training loss: 2.14667
Global Iter: 232100 training acc: 0.1875
Global Iter: 232200 training loss: 1.9874
Global Iter: 232200 training acc: 0.1875
Global Iter: 232300 training loss: 2.08645
Global Iter: 232300 training acc: 0.1875
Global Iter: 232400 training loss: 1.96017
Global Iter: 232400 training acc: 0.125
Global Iter: 232500 training loss: 1.99174
Global Iter: 232500 training acc: 0.15625
Global Iter: 232600 training loss: 1.98866
Global Iter: 232600 training acc: 0.09375
Global Iter: 232700 training loss: 2.07164
Global Iter: 232700 training acc: 0.25
Global Iter: 232800 training loss: 1.93433
Global Iter: 232800 training acc: 0.125
Global Iter: 232900 training loss: 2.04199
Global Iter: 232900 training acc: 0.15625
Global Iter: 233000 training loss: 1.99527
Global Iter: 233000 training acc: 0.125
Global Iter: 233100 training loss: 1.92828
Global Iter: 233100 training acc: 0.21875
Global Iter: 233200 training loss: 1.94651
Global Iter: 233200 training acc: 0.125
Global Iter: 233300 training loss: 1.93965
Global Iter: 233300 training acc: 0.0625
Global Iter: 233400 training loss: 1.99668
Global Iter: 233400 training acc: 0.0625
Global Iter: 233500 training loss: 1.90346
Global Iter: 233500 training acc: 0.21875
Global Iter: 233600 training loss:2017-06-21 11:24:57.538164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-235299
 1.93965
Global Iter: 233600 training acc: 0.3125
Global Iter: 233700 training loss: 2.01691
Global Iter: 233700 training acc: 0.1875
Global Iter: 233800 training loss: 1.9034
Global Iter: 233800 training acc: 0.1875
Global Iter: 233900 training loss: 1.94546
Global Iter: 233900 training acc: 0.25
Global Iter: 234000 training loss: 1.87462
Global Iter: 234000 training acc: 0.3125
Global Iter: 234100 training loss: 1.99504
Global Iter: 234100 training acc: 0.21875
Global Iter: 234200 training loss: 1.97865
Global Iter: 234200 training acc: 0.09375
Global Iter: 234300 training loss: 1.99766
Global Iter: 234300 training acc: 0.21875
Global Iter: 234400 training loss: 2.02098
Global Iter: 234400 training acc: 0.09375
Global Iter: 234500 training loss: 1.946
Global Iter: 234500 training acc: 0.15625
Global Iter: 234600 training loss: 1.93737
Global Iter: 234600 training acc: 0.21875
Global Iter: 234700 training loss: 1.90652
Global Iter: 234700 training acc: 0.25
Global Iter: 234800 training loss: 1.92769
Global Iter: 234800 training acc: 0.1875
Global Iter: 234900 training loss: 2.00793
Global Iter: 234900 training acc: 0.21875
Global Iter: 235000 training loss: 1.9822
Global Iter: 235000 training acc: 0.1875
Global Iter: 235100 training loss: 1.90994
Global Iter: 235100 training acc: 0.1875
Global Iter: 235200 training loss: 1.96512
Global Iter: 235200 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-235299
Number of Patches: 269734
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-235299
Global Iter: 235300 training loss: 2.10572
Global Iter: 235300 training acc: 0.125
Global Iter: 235400 training loss: 2.01105
Global Iter: 235400 training acc: 0.09375
Global Iter: 235500 training loss: 1.95432
Global Iter: 235500 training acc: 0.28125
Global Iter: 235600 training loss: 2.00713
Global Iter: 235600 training acc: 0.1875
Global Iter: 235700 training loss: 1.98099
Global Iter: 235700 training acc: 0.125
Global Iter: 235800 training loss: 1.92997
Global Iter: 235800 training acc: 0.28125
Global Iter: 235900 training loss: 1.94927
Global Iter: 235900 training acc: 0.1875
Global Iter: 236000 training loss: 1.894
Global Iter: 236000 training acc: 0.1875
Global Iter: 236100 training loss: 1.9882
Global Iter: 236100 training acc: 0.3125
Global Iter: 236200 training loss: 1.9989
Global Iter: 236200 training acc: 0.21875
Global Iter: 236300 training loss: 1.98432
Global Iter: 236300 training acc: 0.15625
Global Iter: 236400 training loss: 1.97327
Global Iter: 236400 training acc: 0.0625
Global Iter: 236500 training loss: 2.10602
Global Iter: 236500 training acc: 0.125
Global Iter: 236600 training loss: 2.02701
Global Iter: 236600 training acc: 0.21875
Global Iter: 236700 training loss: 2.04541
Global Iter: 236700 training acc: 0.25
Global Iter: 236800 training loss: 2.03025
Global Iter: 236800 training acc: 0.125
Global Iter: 236900 training loss: 1.92428
Global Iter: 236900 training acc: 0.28125
Global Iter: 237000 training loss: 2.0389
Global Iter: 237000 training acc: 0.09375
Global Iter: 237100 training loss: 1.99193
Global Iter: 237100 training acc: 0.15625
Global Iter: 237200 training loss: 1.92144
Global Iter: 237200 training acc: 0.15625
Global Iter: 237300 training loss: 2.02231
Global Iter: 237300 training acc: 0.09375
Global Iter: 237400 training loss: 2.05018
Global Iter: 237400 training acc: 0.15625
Global Iter: 237500 training loss: 1.98998
Global Iter: 237500 training acc: 0.25
Global Iter: 237600 training loss: 2.04236
Global Iter: 237600 training acc: 0.1875
Global Iter: 237700 training loss: 1.95216
Global Iter: 237700 training acc: 0.1875
Global Iter: 237800 training loss: 1.94031
Global Iter: 237800 training acc: 0.15625
Global Iter: 237900 training loss: 2.12403
Global Iter: 237900 training acc: 0.15625
Global Iter: 238000 training loss: 1.92697
Global Iter: 238000 training acc: 0.1875
Global Iter: 238100 training loss: 1.96175
Global Iter: 238100 training acc: 0.28125
Global Iter: 238200 training loss: 1.89704
Global Iter: 238200 training acc: 0.21875
Global Iter: 238300 training loss: 1.97349
Global Iter: 238300 training acc: 0.09375
Global Iter: 238400 training loss: 2.01534
Global Iter: 238400 training acc: 0.1875
Global Iter: 238500 training loss: 2.01787
Global Iter: 238500 training acc: 0.1875
Global Iter: 238600 training loss: 1.92213
Global Iter: 238600 training acc: 0.1875
Global Iter: 238700 training loss: 1.8912
Global Iter: 238700 training acc: 0.21875
Global Iter: 238800 training loss: 1.89789
Global Iter: 238800 training acc: 0.3125
Global Iter: 238900 training loss: 1.90646
Global Iter: 238900 training acc: 0.21875
Global Iter: 239000 training loss: 1.92379
Global Iter: 239000 training acc: 0.1875
Global Iter: 239100 training loss: 1.93208
Global Iter: 239100 training acc: 0.15625
Global Iter: 239200 training loss: 2.05914
Global Iter: 239200 training acc: 0.125
Global Iter: 239300 training loss: 1.89209
Global Iter: 239300 training acc: 0.28125
Global Iter: 239400 training loss: 1.98398
Global Iter: 239400 training acc: 0.1875
Global Iter: 239500 training loss: 1.98069
Global Iter: 239500 training acc: 0.21875
Global Iter: 239600 training loss: 1.91026
Global Iter: 239600 training acc: 0.28125
Global Iter: 239700 training loss: 2.04284
Global Iter: 239700 training acc: 0.1875
Global Iter: 239800 training loss: 2.02671
Global Iter: 239800 training acc: 0.125
Global Iter: 239900 training loss: 1.93404
Global Iter: 239900 training acc: 0.1875
Global Iter: 240000 training loss: 2.02232
Global Iter: 240000 training acc: 0.21875
Global Iter: 240100 training loss: 1.95866
Global Iter: 240100 training acc: 0.1875
Global Iter: 240200 training loss: 1.91054
Global Iter: 240200 training acc: 0.28125
Global Iter: 240300 training loss: 1.94907
Global Iter: 240300 training acc: 0.25
Global Iter: 240400 training loss: 2.05058
Global Iter: 240400 training acc: 0.15625
Global Iter: 240500 training loss: 1.96081
Global Iter: 240500 training acc: 0.1875
Global Iter: 240600 training loss: 2.03322
Global Iter: 240600 training acc: 0.28125
Global Iter: 240700 training loss: 1.97666
Global Iter: 240700 training acc: 0.125
Global Iter: 240800 training loss: 1.98817
Global Iter: 240800 training acc: 0.21875
Global Iter: 240900 training loss: 1.89769
Global Iter: 240900 training acc: 0.3125
Global Iter: 241000 training loss: 2.04326
Global Iter: 241000 training acc: 0.15625
Global Iter: 241100 training loss: 1.99512
Global Iter: 241100 training acc: 0.21875
Global Iter: 241200 training loss: 2.0245
Global Iter: 241200 training acc: 0.1875
Global Iter: 241300 training loss: 2.06465
Global Iter: 241300 training acc: 0.1875
Global Iter: 241400 training loss: 1.97399
Global Iter: 241400 training acc: 0.15625
Global Iter: 241500 training loss: 2.08003
Global Iter: 241500 training acc: 0.1875
Global Iter: 241600 training loss: 2.1254
Global Iter: 241600 training acc: 0.1875
Global Iter: 241700 training loss: 1.98182
Global Iter: 241700 training acc: 0.09375
Global Iter: 241800 training loss: 1.93744
Global Iter: 241800 training acc: 0.1875
Global Iter: 241900 training loss: 2.02051
Global Iter: 241900 training acc: 0.34375
Global Iter: 242000 training loss: 2.01852
Global Iter: 242000 training acc: 0.15625
Global Iter: 242100 training loss: 1.9044
Global Iter: 242100 training acc: 0.1875
Global Iter: 242200 training loss: 2.01346
Global Iter: 242200 training acc: 0.09375
Global Iter: 242300 training loss: 2.13573
Global Iter: 242300 training acc: 0.09375
Global Iter: 242400 training loss: 1.86255
Global Iter: 242400 training acc: 0.3125
Global Iter: 242500 training loss: 1.99355
Global Iter: 242500 training acc: 0.1875
Global Iter: 242600 training loss: 2.04528
Global Iter: 242600 training acc: 0.15625
Global Iter: 242700 training loss: 2.0729
Global Iter: 242700 training acc: 0.125
Global Iter: 242800 training loss: 1.99929
Global Iter: 242800 training acc: 0.125
Global Iter: 242900 training loss: 1.99096
Global Iter: 242900 training acc: 0.15625
Global Iter: 243000 training loss: 1.89912
Global Iter: 243000 training acc: 0.28125
Global Iter: 243100 training loss: 2.06753
Global Iter: 243100 training acc: 0.15625
Global Iter: 243200 training loss: 1.93664
Global Iter: 243200 training acc: 0.1875
Global Iter: 243300 training loss: 1.96751
Global Iter: 243300 training acc: 0.1875
Global Iter: 243400 training loss: 2.04358
Global Iter: 243400 training acc: 0.15625
Global Iter: 243500 training loss: 2.01079
Global Iter: 243500 training acc: 0.1875
Global Iter: 243600 training loss: 1.97773
Global Iter: 243600 training acc: 0.15625
Global Iter: 243700 training loss: 2.04978
Global Iter: 243700 training acc: 0.1875
Global Iter: 243800 training loss: 1.99843
Global Iter: 243800 training acc: 0.21875
Global Iter: 243900 training loss: 1.92552
Global Iter: 243900 training acc: 0.28125
Global Iter: 244000 training loss: 2.06401
Global Iter: 244000 training acc: 0.21875
Global Iter: 244100 training loss: 1.89025
Global Iter: 244100 training acc: 0.34375
Global Iter: 244200 training loss: 1.92758
Global Iter: 244200 training acc: 0.1875
Global Iter: 244300 training loss: 1.92803
Global Iter: 244300 training acc: 0.15625
Global Iter: 244400 training loss: 1.99506
Global Iter: 244400 training acc: 0.15625
Global Iter: 244500 training loss: 1.98877
Global Iter: 244500 training acc: 0.3125
Global Iter: 244600 training loss: 1.97826
Global Iter: 244600 training acc: 0.125
Global Iter: 244700 training loss: 1.88204
Global Iter: 244700 training acc: 0.375
Global Iter: 244800 training loss: 1.92445
Global Iter: 244800 training acc: 0.28125
Global Iter: 244900 training loss: 2.03815
Global Iter: 244900 training acc: 0.125
Global Iter: 245000 training loss: 1.96539
Global Iter: 245000 training acc: 0.28125
Global Iter: 245100 training loss: 1.98703
Global Iter: 245100 training acc: 0.125
Global Iter: 245200 training loss: 2.04095
Global Iter: 245200 training acc: 0.1875
Global Iter: 245300 training loss: 1.90821
Global Iter: 245300 training acc: 0.15625
Global Iter: 245400 training loss: 2.00339
Global Iter: 245400 training acc: 0.21875
Global Iter: 245500 training loss: 1.95443
Global Iter: 245500 training acc: 0.1875
Global Iter: 245600 training loss: 1.95245
Global Iter: 245600 training acc: 0.21875
Global Iter: 245700 training loss: 1.92908
Global Iter: 245700 training acc: 0.25
Global Iter: 245800 training loss: 2.01601
Global Iter: 245800 training acc: 0.1875
Global Iter: 245900 training loss: 1.92253
Global Iter: 245900 training acc: 0.25
Global Iter: 246000 training loss: 2.00665
Global Iter: 246000 training acc: 0.25
Global Iter: 246100 training loss: 1.94255
Global Iter: 246100 training acc: 0.21875
Global Iter: 246200 training loss: 2.03332
Global Iter: 246200 training acc: 0.1875
Global Iter: 246300 training loss: 1.98902
Global Iter: 246300 training acc: 0.15625
Global Iter: 246400 training loss: 2.02588
Global Iter: 246400 training acc: 0.0625
Global Iter: 246500 training loss: 2.04573
Global Iter: 246500 training acc: 0.15625
Global Iter: 246600 training loss: 1.97263
Global Iter: 246600 training acc: 0.09375
Global Iter: 246700 training loss: 2.06068
Global Iter: 246700 training acc: 0.125
Global Iter: 246800 training loss: 2.03084
Global Iter: 246800 training acc: 0.3125
Global Iter: 246900 training loss: 1.89496
Global Iter: 246900 training acc: 0.125
Global Iter: 247000 training loss: 1.95635
Global Iter: 247000 training acc: 0.125
Global Iter: 247100 training loss: 2.11581
Global Iter: 247100 training acc: 0.28125
Global Iter: 247200 training loss: 2.0567
Global Iter: 247200 training acc: 0.21875
Global Iter: 247300 training loss: 2.04622
Global Iter: 247300 training acc: 0.125
Global Iter: 247400 training loss: 1.95563
Global Iter: 247400 training acc: 0.25
Global Iter: 247500 training loss: 2.09775
Global Iter: 247500 training acc: 0.3125
Global Iter: 247600 training loss: 2.02085
Global Iter: 247600 training acc: 0.21875
Global Iter: 247700 training loss: 1.94591
Global Iter: 247700 training acc: 0.375
Global Iter: 247800 training loss: 1.96031
Global Iter: 247800 training acc: 0.21875
Global Iter: 247900 training loss: 1.87987
Global Iter: 247900 traini2017-06-21 11:54:24.037600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-252158
ng acc: 0.34375
Global Iter: 248000 training loss: 1.99694
Global Iter: 248000 training acc: 0.21875
Global Iter: 248100 training loss: 2.01996
Global Iter: 248100 training acc: 0.1875
Global Iter: 248200 training loss: 2.04674
Global Iter: 248200 training acc: 0.125
Global Iter: 248300 training loss: 2.08309
Global Iter: 248300 training acc: 0.03125
Global Iter: 248400 training loss: 2.12222
Global Iter: 248400 training acc: 0.1875
Global Iter: 248500 training loss: 1.97939
Global Iter: 248500 training acc: 0.15625
Global Iter: 248600 training loss: 2.05711
Global Iter: 248600 training acc: 0.1875
Global Iter: 248700 training loss: 1.95139
Global Iter: 248700 training acc: 0.15625
Global Iter: 248800 training loss: 2.01082
Global Iter: 248800 training acc: 0.34375
Global Iter: 248900 training loss: 1.96743
Global Iter: 248900 training acc: 0.1875
Global Iter: 249000 training loss: 1.94475
Global Iter: 249000 training acc: 0.15625
Global Iter: 249100 training loss: 1.92622
Global Iter: 249100 training acc: 0.15625
Global Iter: 249200 training loss: 2.1432
Global Iter: 249200 training acc: 0.15625
Global Iter: 249300 training loss: 2.00487
Global Iter: 249300 training acc: 0.09375
Global Iter: 249400 training loss: 1.94064
Global Iter: 249400 training acc: 0.21875
Global Iter: 249500 training loss: 2.20271
Global Iter: 249500 training acc: 0.125
Global Iter: 249600 training loss: 1.8986
Global Iter: 249600 training acc: 0.25
Global Iter: 249700 training loss: 2.0545
Global Iter: 249700 training acc: 0.1875
Global Iter: 249800 training loss: 1.88514
Global Iter: 249800 training acc: 0.21875
Global Iter: 249900 training loss: 2.05043
Global Iter: 249900 training acc: 0.25
Global Iter: 250000 training loss: 2.02042
Global Iter: 250000 training acc: 0.125
Global Iter: 250100 training loss: 1.95891
Global Iter: 250100 training acc: 0.15625
Global Iter: 250200 training loss: 1.91567
Global Iter: 250200 training acc: 0.1875
Global Iter: 250300 training loss: 1.88866
Global Iter: 250300 training acc: 0.1875
Global Iter: 250400 training loss: 2.01862
Global Iter: 250400 training acc: 0.1875
Global Iter: 250500 training loss: 1.9516
Global Iter: 250500 training acc: 0.0625
Global Iter: 250600 training loss: 2.05633
Global Iter: 250600 training acc: 0.125
Global Iter: 250700 training loss: 1.99128
Global Iter: 250700 training acc: 0.1875
Global Iter: 250800 training loss: 1.88359
Global Iter: 250800 training acc: 0.3125
Global Iter: 250900 training loss: 1.95974
Global Iter: 250900 training acc: 0.125
Global Iter: 251000 training loss: 2.04477
Global Iter: 251000 training acc: 0.15625
Global Iter: 251100 training loss: 2.01783
Global Iter: 251100 training acc: 0.21875
Global Iter: 251200 training loss: 1.92882
Global Iter: 251200 training acc: 0.21875
Global Iter: 251300 training loss: 1.99545
Global Iter: 251300 training acc: 0.15625
Global Iter: 251400 training loss: 2.02279
Global Iter: 251400 training acc: 0.15625
Global Iter: 251500 training loss: 2.06095
Global Iter: 251500 training acc: 0.25
Global Iter: 251600 training loss: 1.92599
Global Iter: 251600 training acc: 0.28125
Global Iter: 251700 training loss: 1.94019
Global Iter: 251700 training acc: 0.15625
Global Iter: 251800 training loss: 1.97282
Global Iter: 251800 training acc: 0.15625
Global Iter: 251900 training loss: 2.00158
Global Iter: 251900 training acc: 0.125
Global Iter: 252000 training loss: 1.86883
Global Iter: 252000 training acc: 0.34375
Global Iter: 252100 training loss: 2.05287
Global Iter: 252100 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-252158
Number of Patches: 267037
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-252158
Global Iter: 252200 training loss: 1.95263
Global Iter: 252200 training acc: 0.21875
Global Iter: 252300 training loss: 2.0179
Global Iter: 252300 training acc: 0.125
Global Iter: 252400 training loss: 1.96092
Global Iter: 252400 training acc: 0.1875
Global Iter: 252500 training loss: 2.00338
Global Iter: 252500 training acc: 0.21875
Global Iter: 252600 training loss: 2.07919
Global Iter: 252600 training acc: 0.1875
Global Iter: 252700 training loss: 1.97053
Global Iter: 252700 training acc: 0.15625
Global Iter: 252800 training loss: 1.95106
Global Iter: 252800 training acc: 0.25
Global Iter: 252900 training loss: 1.9997
Global Iter: 252900 training acc: 0.25
Global Iter: 253000 training loss: 1.87259
Global Iter: 253000 training acc: 0.28125
Global Iter: 253100 training loss: 1.9842
Global Iter: 253100 training acc: 0.25
Global Iter: 253200 training loss: 1.94985
Global Iter: 253200 training acc: 0.21875
Global Iter: 253300 training loss: 2.15695
Global Iter: 253300 training acc: 0.09375
Global Iter: 253400 training loss: 2.01771
Global Iter: 253400 training acc: 0.09375
Global Iter: 253500 training loss: 1.98377
Global Iter: 253500 training acc: 0.25
Global Iter: 253600 training loss: 2.03922
Global Iter: 253600 training acc: 0.1875
Global Iter: 253700 training loss: 2.03848
Global Iter: 253700 training acc: 0.09375
Global Iter: 253800 training loss: 2.01578
Global Iter: 253800 training acc: 0.21875
Global Iter: 253900 training loss: 1.95895
Global Iter: 253900 training acc: 0.3125
Global Iter: 254000 training loss: 1.989
Global Iter: 254000 training acc: 0.3125
Global Iter: 254100 training loss: 2.01205
Global Iter: 254100 training acc: 0.1875
Global Iter: 254200 training loss: 2.0407
Global Iter: 254200 training acc: 0.09375
Global Iter: 254300 training loss: 1.95827
Global Iter: 254300 training acc: 0.21875
Global Iter: 254400 training loss: 1.88625
Global Iter: 254400 training acc: 0.28125
Global Iter: 254500 training loss: 2.02561
Global Iter: 254500 training acc: 0.125
Global Iter: 254600 training loss: 1.98906
Global Iter: 254600 training acc: 0.1875
Global Iter: 254700 training loss: 2.07117
Global Iter: 254700 training acc: 0.1875
Global Iter: 254800 training loss: 1.93915
Global Iter: 254800 training acc: 0.1875
Global Iter: 254900 training loss: 2.05888
Global Iter: 254900 training acc: 0.25
Global Iter: 255000 training loss: 1.94252
Global Iter: 255000 training acc: 0.25
Global Iter: 255100 training loss: 2.03139
Global Iter: 255100 training acc: 0.1875
Global Iter: 255200 training loss: 1.9692
Global Iter: 255200 training acc: 0.09375
Global Iter: 255300 training loss: 2.01613
Global Iter: 255300 training acc: 0.15625
Global Iter: 255400 training loss: 1.97719
Global Iter: 255400 training acc: 0.15625
Global Iter: 255500 training loss: 1.99831
Global Iter: 255500 training acc: 0.09375
Global Iter: 255600 training loss: 2.07252
Global Iter: 255600 training acc: 0.15625
Global Iter: 255700 training loss: 1.89169
Global Iter: 255700 training acc: 0.15625
Global Iter: 255800 training loss: 1.93553
Global Iter: 255800 training acc: 0.25
Global Iter: 255900 training loss: 1.96378
Global Iter: 255900 training acc: 0.15625
Global Iter: 256000 training loss: 1.96473
Global Iter: 256000 training acc: 0.09375
Global Iter: 256100 training loss: 2.06574
Global Iter: 256100 training acc: 0.1875
Global Iter: 256200 training loss: 2.0649
Global Iter: 256200 training acc: 0.125
Global Iter: 256300 training loss: 1.94437
Global Iter: 256300 training acc: 0.125
Global Iter: 256400 training loss: 1.96735
Global Iter: 256400 training acc: 0.21875
Global Iter: 256500 training loss: 1.97843
Global Iter: 256500 training acc: 0.25
Global Iter: 256600 training loss: 1.93513
Global Iter: 256600 training acc: 0.15625
Global Iter: 256700 training loss: 2.06032
Global Iter: 256700 training acc: 0.09375
Global Iter: 256800 training loss: 1.99207
Global Iter: 256800 training acc: 0.1875
Global Iter: 256900 training loss: 1.96985
Global Iter: 256900 training acc: 0.21875
Global Iter: 257000 training loss: 2.05501
Global Iter: 257000 training acc: 0.125
Global Iter: 257100 training loss: 2.05871
Global Iter: 257100 training acc: 0.125
Global Iter: 257200 training loss: 2.00452
Global Iter: 257200 training acc: 0.15625
Global Iter: 257300 training loss: 1.99312
Global Iter: 257300 training acc: 0.21875
Global Iter: 257400 training loss: 2.07305
Global Iter: 257400 training acc: 0.125
Global Iter: 257500 training loss: 1.89201
Global Iter: 257500 training acc: 0.21875
Global Iter: 257600 training loss: 2.02206
Global Iter: 257600 training acc: 0.1875
Global Iter: 257700 training loss: 2.14532
Global Iter: 257700 training acc: 0.125
Global Iter: 257800 training loss: 1.97928
Global Iter: 257800 training acc: 0.0625
Global Iter: 257900 training loss: 1.95789
Global Iter: 257900 training acc: 0.15625
Global Iter: 258000 training loss: 2.08563
Global Iter: 258000 training acc: 0.1875
Global Iter: 258100 training loss: 2.09768
Global Iter: 258100 training acc: 0.28125
Global Iter: 258200 training loss: 1.9797
Global Iter: 258200 training acc: 0.25
Global Iter: 258300 training loss: 2.05531
Global Iter: 258300 training acc: 0.125
Global Iter: 258400 training loss: 2.08834
Global Iter: 258400 training acc: 0.125
Global Iter: 258500 training loss: 2.15429
Global Iter: 258500 training acc: 0.125
Global Iter: 258600 training loss: 1.93773
Global Iter: 258600 training acc: 0.15625
Global Iter: 258700 training loss: 1.96823
Global Iter: 258700 training acc: 0.125
Global Iter: 258800 training loss: 1.93948
Global Iter: 258800 training acc: 0.25
Global Iter: 258900 training loss: 2.02209
Global Iter: 258900 training acc: 0.1875
Global Iter: 259000 training loss: 1.94775
Global Iter: 259000 training acc: 0.1875
Global Iter: 259100 training loss: 1.99534
Global Iter: 259100 training acc: 0.15625
Global Iter: 259200 training loss: 2.07812
Global Iter: 259200 training acc: 0.1875
Global Iter: 259300 training loss: 2.01708
Global Iter: 259300 training acc: 0.25
Global Iter: 259400 training loss: 1.88411
Global Iter: 259400 training acc: 0.3125
Global Iter: 259500 training loss: 1.90273
Global Iter: 259500 training acc: 0.375
Global Iter: 259600 training loss: 1.99495
Global Iter: 259600 training acc: 0.3125
Global Iter: 259700 training loss: 2.00454
Global Iter: 259700 training acc: 0.21875
Global Iter: 259800 training loss: 2.02846
Global Iter: 259800 training acc: 0.09375
Global Iter: 259900 training loss: 1.92285
Global Iter: 259900 training acc: 0.28125
Global Iter: 260000 training loss: 2.02021
Global Iter: 260000 training acc: 0.1875
Global Iter: 260100 training loss: 1.94575
Global Iter: 260100 training acc: 0.25
Global Iter: 260200 training loss: 1.9713
Global Iter: 260200 training acc: 0.15625
Global Iter: 260300 training loss: 1.91764
Global Iter: 260300 training acc: 0.21875
Global Iter: 260400 training loss: 1.8873
Global Iter: 260400 training acc: 0.3125
Global Iter: 260500 training loss: 1.99137
Global Iter: 260500 training acc: 0.1875
Global Iter: 260600 training loss: 1.96953
Global Iter: 260600 training acc: 0.25
Global Iter: 260700 training loss: 2.02929
Global Iter: 260700 training acc: 0.09375
Global Iter: 260800 training loss: 1.92833
Global Iter: 260800 training acc: 0.21875
Global Iter: 260900 training loss: 1.91618
Global Iter: 260900 training acc: 0.15625
Global Iter: 261000 training loss: 2.04041
Global Iter: 261000 training acc: 0.1875
Global Iter: 261100 training loss: 1.96213
Global Iter: 261100 training acc: 0.28125
Global Iter: 261200 training loss: 1.97359
Global Iter: 261200 training acc: 0.125
Global Iter: 261300 training loss: 2.00526
Global Iter: 261300 training acc: 0.1875
Global Iter: 261400 training loss: 2.01515
Global Iter: 261400 training acc: 0.09375
Global Iter: 261500 training loss: 2.08651
Global Iter: 261500 training acc: 0.1875
Global Iter: 261600 training loss: 2.05824
Global Iter: 261600 training acc: 0.15625
Global Iter: 261700 training loss: 2.02299
Global Iter: 261700 training acc: 0.25
Global Iter: 261800 training loss: 2.07601
Global Iter: 261800 training acc: 0.09375
Global Iter: 261900 training loss: 1.9505
Global Iter: 261900 training acc: 0.3125
Global Iter: 262000 training loss: 2.01887
Global Iter: 262000 training acc: 0.09375
Global Iter: 262100 training loss: 1.89685
Global Iter: 262100 training acc: 0.25
Global Iter: 262200 training loss: 2.01066
Global Iter: 262200 training acc: 0.15625
Global Iter: 262300 training loss: 2.02207
Global Iter: 262300 training acc: 0.125
Global Iter: 262400 training loss: 1.86864
Global Iter: 262400 training acc: 0.34375
Global Iter: 262500 training loss: 2.0828
Global Iter: 262500 training acc: 0.28125
Global Iter: 262600 training loss: 1.97083
Global Iter: 262600 training acc: 0.125
Global Iter: 262700 training loss: 2.02968
Global Iter: 262700 training acc: 0.09375
Global Iter: 262800 training loss: 1.98075
Global Iter: 262800 training acc: 0.1875
Global Iter: 262900 training loss: 2.05505
Global Iter: 262900 training acc: 0.125
Global Iter: 263000 training loss: 1.9669
Global Iter: 263000 training acc: 0.25
Global Iter: 263100 training loss: 2.04037
Global Iter: 263100 training acc: 0.21875
Global Iter: 263200 training loss: 1.9292
Global Iter: 263200 training acc: 0.21875
Global Iter: 263300 training loss: 1.93508
Global Iter: 263300 training acc: 0.21875
Global Iter: 263400 training loss: 2.02644
Global Iter: 263400 training acc: 0.15625
Global Iter: 263500 training loss: 2.10861
Global Iter: 263500 training acc: 0.09375
Global Iter: 263600 training loss: 2.04377
Global Iter: 263600 training acc: 0.1875
Global Iter: 263700 training loss: 2.02256
Global Iter: 263700 training acc: 0.3125
Global Iter: 263800 training loss: 1.95644
Global Iter: 263800 training acc: 0.3125
Global Iter: 263900 training loss: 1.97135
Global Iter: 263900 training acc: 0.28125
Global Iter: 264000 training loss: 2.01119
Global Iter: 264000 training acc: 0.21875
Global Iter: 264100 training loss: 1.95748
Global Iter: 264100 training acc: 0.15625
Global Iter: 264200 training loss: 2.02218
Global Iter: 264200 training acc: 0.15625
Global Iter: 264300 training loss: 1.90693
Global Iter: 264300 training acc: 0.21875
Global Iter: 264400 training loss: 1.85791
Global Iter: 264400 training acc: 0.34375
Global Iter: 264500 training loss: 1.89482
Global Iter: 264500 training acc: 0.3125
Global Iter: 264600 training loss: 1.91401
Global Iter: 264600 training acc: 0.25
Global Iter: 264700 training loss: 1.99029
Global Iter: 264700 training acc: 0.15625
Global Iter: 264800 training loss: 1.95796
Global Iter: 264800 training acc: 0.15625
Global Iter: 264900 training loss: 2.07887
Global Iter: 264900 training acc: 0.15625
Global Iter: 265000 training loss: 2.06196
Global Iter: 265000 training acc: 0.21875
Global Iter: 265100 training loss: 1.91587
Global Iter: 265100 training acc: 0.125
Global Iter: 265200 training loss: 2.10429
Global Iter: 265200 training acc: 0.15625
Global Iter: 265300 training loss: 2.06765
Global Iter: 265300 training acc: 0.15625
Global Iter: 265400 training loss: 2.10951
Global Iter: 265400 training acc: 0.15625
Global Iter: 265500 training loss: 1.95376
Global Iter: 265500 training acc: 0.21875
Global Iter: 265600 training loss: 1.9227
Global Iter: 265600 training acc: 0.1875
Global Iter: 265700 training loss: 1.98365
Global Iter: 265700 training acc: 0.28125
Global Iter: 265800 training loss: 1.9109
Global Iter: 265800 training acc: 0.3125
Global Iter: 265900 training loss: 1.93903
Global Iter: 265900 training acc: 0.25
Global Iter: 266000 training loss: 1.99484
Global Iter: 266000 training acc: 0.125
Global Iter: 266100 training loss: 2.02606
Global Iter: 266100 training acc: 0.1875
Global Iter: 266200 training loss: 1.95164
Global Iter: 266200 training acc: 0.15625
Global Iter: 266300 training loss: 2.069
Global Iter: 266300 training acc: 0.21875
Global Iter: 266400 training loss: 2.04369
Global Iter: 266400 training acc: 0.125
Global Iter: 266500 training loss: 1.97023
Global Iter: 266500 training acc: 0.1875
Global Iter: 266600 training loss: 2.09173
Global Iter: 266600 training acc: 0.125
Global Iter: 266700 training loss: 2.08512
Global Iter: 266700 training acc: 0.15625
Global Iter: 266800 training loss: 1.97152
Global Iter: 266800 training acc: 0.1875
Global Iter: 266900 training loss: 2.09104
Global Iter: 266900 training acc: 0.15625
Global Iter: 267000 training loss: 1.92651
Global Iter: 267000 training acc: 0.1875
Global Iter: 267100 training loss: 1.9562
Global Iter: 267100 training acc: 0.125
Global Iter: 267202017-06-21 12:23:46.578506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-268848
0 training loss: 1.90472
Global Iter: 267200 training acc: 0.25
Global Iter: 267300 training loss: 2.06487
Global Iter: 267300 training acc: 0.1875
Global Iter: 267400 training loss: 2.00237
Global Iter: 267400 training acc: 0.1875
Global Iter: 267500 training loss: 1.86316
Global Iter: 267500 training acc: 0.3125
Global Iter: 267600 training loss: 1.95553
Global Iter: 267600 training acc: 0.21875
Global Iter: 267700 training loss: 2.05602
Global Iter: 267700 training acc: 0.125
Global Iter: 267800 training loss: 1.99752
Global Iter: 267800 training acc: 0.125
Global Iter: 267900 training loss: 1.96897
Global Iter: 267900 training acc: 0.21875
Global Iter: 268000 training loss: 1.84859
Global Iter: 268000 training acc: 0.34375
Global Iter: 268100 training loss: 2.14295
Global Iter: 268100 training acc: 0.15625
Global Iter: 268200 training loss: 2.04877
Global Iter: 268200 training acc: 0.1875
Global Iter: 268300 training loss: 1.93224
Global Iter: 268300 training acc: 0.21875
Global Iter: 268400 training loss: 2.03192
Global Iter: 268400 training acc: 0.15625
Global Iter: 268500 training loss: 1.93862
Global Iter: 268500 training acc: 0.1875
Global Iter: 268600 training loss: 1.94846
Global Iter: 268600 training acc: 0.25
Global Iter: 268700 training loss: 2.0761
Global Iter: 268700 training acc: 0.125
Global Iter: 268800 training loss: 1.95002
Global Iter: 268800 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-268848
Number of Patches: 264367
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-268848
Global Iter: 268900 training loss: 1.95191
Global Iter: 268900 training acc: 0.21875
Global Iter: 269000 training loss: 2.02407
Global Iter: 269000 training acc: 0.15625
Global Iter: 269100 training loss: 1.96857
Global Iter: 269100 training acc: 0.09375
Global Iter: 269200 training loss: 2.00016
Global Iter: 269200 training acc: 0.125
Global Iter: 269300 training loss: 1.92436
Global Iter: 269300 training acc: 0.1875
Global Iter: 269400 training loss: 1.98654
Global Iter: 269400 training acc: 0.125
Global Iter: 269500 training loss: 2.05624
Global Iter: 269500 training acc: 0.21875
Global Iter: 269600 training loss: 2.0393
Global Iter: 269600 training acc: 0.1875
Global Iter: 269700 training loss: 1.87763
Global Iter: 269700 training acc: 0.15625
Global Iter: 269800 training loss: 1.97179
Global Iter: 269800 training acc: 0.25
Global Iter: 269900 training loss: 2.0624
Global Iter: 269900 training acc: 0.0625
Global Iter: 270000 training loss: 1.99446
Global Iter: 270000 training acc: 0.25
Global Iter: 270100 training loss: 1.93656
Global Iter: 270100 training acc: 0.1875
Global Iter: 270200 training loss: 1.90534
Global Iter: 270200 training acc: 0.125
Global Iter: 270300 training loss: 2.03321
Global Iter: 270300 training acc: 0.15625
Global Iter: 270400 training loss: 1.99539
Global Iter: 270400 training acc: 0.15625
Global Iter: 270500 training loss: 2.04397
Global Iter: 270500 training acc: 0.25
Global Iter: 270600 training loss: 2.05259
Global Iter: 270600 training acc: 0.125
Global Iter: 270700 training loss: 1.87998
Global Iter: 270700 training acc: 0.1875
Global Iter: 270800 training loss: 1.94774
Global Iter: 270800 training acc: 0.34375
Global Iter: 270900 training loss: 1.97515
Global Iter: 270900 training acc: 0.21875
Global Iter: 271000 training loss: 1.97781
Global Iter: 271000 training acc: 0.1875
Global Iter: 271100 training loss: 1.94803
Global Iter: 271100 training acc: 0.21875
Global Iter: 271200 training loss: 1.99594
Global Iter: 271200 training acc: 0.25
Global Iter: 271300 training loss: 2.03343
Global Iter: 271300 training acc: 0.15625
Global Iter: 271400 training loss: 2.02586
Global Iter: 271400 training acc: 0.1875
Global Iter: 271500 training loss: 1.93612
Global Iter: 271500 training acc: 0.15625
Global Iter: 271600 training loss: 1.93622
Global Iter: 271600 training acc: 0.1875
Global Iter: 271700 training loss: 1.9778
Global Iter: 271700 training acc: 0.15625
Global Iter: 271800 training loss: 2.00065
Global Iter: 271800 training acc: 0.15625
Global Iter: 271900 training loss: 2.00672
Global Iter: 271900 training acc: 0.28125
Global Iter: 272000 training loss: 2.03763
Global Iter: 272000 training acc: 0.1875
Global Iter: 272100 training loss: 2.08906
Global Iter: 272100 training acc: 0.25
Global Iter: 272200 training loss: 1.91443
Global Iter: 272200 training acc: 0.21875
Global Iter: 272300 training loss: 2.03346
Global Iter: 272300 training acc: 0.21875
Global Iter: 272400 training loss: 1.96301
Global Iter: 272400 training acc: 0.0625
Global Iter: 272500 training loss: 1.94705
Global Iter: 272500 training acc: 0.3125
Global Iter: 272600 training loss: 1.98958
Global Iter: 272600 training acc: 0.28125
Global Iter: 272700 training loss: 1.94181
Global Iter: 272700 training acc: 0.21875
Global Iter: 272800 training loss: 2.01572
Global Iter: 272800 training acc: 0.125
Global Iter: 272900 training loss: 1.91682
Global Iter: 272900 training acc: 0.21875
Global Iter: 273000 training loss: 1.89921
Global Iter: 273000 training acc: 0.3125
Global Iter: 273100 training loss: 1.94032
Global Iter: 273100 training acc: 0.21875
Global Iter: 273200 training loss: 1.95057
Global Iter: 273200 training acc: 0.25
Global Iter: 273300 training loss: 2.1059
Global Iter: 273300 training acc: 0.125
Global Iter: 273400 training loss: 2.12111
Global Iter: 273400 training acc: 0.15625
Global Iter: 273500 training loss: 1.98843
Global Iter: 273500 training acc: 0.15625
Global Iter: 273600 training loss: 1.9816
Global Iter: 273600 training acc: 0.1875
Global Iter: 273700 training loss: 1.91463
Global Iter: 273700 training acc: 0.375
Global Iter: 273800 training loss: 2.05799
Global Iter: 273800 training acc: 0.125
Global Iter: 273900 training loss: 1.99319
Global Iter: 273900 training acc: 0.1875
Global Iter: 274000 training loss: 2.01662
Global Iter: 274000 training acc: 0.25
Global Iter: 274100 training loss: 1.94278
Global Iter: 274100 training acc: 0.15625
Global Iter: 274200 training loss: 1.88956
Global Iter: 274200 training acc: 0.1875
Global Iter: 274300 training loss: 2.00773
Global Iter: 274300 training acc: 0.125
Global Iter: 274400 training loss: 1.96469
Global Iter: 274400 training acc: 0.21875
Global Iter: 274500 training loss: 2.05358
Global Iter: 274500 training acc: 0.125
Global Iter: 274600 training loss: 1.88323
Global Iter: 274600 training acc: 0.28125
Global Iter: 274700 training loss: 1.98146
Global Iter: 274700 training acc: 0.15625
Global Iter: 274800 training loss: 1.98228
Global Iter: 274800 training acc: 0.15625
Global Iter: 274900 training loss: 2.07009
Global Iter: 274900 training acc: 0.1875
Global Iter: 275000 training loss: 1.90575
Global Iter: 275000 training acc: 0.3125
Global Iter: 275100 training loss: 2.02247
Global Iter: 275100 training acc: 0.09375
Global Iter: 275200 training loss: 1.9056
Global Iter: 275200 training acc: 0.28125
Global Iter: 275300 training loss: 2.04757
Global Iter: 275300 training acc: 0.15625
Global Iter: 275400 training loss: 2.17623
Global Iter: 275400 training acc: 0.1875
Global Iter: 275500 training loss: 1.93519
Global Iter: 275500 training acc: 0.21875
Global Iter: 275600 training loss: 1.95306
Global Iter: 275600 training acc: 0.1875
Global Iter: 275700 training loss: 1.88738
Global Iter: 275700 training acc: 0.25
Global Iter: 275800 training loss: 2.04451
Global Iter: 275800 training acc: 0.3125
Global Iter: 275900 training loss: 2.04144
Global Iter: 275900 training acc: 0.1875
Global Iter: 276000 training loss: 1.98864
Global Iter: 276000 training acc: 0.09375
Global Iter: 276100 training loss: 2.14548
Global Iter: 276100 training acc: 0.1875
Global Iter: 276200 training loss: 2.02417
Global Iter: 276200 training acc: 0.15625
Global Iter: 276300 training loss: 1.95964
Global Iter: 276300 training acc: 0.1875
Global Iter: 276400 training loss: 2.10168
Global Iter: 276400 training acc: 0.0625
Global Iter: 276500 training loss: 1.98542
Global Iter: 276500 training acc: 0.1875
Global Iter: 276600 training loss: 1.97716
Global Iter: 276600 training acc: 0.1875
Global Iter: 276700 training loss: 2.02692
Global Iter: 276700 training acc: 0.1875
Global Iter: 276800 training loss: 2.03223
Global Iter: 276800 training acc: 0.21875
Global Iter: 276900 training loss: 1.97582
Global Iter: 276900 training acc: 0.15625
Global Iter: 277000 training loss: 2.08588
Global Iter: 277000 training acc: 0.125
Global Iter: 277100 training loss: 2.09175
Global Iter: 277100 training acc: 0.15625
Global Iter: 277200 training loss: 1.9874
Global Iter: 277200 training acc: 0.125
Global Iter: 277300 training loss: 2.03722
Global Iter: 277300 training acc: 0.1875
Global Iter: 277400 training loss: 1.9737
Global Iter: 277400 training acc: 0.15625
Global Iter: 277500 training loss: 1.94838
Global Iter: 277500 training acc: 0.15625
Global Iter: 277600 training loss: 1.96331
Global Iter: 277600 training acc: 0.0625
Global Iter: 277700 training loss: 2.01365
Global Iter: 277700 training acc: 0.28125
Global Iter: 277800 training loss: 2.06698
Global Iter: 277800 training acc: 0.125
Global Iter: 277900 training loss: 1.91819
Global Iter: 277900 training acc: 0.15625
Global Iter: 278000 training loss: 1.97444
Global Iter: 278000 training acc: 0.25
Global Iter: 278100 training loss: 2.04491
Global Iter: 278100 training acc: 0.15625
Global Iter: 278200 training loss: 1.96597
Global Iter: 278200 training acc: 0.3125
Global Iter: 278300 training loss: 1.98028
Global Iter: 278300 training acc: 0.1875
Global Iter: 278400 training loss: 1.93693
Global Iter: 278400 training acc: 0.375
Global Iter: 278500 training loss: 1.96948
Global Iter: 278500 training acc: 0.25
Global Iter: 278600 training loss: 1.97271
Global Iter: 278600 training acc: 0.25
Global Iter: 278700 training loss: 1.97799
Global Iter: 278700 training acc: 0.125
Global Iter: 278800 training loss: 1.99484
Global Iter: 278800 training acc: 0.09375
Global Iter: 278900 training loss: 1.98252
Global Iter: 278900 training acc: 0.15625
Global Iter: 279000 training loss: 1.92385
Global Iter: 279000 training acc: 0.15625
Global Iter: 279100 training loss: 2.00889
Global Iter: 279100 training acc: 0.125
Global Iter: 279200 training loss: 1.96304
Global Iter: 279200 training acc: 0.15625
Global Iter: 279300 training loss: 1.89762
Global Iter: 279300 training acc: 0.3125
Global Iter: 279400 training loss: 1.92849
Global Iter: 279400 training acc: 0.25
Global Iter: 279500 training loss: 1.96146
Global Iter: 279500 training acc: 0.3125
Global Iter: 279600 training loss: 1.97863
Global Iter: 279600 training acc: 0.125
Global Iter: 279700 training loss: 1.85222
Global Iter: 279700 training acc: 0.3125
Global Iter: 279800 training loss: 2.00074
Global Iter: 279800 training acc: 0.1875
Global Iter: 279900 training loss: 2.00779
Global Iter: 279900 training acc: 0.1875
Global Iter: 280000 training loss: 2.01293
Global Iter: 280000 training acc: 0.1875
Global Iter: 280100 training loss: 2.04262
Global Iter: 280100 training acc: 0.09375
Global Iter: 280200 training loss: 2.04069
Global Iter: 280200 training acc: 0.15625
Global Iter: 280300 training loss: 1.97584
Global Iter: 280300 training acc: 0.21875
Global Iter: 280400 training loss: 2.05049
Global Iter: 280400 training acc: 0.1875
Global Iter: 280500 training loss: 2.06548
Global Iter: 280500 training acc: 0.125
Global Iter: 280600 training loss: 1.91272
Global Iter: 280600 training acc: 0.28125
Global Iter: 280700 training loss: 2.07706
Global Iter: 280700 training acc: 0.09375
Global Iter: 280800 training loss: 2.04089
Global Iter: 280800 training acc: 0.1875
Global Iter: 280900 training loss: 1.98588
Global Iter: 280900 training acc: 0.15625
Global Iter: 281000 training loss: 2.05013
Global Iter: 281000 training acc: 0.09375
Global Iter: 281100 training loss: 1.94549
Global Iter: 281100 training acc: 0.21875
Global Iter: 281200 training loss: 2.04822
Global Iter: 281200 training acc: 0.125
Global Iter: 281300 training loss: 2.2096
Global Iter: 281300 training acc: 0.1875
Global Iter: 281400 training loss: 1.99749
Global Iter: 281400 training acc: 0.125
Global Iter: 281500 training loss: 2.02879
Global Iter: 281500 training 2017-06-21 12:53:00.665377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-285371
acc: 0.15625
Global Iter: 281600 training loss: 2.06369
Global Iter: 281600 training acc: 0.1875
Global Iter: 281700 training loss: 1.89828
Global Iter: 281700 training acc: 0.25
Global Iter: 281800 training loss: 1.96053
Global Iter: 281800 training acc: 0.15625
Global Iter: 281900 training loss: 2.07902
Global Iter: 281900 training acc: 0.15625
Global Iter: 282000 training loss: 1.94573
Global Iter: 282000 training acc: 0.25
Global Iter: 282100 training loss: 1.96874
Global Iter: 282100 training acc: 0.09375
Global Iter: 282200 training loss: 2.01242
Global Iter: 282200 training acc: 0.1875
Global Iter: 282300 training loss: 1.98553
Global Iter: 282300 training acc: 0.09375
Global Iter: 282400 training loss: 1.91242
Global Iter: 282400 training acc: 0.1875
Global Iter: 282500 training loss: 1.97411
Global Iter: 282500 training acc: 0.1875
Global Iter: 282600 training loss: 2.00736
Global Iter: 282600 training acc: 0.15625
Global Iter: 282700 training loss: 2.04781
Global Iter: 282700 training acc: 0.1875
Global Iter: 282800 training loss: 1.91985
Global Iter: 282800 training acc: 0.28125
Global Iter: 282900 training loss: 1.95448
Global Iter: 282900 training acc: 0.125
Global Iter: 283000 training loss: 1.92151
Global Iter: 283000 training acc: 0.28125
Global Iter: 283100 training loss: 2.05199
Global Iter: 283100 training acc: 0.09375
Global Iter: 283200 training loss: 1.93252
Global Iter: 283200 training acc: 0.3125
Global Iter: 283300 training loss: 1.96407
Global Iter: 283300 training acc: 0.21875
Global Iter: 283400 training loss: 1.99655
Global Iter: 283400 training acc: 0.09375
Global Iter: 283500 training loss: 2.02096
Global Iter: 283500 training acc: 0.1875
Global Iter: 283600 training loss: 2.01988
Global Iter: 283600 training acc: 0.09375
Global Iter: 283700 training loss: 1.96122
Global Iter: 283700 training acc: 0.21875
Global Iter: 283800 training loss: 2.0467
Global Iter: 283800 training acc: 0.15625
Global Iter: 283900 training loss: 1.83797
Global Iter: 283900 training acc: 0.1875
Global Iter: 284000 training loss: 2.11787
Global Iter: 284000 training acc: 0.1875
Global Iter: 284100 training loss: 1.97262
Global Iter: 284100 training acc: 0.1875
Global Iter: 284200 training loss: 1.99336
Global Iter: 284200 training acc: 0.09375
Global Iter: 284300 training loss: 2.03668
Global Iter: 284300 training acc: 0.125
Global Iter: 284400 training loss: 2.08924
Global Iter: 284400 training acc: 0.125
Global Iter: 284500 training loss: 1.95048
Global Iter: 284500 training acc: 0.21875
Global Iter: 284600 training loss: 2.09031
Global Iter: 284600 training acc: 0.15625
Global Iter: 284700 training loss: 1.93581
Global Iter: 284700 training acc: 0.15625
Global Iter: 284800 training loss: 1.99048
Global Iter: 284800 training acc: 0.1875
Global Iter: 284900 training loss: 1.92637
Global Iter: 284900 training acc: 0.125
Global Iter: 285000 training loss: 1.98482
Global Iter: 285000 training acc: 0.25
Global Iter: 285100 training loss: 1.90935
Global Iter: 285100 training acc: 0.1875
Global Iter: 285200 training loss: 1.95055
Global Iter: 285200 training acc: 0.25
Global Iter: 285300 training loss: 2.11592
Global Iter: 285300 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-285371
Number of Patches: 261724
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-285371
Global Iter: 285400 training loss: 1.94142
Global Iter: 285400 training acc: 0.21875
Global Iter: 285500 training loss: 2.01746
Global Iter: 285500 training acc: 0.21875
Global Iter: 285600 training loss: 2.0234
Global Iter: 285600 training acc: 0.15625
Global Iter: 285700 training loss: 2.17015
Global Iter: 285700 training acc: 0.125
Global Iter: 285800 training loss: 1.9529
Global Iter: 285800 training acc: 0.28125
Global Iter: 285900 training loss: 2.10917
Global Iter: 285900 training acc: 0.125
Global Iter: 286000 training loss: 1.99423
Global Iter: 286000 training acc: 0.1875
Global Iter: 286100 training loss: 2.01315
Global Iter: 286100 training acc: 0.25
Global Iter: 286200 training loss: 2.02367
Global Iter: 286200 training acc: 0.21875
Global Iter: 286300 training loss: 2.07554
Global Iter: 286300 training acc: 0.125
Global Iter: 286400 training loss: 1.98364
Global Iter: 286400 training acc: 0.09375
Global Iter: 286500 training loss: 2.00237
Global Iter: 286500 training acc: 0.125
Global Iter: 286600 training loss: 1.92907
Global Iter: 286600 training acc: 0.28125
Global Iter: 286700 training loss: 2.07002
Global Iter: 286700 training acc: 0.09375
Global Iter: 286800 training loss: 1.93735
Global Iter: 286800 training acc: 0.25
Global Iter: 286900 training loss: 2.09888
Global Iter: 286900 training acc: 0.0625
Global Iter: 287000 training loss: 1.88124
Global Iter: 287000 training acc: 0.21875
Global Iter: 287100 training loss: 2.02503
Global Iter: 287100 training acc: 0.15625
Global Iter: 287200 training loss: 1.96411
Global Iter: 287200 training acc: 0.3125
Global Iter: 287300 training loss: 1.99392
Global Iter: 287300 training acc: 0.21875
Global Iter: 287400 training loss: 1.9421
Global Iter: 287400 training acc: 0.1875
Global Iter: 287500 training loss: 1.95102
Global Iter: 287500 training acc: 0.15625
Global Iter: 287600 training loss: 2.00698
Global Iter: 287600 training acc: 0.21875
Global Iter: 287700 training loss: 2.00092
Global Iter: 287700 training acc: 0.0625
Global Iter: 287800 training loss: 1.97028
Global Iter: 287800 training acc: 0.3125
Global Iter: 287900 training loss: 1.98478
Global Iter: 287900 training acc: 0.1875
Global Iter: 288000 training loss: 2.02537
Global Iter: 288000 training acc: 0.375
Global Iter: 288100 training loss: 2.01377
Global Iter: 288100 training acc: 0.03125
Global Iter: 288200 training loss: 1.88031
Global Iter: 288200 training acc: 0.34375
Global Iter: 288300 training loss: 1.99332
Global Iter: 288300 training acc: 0.21875
Global Iter: 288400 training loss: 1.92225
Global Iter: 288400 training acc: 0.3125
Global Iter: 288500 training loss: 1.92999
Global Iter: 288500 training acc: 0.1875
Global Iter: 288600 training loss: 2.07798
Global Iter: 288600 training acc: 0.25
Global Iter: 288700 training loss: 1.90552
Global Iter: 288700 training acc: 0.28125
Global Iter: 288800 training loss: 2.0185
Global Iter: 288800 training acc: 0.21875
Global Iter: 288900 training loss: 2.07515
Global Iter: 288900 training acc: 0.125
Global Iter: 289000 training loss: 2.04236
Global Iter: 289000 training acc: 0.1875
Global Iter: 289100 training loss: 1.99697
Global Iter: 289100 training acc: 0.125
Global Iter: 289200 training loss: 1.94437
Global Iter: 289200 training acc: 0.21875
Global Iter: 289300 training loss: 2.06225
Global Iter: 289300 training acc: 0.21875
Global Iter: 289400 training loss: 1.92903
Global Iter: 289400 training acc: 0.25
Global Iter: 289500 training loss: 2.01319
Global Iter: 289500 training acc: 0.09375
Global Iter: 289600 training loss: 1.99578
Global Iter: 289600 training acc: 0.21875
Global Iter: 289700 training loss: 1.94937
Global Iter: 289700 training acc: 0.21875
Global Iter: 289800 training loss: 1.90692
Global Iter: 289800 training acc: 0.1875
Global Iter: 289900 training loss: 1.99733
Global Iter: 289900 training acc: 0.1875
Global Iter: 290000 training loss: 1.99192
Global Iter: 290000 training acc: 0.1875
Global Iter: 290100 training loss: 1.97311
Global Iter: 290100 training acc: 0.15625
Global Iter: 290200 training loss: 1.95841
Global Iter: 290200 training acc: 0.28125
Global Iter: 290300 training loss: 2.1174
Global Iter: 290300 training acc: 0.1875
Global Iter: 290400 training loss: 2.10649
Global Iter: 290400 training acc: 0.09375
Global Iter: 290500 training loss: 2.02344
Global Iter: 290500 training acc: 0.03125
Global Iter: 290600 training loss: 1.98279
Global Iter: 290600 training acc: 0.21875
Global Iter: 290700 training loss: 2.05429
Global Iter: 290700 training acc: 0.21875
Global Iter: 290800 training loss: 2.08526
Global Iter: 290800 training acc: 0.1875
Global Iter: 290900 training loss: 1.98923
Global Iter: 290900 training acc: 0.21875
Global Iter: 291000 training loss: 2.09892
Global Iter: 291000 training acc: 0.125
Global Iter: 291100 training loss: 2.08897
Global Iter: 291100 training acc: 0.21875
Global Iter: 291200 training loss: 1.95531
Global Iter: 291200 training acc: 0.1875
Global Iter: 291300 training loss: 2.0711
Global Iter: 291300 training acc: 0.15625
Global Iter: 291400 training loss: 1.98111
Global Iter: 291400 training acc: 0.1875
Global Iter: 291500 training loss: 1.99772
Global Iter: 291500 training acc: 0.125
Global Iter: 291600 training loss: 1.92951
Global Iter: 291600 training acc: 0.28125
Global Iter: 291700 training loss: 2.02452
Global Iter: 291700 training acc: 0.125
Global Iter: 291800 training loss: 2.04251
Global Iter: 291800 training acc: 0.25
Global Iter: 291900 training loss: 1.93391
Global Iter: 291900 training acc: 0.28125
Global Iter: 292000 training loss: 1.96533
Global Iter: 292000 training acc: 0.125
Global Iter: 292100 training loss: 2.02198
Global Iter: 292100 training acc: 0.21875
Global Iter: 292200 training loss: 1.95834
Global Iter: 292200 training acc: 0.09375
Global Iter: 292300 training loss: 1.96506
Global Iter: 292300 training acc: 0.125
Global Iter: 292400 training loss: 2.03058
Global Iter: 292400 training acc: 0.15625
Global Iter: 292500 training loss: 1.92762
Global Iter: 292500 training acc: 0.21875
Global Iter: 292600 training loss: 2.01508
Global Iter: 292600 training acc: 0.25
Global Iter: 292700 training loss: 2.14955
Global Iter: 292700 training acc: 0.09375
Global Iter: 292800 training loss: 2.02756
Global Iter: 292800 training acc: 0.125
Global Iter: 292900 training loss: 1.95099
Global Iter: 292900 training acc: 0.1875
Global Iter: 293000 training loss: 2.04583
Global Iter: 293000 training acc: 0.15625
Global Iter: 293100 training loss: 1.98532
Global Iter: 293100 training acc: 0.15625
Global Iter: 293200 training loss: 2.17388
Global Iter: 293200 training acc: 0.1875
Global Iter: 293300 training loss: 1.95083
Global Iter: 293300 training acc: 0.28125
Global Iter: 293400 training loss: 1.90887
Global Iter: 293400 training acc: 0.1875
Global Iter: 293500 training loss: 1.92713
Global Iter: 293500 training acc: 0.28125
Global Iter: 293600 training loss: 2.0099
Global Iter: 293600 training acc: 0.15625
Global Iter: 293700 training loss: 1.95198
Global Iter: 293700 training acc: 0.1875
Global Iter: 293800 training loss: 2.08718
Global Iter: 293800 training acc: 0.15625
Global Iter: 293900 training loss: 1.91388
Global Iter: 293900 training acc: 0.34375
Global Iter: 294000 training loss: 1.99263
Global Iter: 294000 training acc: 0.125
Global Iter: 294100 training loss: 2.00022
Global Iter: 294100 training acc: 0.21875
Global Iter: 294200 training loss: 1.91704
Global Iter: 294200 training acc: 0.34375
Global Iter: 294300 training loss: 1.96181
Global Iter: 294300 training acc: 0.1875
Global Iter: 294400 training loss: 2.03378
Global Iter: 294400 training acc: 0.125
Global Iter: 294500 training loss: 2.03308
Global Iter: 294500 training acc: 0.125
Global Iter: 294600 training loss: 2.09847
Global Iter: 294600 training acc: 0.09375
Global Iter: 294700 training loss: 2.04211
Global Iter: 294700 training acc: 0.09375
Global Iter: 294800 training loss: 2.0746
Global Iter: 294800 training acc: 0.125
Global Iter: 294900 training loss: 1.95586
Global Iter: 294900 training acc: 0.21875
Global Iter: 295000 training loss: 1.93575
Global Iter: 295000 training acc: 0.1875
Global Iter: 295100 training loss: 1.96836
Global Iter: 295100 training acc: 0.125
Global Iter: 295200 training loss: 1.97556
Global Iter: 295200 training acc: 0.125
Global Iter: 295300 training loss: 2.03473
Global Iter: 295300 training acc: 0.21875
Global Iter: 295400 training loss: 2.00247
Global Iter: 295400 training acc: 0.1875
Global Iter: 295500 training loss: 1.99005
Global Iter: 295500 training acc: 0.15625
Global Iter: 295600 training loss: 2.04776
Global Iter: 295600 training acc: 0.125
Global Iter: 295700 training loss: 1.87122
Global Iter: 295700 training acc: 0.3125
Global Iter: 295800 training loss: 1.93002
Global Iter: 295800 training acc: 0.1875
Global Iter: 295900 training loss: 2.06987
Global Iter: 295900 training acc: 0.15625
Global Iter: 296000 training loss: 1.98026
Global Iter: 296000 training acc: 0.1875
Global Iter: 296100 training loss: 1.94497
Global Iter: 296100 training acc: 0.15625
Global Iter: 296200 training loss: 1.98969
Global Iter: 296200 training acc: 0.09375
Global Iter: 296300 training loss: 1.91236
Global Iter: 296300 training acc: 0.21875
Global Iter: 296400 training loss: 2.07045
Global Iter: 296400 training acc: 0.25
Global Iter: 296500 training loss: 1.99198
Global Iter: 296500 training acc: 0.28125
Global Iter: 296600 training loss: 2.08235
Global Iter: 296600 training acc: 0.125
Global Iter: 296700 training loss: 1.97407
Global Iter: 296700 training acc: 0.09375
Global Iter: 296800 training loss: 1.91764
Global Iter: 296800 training acc: 0.25
Global Iter: 296900 training loss: 1.96187
Global Iter: 296900 training acc: 0.09375
Global Iter: 297000 training loss: 2.12307
Global Iter: 297000 training acc: 0.03125
Global Iter: 297100 training loss: 2.02996
Global Iter: 297100 training acc: 0.125
Global Iter: 297200 training loss: 1.93593
Global Iter: 297200 training acc: 0.21875
Global Iter: 297300 training loss: 1.99756
Global Iter: 297300 training acc: 0.25
Global Iter: 297400 training loss: 2.01271
Global Iter: 297400 training acc: 0.1875
Global Iter: 297500 training loss: 1.90659
Global Iter: 297500 training acc: 0.3125
Global Iter: 297600 training loss: 2.14895
Global Iter: 297600 training acc: 0.0625
Global Iter: 297700 training loss: 2.09789
Global Iter: 297700 training acc: 0.09375
Global Iter: 297800 training loss: 1.9514
Global Iter: 297800 training acc: 0.1875
Global Iter: 297900 training loss: 2.01965
Global Iter: 297900 training acc: 0.21875
Global Iter: 298000 training loss: 1.93958
Global Iter: 298000 training acc: 0.15625
Global Iter: 298100 training loss: 2.09161
Global Iter: 298100 training acc: 0.0625
Global Iter: 298200 training loss: 1.94041
Global Iter: 298200 training acc: 0.28125
Global Iter: 298300 training loss: 2.00437
Global Iter: 298300 training acc: 0.0625
Global Iter: 298400 training loss: 2.04392
Global Iter: 298400 training acc: 0.3125
Global Iter: 298500 training loss: 1.95805
Global Iter: 298500 training acc: 0.21875
Global Iter: 298600 training loss: 1.95796
Global Iter: 298600 training acc: 0.1875
Global Iter: 298700 training loss: 2.04477
Global Iter: 298700 training acc: 0.25
Global Iter: 298800 training loss: 1.94834
Global Iter: 298800 training acc: 0.1875
Global Iter: 298900 training loss: 2.00487
Global Iter: 298900 training acc: 0.15625
Global Iter: 299000 training loss: 1.98859
Global Iter: 299000 training acc: 0.09375
Global Iter: 299100 training loss: 1.96651
Global Iter: 299100 training acc: 0.25
Global Iter: 299200 training loss: 2.03147
Global Iter: 299200 training acc: 0.15625
Global Iter: 299300 training loss: 1.9852
Global Iter: 299300 training acc: 0.15625
Global Iter: 299400 training loss: 1.9794
Global Iter: 299400 training acc: 0.21875
Global Iter: 299500 training loss: 1.92464
Global Iter: 299500 training acc: 0.15625
Global Iter: 299600 training loss: 2.0355
Global Iter: 299600 training acc: 0.09375
Global Iter: 299700 training loss: 1.96521
Global Iter: 299700 training acc: 0.21875
Global Iter: 299800 training loss: 2.08047
Global Iter: 299800 training acc: 0.09375
Global Iter: 299900 training loss: 1.94259
Global Iter: 299900 training acc: 0.3125
Global Iter: 300000 training loss: 2.00395
Global Iter: 300000 training acc: 0.1875
Global Iter: 300100 training loss: 2.1216
Global Iter: 300100 training acc: 0.09375
Global Iter: 300200 training loss: 1.9472
Global Iter: 300200 training acc: 0.15625
Global Iter: 300300 training loss: 2.16551
Global Iter: 300300 training acc: 0.125
Global Iter: 300400 training loss: 2.18874
Global Iter: 300400 training acc: 0.09375
Global Iter: 300500 training loss: 2.00968
Global Iter: 300500 training acc: 0.1875
Global Iter: 300600 training loss: 2.12967
Global Iter: 300600 training acc: 0.03125
Global Iter: 300700 training loss: 1.98623
Global Iter: 300700 training acc: 0.2017-06-21 13:22:11.041680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-301729
1875
Global Iter: 300800 training loss: 2.02952
Global Iter: 300800 training acc: 0.25
Global Iter: 300900 training loss: 2.03834
Global Iter: 300900 training acc: 0.15625
Global Iter: 301000 training loss: 2.10008
Global Iter: 301000 training acc: 0.125
Global Iter: 301100 training loss: 1.95841
Global Iter: 301100 training acc: 0.1875
Global Iter: 301200 training loss: 2.14043
Global Iter: 301200 training acc: 0.0625
Global Iter: 301300 training loss: 2.05824
Global Iter: 301300 training acc: 0.28125
Global Iter: 301400 training loss: 1.89869
Global Iter: 301400 training acc: 0.28125
Global Iter: 301500 training loss: 1.99179
Global Iter: 301500 training acc: 0.125
Global Iter: 301600 training loss: 2.18241
Global Iter: 301600 training acc: 0.125
Global Iter: 301700 training loss: 2.07268
Global Iter: 301700 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-301729
Number of Patches: 259107
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-301729
Global Iter: 301800 training loss: 2.00959
Global Iter: 301800 training acc: 0.03125
Global Iter: 301900 training loss: 2.17848
Global Iter: 301900 training acc: 0.125
Global Iter: 302000 training loss: 1.99517
Global Iter: 302000 training acc: 0.1875
Global Iter: 302100 training loss: 2.00228
Global Iter: 302100 training acc: 0.1875
Global Iter: 302200 training loss: 1.9345
Global Iter: 302200 training acc: 0.28125
Global Iter: 302300 training loss: 2.0554
Global Iter: 302300 training acc: 0.0625
Global Iter: 302400 training loss: 2.0642
Global Iter: 302400 training acc: 0.09375
Global Iter: 302500 training loss: 2.0139
Global Iter: 302500 training acc: 0.25
Global Iter: 302600 training loss: 1.95464
Global Iter: 302600 training acc: 0.1875
Global Iter: 302700 training loss: 2.01189
Global Iter: 302700 training acc: 0.21875
Global Iter: 302800 training loss: 1.92906
Global Iter: 302800 training acc: 0.15625
Global Iter: 302900 training loss: 1.94612
Global Iter: 302900 training acc: 0.1875
Global Iter: 303000 training loss: 1.93941
Global Iter: 303000 training acc: 0.1875
Global Iter: 303100 training loss: 1.90526
Global Iter: 303100 training acc: 0.15625
Global Iter: 303200 training loss: 2.04022
Global Iter: 303200 training acc: 0.125
Global Iter: 303300 training loss: 1.98517
Global Iter: 303300 training acc: 0.0625
Global Iter: 303400 training loss: 1.9402
Global Iter: 303400 training acc: 0.25
Global Iter: 303500 training loss: 2.00861
Global Iter: 303500 training acc: 0.125
Global Iter: 303600 training loss: 2.04318
Global Iter: 303600 training acc: 0.15625
Global Iter: 303700 training loss: 1.90035
Global Iter: 303700 training acc: 0.25
Global Iter: 303800 training loss: 1.94341
Global Iter: 303800 training acc: 0.1875
Global Iter: 303900 training loss: 2.15746
Global Iter: 303900 training acc: 0.09375
Global Iter: 304000 training loss: 2.02548
Global Iter: 304000 training acc: 0.1875
Global Iter: 304100 training loss: 1.99095
Global Iter: 304100 training acc: 0.28125
Global Iter: 304200 training loss: 2.03534
Global Iter: 304200 training acc: 0.0625
Global Iter: 304300 training loss: 1.95287
Global Iter: 304300 training acc: 0.15625
Global Iter: 304400 training loss: 1.83941
Global Iter: 304400 training acc: 0.40625
Global Iter: 304500 training loss: 1.93422
Global Iter: 304500 training acc: 0.25
Global Iter: 304600 training loss: 2.05172
Global Iter: 304600 training acc: 0.125
Global Iter: 304700 training loss: 1.93381
Global Iter: 304700 training acc: 0.1875
Global Iter: 304800 training loss: 1.98121
Global Iter: 304800 training acc: 0.21875
Global Iter: 304900 training loss: 1.92943
Global Iter: 304900 training acc: 0.3125
Global Iter: 305000 training loss: 1.97914
Global Iter: 305000 training acc: 0.125
Global Iter: 305100 training loss: 2.0567
Global Iter: 305100 training acc: 0.09375
Global Iter: 305200 training loss: 2.05338
Global Iter: 305200 training acc: 0.125
Global Iter: 305300 training loss: 2.30263
Global Iter: 305300 training acc: 0.03125
Global Iter: 305400 training loss: 1.94344
Global Iter: 305400 training acc: 0.125
Global Iter: 305500 training loss: 2.03752
Global Iter: 305500 training acc: 0.125
Global Iter: 305600 training loss: 2.04347
Global Iter: 305600 training acc: 0.21875
Global Iter: 305700 training loss: 1.97531
Global Iter: 305700 training acc: 0.125
Global Iter: 305800 training loss: 2.00391
Global Iter: 305800 training acc: 0.09375
Global Iter: 305900 training loss: 2.05591
Global Iter: 305900 training acc: 0.15625
Global Iter: 306000 training loss: 1.95938
Global Iter: 306000 training acc: 0.1875
Global Iter: 306100 training loss: 1.96332
Global Iter: 306100 training acc: 0.1875
Global Iter: 306200 training loss: 1.99178
Global Iter: 306200 training acc: 0.15625
Global Iter: 306300 training loss: 2.04188
Global Iter: 306300 training acc: 0.1875
Global Iter: 306400 training loss: 2.00299
Global Iter: 306400 training acc: 0.1875
Global Iter: 306500 training loss: 2.00062
Global Iter: 306500 training acc: 0.21875
Global Iter: 306600 training loss: 1.90262
Global Iter: 306600 training acc: 0.28125
Global Iter: 306700 training loss: 2.06347
Global Iter: 306700 training acc: 0.1875
Global Iter: 306800 training loss: 1.94807
Global Iter: 306800 training acc: 0.21875
Global Iter: 306900 training loss: 1.95637
Global Iter: 306900 training acc: 0.15625
Global Iter: 307000 training loss: 1.99988
Global Iter: 307000 training acc: 0.21875
Global Iter: 307100 training loss: 2.11333
Global Iter: 307100 training acc: 0.03125
Global Iter: 307200 training loss: 2.05272
Global Iter: 307200 training acc: 0.25
Global Iter: 307300 training loss: 1.99349
Global Iter: 307300 training acc: 0.0625
Global Iter: 307400 training loss: 1.89818
Global Iter: 307400 training acc: 0.21875
Global Iter: 307500 training loss: 1.99723
Global Iter: 307500 training acc: 0.125
Global Iter: 307600 training loss: 2.05846
Global Iter: 307600 training acc: 0.125
Global Iter: 307700 training loss: 1.97966
Global Iter: 307700 training acc: 0.1875
Global Iter: 307800 training loss: 2.02344
Global Iter: 307800 training acc: 0.15625
Global Iter: 307900 training loss: 1.98821
Global Iter: 307900 training acc: 0.21875
Global Iter: 308000 training loss: 2.00897
Global Iter: 308000 training acc: 0.0625
Global Iter: 308100 training loss: 1.97679
Global Iter: 308100 training acc: 0.21875
Global Iter: 308200 training loss: 2.05552
Global Iter: 308200 training acc: 0.15625
Global Iter: 308300 training loss: 1.89722
Global Iter: 308300 training acc: 0.3125
Global Iter: 308400 training loss: 1.99629
Global Iter: 308400 training acc: 0.1875
Global Iter: 308500 training loss: 1.89466
Global Iter: 308500 training acc: 0.21875
Global Iter: 308600 training loss: 1.99792
Global Iter: 308600 training acc: 0.15625
Global Iter: 308700 training loss: 2.06193
Global Iter: 308700 training acc: 0.125
Global Iter: 308800 training loss: 2.01451
Global Iter: 308800 training acc: 0.09375
Global Iter: 308900 training loss: 2.08735
Global Iter: 308900 training acc: 0.09375
Global Iter: 309000 training loss: 2.05423
Global Iter: 309000 training acc: 0.0625
Global Iter: 309100 training loss: 2.15312
Global Iter: 309100 training acc: 0.125
Global Iter: 309200 training loss: 2.04592
Global Iter: 309200 training acc: 0.125
Global Iter: 309300 training loss: 1.95593
Global Iter: 309300 training acc: 0.15625
Global Iter: 309400 training loss: 1.98718
Global Iter: 309400 training acc: 0.1875
Global Iter: 309500 training loss: 2.03265
Global Iter: 309500 training acc: 0.15625
Global Iter: 309600 training loss: 2.14476
Global Iter: 309600 training acc: 0.125
Global Iter: 309700 training loss: 2.00554
Global Iter: 309700 training acc: 0.15625
Global Iter: 309800 training loss: 1.93211
Global Iter: 309800 training acc: 0.15625
Global Iter: 309900 training loss: 2.06581
Global Iter: 309900 training acc: 0.15625
Global Iter: 310000 training loss: 2.06287
Global Iter: 310000 training acc: 0.125
Global Iter: 310100 training loss: 2.04856
Global Iter: 310100 training acc: 0.15625
Global Iter: 310200 training loss: 2.05703
Global Iter: 310200 training acc: 0.03125
Global Iter: 310300 training loss: 1.90985
Global Iter: 310300 training acc: 0.25
Global Iter: 310400 training loss: 1.91162
Global Iter: 310400 training acc: 0.3125
Global Iter: 310500 training loss: 1.91819
Global Iter: 310500 training acc: 0.28125
Global Iter: 310600 training loss: 2.03109
Global Iter: 310600 training acc: 0.125
Global Iter: 310700 training loss: 1.99058
Global Iter: 310700 training acc: 0.125
Global Iter: 310800 training loss: 1.92077
Global Iter: 310800 training acc: 0.15625
Global Iter: 310900 training loss: 2.03379
Global Iter: 310900 training acc: 0.1875
Global Iter: 311000 training loss: 1.99528
Global Iter: 311000 training acc: 0.21875
Global Iter: 311100 training loss: 1.99443
Global Iter: 311100 training acc: 0.25
Global Iter: 311200 training loss: 1.94658
Global Iter: 311200 training acc: 0.15625
Global Iter: 311300 training loss: 2.03216
Global Iter: 311300 training acc: 0.1875
Global Iter: 311400 training loss: 2.1039
Global Iter: 311400 training acc: 0.125
Global Iter: 311500 training loss: 2.02455
Global Iter: 311500 training acc: 0.1875
Global Iter: 311600 training loss: 1.89757
Global Iter: 311600 training acc: 0.21875
Global Iter: 311700 training loss: 1.98214
Global Iter: 311700 training acc: 0.15625
Global Iter: 311800 training loss: 1.95027
Global Iter: 311800 training acc: 0.09375
Global Iter: 311900 training loss: 1.88988
Global Iter: 311900 training acc: 0.21875
Global Iter: 312000 training loss: 1.98175
Global Iter: 312000 training acc: 0.21875
Global Iter: 312100 training loss: 1.98081
Global Iter: 312100 training acc: 0.25
Global Iter: 312200 training loss: 2.0396
Global Iter: 312200 training acc: 0.125
Global Iter: 312300 training loss: 1.96545
Global Iter: 312300 training acc: 0.15625
Global Iter: 312400 training loss: 2.01662
Global Iter: 312400 training acc: 0.25
Global Iter: 312500 training loss: 1.99525
Global Iter: 312500 training acc: 0.25
Global Iter: 312600 training loss: 1.99318
Global Iter: 312600 training acc: 0.21875
Global Iter: 312700 training loss: 1.98006
Global Iter: 312700 training acc: 0.15625
Global Iter: 312800 training loss: 1.9542
Global Iter: 312800 training acc: 0.25
Global Iter: 312900 training loss: 1.99795
Global Iter: 312900 training acc: 0.21875
Global Iter: 313000 training loss: 2.02394
Global Iter: 313000 training acc: 0.125
Global Iter: 313100 training loss: 1.90048
Global Iter: 313100 training acc: 0.21875
Global Iter: 313200 training loss: 1.92315
Global Iter: 313200 training acc: 0.15625
Global Iter: 313300 training loss: 1.92382
Global Iter: 313300 training acc: 0.21875
Global Iter: 313400 training loss: 1.98185
Global Iter: 313400 training acc: 0.1875
Global Iter: 313500 training loss: 1.91659
Global Iter: 313500 training acc: 0.21875
Global Iter: 313600 training loss: 1.97403
Global Iter: 313600 training acc: 0.09375
Global Iter: 313700 training loss: 2.08545
Global Iter: 313700 training acc: 0.1875
Global Iter: 313800 training loss: 1.94319
Global Iter: 313800 training acc: 0.1875
Global Iter: 313900 training loss: 1.99732
Global Iter: 313900 training acc: 0.28125
Global Iter: 314000 training loss: 1.98756
Global Iter: 314000 training acc: 0.125
Global Iter: 314100 training loss: 1.91654
Global Iter: 314100 training acc: 0.15625
Global Iter: 314200 training loss: 2.04163
Global Iter: 314200 training acc: 0.21875
Global Iter: 314300 training loss: 1.96967
Global Iter: 314300 training acc: 0.125
Global Iter: 314400 training loss: 2.03756
Global Iter: 314400 training acc: 0.15625
Global Iter: 314500 training loss: 1.80988
Global Iter: 314500 training acc: 0.40625
Global Iter: 314600 training loss: 1.97302
Global Iter: 314600 training acc: 0.1875
Global Iter: 314700 training loss: 1.99354
Global Iter: 314700 training acc: 0.21875
Global Iter: 314800 training loss: 1.95664
Global Iter: 314800 training acc: 0.15625
Global Iter: 314900 training loss: 1.9479
Global Iter: 314900 training acc: 0.15625
Global Iter: 315000 training loss: 1.97129
Global Iter: 315000 training acc: 0.09375
Global Iter: 315100 training loss: 1.2017-06-21 13:50:48.565293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-317924
98007
Global Iter: 315100 training acc: 0.21875
Global Iter: 315200 training loss: 1.95927
Global Iter: 315200 training acc: 0.125
Global Iter: 315300 training loss: 1.97509
Global Iter: 315300 training acc: 0.28125
Global Iter: 315400 training loss: 1.84272
Global Iter: 315400 training acc: 0.34375
Global Iter: 315500 training loss: 1.9592
Global Iter: 315500 training acc: 0.25
Global Iter: 315600 training loss: 2.02552
Global Iter: 315600 training acc: 0.125
Global Iter: 315700 training loss: 2.05219
Global Iter: 315700 training acc: 0.0625
Global Iter: 315800 training loss: 1.88968
Global Iter: 315800 training acc: 0.34375
Global Iter: 315900 training loss: 1.90139
Global Iter: 315900 training acc: 0.25
Global Iter: 316000 training loss: 2.04644
Global Iter: 316000 training acc: 0.09375
Global Iter: 316100 training loss: 2.04323
Global Iter: 316100 training acc: 0.125
Global Iter: 316200 training loss: 1.97834
Global Iter: 316200 training acc: 0.21875
Global Iter: 316300 training loss: 1.95819
Global Iter: 316300 training acc: 0.21875
Global Iter: 316400 training loss: 2.07115
Global Iter: 316400 training acc: 0.125
Global Iter: 316500 training loss: 1.97162
Global Iter: 316500 training acc: 0.125
Global Iter: 316600 training loss: 2.09696
Global Iter: 316600 training acc: 0.125
Global Iter: 316700 training loss: 1.91968
Global Iter: 316700 training acc: 0.28125
Global Iter: 316800 training loss: 2.04247
Global Iter: 316800 training acc: 0.15625
Global Iter: 316900 training loss: 2.02313
Global Iter: 316900 training acc: 0.15625
Global Iter: 317000 training loss: 2.00811
Global Iter: 317000 training acc: 0.15625
Global Iter: 317100 training loss: 1.92719
Global Iter: 317100 training acc: 0.3125
Global Iter: 317200 training loss: 1.93755
Global Iter: 317200 training acc: 0.1875
Global Iter: 317300 training loss: 1.92927
Global Iter: 317300 training acc: 0.28125
Global Iter: 317400 training loss: 2.07119
Global Iter: 317400 training acc: 0.21875
Global Iter: 317500 training loss: 1.98027
Global Iter: 317500 training acc: 0.15625
Global Iter: 317600 training loss: 1.93259
Global Iter: 317600 training acc: 0.1875
Global Iter: 317700 training loss: 2.08069
Global Iter: 317700 training acc: 0.1875
Global Iter: 317800 training loss: 2.0082
Global Iter: 317800 training acc: 0.15625
Global Iter: 317900 training loss: 1.9671
Global Iter: 317900 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-317924
Number of Patches: 256516
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-317924
Global Iter: 318000 training loss: 1.99533
Global Iter: 318000 training acc: 0.09375
Global Iter: 318100 training loss: 1.94758
Global Iter: 318100 training acc: 0.25
Global Iter: 318200 training loss: 1.90749
Global Iter: 318200 training acc: 0.28125
Global Iter: 318300 training loss: 1.92046
Global Iter: 318300 training acc: 0.15625
Global Iter: 318400 training loss: 2.02286
Global Iter: 318400 training acc: 0.125
Global Iter: 318500 training loss: 1.88938
Global Iter: 318500 training acc: 0.40625
Global Iter: 318600 training loss: 1.94673
Global Iter: 318600 training acc: 0.15625
Global Iter: 318700 training loss: 1.89979
Global Iter: 318700 training acc: 0.28125
Global Iter: 318800 training loss: 1.86892
Global Iter: 318800 training acc: 0.375
Global Iter: 318900 training loss: 1.95543
Global Iter: 318900 training acc: 0.21875
Global Iter: 319000 training loss: 2.2011
Global Iter: 319000 training acc: 0.03125
Global Iter: 319100 training loss: 1.9393
Global Iter: 319100 training acc: 0.28125
Global Iter: 319200 training loss: 1.97926
Global Iter: 319200 training acc: 0.1875
Global Iter: 319300 training loss: 1.86178
Global Iter: 319300 training acc: 0.25
Global Iter: 319400 training loss: 1.99766
Global Iter: 319400 training acc: 0.125
Global Iter: 319500 training loss: 1.89755
Global Iter: 319500 training acc: 0.28125
Global Iter: 319600 training loss: 2.01716
Global Iter: 319600 training acc: 0.1875
Global Iter: 319700 training loss: 1.95162
Global Iter: 319700 training acc: 0.15625
Global Iter: 319800 training loss: 2.01827
Global Iter: 319800 training acc: 0.125
Global Iter: 319900 training loss: 2.0627
Global Iter: 319900 training acc: 0.21875
Global Iter: 320000 training loss: 1.9288
Global Iter: 320000 training acc: 0.21875
Global Iter: 320100 training loss: 1.9025
Global Iter: 320100 training acc: 0.21875
Global Iter: 320200 training loss: 1.88569
Global Iter: 320200 training acc: 0.375
Global Iter: 320300 training loss: 2.02232
Global Iter: 320300 training acc: 0.21875
Global Iter: 320400 training loss: 1.98962
Global Iter: 320400 training acc: 0.09375
Global Iter: 320500 training loss: 1.96296
Global Iter: 320500 training acc: 0.15625
Global Iter: 320600 training loss: 1.9831
Global Iter: 320600 training acc: 0.0625
Global Iter: 320700 training loss: 1.99305
Global Iter: 320700 training acc: 0.15625
Global Iter: 320800 training loss: 1.97931
Global Iter: 320800 training acc: 0.21875
Global Iter: 320900 training loss: 2.05615
Global Iter: 320900 training acc: 0.125
Global Iter: 321000 training loss: 1.98359
Global Iter: 321000 training acc: 0.125
Global Iter: 321100 training loss: 2.0968
Global Iter: 321100 training acc: 0.1875
Global Iter: 321200 training loss: 1.96093
Global Iter: 321200 training acc: 0.1875
Global Iter: 321300 training loss: 2.00817
Global Iter: 321300 training acc: 0.1875
Global Iter: 321400 training loss: 1.97514
Global Iter: 321400 training acc: 0.1875
Global Iter: 321500 training loss: 1.97296
Global Iter: 321500 training acc: 0.375
Global Iter: 321600 training loss: 2.02705
Global Iter: 321600 training acc: 0.15625
Global Iter: 321700 training loss: 1.95408
Global Iter: 321700 training acc: 0.15625
Global Iter: 321800 training loss: 1.89824
Global Iter: 321800 training acc: 0.21875
Global Iter: 321900 training loss: 2.06064
Global Iter: 321900 training acc: 0.15625
Global Iter: 322000 training loss: 1.93839
Global Iter: 322000 training acc: 0.15625
Global Iter: 322100 training loss: 2.00194
Global Iter: 322100 training acc: 0.15625
Global Iter: 322200 training loss: 2.02605
Global Iter: 322200 training acc: 0.1875
Global Iter: 322300 training loss: 1.91415
Global Iter: 322300 training acc: 0.28125
Global Iter: 322400 training loss: 1.97912
Global Iter: 322400 training acc: 0.21875
Global Iter: 322500 training loss: 1.95269
Global Iter: 322500 training acc: 0.25
Global Iter: 322600 training loss: 1.88201
Global Iter: 322600 training acc: 0.28125
Global Iter: 322700 training loss: 1.99238
Global Iter: 322700 training acc: 0.1875
Global Iter: 322800 training loss: 1.94513
Global Iter: 322800 training acc: 0.15625
Global Iter: 322900 training loss: 2.03858
Global Iter: 322900 training acc: 0.15625
Global Iter: 323000 training loss: 1.96779
Global Iter: 323000 training acc: 0.21875
Global Iter: 323100 training loss: 1.94916
Global Iter: 323100 training acc: 0.15625
Global Iter: 323200 training loss: 2.01655
Global Iter: 323200 training acc: 0.15625
Global Iter: 323300 training loss: 2.07049
Global Iter: 323300 training acc: 0.125
Global Iter: 323400 training loss: 2.04039
Global Iter: 323400 training acc: 0.15625
Global Iter: 323500 training loss: 1.92876
Global Iter: 323500 training acc: 0.25
Global Iter: 323600 training loss: 1.94606
Global Iter: 323600 training acc: 0.1875
Global Iter: 323700 training loss: 1.97394
Global Iter: 323700 training acc: 0.15625
Global Iter: 323800 training loss: 1.98636
Global Iter: 323800 training acc: 0.0625
Global Iter: 323900 training loss: 1.99915
Global Iter: 323900 training acc: 0.15625
Global Iter: 324000 training loss: 1.91911
Global Iter: 324000 training acc: 0.21875
Global Iter: 324100 training loss: 2.0243
Global Iter: 324100 training acc: 0.21875
Global Iter: 324200 training loss: 1.93199
Global Iter: 324200 training acc: 0.15625
Global Iter: 324300 training loss: 1.94356
Global Iter: 324300 training acc: 0.3125
Global Iter: 324400 training loss: 1.89631
Global Iter: 324400 training acc: 0.28125
Global Iter: 324500 training loss: 1.98405
Global Iter: 324500 training acc: 0.125
Global Iter: 324600 training loss: 1.92749
Global Iter: 324600 training acc: 0.25
Global Iter: 324700 training loss: 2.04501
Global Iter: 324700 training acc: 0.125
Global Iter: 324800 training loss: 2.03587
Global Iter: 324800 training acc: 0.0625
Global Iter: 324900 training loss: 2.05334
Global Iter: 324900 training acc: 0.25
Global Iter: 325000 training loss: 1.96317
Global Iter: 325000 training acc: 0.15625
Global Iter: 325100 training loss: 1.96632
Global Iter: 325100 training acc: 0.0625
Global Iter: 325200 training loss: 2.07476
Global Iter: 325200 training acc: 0.15625
Global Iter: 325300 training loss: 1.99189
Global Iter: 325300 training acc: 0.15625
Global Iter: 325400 training loss: 1.94944
Global Iter: 325400 training acc: 0.25
Global Iter: 325500 training loss: 2.05659
Global Iter: 325500 training acc: 0.21875
Global Iter: 325600 training loss: 1.91625
Global Iter: 325600 training acc: 0.25
Global Iter: 325700 training loss: 1.94405
Global Iter: 325700 training acc: 0.3125
Global Iter: 325800 training loss: 2.09089
Global Iter: 325800 training acc: 0.125
Global Iter: 325900 training loss: 2.00576
Global Iter: 325900 training acc: 0.15625
Global Iter: 326000 training loss: 1.92362
Global Iter: 326000 training acc: 0.15625
Global Iter: 326100 training loss: 1.92964
Global Iter: 326100 training acc: 0.1875
Global Iter: 326200 training loss: 1.88239
Global Iter: 326200 training acc: 0.28125
Global Iter: 326300 training loss: 1.9772
Global Iter: 326300 training acc: 0.21875
Global Iter: 326400 training loss: 1.88447
Global Iter: 326400 training acc: 0.3125
Global Iter: 326500 training loss: 1.94724
Global Iter: 326500 training acc: 0.1875
Global Iter: 326600 training loss: 1.9684
Global Iter: 326600 training acc: 0.09375
Global Iter: 326700 training loss: 2.07367
Global Iter: 326700 training acc: 0.09375
Global Iter: 326800 training loss: 2.0133
Global Iter: 326800 training acc: 0.15625
Global Iter: 326900 training loss: 2.01401
Global Iter: 326900 training acc: 0.21875
Global Iter: 327000 training loss: 1.92653
Global Iter: 327000 training acc: 0.21875
Global Iter: 327100 training loss: 1.95443
Global Iter: 327100 training acc: 0.3125
Global Iter: 327200 training loss: 1.87795
Global Iter: 327200 training acc: 0.375
Global Iter: 327300 training loss: 1.97896
Global Iter: 327300 training acc: 0.25
Global Iter: 327400 training loss: 1.99051
Global Iter: 327400 training acc: 0.25
Global Iter: 327500 training loss: 1.97193
Global Iter: 327500 training acc: 0.1875
Global Iter: 327600 training loss: 1.9332
Global Iter: 327600 training acc: 0.1875
Global Iter: 327700 training loss: 1.91466
Global Iter: 327700 training acc: 0.3125
Global Iter: 327800 training loss: 1.93121
Global Iter: 327800 training acc: 0.3125
Global Iter: 327900 training loss: 1.90252
Global Iter: 327900 training acc: 0.25
Global Iter: 328000 training loss: 2.06105
Global Iter: 328000 training acc: 0.03125
Global Iter: 328100 training loss: 1.98554
Global Iter: 328100 training acc: 0.25
Global Iter: 328200 training loss: 1.94487
Global Iter: 328200 training acc: 0.15625
Global Iter: 328300 training loss: 2.01633
Global Iter: 328300 training acc: 0.21875
Global Iter: 328400 training loss: 1.99298
Global Iter: 328400 training acc: 0.15625
Global Iter: 328500 training loss: 1.99175
Global Iter: 328500 training acc: 0.21875
Global Iter: 328600 training loss: 1.99917
Global Iter: 328600 training acc: 0.15625
Global Iter: 328700 training loss: 1.95078
Global Iter: 328700 training acc: 0.21875
Global Iter: 328800 training loss: 2.01368
Global Iter: 328800 training acc: 0.15625
Global Iter: 328900 training loss: 2.0589
Global Iter: 328900 training acc: 0.125
Global Iter: 329000 training loss: 1.95604
Global Iter: 329000 training acc: 0.21875
Global Iter: 329100 training loss: 1.9445
Global Iter: 329100 training acc: 0.21875
Global Iter: 329200 training loss: 1.9048
Global Iter: 329200 training acc: 0.15625
Global Iter: 329300 training loss: 2.04959
Global Iter: 329300 training acc: 0.15625
Global Iter: 329400 training loss: 1.99201
Global Iter: 329400 training acc2017-06-21 14:18:59.748427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-333957
: 0.25
Global Iter: 329500 training loss: 1.94493
Global Iter: 329500 training acc: 0.21875
Global Iter: 329600 training loss: 1.99653
Global Iter: 329600 training acc: 0.25
Global Iter: 329700 training loss: 1.91715
Global Iter: 329700 training acc: 0.21875
Global Iter: 329800 training loss: 2.04254
Global Iter: 329800 training acc: 0.15625
Global Iter: 329900 training loss: 1.9932
Global Iter: 329900 training acc: 0.1875
Global Iter: 330000 training loss: 2.07001
Global Iter: 330000 training acc: 0.15625
Global Iter: 330100 training loss: 1.93284
Global Iter: 330100 training acc: 0.15625
Global Iter: 330200 training loss: 2.04933
Global Iter: 330200 training acc: 0.125
Global Iter: 330300 training loss: 1.96662
Global Iter: 330300 training acc: 0.1875
Global Iter: 330400 training loss: 2.18489
Global Iter: 330400 training acc: 0.03125
Global Iter: 330500 training loss: 1.97179
Global Iter: 330500 training acc: 0.15625
Global Iter: 330600 training loss: 1.96438
Global Iter: 330600 training acc: 0.125
Global Iter: 330700 training loss: 1.98124
Global Iter: 330700 training acc: 0.21875
Global Iter: 330800 training loss: 1.9821
Global Iter: 330800 training acc: 0.09375
Global Iter: 330900 training loss: 1.99747
Global Iter: 330900 training acc: 0.21875
Global Iter: 331000 training loss: 1.90239
Global Iter: 331000 training acc: 0.21875
Global Iter: 331100 training loss: 2.06557
Global Iter: 331100 training acc: 0.125
Global Iter: 331200 training loss: 1.95372
Global Iter: 331200 training acc: 0.09375
Global Iter: 331300 training loss: 1.8973
Global Iter: 331300 training acc: 0.15625
Global Iter: 331400 training loss: 2.00852
Global Iter: 331400 training acc: 0.3125
Global Iter: 331500 training loss: 2.08226
Global Iter: 331500 training acc: 0.15625
Global Iter: 331600 training loss: 1.99321
Global Iter: 331600 training acc: 0.09375
Global Iter: 331700 training loss: 2.00493
Global Iter: 331700 training acc: 0.1875
Global Iter: 331800 training loss: 2.10557
Global Iter: 331800 training acc: 0.09375
Global Iter: 331900 training loss: 2.0343
Global Iter: 331900 training acc: 0.09375
Global Iter: 332000 training loss: 1.95212
Global Iter: 332000 training acc: 0.1875
Global Iter: 332100 training loss: 1.95809
Global Iter: 332100 training acc: 0.09375
Global Iter: 332200 training loss: 1.98466
Global Iter: 332200 training acc: 0.1875
Global Iter: 332300 training loss: 2.17657
Global Iter: 332300 training acc: 0.21875
Global Iter: 332400 training loss: 1.91836
Global Iter: 332400 training acc: 0.28125
Global Iter: 332500 training loss: 2.07872
Global Iter: 332500 training acc: 0.125
Global Iter: 332600 training loss: 1.92585
Global Iter: 332600 training acc: 0.15625
Global Iter: 332700 training loss: 2.05872
Global Iter: 332700 training acc: 0.125
Global Iter: 332800 training loss: 1.94218
Global Iter: 332800 training acc: 0.21875
Global Iter: 332900 training loss: 1.95553
Global Iter: 332900 training acc: 0.125
Global Iter: 333000 training loss: 2.00101
Global Iter: 333000 training acc: 0.21875
Global Iter: 333100 training loss: 1.99666
Global Iter: 333100 training acc: 0.125
Global Iter: 333200 training loss: 1.98313
Global Iter: 333200 training acc: 0.34375
Global Iter: 333300 training loss: 1.94836
Global Iter: 333300 training acc: 0.3125
Global Iter: 333400 training loss: 1.93005
Global Iter: 333400 training acc: 0.28125
Global Iter: 333500 training loss: 1.98197
Global Iter: 333500 training acc: 0.25
Global Iter: 333600 training loss: 1.99162
Global Iter: 333600 training acc: 0.1875
Global Iter: 333700 training loss: 1.89167
Global Iter: 333700 training acc: 0.1875
Global Iter: 333800 training loss: 2.22598
Global Iter: 333800 training acc: 0.09375
Global Iter: 333900 training loss: 2.06076
Global Iter: 333900 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-333957
Number of Patches: 253951
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-333957
Global Iter: 334000 training loss: 1.9972
Global Iter: 334000 training acc: 0.125
Global Iter: 334100 training loss: 2.0703
Global Iter: 334100 training acc: 0.21875
Global Iter: 334200 training loss: 2.05105
Global Iter: 334200 training acc: 0.25
Global Iter: 334300 training loss: 2.0217
Global Iter: 334300 training acc: 0.125
Global Iter: 334400 training loss: 2.0362
Global Iter: 334400 training acc: 0.21875
Global Iter: 334500 training loss: 1.99276
Global Iter: 334500 training acc: 0.1875
Global Iter: 334600 training loss: 1.97215
Global Iter: 334600 training acc: 0.0
Global Iter: 334700 training loss: 2.07005
Global Iter: 334700 training acc: 0.1875
Global Iter: 334800 training loss: 2.20615
Global Iter: 334800 training acc: 0.15625
Global Iter: 334900 training loss: 2.06816
Global Iter: 334900 training acc: 0.15625
Global Iter: 335000 training loss: 2.06742
Global Iter: 335000 training acc: 0.15625
Global Iter: 335100 training loss: 2.11848
Global Iter: 335100 training acc: 0.21875
Global Iter: 335200 training loss: 1.95077
Global Iter: 335200 training acc: 0.21875
Global Iter: 335300 training loss: 2.05625
Global Iter: 335300 training acc: 0.1875
Global Iter: 335400 training loss: 2.08318
Global Iter: 335400 training acc: 0.09375
Global Iter: 335500 training loss: 1.95761
Global Iter: 335500 training acc: 0.1875
Global Iter: 335600 training loss: 1.98763
Global Iter: 335600 training acc: 0.1875
Global Iter: 335700 training loss: 1.97898
Global Iter: 335700 training acc: 0.21875
Global Iter: 335800 training loss: 1.93807
Global Iter: 335800 training acc: 0.1875
Global Iter: 335900 training loss: 1.99413
Global Iter: 335900 training acc: 0.3125
Global Iter: 336000 training loss: 2.0916
Global Iter: 336000 training acc: 0.15625
Global Iter: 336100 training loss: 2.01849
Global Iter: 336100 training acc: 0.3125
Global Iter: 336200 training loss: 1.92421
Global Iter: 336200 training acc: 0.21875
Global Iter: 336300 training loss: 1.96141
Global Iter: 336300 training acc: 0.125
Global Iter: 336400 training loss: 1.95533
Global Iter: 336400 training acc: 0.1875
Global Iter: 336500 training loss: 1.90254
Global Iter: 336500 training acc: 0.28125
Global Iter: 336600 training loss: 2.03679
Global Iter: 336600 training acc: 0.125
Global Iter: 336700 training loss: 2.11986
Global Iter: 336700 training acc: 0.09375
Global Iter: 336800 training loss: 1.99391
Global Iter: 336800 training acc: 0.15625
Global Iter: 336900 training loss: 2.04091
Global Iter: 336900 training acc: 0.15625
Global Iter: 337000 training loss: 1.90775
Global Iter: 337000 training acc: 0.28125
Global Iter: 337100 training loss: 1.93748
Global Iter: 337100 training acc: 0.1875
Global Iter: 337200 training loss: 1.92572
Global Iter: 337200 training acc: 0.21875
Global Iter: 337300 training loss: 1.939
Global Iter: 337300 training acc: 0.34375
Global Iter: 337400 training loss: 1.91745
Global Iter: 337400 training acc: 0.28125
Global Iter: 337500 training loss: 2.0527
Global Iter: 337500 training acc: 0.125
Global Iter: 337600 training loss: 1.91976
Global Iter: 337600 training acc: 0.1875
Global Iter: 337700 training loss: 1.99485
Global Iter: 337700 training acc: 0.25
Global Iter: 337800 training loss: 1.94102
Global Iter: 337800 training acc: 0.28125
Global Iter: 337900 training loss: 1.96703
Global Iter: 337900 training acc: 0.1875
Global Iter: 338000 training loss: 1.96959
Global Iter: 338000 training acc: 0.21875
Global Iter: 338100 training loss: 1.99305
Global Iter: 338100 training acc: 0.1875
Global Iter: 338200 training loss: 1.96299
Global Iter: 338200 training acc: 0.21875
Global Iter: 338300 training loss: 2.04441
Global Iter: 338300 training acc: 0.21875
Global Iter: 338400 training loss: 2.12392
Global Iter: 338400 training acc: 0.09375
Global Iter: 338500 training loss: 1.96741
Global Iter: 338500 training acc: 0.15625
Global Iter: 338600 training loss: 1.99424
Global Iter: 338600 training acc: 0.15625
Global Iter: 338700 training loss: 1.854
Global Iter: 338700 training acc: 0.28125
Global Iter: 338800 training loss: 1.89835
Global Iter: 338800 training acc: 0.25
Global Iter: 338900 training loss: 2.1072
Global Iter: 338900 training acc: 0.0625
Global Iter: 339000 training loss: 1.96907
Global Iter: 339000 training acc: 0.25
Global Iter: 339100 training loss: 1.99747
Global Iter: 339100 training acc: 0.1875
Global Iter: 339200 training loss: 1.98917
Global Iter: 339200 training acc: 0.09375
Global Iter: 339300 training loss: 1.95056
Global Iter: 339300 training acc: 0.1875
Global Iter: 339400 training loss: 2.02577
Global Iter: 339400 training acc: 0.25
Global Iter: 339500 training loss: 2.03537
Global Iter: 339500 training acc: 0.09375
Global Iter: 339600 training loss: 1.9114
Global Iter: 339600 training acc: 0.34375
Global Iter: 339700 training loss: 1.9508
Global Iter: 339700 training acc: 0.09375
Global Iter: 339800 training loss: 2.18962
Global Iter: 339800 training acc: 0.15625
Global Iter: 339900 training loss: 2.00588
Global Iter: 339900 training acc: 0.3125
Global Iter: 340000 training loss: 1.97254
Global Iter: 340000 training acc: 0.15625
Global Iter: 340100 training loss: 2.04403
Global Iter: 340100 training acc: 0.09375
Global Iter: 340200 training loss: 2.12219
Global Iter: 340200 training acc: 0.15625
Global Iter: 340300 training loss: 2.0562
Global Iter: 340300 training acc: 0.15625
Global Iter: 340400 training loss: 1.87023
Global Iter: 340400 training acc: 0.28125
Global Iter: 340500 training loss: 1.99419
Global Iter: 340500 training acc: 0.1875
Global Iter: 340600 training loss: 1.93438
Global Iter: 340600 training acc: 0.25
Global Iter: 340700 training loss: 2.00763
Global Iter: 340700 training acc: 0.15625
Global Iter: 340800 training loss: 2.0229
Global Iter: 340800 training acc: 0.125
Global Iter: 340900 training loss: 1.94669
Global Iter: 340900 training acc: 0.1875
Global Iter: 341000 training loss: 1.95404
Global Iter: 341000 training acc: 0.1875
Global Iter: 341100 training loss: 1.90506
Global Iter: 341100 training acc: 0.25
Global Iter: 341200 training loss: 2.06216
Global Iter: 341200 training acc: 0.1875
Global Iter: 341300 training loss: 2.04462
Global Iter: 341300 training acc: 0.1875
Global Iter: 341400 training loss: 2.08257
Global Iter: 341400 training acc: 0.125
Global Iter: 341500 training loss: 2.07044
Global Iter: 341500 training acc: 0.125
Global Iter: 341600 training loss: 1.9588
Global Iter: 341600 training acc: 0.15625
Global Iter: 341700 training loss: 2.07855
Global Iter: 341700 training acc: 0.15625
Global Iter: 341800 training loss: 2.07544
Global Iter: 341800 training acc: 0.21875
Global Iter: 341900 training loss: 2.02643
Global Iter: 341900 training acc: 0.15625
Global Iter: 342000 training loss: 1.90781
Global Iter: 342000 training acc: 0.28125
Global Iter: 342100 training loss: 1.99543
Global Iter: 342100 training acc: 0.21875
Global Iter: 342200 training loss: 1.97898
Global Iter: 342200 training acc: 0.28125
Global Iter: 342300 training loss: 1.821
Global Iter: 342300 training acc: 0.375
Global Iter: 342400 training loss: 1.93852
Global Iter: 342400 training acc: 0.125
Global Iter: 342500 training loss: 2.20457
Global Iter: 342500 training acc: 0.03125
Global Iter: 342600 training loss: 2.08298
Global Iter: 342600 training acc: 0.21875
Global Iter: 342700 training loss: 2.11635
Global Iter: 342700 training acc: 0.1875
Global Iter: 342800 training loss: 1.92019
Global Iter: 342800 training acc: 0.28125
Global Iter: 342900 training loss: 2.0502
Global Iter: 342900 training acc: 0.09375
Global Iter: 343000 training loss: 1.98004
Global Iter: 343000 training acc: 0.15625
Global Iter: 343100 training loss: 2.03752
Global Iter: 343100 training acc: 0.28125
Global Iter: 343200 training loss: 1.97132
Global Iter: 343200 training acc: 0.15625
Global Iter: 343300 training loss: 2.08125
Global Iter: 343300 training acc: 0.125
Global Iter: 343400 training loss: 1.99073
Global Iter: 343400 training acc: 0.25
Global Iter: 343500 training loss: 2.04847
Global Iter: 343500 training acc: 0.1875
Global Iter: 343600 training loss: 1.95026
Global Iter: 343600 training acc: 0.1875
Global Iter: 343700 training loss: 1.93253
Global Iter: 343700 training acc: 0.34375
Global Iter: 343800 training loss: 2.02774
Global Iter: 343800 training acc: 0.21875
Global Iter: 343900 training loss: 1.946
Global Iter: 343900 training acc: 0.1875
Global Iter: 344000 training loss: 2.06484
Global Iter: 344000 training acc: 0.25
Global Iter: 344100 training loss: 2.02924
Global Iter: 344100 training acc: 0.21875
Global Iter: 344200 training loss: 1.96184
Global Iter: 344200 training acc: 0.09375
Global Iter: 344300 training loss: 2.00398
Global Iter: 344300 training acc: 0.21875
Global Iter: 344400 training loss: 2.04388
Global Iter: 344400 training acc: 0.0625
Global Iter: 344500 training loss: 1.95472
Global Iter: 344500 training acc: 0.1875
Global Iter: 344600 training loss: 1.96227
Global Iter: 344600 training acc: 0.28125
Global Iter: 344700 training loss: 1.99019
Global Iter: 344700 training acc: 0.15625
Global Iter: 344800 training loss: 1.94258
Global Iter: 344800 training acc: 0.125
Global Iter: 344900 training loss: 2.04144
Global Iter: 344900 training acc: 0.125
Global Iter: 345000 training loss: 1.95709
Global Iter: 345000 training acc: 0.3125
Global Iter: 345100 training loss: 2.02372
Global Iter: 345100 training acc: 0.15625
Global Iter: 345200 training loss: 1.85275
Global Iter: 345200 training acc: 0.4375
Global Iter: 345300 training loss: 2.00682
Global Iter: 345300 training acc: 0.28125
Global Iter: 345400 training loss: 2.07779
Global Iter: 345400 training acc: 0.15625
Global Iter: 345500 training loss: 1.9519
Global Iter: 345500 training acc: 0.1875
Global Iter: 345600 training loss: 1.95526
Global Iter: 345600 training acc: 0.34375
Global Iter: 345700 training loss: 2.06533
Global Iter: 345700 training acc: 0.09375
Global Iter: 345800 training loss: 2.01662
Global Iter: 345800 training acc: 0.15625
Global Iter: 345900 training loss: 1.96853
Global Iter: 345900 training acc: 0.28125
Global Iter: 346000 training loss: 1.91002
Global Iter: 346000 training acc: 0.1875
Global Iter: 346100 training loss: 2.05184
Global Iter: 346100 training acc: 0.25
Global Iter: 346200 training loss: 2.03599
Global Iter: 346200 training acc: 0.125
Global Iter: 346300 training loss: 2.04309
Global Iter: 346300 training acc: 0.15625
Global Iter: 346400 training loss: 1.90666
Global Iter: 346400 training acc: 0.15625
Global Iter: 346500 training loss: 1.99416
Global Iter: 346500 training acc: 0.125
Global Iter: 346600 training loss: 2.01084
Global Iter: 346600 training acc: 0.15625
Global Iter: 346700 training loss: 1.98341
Global Iter: 346700 training acc: 0.28125
Global Iter: 346800 training loss: 1.94867
Global Iter: 346800 training acc: 0.15625
Global Iter: 346900 training loss: 1.93862
Global Iter: 346900 training acc: 0.21875
Global Iter: 347000 training loss: 2.00372
Global Iter: 347000 training acc: 0.15625
Global Iter: 347100 training loss: 1.93846
Global Iter: 347100 training acc: 0.1875
Global Iter: 347200 training loss: 2.11972
Global Iter: 347200 training acc: 0.1875
Global Iter: 347300 training loss: 1.9475
Global Iter: 347300 training acc: 0.125
Global Iter: 347400 training loss: 1.95363
Global Iter: 347400 training acc: 0.15625
Global Iter: 347500 training loss: 2.03089
Global Iter: 347500 training acc: 0.125
Global Iter: 347600 training loss: 2.04199
Global Iter: 347600 training acc: 0.3125
Global Iter: 347700 training loss: 1.93972
Global Iter: 347700 training acc: 0.15625
Global Iter: 347800 training loss: 1.96765
Global Iter: 347800 training acc: 0.1875
Global Iter: 347900 training loss: 1.96644
Global Iter: 347900 training acc: 0.15625
Global Iter: 348000 training loss: 2.10008
Global Iter: 348000 training acc: 0.21875
Global Iter: 348100 training loss: 1.9306
Global Iter: 348100 training acc: 0.1875
Global Iter: 348200 training loss: 1.91773
Global Iter: 348200 training acc: 0.1875
Global Iter: 348300 training loss: 2.05514
Global Iter: 348300 training acc: 0.1875
Global Iter: 348400 training loss: 1.97668
Global Iter: 348400 training acc: 0.21875
Global Iter: 348500 training loss: 2.05506
Global Iter: 348500 training acc: 0.15625
Global Iter: 348600 training loss: 2.05329
Global Iter: 348600 training acc: 0.06252017-06-21 14:46:42.186005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-349829

Global Iter: 348700 training loss: 1.99476
Global Iter: 348700 training acc: 0.15625
Global Iter: 348800 training loss: 1.96277
Global Iter: 348800 training acc: 0.28125
Global Iter: 348900 training loss: 1.95877
Global Iter: 348900 training acc: 0.1875
Global Iter: 349000 training loss: 2.04594
Global Iter: 349000 training acc: 0.15625
Global Iter: 349100 training loss: 1.88231
Global Iter: 349100 training acc: 0.1875
Global Iter: 349200 training loss: 1.8882
Global Iter: 349200 training acc: 0.1875
Global Iter: 349300 training loss: 1.91714
Global Iter: 349300 training acc: 0.21875
Global Iter: 349400 training loss: 1.99247
Global Iter: 349400 training acc: 0.25
Global Iter: 349500 training loss: 2.03483
Global Iter: 349500 training acc: 0.09375
Global Iter: 349600 training loss: 2.04816
Global Iter: 349600 training acc: 0.15625
Global Iter: 349700 training loss: 1.95979
Global Iter: 349700 training acc: 0.15625
Global Iter: 349800 training loss: 1.89314
Global Iter: 349800 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-349829
Number of Patches: 251412
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-349829
Global Iter: 349900 training loss: 1.98765
Global Iter: 349900 training acc: 0.125
Global Iter: 350000 training loss: 1.95709
Global Iter: 350000 training acc: 0.15625
Global Iter: 350100 training loss: 2.00553
Global Iter: 350100 training acc: 0.09375
Global Iter: 350200 training loss: 2.02718
Global Iter: 350200 training acc: 0.125
Global Iter: 350300 training loss: 1.94236
Global Iter: 350300 training acc: 0.1875
Global Iter: 350400 training loss: 1.984
Global Iter: 350400 training acc: 0.15625
Global Iter: 350500 training loss: 1.96263
Global Iter: 350500 training acc: 0.1875
Global Iter: 350600 training loss: 2.0191
Global Iter: 350600 training acc: 0.125
Global Iter: 350700 training loss: 2.01744
Global Iter: 350700 training acc: 0.125
Global Iter: 350800 training loss: 2.01261
Global Iter: 350800 training acc: 0.25
Global Iter: 350900 training loss: 1.96436
Global Iter: 350900 training acc: 0.1875
Global Iter: 351000 training loss: 1.99293
Global Iter: 351000 training acc: 0.21875
Global Iter: 351100 training loss: 2.12228
Global Iter: 351100 training acc: 0.0
Global Iter: 351200 training loss: 1.99395
Global Iter: 351200 training acc: 0.15625
Global Iter: 351300 training loss: 1.95658
Global Iter: 351300 training acc: 0.21875
Global Iter: 351400 training loss: 2.05797
Global Iter: 351400 training acc: 0.125
Global Iter: 351500 training loss: 1.90348
Global Iter: 351500 training acc: 0.21875
Global Iter: 351600 training loss: 2.16862
Global Iter: 351600 training acc: 0.15625
Global Iter: 351700 training loss: 1.93362
Global Iter: 351700 training acc: 0.21875
Global Iter: 351800 training loss: 2.03589
Global Iter: 351800 training acc: 0.21875
Global Iter: 351900 training loss: 1.97198
Global Iter: 351900 training acc: 0.28125
Global Iter: 352000 training loss: 1.98159
Global Iter: 352000 training acc: 0.28125
Global Iter: 352100 training loss: 1.96783
Global Iter: 352100 training acc: 0.28125
Global Iter: 352200 training loss: 2.03917
Global Iter: 352200 training acc: 0.125
Global Iter: 352300 training loss: 2.01037
Global Iter: 352300 training acc: 0.28125
Global Iter: 352400 training loss: 2.0133
Global Iter: 352400 training acc: 0.28125
Global Iter: 352500 training loss: 2.023
Global Iter: 352500 training acc: 0.3125
Global Iter: 352600 training loss: 1.97343
Global Iter: 352600 training acc: 0.25
Global Iter: 352700 training loss: 2.10449
Global Iter: 352700 training acc: 0.09375
Global Iter: 352800 training loss: 1.91294
Global Iter: 352800 training acc: 0.21875
Global Iter: 352900 training loss: 2.04064
Global Iter: 352900 training acc: 0.21875
Global Iter: 353000 training loss: 1.98317
Global Iter: 353000 training acc: 0.125
Global Iter: 353100 training loss: 2.00102
Global Iter: 353100 training acc: 0.125
Global Iter: 353200 training loss: 2.08792
Global Iter: 353200 training acc: 0.15625
Global Iter: 353300 training loss: 2.02609
Global Iter: 353300 training acc: 0.125
Global Iter: 353400 training loss: 1.86014
Global Iter: 353400 training acc: 0.28125
Global Iter: 353500 training loss: 2.05374
Global Iter: 353500 training acc: 0.125
Global Iter: 353600 training loss: 2.03204
Global Iter: 353600 training acc: 0.21875
Global Iter: 353700 training loss: 2.08719
Global Iter: 353700 training acc: 0.125
Global Iter: 353800 training loss: 1.9536
Global Iter: 353800 training acc: 0.125
Global Iter: 353900 training loss: 1.94277
Global Iter: 353900 training acc: 0.28125
Global Iter: 354000 training loss: 1.97195
Global Iter: 354000 training acc: 0.1875
Global Iter: 354100 training loss: 1.89792
Global Iter: 354100 training acc: 0.28125
Global Iter: 354200 training loss: 2.03504
Global Iter: 354200 training acc: 0.125
Global Iter: 354300 training loss: 2.08098
Global Iter: 354300 training acc: 0.0625
Global Iter: 354400 training loss: 1.9856
Global Iter: 354400 training acc: 0.1875
Global Iter: 354500 training loss: 1.97513
Global Iter: 354500 training acc: 0.21875
Global Iter: 354600 training loss: 2.02556
Global Iter: 354600 training acc: 0.1875
Global Iter: 354700 training loss: 2.11189
Global Iter: 354700 training acc: 0.09375
Global Iter: 354800 training loss: 1.9413
Global Iter: 354800 training acc: 0.3125
Global Iter: 354900 training loss: 2.03492
Global Iter: 354900 training acc: 0.25
Global Iter: 355000 training loss: 1.95643
Global Iter: 355000 training acc: 0.21875
Global Iter: 355100 training loss: 1.98785
Global Iter: 355100 training acc: 0.1875
Global Iter: 355200 training loss: 1.96939
Global Iter: 355200 training acc: 0.125
Global Iter: 355300 training loss: 1.88777
Global Iter: 355300 training acc: 0.34375
Global Iter: 355400 training loss: 2.05992
Global Iter: 355400 training acc: 0.09375
Global Iter: 355500 training loss: 1.95729
Global Iter: 355500 training acc: 0.21875
Global Iter: 355600 training loss: 2.01415
Global Iter: 355600 training acc: 0.125
Global Iter: 355700 training loss: 2.02942
Global Iter: 355700 training acc: 0.15625
Global Iter: 355800 training loss: 2.12546
Global Iter: 355800 training acc: 0.15625
Global Iter: 355900 training loss: 2.04451
Global Iter: 355900 training acc: 0.09375
Global Iter: 356000 training loss: 1.95232
Global Iter: 356000 training acc: 0.25
Global Iter: 356100 training loss: 1.99391
Global Iter: 356100 training acc: 0.21875
Global Iter: 356200 training loss: 1.98414
Global Iter: 356200 training acc: 0.25
Global Iter: 356300 training loss: 2.01114
Global Iter: 356300 training acc: 0.125
Global Iter: 356400 training loss: 1.99265
Global Iter: 356400 training acc: 0.25
Global Iter: 356500 training loss: 1.93506
Global Iter: 356500 training acc: 0.3125
Global Iter: 356600 training loss: 1.87794
Global Iter: 356600 training acc: 0.15625
Global Iter: 356700 training loss: 1.96985
Global Iter: 356700 training acc: 0.375
Global Iter: 356800 training loss: 2.04399
Global Iter: 356800 training acc: 0.1875
Global Iter: 356900 training loss: 1.95715
Global Iter: 356900 training acc: 0.09375
Global Iter: 357000 training loss: 1.8798
Global Iter: 357000 training acc: 0.28125
Global Iter: 357100 training loss: 1.98091
Global Iter: 357100 training acc: 0.21875
Global Iter: 357200 training loss: 2.07332
Global Iter: 357200 training acc: 0.15625
Global Iter: 357300 training loss: 2.01375
Global Iter: 357300 training acc: 0.1875
Global Iter: 357400 training loss: 1.92721
Global Iter: 357400 training acc: 0.3125
Global Iter: 357500 training loss: 1.96056
Global Iter: 357500 training acc: 0.125
Global Iter: 357600 training loss: 2.02494
Global Iter: 357600 training acc: 0.15625
Global Iter: 357700 training loss: 1.97168
Global Iter: 357700 training acc: 0.125
Global Iter: 357800 training loss: 1.96668
Global Iter: 357800 training acc: 0.09375
Global Iter: 357900 training loss: 2.04336
Global Iter: 357900 training acc: 0.15625
Global Iter: 358000 training loss: 2.00332
Global Iter: 358000 training acc: 0.1875
Global Iter: 358100 training loss: 1.94493
Global Iter: 358100 training acc: 0.21875
Global Iter: 358200 training loss: 2.08336
Global Iter: 358200 training acc: 0.125
Global Iter: 358300 training loss: 1.97356
Global Iter: 358300 training acc: 0.15625
Global Iter: 358400 training loss: 1.91074
Global Iter: 358400 training acc: 0.21875
Global Iter: 358500 training loss: 2.11973
Global Iter: 358500 training acc: 0.125
Global Iter: 358600 training loss: 2.02299
Global Iter: 358600 training acc: 0.0625
Global Iter: 358700 training loss: 1.85011
Global Iter: 358700 training acc: 0.3125
Global Iter: 358800 training loss: 2.08075
Global Iter: 358800 training acc: 0.125
Global Iter: 358900 training loss: 1.93894
Global Iter: 358900 training acc: 0.1875
Global Iter: 359000 training loss: 1.95226
Global Iter: 359000 training acc: 0.28125
Global Iter: 359100 training loss: 1.95245
Global Iter: 359100 training acc: 0.1875
Global Iter: 359200 training loss: 1.98471
Global Iter: 359200 training acc: 0.15625
Global Iter: 359300 training loss: 2.07491
Global Iter: 359300 training acc: 0.0625
Global Iter: 359400 training loss: 2.06824
Global Iter: 359400 training acc: 0.15625
Global Iter: 359500 training loss: 1.99492
Global Iter: 359500 training acc: 0.28125
Global Iter: 359600 training loss: 2.01782
Global Iter: 359600 training acc: 0.28125
Global Iter: 359700 training loss: 2.01571
Global Iter: 359700 training acc: 0.125
Global Iter: 359800 training loss: 1.93967
Global Iter: 359800 training acc: 0.1875
Global Iter: 359900 training loss: 1.98971
Global Iter: 359900 training acc: 0.21875
Global Iter: 360000 training loss: 1.9975
Global Iter: 360000 training acc: 0.125
Global Iter: 360100 training loss: 1.99344
Global Iter: 360100 training acc: 0.125
Global Iter: 360200 training loss: 2.01647
Global Iter: 360200 training acc: 0.15625
Global Iter: 360300 training loss: 2.02046
Global Iter: 360300 training acc: 0.21875
Global Iter: 360400 training loss: 2.0424
Global Iter: 360400 training acc: 0.25
Global Iter: 360500 training loss: 2.08426
Global Iter: 360500 training acc: 0.09375
Global Iter: 360600 training loss: 2.00901
Global Iter: 360600 training acc: 0.09375
Global Iter: 360700 training loss: 2.00956
Global Iter: 360700 training acc: 0.25
Global Iter: 360800 training loss: 1.88724
Global Iter: 360800 training acc: 0.3125
Global Iter: 360900 training loss: 1.94657
Global Iter: 360900 training acc: 0.21875
Global Iter: 361000 training loss: 2.00278
Global Iter: 361000 training acc: 0.1875
Global Iter: 361100 training loss: 2.03204
Global Iter: 361100 training acc: 0.21875
Global Iter: 361200 training loss: 1.94936
Global Iter: 361200 training acc: 0.1875
Global Iter: 361300 training loss: 2.0107
Global Iter: 361300 training acc: 0.15625
Global Iter: 361400 training loss: 2.03117
Global Iter: 361400 training acc: 0.125
Global Iter: 361500 training loss: 1.99587
Global Iter: 361500 training acc: 0.125
Global Iter: 361600 training loss: 1.92532
Global Iter: 361600 training acc: 0.25
Global Iter: 361700 training loss: 1.91513
Global Iter: 361700 training acc: 0.1875
Global Iter: 361800 training loss: 1.9984
Global Iter: 361800 training acc: 0.09375
Global Iter: 361900 training loss: 1.91492
Global Iter: 361900 training acc: 0.21875
Global Iter: 362000 training loss: 1.93063
Global Iter: 362000 training acc: 0.15625
Global Iter: 362100 training loss: 2.01591
Global Iter: 362100 training acc: 0.125
Global Iter: 362200 training loss: 2.10696
Global Iter: 362200 training acc: 0.0625
Global Iter: 362300 training loss: 1.94335
Global Iter: 362300 training acc: 0.25
Global Iter: 362400 training loss: 2.02132
Global Iter: 362400 training acc: 0.15625
Global Iter: 362500 training loss: 1.95246
Global Iter: 362500 training acc: 0.21875
Global Iter: 362600 training loss: 1.91996
Global Iter: 362600 training acc: 0.25
Global Iter: 362700 training loss: 2.05213
Global Iter: 362700 training acc: 0.125
Global Iter: 362800 training loss: 1.97815
Global Iter: 362800 training acc: 0.15625
Global Iter: 362900 training loss: 1.90926
Global Iter: 362900 training acc: 0.28125
Global Iter: 363000 training loss: 1.96724
Global Iter2017-06-21 15:14:16.455076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-365543
: 363000 training acc: 0.21875
Global Iter: 363100 training loss: 1.97945
Global Iter: 363100 training acc: 0.125
Global Iter: 363200 training loss: 1.95736
Global Iter: 363200 training acc: 0.21875
Global Iter: 363300 training loss: 1.98862
Global Iter: 363300 training acc: 0.25
Global Iter: 363400 training loss: 1.98497
Global Iter: 363400 training acc: 0.28125
Global Iter: 363500 training loss: 1.96122
Global Iter: 363500 training acc: 0.21875
Global Iter: 363600 training loss: 2.02864
Global Iter: 363600 training acc: 0.15625
Global Iter: 363700 training loss: 1.96679
Global Iter: 363700 training acc: 0.25
Global Iter: 363800 training loss: 2.01178
Global Iter: 363800 training acc: 0.25
Global Iter: 363900 training loss: 1.98415
Global Iter: 363900 training acc: 0.34375
Global Iter: 364000 training loss: 2.08232
Global Iter: 364000 training acc: 0.21875
Global Iter: 364100 training loss: 2.013
Global Iter: 364100 training acc: 0.125
Global Iter: 364200 training loss: 2.07457
Global Iter: 364200 training acc: 0.09375
Global Iter: 364300 training loss: 2.06986
Global Iter: 364300 training acc: 0.15625
Global Iter: 364400 training loss: 1.91291
Global Iter: 364400 training acc: 0.3125
Global Iter: 364500 training loss: 2.0325
Global Iter: 364500 training acc: 0.21875
Global Iter: 364600 training loss: 1.96161
Global Iter: 364600 training acc: 0.28125
Global Iter: 364700 training loss: 2.07225
Global Iter: 364700 training acc: 0.03125
Global Iter: 364800 training loss: 1.99335
Global Iter: 364800 training acc: 0.09375
Global Iter: 364900 training loss: 2.00764
Global Iter: 364900 training acc: 0.21875
Global Iter: 365000 training loss: 2.00962
Global Iter: 365000 training acc: 0.21875
Global Iter: 365100 training loss: 1.88152
Global Iter: 365100 training acc: 0.28125
Global Iter: 365200 training loss: 2.09426
Global Iter: 365200 training acc: 0.1875
Global Iter: 365300 training loss: 1.91397
Global Iter: 365300 training acc: 0.34375
Global Iter: 365400 training loss: 1.95288
Global Iter: 365400 training acc: 0.1875
Global Iter: 365500 training loss: 1.93526
Global Iter: 365500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-365543
Number of Patches: 248898
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-365543
Global Iter: 365600 training loss: 2.08477
Global Iter: 365600 training acc: 0.1875
Global Iter: 365700 training loss: 2.00048
Global Iter: 365700 training acc: 0.3125
Global Iter: 365800 training loss: 2.06038
Global Iter: 365800 training acc: 0.25
Global Iter: 365900 training loss: 2.02132
Global Iter: 365900 training acc: 0.09375
Global Iter: 366000 training loss: 2.01991
Global Iter: 366000 training acc: 0.15625
Global Iter: 366100 training loss: 2.1121
Global Iter: 366100 training acc: 0.15625
Global Iter: 366200 training loss: 2.03446
Global Iter: 366200 training acc: 0.15625
Global Iter: 366300 training loss: 2.02447
Global Iter: 366300 training acc: 0.09375
Global Iter: 366400 training loss: 2.03918
Global Iter: 366400 training acc: 0.15625
Global Iter: 366500 training loss: 1.96209
Global Iter: 366500 training acc: 0.1875
Global Iter: 366600 training loss: 2.08326
Global Iter: 366600 training acc: 0.15625
Global Iter: 366700 training loss: 2.00438
Global Iter: 366700 training acc: 0.125
Global Iter: 366800 training loss: 1.94205
Global Iter: 366800 training acc: 0.21875
Global Iter: 366900 training loss: 2.00157
Global Iter: 366900 training acc: 0.25
Global Iter: 367000 training loss: 2.06329
Global Iter: 367000 training acc: 0.125
Global Iter: 367100 training loss: 2.11502
Global Iter: 367100 training acc: 0.125
Global Iter: 367200 training loss: 2.01547
Global Iter: 367200 training acc: 0.1875
Global Iter: 367300 training loss: 2.06216
Global Iter: 367300 training acc: 0.21875
Global Iter: 367400 training loss: 2.1265
Global Iter: 367400 training acc: 0.15625
Global Iter: 367500 training loss: 1.98968
Global Iter: 367500 training acc: 0.15625
Global Iter: 367600 training loss: 1.98277
Global Iter: 367600 training acc: 0.1875
Global Iter: 367700 training loss: 2.01934
Global Iter: 367700 training acc: 0.125
Global Iter: 367800 training loss: 1.97177
Global Iter: 367800 training acc: 0.125
Global Iter: 367900 training loss: 1.89229
Global Iter: 367900 training acc: 0.25
Global Iter: 368000 training loss: 2.0712
Global Iter: 368000 training acc: 0.125
Global Iter: 368100 training loss: 1.98446
Global Iter: 368100 training acc: 0.1875
Global Iter: 368200 training loss: 1.93231
Global Iter: 368200 training acc: 0.21875
Global Iter: 368300 training loss: 1.97131
Global Iter: 368300 training acc: 0.21875
Global Iter: 368400 training loss: 2.00699
Global Iter: 368400 training acc: 0.125
Global Iter: 368500 training loss: 1.90228
Global Iter: 368500 training acc: 0.21875
Global Iter: 368600 training loss: 2.12077
Global Iter: 368600 training acc: 0.1875
Global Iter: 368700 training loss: 1.88939
Global Iter: 368700 training acc: 0.25
Global Iter: 368800 training loss: 2.05542
Global Iter: 368800 training acc: 0.125
Global Iter: 368900 training loss: 2.01408
Global Iter: 368900 training acc: 0.1875
Global Iter: 369000 training loss: 1.93823
Global Iter: 369000 training acc: 0.25
Global Iter: 369100 training loss: 1.96866
Global Iter: 369100 training acc: 0.1875
Global Iter: 369200 training loss: 1.97891
Global Iter: 369200 training acc: 0.28125
Global Iter: 369300 training loss: 2.10839
Global Iter: 369300 training acc: 0.15625
Global Iter: 369400 training loss: 2.06701
Global Iter: 369400 training acc: 0.21875
Global Iter: 369500 training loss: 1.99513
Global Iter: 369500 training acc: 0.21875
Global Iter: 369600 training loss: 2.0347
Global Iter: 369600 training acc: 0.25
Global Iter: 369700 training loss: 1.8557
Global Iter: 369700 training acc: 0.25
Global Iter: 369800 training loss: 1.96386
Global Iter: 369800 training acc: 0.28125
Global Iter: 369900 training loss: 2.06437
Global Iter: 369900 training acc: 0.125
Global Iter: 370000 training loss: 1.99024
Global Iter: 370000 training acc: 0.1875
Global Iter: 370100 training loss: 1.98608
Global Iter: 370100 training acc: 0.1875
Global Iter: 370200 training loss: 2.01628
Global Iter: 370200 training acc: 0.15625
Global Iter: 370300 training loss: 1.95771
Global Iter: 370300 training acc: 0.25
Global Iter: 370400 training loss: 1.98112
Global Iter: 370400 training acc: 0.15625
Global Iter: 370500 training loss: 2.15223
Global Iter: 370500 training acc: 0.21875
Global Iter: 370600 training loss: 1.8691
Global Iter: 370600 training acc: 0.3125
Global Iter: 370700 training loss: 1.90395
Global Iter: 370700 training acc: 0.28125
Global Iter: 370800 training loss: 1.9893
Global Iter: 370800 training acc: 0.09375
Global Iter: 370900 training loss: 2.08832
Global Iter: 370900 training acc: 0.15625
Global Iter: 371000 training loss: 2.04924
Global Iter: 371000 training acc: 0.125
Global Iter: 371100 training loss: 1.89595
Global Iter: 371100 training acc: 0.21875
Global Iter: 371200 training loss: 1.99694
Global Iter: 371200 training acc: 0.15625
Global Iter: 371300 training loss: 1.96403
Global Iter: 371300 training acc: 0.125
Global Iter: 371400 training loss: 2.00132
Global Iter: 371400 training acc: 0.28125
Global Iter: 371500 training loss: 1.86158
Global Iter: 371500 training acc: 0.3125
Global Iter: 371600 training loss: 1.95261
Global Iter: 371600 training acc: 0.3125
Global Iter: 371700 training loss: 1.93241
Global Iter: 371700 training acc: 0.15625
Global Iter: 371800 training loss: 1.96417
Global Iter: 371800 training acc: 0.15625
Global Iter: 371900 training loss: 1.96918
Global Iter: 371900 training acc: 0.21875
Global Iter: 372000 training loss: 2.02351
Global Iter: 372000 training acc: 0.09375
Global Iter: 372100 training loss: 1.96495
Global Iter: 372100 training acc: 0.15625
Global Iter: 372200 training loss: 1.91773
Global Iter: 372200 training acc: 0.0625
Global Iter: 372300 training loss: 2.05395
Global Iter: 372300 training acc: 0.28125
Global Iter: 372400 training loss: 2.02416
Global Iter: 372400 training acc: 0.0625
Global Iter: 372500 training loss: 2.01365
Global Iter: 372500 training acc: 0.125
Global Iter: 372600 training loss: 2.02021
Global Iter: 372600 training acc: 0.0625
Global Iter: 372700 training loss: 1.86501
Global Iter: 372700 training acc: 0.34375
Global Iter: 372800 training loss: 1.96643
Global Iter: 372800 training acc: 0.125
Global Iter: 372900 training loss: 2.02293
Global Iter: 372900 training acc: 0.1875
Global Iter: 373000 training loss: 2.06904
Global Iter: 373000 training acc: 0.15625
Global Iter: 373100 training loss: 1.97007
Global Iter: 373100 training acc: 0.15625
Global Iter: 373200 training loss: 1.97947
Global Iter: 373200 training acc: 0.09375
Global Iter: 373300 training loss: 2.06621
Global Iter: 373300 training acc: 0.1875
Global Iter: 373400 training loss: 1.95395
Global Iter: 373400 training acc: 0.15625
Global Iter: 373500 training loss: 2.02667
Global Iter: 373500 training acc: 0.1875
Global Iter: 373600 training loss: 2.01519
Global Iter: 373600 training acc: 0.21875
Global Iter: 373700 training loss: 1.98985
Global Iter: 373700 training acc: 0.3125
Global Iter: 373800 training loss: 2.05044
Global Iter: 373800 training acc: 0.1875
Global Iter: 373900 training loss: 1.95777
Global Iter: 373900 training acc: 0.125
Global Iter: 374000 training loss: 1.95683
Global Iter: 374000 training acc: 0.15625
Global Iter: 374100 training loss: 1.91942
Global Iter: 374100 training acc: 0.1875
Global Iter: 374200 training loss: 1.93853
Global Iter: 374200 training acc: 0.28125
Global Iter: 374300 training loss: 1.95435
Global Iter: 374300 training acc: 0.25
Global Iter: 374400 training loss: 1.95594
Global Iter: 374400 training acc: 0.1875
Global Iter: 374500 training loss: 1.86746
Global Iter: 374500 training acc: 0.28125
Global Iter: 374600 training loss: 1.96179
Global Iter: 374600 training acc: 0.3125
Global Iter: 374700 training loss: 2.0373
Global Iter: 374700 training acc: 0.21875
Global Iter: 374800 training loss: 2.0558
Global Iter: 374800 training acc: 0.125
Global Iter: 374900 training loss: 2.1498
Global Iter: 374900 training acc: 0.1875
Global Iter: 375000 training loss: 2.06814
Global Iter: 375000 training acc: 0.1875
Global Iter: 375100 training loss: 2.09754
Global Iter: 375100 training acc: 0.09375
Global Iter: 375200 training loss: 2.02547
Global Iter: 375200 training acc: 0.09375
Global Iter: 375300 training loss: 1.96299
Global Iter: 375300 training acc: 0.21875
Global Iter: 375400 training loss: 1.91964
Global Iter: 375400 training acc: 0.1875
Global Iter: 375500 training loss: 1.92515
Global Iter: 375500 training acc: 0.25
Global Iter: 375600 training loss: 2.05368
Global Iter: 375600 training acc: 0.09375
Global Iter: 375700 training loss: 2.08016
Global Iter: 375700 training acc: 0.1875
Global Iter: 375800 training loss: 1.94186
Global Iter: 375800 training acc: 0.1875
Global Iter: 375900 training loss: 2.03824
Global Iter: 375900 training acc: 0.21875
Global Iter: 376000 training loss: 2.00617
Global Iter: 376000 training acc: 0.1875
Global Iter: 376100 training loss: 1.87969
Global Iter: 376100 training acc: 0.28125
Global Iter: 376200 training loss: 1.94632
Global Iter: 376200 training acc: 0.15625
Global Iter: 376300 training loss: 2.0015
Global Iter: 376300 training acc: 0.15625
Global Iter: 376400 training loss: 1.96326
Global Iter: 376400 training acc: 0.09375
Global Iter: 376500 training loss: 1.96142
Global Iter: 376500 training acc: 0.09375
Global Iter: 376600 training loss: 1.97435
Global Iter: 376600 training acc: 0.15625
Global Iter: 376700 training loss: 2.05733
Global Iter: 376700 training acc: 0.3125
Global Iter: 376800 training loss: 2.07466
Global Iter: 376800 training acc: 0.21875
Global Iter: 376900 training loss: 1.89766
Global Iter: 376900 training acc: 0.34375
Global Iter: 377000 training loss: 1.94257
Global Iter: 377000 training acc: 0.15625
Global Iter: 377100 training loss: 1.92194
Global Iter: 377100 training acc: 0.15625
Global Iter: 377200 training loss: 2.03346
Global Iter: 377200 training acc: 0.25
Global Iter: 377300 training loss: 1.97536
Global Iter: 377300 training acc: 0.4375
Global2017-06-21 15:41:31.126564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-381100
 Iter: 377400 training loss: 1.97104
Global Iter: 377400 training acc: 0.1875
Global Iter: 377500 training loss: 2.00859
Global Iter: 377500 training acc: 0.25
Global Iter: 377600 training loss: 1.99069
Global Iter: 377600 training acc: 0.1875
Global Iter: 377700 training loss: 1.97105
Global Iter: 377700 training acc: 0.21875
Global Iter: 377800 training loss: 1.97507
Global Iter: 377800 training acc: 0.0625
Global Iter: 377900 training loss: 1.92917
Global Iter: 377900 training acc: 0.1875
Global Iter: 378000 training loss: 1.93368
Global Iter: 378000 training acc: 0.15625
Global Iter: 378100 training loss: 2.01176
Global Iter: 378100 training acc: 0.1875
Global Iter: 378200 training loss: 1.95504
Global Iter: 378200 training acc: 0.125
Global Iter: 378300 training loss: 2.02533
Global Iter: 378300 training acc: 0.21875
Global Iter: 378400 training loss: 1.9937
Global Iter: 378400 training acc: 0.1875
Global Iter: 378500 training loss: 2.03324
Global Iter: 378500 training acc: 0.09375
Global Iter: 378600 training loss: 1.91665
Global Iter: 378600 training acc: 0.1875
Global Iter: 378700 training loss: 1.98878
Global Iter: 378700 training acc: 0.09375
Global Iter: 378800 training loss: 2.02337
Global Iter: 378800 training acc: 0.21875
Global Iter: 378900 training loss: 1.93092
Global Iter: 378900 training acc: 0.21875
Global Iter: 379000 training loss: 2.06297
Global Iter: 379000 training acc: 0.21875
Global Iter: 379100 training loss: 1.9091
Global Iter: 379100 training acc: 0.25
Global Iter: 379200 training loss: 1.9212
Global Iter: 379200 training acc: 0.28125
Global Iter: 379300 training loss: 2.01026
Global Iter: 379300 training acc: 0.09375
Global Iter: 379400 training loss: 1.99424
Global Iter: 379400 training acc: 0.25
Global Iter: 379500 training loss: 2.01923
Global Iter: 379500 training acc: 0.1875
Global Iter: 379600 training loss: 1.9386
Global Iter: 379600 training acc: 0.25
Global Iter: 379700 training loss: 1.93884
Global Iter: 379700 training acc: 0.15625
Global Iter: 379800 training loss: 2.05082
Global Iter: 379800 training acc: 0.15625
Global Iter: 379900 training loss: 1.91458
Global Iter: 379900 training acc: 0.21875
Global Iter: 380000 training loss: 1.995
Global Iter: 380000 training acc: 0.21875
Global Iter: 380100 training loss: 2.05091
Global Iter: 380100 training acc: 0.125
Global Iter: 380200 training loss: 2.04193
Global Iter: 380200 training acc: 0.15625
Global Iter: 380300 training loss: 1.98525
Global Iter: 380300 training acc: 0.21875
Global Iter: 380400 training loss: 1.93213
Global Iter: 380400 training acc: 0.1875
Global Iter: 380500 training loss: 1.93208
Global Iter: 380500 training acc: 0.21875
Global Iter: 380600 training loss: 1.96624
Global Iter: 380600 training acc: 0.15625
Global Iter: 380700 training loss: 1.99912
Global Iter: 380700 training acc: 0.0625
Global Iter: 380800 training loss: 2.11154
Global Iter: 380800 training acc: 0.1875
Global Iter: 380900 training loss: 2.00566
Global Iter: 380900 training acc: 0.125
Global Iter: 381000 training loss: 2.12576
Global Iter: 381000 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-381100
Number of Patches: 246410
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-381100
Global Iter: 381200 training loss: 1.98485
Global Iter: 381200 training acc: 0.125
Global Iter: 381300 training loss: 2.06191
Global Iter: 381300 training acc: 0.21875
Global Iter: 381400 training loss: 1.91789
Global Iter: 381400 training acc: 0.25
Global Iter: 381500 training loss: 1.94115
Global Iter: 381500 training acc: 0.0625
Global Iter: 381600 training loss: 1.95825
Global Iter: 381600 training acc: 0.25
Global Iter: 381700 training loss: 2.13817
Global Iter: 381700 training acc: 0.09375
Global Iter: 381800 training loss: 1.92757
Global Iter: 381800 training acc: 0.21875
Global Iter: 381900 training loss: 1.89165
Global Iter: 381900 training acc: 0.25
Global Iter: 382000 training loss: 1.88326
Global Iter: 382000 training acc: 0.3125
Global Iter: 382100 training loss: 1.94464
Global Iter: 382100 training acc: 0.15625
Global Iter: 382200 training loss: 2.01263
Global Iter: 382200 training acc: 0.15625
Global Iter: 382300 training loss: 2.00879
Global Iter: 382300 training acc: 0.15625
Global Iter: 382400 training loss: 2.09578
Global Iter: 382400 training acc: 0.1875
Global Iter: 382500 training loss: 2.00456
Global Iter: 382500 training acc: 0.25
Global Iter: 382600 training loss: 1.93556
Global Iter: 382600 training acc: 0.25
Global Iter: 382700 training loss: 2.03632
Global Iter: 382700 training acc: 0.25
Global Iter: 382800 training loss: 2.05013
Global Iter: 382800 training acc: 0.21875
Global Iter: 382900 training loss: 1.98534
Global Iter: 382900 training acc: 0.21875
Global Iter: 383000 training loss: 1.93769
Global Iter: 383000 training acc: 0.21875
Global Iter: 383100 training loss: 1.97313
Global Iter: 383100 training acc: 0.25
Global Iter: 383200 training loss: 1.93702
Global Iter: 383200 training acc: 0.09375
Global Iter: 383300 training loss: 1.94658
Global Iter: 383300 training acc: 0.28125
Global Iter: 383400 training loss: 1.88944
Global Iter: 383400 training acc: 0.3125
Global Iter: 383500 training loss: 2.10537
Global Iter: 383500 training acc: 0.09375
Global Iter: 383600 training loss: 2.00542
Global Iter: 383600 training acc: 0.1875
Global Iter: 383700 training loss: 2.06329
Global Iter: 383700 training acc: 0.125
Global Iter: 383800 training loss: 1.95637
Global Iter: 383800 training acc: 0.21875
Global Iter: 383900 training loss: 2.15249
Global Iter: 383900 training acc: 0.15625
Global Iter: 384000 training loss: 1.96573
Global Iter: 384000 training acc: 0.125
Global Iter: 384100 training loss: 1.98138
Global Iter: 384100 training acc: 0.21875
Global Iter: 384200 training loss: 2.05082
Global Iter: 384200 training acc: 0.21875
Global Iter: 384300 training loss: 1.97298
Global Iter: 384300 training acc: 0.1875
Global Iter: 384400 training loss: 1.87969
Global Iter: 384400 training acc: 0.34375
Global Iter: 384500 training loss: 1.89019
Global Iter: 384500 training acc: 0.25
Global Iter: 384600 training loss: 2.07227
Global Iter: 384600 training acc: 0.125
Global Iter: 384700 training loss: 1.90304
Global Iter: 384700 training acc: 0.28125
Global Iter: 384800 training loss: 2.11239
Global Iter: 384800 training acc: 0.15625
Global Iter: 384900 training loss: 1.98243
Global Iter: 384900 training acc: 0.1875
Global Iter: 385000 training loss: 1.88772
Global Iter: 385000 training acc: 0.21875
Global Iter: 385100 training loss: 2.03131
Global Iter: 385100 training acc: 0.09375
Global Iter: 385200 training loss: 1.98532
Global Iter: 385200 training acc: 0.15625
Global Iter: 385300 training loss: 1.98021
Global Iter: 385300 training acc: 0.21875
Global Iter: 385400 training loss: 1.98415
Global Iter: 385400 training acc: 0.09375
Global Iter: 385500 training loss: 2.02994
Global Iter: 385500 training acc: 0.125
Global Iter: 385600 training loss: 2.02702
Global Iter: 385600 training acc: 0.1875
Global Iter: 385700 training loss: 1.90847
Global Iter: 385700 training acc: 0.1875
Global Iter: 385800 training loss: 1.96577
Global Iter: 385800 training acc: 0.21875
Global Iter: 385900 training loss: 2.21607
Global Iter: 385900 training acc: 0.125
Global Iter: 386000 training loss: 1.98159
Global Iter: 386000 training acc: 0.1875
Global Iter: 386100 training loss: 1.9486
Global Iter: 386100 training acc: 0.21875
Global Iter: 386200 training loss: 1.927
Global Iter: 386200 training acc: 0.25
Global Iter: 386300 training loss: 2.06056
Global Iter: 386300 training acc: 0.1875
Global Iter: 386400 training loss: 1.96269
Global Iter: 386400 training acc: 0.21875
Global Iter: 386500 training loss: 2.03654
Global Iter: 386500 training acc: 0.15625
Global Iter: 386600 training loss: 1.96633
Global Iter: 386600 training acc: 0.34375
Global Iter: 386700 training loss: 1.99096
Global Iter: 386700 training acc: 0.3125
Global Iter: 386800 training loss: 1.95849
Global Iter: 386800 training acc: 0.15625
Global Iter: 386900 training loss: 1.90624
Global Iter: 386900 training acc: 0.28125
Global Iter: 387000 training loss: 1.95425
Global Iter: 387000 training acc: 0.25
Global Iter: 387100 training loss: 1.95726
Global Iter: 387100 training acc: 0.1875
Global Iter: 387200 training loss: 2.06281
Global Iter: 387200 training acc: 0.09375
Global Iter: 387300 training loss: 2.12606
Global Iter: 387300 training acc: 0.125
Global Iter: 387400 training loss: 2.03141
Global Iter: 387400 training acc: 0.1875
Global Iter: 387500 training loss: 1.94569
Global Iter: 387500 training acc: 0.1875
Global Iter: 387600 training loss: 2.11335
Global Iter: 387600 training acc: 0.15625
Global Iter: 387700 training loss: 1.96167
Global Iter: 387700 training acc: 0.1875
Global Iter: 387800 training loss: 2.07525
Global Iter: 387800 training acc: 0.1875
Global Iter: 387900 training loss: 2.05039
Global Iter: 387900 training acc: 0.15625
Global Iter: 388000 training loss: 1.96648
Global Iter: 388000 training acc: 0.125
Global Iter: 388100 training loss: 1.86571
Global Iter: 388100 training acc: 0.21875
Global Iter: 388200 training loss: 1.90758
Global Iter: 388200 training acc: 0.21875
Global Iter: 388300 training loss: 2.11264
Global Iter: 388300 training acc: 0.09375
Global Iter: 388400 training loss: 2.03927
Global Iter: 388400 training acc: 0.125
Global Iter: 388500 training loss: 1.98079
Global Iter: 388500 training acc: 0.125
Global Iter: 388600 training loss: 2.03605
Global Iter: 388600 training acc: 0.28125
Global Iter: 388700 training loss: 1.93748
Global Iter: 388700 training acc: 0.28125
Global Iter: 388800 training loss: 1.98205
Global Iter: 388800 training acc: 0.28125
Global Iter: 388900 training loss: 2.02531
Global Iter: 388900 training acc: 0.1875
Global Iter: 389000 training loss: 1.98612
Global Iter: 389000 training acc: 0.15625
Global Iter: 389100 training loss: 2.00221
Global Iter: 389100 training acc: 0.125
Global Iter: 389200 training loss: 1.97066
Global Iter: 389200 training acc: 0.21875
Global Iter: 389300 training loss: 1.94453
Global Iter: 389300 training acc: 0.21875
Global Iter: 389400 training loss: 1.88543
Global Iter: 389400 training acc: 0.21875
Global Iter: 389500 training loss: 1.98356
Global Iter: 389500 training acc: 0.25
Global Iter: 389600 training loss: 2.05764
Global Iter: 389600 training acc: 0.34375
Global Iter: 389700 training loss: 1.93977
Global Iter: 389700 training acc: 0.15625
Global Iter: 389800 training loss: 1.91009
Global Iter: 389800 training acc: 0.34375
Global Iter: 389900 training loss: 1.95149
Global Iter: 389900 training acc: 0.1875
Global Iter: 390000 training loss: 1.99644
Global Iter: 390000 training acc: 0.1875
Global Iter: 390100 training loss: 2.08189
Global Iter: 390100 training acc: 0.0625
Global Iter: 390200 training loss: 1.88039
Global Iter: 390200 training acc: 0.3125
Global Iter: 390300 training loss: 1.92439
Global Iter: 390300 training acc: 0.15625
Global Iter: 390400 training loss: 1.91632
Global Iter: 390400 training acc: 0.125
Global Iter: 390500 training loss: 1.91604
Global Iter: 390500 training acc: 0.34375
Global Iter: 390600 training loss: 1.92006
Global Iter: 390600 training acc: 0.1875
Global Iter: 390700 training loss: 1.95395
Global Iter: 390700 training acc: 0.25
Global Iter: 390800 training loss: 1.99922
Global Iter: 390800 training acc: 0.1875
Global Iter: 390900 training loss: 1.98327
Global Iter: 390900 training acc: 0.28125
Global Iter: 391000 training loss: 1.92855
Global Iter: 391000 training acc: 0.15625
Global Iter: 391100 training loss: 1.98807
Global Iter: 391100 training acc: 0.25
Global Iter: 391200 training loss: 2.03248
Global Iter: 391200 training acc: 0.125
Global Iter: 391300 training loss: 1.94949
Global Iter: 391300 training acc: 0.25
Global Iter: 391400 training loss: 2.02682
Global Iter: 391400 training acc: 0.1875
Global Iter: 391500 training loss: 2.03598
Global Iter: 391500 training acc: 0.21875
Global Iter: 391600 training loss: 2.08166
Global Iter: 391600 training acc: 0.125
Global Iter: 391700 training loss: 1.92102
Global Iter: 391700 training acc: 0.25
Global Iter: 391800 training loss: 2.00199
Global Iter2017-06-21 16:08:24.342373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
: 391800 training acc: 0.3125
Global Iter: 391900 training loss: 2.08252
Global Iter: 391900 training acc: 0.21875
Global Iter: 392000 training loss: 1.90864
Global Iter: 392000 training acc: 0.1875
Global Iter: 392100 training loss: 2.06874
Global Iter: 392100 training acc: 0.0625
Global Iter: 392200 training loss: 1.99514
Global Iter: 392200 training acc: 0.1875
Global Iter: 392300 training loss: 2.10258
Global Iter: 392300 training acc: 0.09375
Global Iter: 392400 training loss: 2.06193
Global Iter: 392400 training acc: 0.1875
Global Iter: 392500 training loss: 2.01894
Global Iter: 392500 training acc: 0.125
Global Iter: 392600 training loss: 1.90724
Global Iter: 392600 training acc: 0.25
Global Iter: 392700 training loss: 2.13556
Global Iter: 392700 training acc: 0.15625
Global Iter: 392800 training loss: 1.87732
Global Iter: 392800 training acc: 0.1875
Global Iter: 392900 training loss: 2.01855
Global Iter: 392900 training acc: 0.0625
Global Iter: 393000 training loss: 1.91948
Global Iter: 393000 training acc: 0.21875
Global Iter: 393100 training loss: 2.15453
Global Iter: 393100 training acc: 0.1875
Global Iter: 393200 training loss: 1.96576
Global Iter: 393200 training acc: 0.28125
Global Iter: 393300 training loss: 2.00205
Global Iter: 393300 training acc: 0.09375
Global Iter: 393400 training loss: 2.08252
Global Iter: 393400 training acc: 0.21875
Global Iter: 393500 training loss: 1.93221
Global Iter: 393500 training acc: 0.1875
Global Iter: 393600 training loss: 2.05033
Global Iter: 393600 training acc: 0.21875
Global Iter: 393700 training loss: 1.97329
Global Iter: 393700 training acc: 0.375
Global Iter: 393800 training loss: 2.01814
Global Iter: 393800 training acc: 0.15625
Global Iter: 393900 training loss: 1.95779
Global Iter: 393900 training acc: 0.1875
Global Iter: 394000 training loss: 1.94553
Global Iter: 394000 training acc: 0.34375
Global Iter: 394100 training loss: 1.95961
Global Iter: 394100 training acc: 0.15625
Global Iter: 394200 training loss: 1.99327
Global Iter: 394200 training acc: 0.25
Global Iter: 394300 training loss: 1.98059
Global Iter: 394300 training acc: 0.21875
Global Iter: 394400 training loss: 1.95412
Global Iter: 394400 training acc: 0.25
Global Iter: 394500 training loss: 1.98602
Global Iter: 394500 training acc: 0.15625
Global Iter: 394600 training loss: 1.96789
Global Iter: 394600 training acc: 0.1875
Global Iter: 394700 training loss: 1.92566
Global Iter: 394700 training acc: 0.21875
Global Iter: 394800 training loss: 1.92198
Global Iter: 394800 training acc: 0.15625
Global Iter: 394900 training loss: 1.99096
Global Iter: 394900 training acc: 0.0625
Global Iter: 395000 training loss: 1.90587
Global Iter: 395000 training acc: 0.3125
Global Iter: 395100 training loss: 2.02909
Global Iter: 395100 training acc: 0.1875
Global Iter: 395200 training loss: 1.95929
Global Iter: 395200 training acc: 0.25
Global Iter: 395300 training loss: 1.94699
Global Iter: 395300 training acc: 0.125
Global Iter: 395400 training loss: 1.90625
Global Iter: 395400 training acc: 0.21875
Global Iter: 395500 training loss: 2.01241
Global Iter: 395500 training acc: 0.15625
Global Iter: 395600 training loss: 1.90896
Global Iter: 395600 training acc: 0.3125
Global Iter: 395700 training loss: 2.04038
Global Iter: 395700 training acc: 0.15625
Global Iter: 395800 training loss: 1.93051
Global Iter: 395800 training acc: 0.21875
Global Iter: 395900 training loss: 1.93003
Global Iter: 395900 training acc: 0.25
Global Iter: 396000 training loss: 2.02362
Global Iter: 396000 training acc: 0.25
Global Iter: 396100 training loss: 1.99481
Global Iter: 396100 training acc: 0.15625
Global Iter: 396200 training loss: 1.92667
Global Iter: 396200 training acc: 0.25
Global Iter: 396300 training loss: 2.11791
Global Iter: 396300 training acc: 0.1875
Global Iter: 396400 training loss: 1.97815
Global Iter: 396400 training acc: 0.15625
Global Iter: 396500 training loss: 1.96485
Global Iter: 396500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-396501
Number of PatINFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-396501
ches: 243946
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-396501
Global Iter: 396600 training loss: 2.0186
Global Iter: 396600 training acc: 0.09375
Global Iter: 396700 training loss: 1.99316
Global Iter: 396700 training acc: 0.15625
Global Iter: 396800 training loss: 1.94683
Global Iter: 396800 training acc: 0.15625
Global Iter: 396900 training loss: 2.07665
Global Iter: 396900 training acc: 0.125
Global Iter: 397000 training loss: 1.95958
Global Iter: 397000 training acc: 0.21875
Global Iter: 397100 training loss: 1.98524
Global Iter: 397100 training acc: 0.1875
Global Iter: 397200 training loss: 1.93783
Global Iter: 397200 training acc: 0.1875
Global Iter: 397300 training loss: 2.02753
Global Iter: 397300 training acc: 0.15625
Global Iter: 397400 training loss: 1.94293
Global Iter: 397400 training acc: 0.21875
Global Iter: 397500 training loss: 1.97014
Global Iter: 397500 training acc: 0.1875
Global Iter: 397600 training loss: 1.90514
Global Iter: 397600 training acc: 0.25
Global Iter: 397700 training loss: 1.9145
Global Iter: 397700 training acc: 0.3125
Global Iter: 397800 training loss: 1.97824
Global Iter: 397800 training acc: 0.375
Global Iter: 397900 training loss: 2.02533
Global Iter: 397900 training acc: 0.1875
Global Iter: 398000 training loss: 2.0886
Global Iter: 398000 training acc: 0.125
Global Iter: 398100 training loss: 1.95932
Global Iter: 398100 training acc: 0.25
Global Iter: 398200 training loss: 1.90585
Global Iter: 398200 training acc: 0.3125
Global Iter: 398300 training loss: 1.94002
Global Iter: 398300 training acc: 0.25
Global Iter: 398400 training loss: 2.00023
Global Iter: 398400 training acc: 0.21875
Global Iter: 398500 training loss: 2.05075
Global Iter: 398500 training acc: 0.125
Global Iter: 398600 training loss: 2.03224
Global Iter: 398600 training acc: 0.1875
Global Iter: 398700 training loss: 1.92623
Global Iter: 398700 training acc: 0.21875
Global Iter: 398800 training loss: 1.86275
Global Iter: 398800 training acc: 0.1875
Global Iter: 398900 training loss: 2.01646
Global Iter: 398900 training acc: 0.09375
Global Iter: 399000 training loss: 1.91922
Global Iter: 399000 training acc: 0.1875
Global Iter: 399100 training loss: 2.04867
Global Iter: 399100 training acc: 0.28125
Global Iter: 399200 training loss: 2.01605
Global Iter: 399200 training acc: 0.15625
Global Iter: 399300 training loss: 1.95801
Global Iter: 399300 training acc: 0.125
Global Iter: 399400 training loss: 2.00497
Global Iter: 399400 training acc: 0.25
Global Iter: 399500 training loss: 1.98141
Global Iter: 399500 training acc: 0.1875
Global Iter: 399600 training loss: 1.96834
Global Iter: 399600 training acc: 0.3125
Global Iter: 399700 training loss: 1.90459
Global Iter: 399700 training acc: 0.28125
Global Iter: 399800 training loss: 2.09988
Global Iter: 399800 training acc: 0.125
Global Iter: 399900 training loss: 1.944
Global Iter: 399900 training acc: 0.21875
Global Iter: 400000 training loss: 1.98526
Global Iter: 400000 training acc: 0.125
Global Iter: 400100 training loss: 2.00706
Global Iter: 400100 training acc: 0.0625
Global Iter: 400200 training loss: 2.01076
Global Iter: 400200 training acc: 0.28125
Global Iter: 400300 training loss: 1.91197
Global Iter: 400300 training acc: 0.21875
Global Iter: 400400 training loss: 2.05802
Global Iter: 400400 training acc: 0.15625
Global Iter: 400500 training loss: 1.95467
Global Iter: 400500 training acc: 0.21875
Global Iter: 400600 training loss: 1.9302
Global Iter: 400600 training acc: 0.21875
Global Iter: 400700 training loss: 1.97781
Global Iter: 400700 training acc: 0.125
Global Iter: 400800 training loss: 1.96149
Global Iter: 400800 training acc: 0.21875
Global Iter: 400900 training loss: 2.13198
Global Iter: 400900 training acc: 0.15625
Global Iter: 401000 training loss: 1.98851
Global Iter: 401000 training acc: 0.1875
Global Iter: 401100 training loss: 1.9537
Global Iter: 401100 training acc: 0.09375
Global Iter: 401200 training loss: 2.02756
Global Iter: 401200 training acc: 0.125
Global Iter: 401300 training loss: 2.01963
Global Iter: 401300 training acc: 0.21875
Global Iter: 401400 training loss: 1.92583
Global Iter: 401400 training acc: 0.1875
Global Iter: 401500 training loss: 2.00026
Global Iter: 401500 training acc: 0.21875
Global Iter: 401600 training loss: 2.02072
Global Iter: 401600 training acc: 0.09375
Global Iter: 401700 training loss: 1.97368
Global Iter: 401700 training acc: 0.28125
Global Iter: 401800 training loss: 2.19038
Global Iter: 401800 training acc: 0.1875
Global Iter: 401900 training loss: 1.93632
Global Iter: 401900 training acc: 0.1875
Global Iter: 402000 training loss: 2.00361
Global Iter: 402000 training acc: 0.1875
Global Iter: 402100 training loss: 2.01845
Global Iter: 402100 training acc: 0.125
Global Iter: 402200 training loss: 2.02164
Global Iter: 402200 training acc: 0.21875
Global Iter: 402300 training loss: 1.90331
Global Iter: 402300 training acc: 0.21875
Global Iter: 402400 training loss: 1.96156
Global Iter: 402400 training acc: 0.09375
Global Iter: 402500 training loss: 1.99266
Global Iter: 402500 training acc: 0.15625
Global Iter: 402600 training loss: 1.94029
Global Iter: 402600 training acc: 0.21875
Global Iter: 402700 training loss: 2.03142
Global Iter: 402700 training acc: 0.1875
Global Iter: 402800 training loss: 2.10739
Global Iter: 402800 training acc: 0.21875
Global Iter: 402900 training loss: 1.90929
Global Iter: 402900 training acc: 0.21875
Global Iter: 403000 training loss: 2.07411
Global Iter: 403000 training acc: 0.0625
Global Iter: 403100 training loss: 2.02829
Global Iter: 403100 training acc: 0.125
Global Iter: 403200 training loss: 1.97138
Global Iter: 403200 training acc: 0.1875
Global Iter: 403300 training loss: 1.96442
Global Iter: 403300 training acc: 0.21875
Global Iter: 403400 training loss: 1.92349
Global Iter: 403400 training acc: 0.25
Global Iter: 403500 training loss: 1.87679
Global Iter: 403500 training acc: 0.15625
Global Iter: 403600 training loss: 1.97204
Global Iter: 403600 training acc: 0.21875
Global Iter: 403700 training loss: 2.13098
Global Iter: 403700 training acc: 0.15625
Global Iter: 403800 training loss: 1.88216
Global Iter: 403800 training acc: 0.34375
Global Iter: 403900 training loss: 2.06293
Global Iter: 403900 training acc: 0.09375
Global Iter: 404000 training loss: 1.98732
Global Iter: 404000 training acc: 0.15625
Global Iter: 404100 training loss: 1.99564
Global Iter: 404100 training acc: 0.21875
Global Iter: 404200 training loss: 1.93115
Global Iter: 404200 training acc: 0.25
Global Iter: 404300 training loss: 2.10763
Global Iter: 404300 training acc: 0.1875
Global Iter: 404400 training loss: 1.99577
Global Iter: 404400 training acc: 0.15625
Global Iter: 404500 training loss: 2.00834
Global Iter: 404500 training acc: 0.21875
Global Iter: 404600 training loss: 1.99447
Global Iter: 404600 training acc: 0.21875
Global Iter: 404700 training loss: 2.14863
Global Iter: 404700 training acc: 0.125
Global Iter: 404800 training loss: 2.07658
Global Iter: 404800 training acc: 0.15625
Global Iter: 404900 training loss: 1.9987
Global Iter: 404900 training acc: 0.1875
Global Iter: 405000 training loss: 2.04406
Global Iter: 405000 training acc: 0.09375
Global Iter: 405100 training loss: 2.01074
Global Iter: 405100 training acc: 0.15625
Global Iter: 405200 training loss: 2.00039
Global Iter: 405200 training acc: 0.125
Global Iter: 405300 training loss: 1.98257
Global Iter: 405300 training acc: 0.1875
Global Iter: 405400 training loss: 1.91603
Global Iter: 405400 training acc: 0.25
Global Iter: 405500 training loss: 2.0143
Global Iter: 405500 training acc: 0.21875
Global Iter: 405600 training loss: 2.23478
Global Iter: 405600 training acc: 0.09375
Global Iter: 405700 training loss: 2.00422
Global Iter: 405700 training acc: 0.09375
Global Iter: 405800 training loss: 1.96906
Global Iter: 405800 training acc: 0.25
Global Iter: 405900 training loss: 2.06468
Global Iter: 405900 training acc: 0.1875
Global Iter: 406000 training loss: 2.02616
Global Iter: 406000 training acc: 0.125
Global Iter: 406100 training loss: 1.92241
Global Iter: 406100 training acc: 0.28125
Global Iter: 406200 training loss: 2.04602
Global Iter: 406200 training acc: 0.25
Global Iter: 406300 training loss: 1.93459
Global Iter: 406300 training acc: 0.28125
Global Iter: 406400 training loss: 2.00263
Global Iter: 406400 training acc: 0.15625
Global Iter: 406500 training loss: 1.96816
Global Iter: 406500 training acc: 0.1875
Global Iter: 406600 training loss: 1.98144
Global Iter: 406600 training acc: 0.1875
Global Iter: 406700 training loss: 2.01369
Global Iter: 406700 training acc: 0.125
Global Iter: 406800 training loss: 1.96912
Global Iter: 406800 training acc: 0.28125
Global Iter: 406900 training loss: 1.91399
Global Iter: 406900 training acc: 0.21875
Global Iter: 407000 training loss: 2.09367
Global Iter: 407000 training acc: 0.15625
Global Iter: 407100 training loss: 2.06286
Global Iter: 407100 training acc: 0.0625
Global Iter: 407200 training loss: 1.92437
Global Iter: 407200 training acc: 0.1875
Global Iter: 407300 training loss: 1.90544
Global Iter: 407300 training acc: 0.1875
Global Iter: 407400 training loss: 1.95975
Global Iter: 407400 training acc: 0.21875
Global Iter: 407500 training loss: 1.94856
Global Iter: 407500 training acc: 0.21875
Global Iter: 407600 training loss: 2.12711
Global Iter: 407600 training acc: 0.21875
Global Iter: 407700 training loss: 2.0281
Global Iter: 407700 training acc: 0.15625
Global Iter: 407800 training loss: 2.02111
Global Iter: 407800 training acc: 0.125
Global Iter: 407900 training loss: 2.02178
Global Iter: 407900 training acc: 0.03125
Global Iter: 408000 training loss: 2.03942
Global Iter: 408000 training acc: 0.09375
Global Iter: 408100 training loss: 1.99359
Global Iter: 408100 training acc: 0.25
Global Iter: 408200 training loss: 2.01006
Global Iter: 408200 training acc: 0.25
Global Iter: 408300 training loss: 2.02822
Global Iter: 408300 training acc: 0.09375
Global Iter: 408400 training loss: 1.94453
Global Iter: 408400 training acc: 0.21875
Global Iter: 408500 training loss: 1.97972
Global Iter: 408500 training acc: 0.0625
Global Iter: 408600 training loss: 2.02651
Global Iter: 408600 training acc: 0.09375
Global Iter: 408700 training loss: 2.07797
Global Iter: 408700 training acc: 0.21875
Global Iter: 408800 training loss: 1.89999
Global Iter: 408800 training acc: 0.21875
Global Iter: 408900 training loss: 1.96355
Global Iter: 408900 training acc: 0.25
Global Iter: 409000 training loss: 1.9895
Global Iter: 409000 training acc: 0.09375
Global Iter: 409100 training loss: 1.89035
Global Iter: 409100 training acc: 0.21875
Global Iter: 409200 training loss: 1.98127
Global Iter: 409200 training acc: 0.21875
Global Iter: 409300 training loss: 2.04462
Global Iter: 409300 training acc: 0.21875
Global Iter: 409400 training loss: 1.84658
Global Iter: 409400 training acc: 0.28125
Global Iter: 409500 training loss: 1.88246
Global Iter: 409500 training acc: 0.34375
Global Iter: 409600 training loss: 1.87977
Global Iter: 409600 training acc: 0.3125
Global Iter: 409700 training loss: 2.00582
Global Iter: 409700 training acc: 0.1875
Global Iter: 409800 training loss: 2.01671
Global Iter: 409800 training acc: 0.1875
Global Iter: 409900 training loss: 1.95719
Global Iter: 409900 training acc: 0.25
Global Iter: 410000 training loss: 2.11539
Global Iter: 410000 training acc: 0.09375
Global Iter: 410100 training loss: 2.07287
Global Iter: 410100 training acc: 0.1875
Global Iter: 410200 training loss: 1.95273
Global Iter: 410200 training acc: 0.34375
Global Iter: 410300 training loss: 2.05163
Global Iter: 410300 training acc: 0.125
Global Iter: 410400 training loss: 1.89936
Global Iter: 410400 training acc: 0.21875
Global Iter: 410500 training loss: 1.91806
Global Iter: 410500 training acc: 0.25
Global Iter: 410600 training loss: 1.94547
Global Iter: 410600 training acc: 0.15625
Global Iter: 410700 training loss: 1.94843
Global Iter: 410700 training acc: 0.25
Global Iter: 410800 training loss: 1.95369
Global Iter: 410800 training acc: 0.3125
Global Iter: 410900 training loss: 1.94489
Global Iter: 410900 training acc: 0.34375
Global Iter: 411000 training loss: 2.03212
Global Iter: 411000 2017-06-21 16:35:20.260536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-411748
training acc: 0.3125
Global Iter: 411100 training loss: 1.98568
Global Iter: 411100 training acc: 0.1875
Global Iter: 411200 training loss: 1.99611
Global Iter: 411200 training acc: 0.25
Global Iter: 411300 training loss: 1.98643
Global Iter: 411300 training acc: 0.125
Global Iter: 411400 training loss: 1.9502
Global Iter: 411400 training acc: 0.1875
Global Iter: 411500 training loss: 1.92577
Global Iter: 411500 training acc: 0.1875
Global Iter: 411600 training loss: 2.00952
Global Iter: 411600 training acc: 0.1875
Global Iter: 411700 training loss: 2.01763
Global Iter: 411700 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-411748
Number of Patches: 241507
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-411748
Global Iter: 411800 training loss: 1.97749
Global Iter: 411800 training acc: 0.28125
Global Iter: 411900 training loss: 2.04769
Global Iter: 411900 training acc: 0.1875
Global Iter: 412000 training loss: 1.93246
Global Iter: 412000 training acc: 0.25
Global Iter: 412100 training loss: 1.94667
Global Iter: 412100 training acc: 0.21875
Global Iter: 412200 training loss: 2.00615
Global Iter: 412200 training acc: 0.09375
Global Iter: 412300 training loss: 1.98799
Global Iter: 412300 training acc: 0.03125
Global Iter: 412400 training loss: 2.00803
Global Iter: 412400 training acc: 0.0625
Global Iter: 412500 training loss: 1.86211
Global Iter: 412500 training acc: 0.21875
Global Iter: 412600 training loss: 1.93244
Global Iter: 412600 training acc: 0.3125
Global Iter: 412700 training loss: 1.98469
Global Iter: 412700 training acc: 0.15625
Global Iter: 412800 training loss: 1.97305
Global Iter: 412800 training acc: 0.25
Global Iter: 412900 training loss: 1.92232
Global Iter: 412900 training acc: 0.125
Global Iter: 413000 training loss: 1.98752
Global Iter: 413000 training acc: 0.21875
Global Iter: 413100 training loss: 1.97782
Global Iter: 413100 training acc: 0.15625
Global Iter: 413200 training loss: 1.94465
Global Iter: 413200 training acc: 0.21875
Global Iter: 413300 training loss: 2.02958
Global Iter: 413300 training acc: 0.125
Global Iter: 413400 training loss: 1.94646
Global Iter: 413400 training acc: 0.1875
Global Iter: 413500 training loss: 1.97795
Global Iter: 413500 training acc: 0.15625
Global Iter: 413600 training loss: 2.0176
Global Iter: 413600 training acc: 0.09375
Global Iter: 413700 training loss: 1.94427
Global Iter: 413700 training acc: 0.25
Global Iter: 413800 training loss: 1.98833
Global Iter: 413800 training acc: 0.21875
Global Iter: 413900 training loss: 1.94847
Global Iter: 413900 training acc: 0.1875
Global Iter: 414000 training loss: 1.94917
Global Iter: 414000 training acc: 0.0625
Global Iter: 414100 training loss: 1.98292
Global Iter: 414100 training acc: 0.1875
Global Iter: 414200 training loss: 1.92731
Global Iter: 414200 training acc: 0.21875
Global Iter: 414300 training loss: 1.95286
Global Iter: 414300 training acc: 0.1875
Global Iter: 414400 training loss: 2.04835
Global Iter: 414400 training acc: 0.0625
Global Iter: 414500 training loss: 1.95778
Global Iter: 414500 training acc: 0.15625
Global Iter: 414600 training loss: 1.92882
Global Iter: 414600 training acc: 0.34375
Global Iter: 414700 training loss: 1.90634
Global Iter: 414700 training acc: 0.15625
Global Iter: 414800 training loss: 1.93833
Global Iter: 414800 training acc: 0.25
Global Iter: 414900 training loss: 2.00843
Global Iter: 414900 training acc: 0.09375
Global Iter: 415000 training loss: 1.92622
Global Iter: 415000 training acc: 0.1875
Global Iter: 415100 training loss: 2.04889
Global Iter: 415100 training acc: 0.21875
Global Iter: 415200 training loss: 2.00762
Global Iter: 415200 training acc: 0.21875
Global Iter: 415300 training loss: 2.03395
Global Iter: 415300 training acc: 0.1875
Global Iter: 415400 training loss: 1.91266
Global Iter: 415400 training acc: 0.375
Global Iter: 415500 training loss: 2.02382
Global Iter: 415500 training acc: 0.25
Global Iter: 415600 training loss: 2.21865
Global Iter: 415600 training acc: 0.0625
Global Iter: 415700 training loss: 1.94754
Global Iter: 415700 training acc: 0.21875
Global Iter: 415800 training loss: 2.02801
Global Iter: 415800 training acc: 0.1875
Global Iter: 415900 training loss: 2.14557
Global Iter: 415900 training acc: 0.0625
Global Iter: 416000 training loss: 1.9866
Global Iter: 416000 training acc: 0.25
Global Iter: 416100 training loss: 1.94771
Global Iter: 416100 training acc: 0.25
Global Iter: 416200 training loss: 2.12278
Global Iter: 416200 training acc: 0.125
Global Iter: 416300 training loss: 1.94388
Global Iter: 416300 training acc: 0.25
Global Iter: 416400 training loss: 2.01762
Global Iter: 416400 training acc: 0.09375
Global Iter: 416500 training loss: 1.96604
Global Iter: 416500 training acc: 0.21875
Global Iter: 416600 training loss: 2.05377
Global Iter: 416600 training acc: 0.15625
Global Iter: 416700 training loss: 1.92866
Global Iter: 416700 training acc: 0.25
Global Iter: 416800 training loss: 2.02808
Global Iter: 416800 training acc: 0.1875
Global Iter: 416900 training loss: 2.01646
Global Iter: 416900 training acc: 0.0625
Global Iter: 417000 training loss: 1.90102
Global Iter: 417000 training acc: 0.15625
Global Iter: 417100 training loss: 2.01572
Global Iter: 417100 training acc: 0.15625
Global Iter: 417200 training loss: 1.93099
Global Iter: 417200 training acc: 0.21875
Global Iter: 417300 training loss: 1.99499
Global Iter: 417300 training acc: 0.1875
Global Iter: 417400 training loss: 1.99495
Global Iter: 417400 training acc: 0.21875
Global Iter: 417500 training loss: 1.95914
Global Iter: 417500 training acc: 0.125
Global Iter: 417600 training loss: 1.98345
Global Iter: 417600 training acc: 0.1875
Global Iter: 417700 training loss: 1.92152
Global Iter: 417700 training acc: 0.15625
Global Iter: 417800 training loss: 1.97911
Global Iter: 417800 training acc: 0.15625
Global Iter: 417900 training loss: 1.93854
Global Iter: 417900 training acc: 0.28125
Global Iter: 418000 training loss: 2.04671
Global Iter: 418000 training acc: 0.21875
Global Iter: 418100 training loss: 2.12396
Global Iter: 418100 training acc: 0.03125
Global Iter: 418200 training loss: 1.92372
Global Iter: 418200 training acc: 0.15625
Global Iter: 418300 training loss: 2.05312
Global Iter: 418300 training acc: 0.0625
Global Iter: 418400 training loss: 1.88129
Global Iter: 418400 training acc: 0.25
Global Iter: 418500 training loss: 2.01074
Global Iter: 418500 training acc: 0.1875
Global Iter: 418600 training loss: 2.03678
Global Iter: 418600 training acc: 0.1875
Global Iter: 418700 training loss: 1.90982
Global Iter: 418700 training acc: 0.25
Global Iter: 418800 training loss: 1.88244
Global Iter: 418800 training acc: 0.1875
Global Iter: 418900 training loss: 2.04938
Global Iter: 418900 training acc: 0.15625
Global Iter: 419000 training loss: 2.05452
Global Iter: 419000 training acc: 0.25
Global Iter: 419100 training loss: 1.96486
Global Iter: 419100 training acc: 0.25
Global Iter: 419200 training loss: 2.02896
Global Iter: 419200 training acc: 0.125
Global Iter: 419300 training loss: 2.09129
Global Iter: 419300 training acc: 0.09375
Global Iter: 419400 training loss: 1.9469
Global Iter: 419400 training acc: 0.25
Global Iter: 419500 training loss: 2.02723
Global Iter: 419500 training acc: 0.15625
Global Iter: 419600 training loss: 1.93552
Global Iter: 419600 training acc: 0.25
Global Iter: 419700 training loss: 1.93164
Global Iter: 419700 training acc: 0.21875
Global Iter: 419800 training loss: 1.95921
Global Iter: 419800 training acc: 0.15625
Global Iter: 419900 training loss: 2.04115
Global Iter: 419900 training acc: 0.28125
Global Iter: 420000 training loss: 2.06605
Global Iter: 420000 training acc: 0.15625
Global Iter: 420100 training loss: 1.94888
Global Iter: 420100 training acc: 0.1875
Global Iter: 420200 training loss: 1.97981
Global Iter: 420200 training acc: 0.21875
Global Iter: 420300 training loss: 2.02915
Global Iter: 420300 training acc: 0.125
Global Iter: 420400 training loss: 2.05753
Global Iter: 420400 training acc: 0.09375
Global Iter: 420500 training loss: 2.06246
Global Iter: 420500 training acc: 0.21875
Global Iter: 420600 training loss: 1.97513
Global Iter: 420600 training acc: 0.1875
Global Iter: 420700 training loss: 1.98299
Global Iter: 420700 training acc: 0.25
Global Iter: 420800 training loss: 1.98333
Global Iter: 420800 training acc: 0.15625
Global Iter: 420900 training loss: 1.86715
Global Iter: 420900 training acc: 0.21875
Global Iter: 421000 training loss: 1.99354
Global Iter: 421000 training acc: 0.125
Global Iter: 421100 training loss: 1.96646
Global Iter: 421100 training acc: 0.25
Global Iter: 421200 training loss: 2.11005
Global Iter: 421200 training acc: 0.09375
Global Iter: 421300 training loss: 2.00212
Global Iter: 421300 training acc: 0.21875
Global Iter: 421400 training loss: 1.87638
Global Iter: 421400 training acc: 0.21875
Global Iter: 421500 training loss: 1.89206
Global Iter: 421500 training acc: 0.1875
Global Iter: 421600 training loss: 1.99251
Global Iter: 421600 training acc: 0.15625
Global Iter: 421700 training loss: 1.91704
Global Iter: 421700 training acc: 0.34375
Global Iter: 421800 training loss: 1.95734
Global Iter: 421800 training acc: 0.28125
Global Iter: 421900 training loss: 2.00855
Global Iter: 421900 training acc: 0.125
Global Iter: 422000 training loss: 2.01099
Global Iter: 422000 training acc: 0.21875
Global Iter: 422100 training loss: 1.95915
Global Iter: 422100 training acc: 0.125
Global Iter: 422200 training loss: 1.9616
Global Iter: 422200 training acc: 0.09375
Global Iter: 422300 training loss: 2.04144
Global Iter: 422300 training acc: 0.09375
Global Iter: 422400 training loss: 2.07982
Global Iter: 422400 training acc: 0.125
Global Iter: 422500 training loss: 2.08851
Global Iter: 422500 training acc: 0.21875
Global Iter: 422600 training loss: 2.11508
Global Iter: 422600 training acc: 0.15625
Global Iter: 422700 training loss: 1.95919
Global Iter: 422700 training acc: 0.21875
Global Iter: 422800 training loss: 1.97223
Global Iter: 422800 training acc: 0.15625
Global Iter: 422900 training loss: 2.07945
Global Iter: 422900 training acc: 0.25
Global Iter: 423000 training loss: 2.02137
Global Iter: 423000 training acc: 0.3125
Global Iter: 423100 training loss: 1.87877
Global Iter: 423100 training acc: 0.1875
Global Iter: 423200 training loss: 1.9555
Global Iter: 423200 training acc: 0.21875
Global Iter: 423300 training loss: 1.94513
Global Iter: 423300 training acc: 0.09375
Global Iter: 423400 training loss: 2.00638
Global Iter: 423400 training acc: 0.1875
Global Iter: 423500 training loss: 2.00635
Global Iter: 423500 training acc: 0.09375
Global Iter: 423600 training loss: 1.92774
Global Iter: 423600 training acc: 0.15625
Global Iter: 423700 training loss: 1.95062
Global Iter: 423700 training acc: 0.15625
Global Iter: 423800 training loss: 2.06434
Global Iter: 423800 training acc: 0.21875
Global Iter: 423900 training loss: 1.97512
Global Iter: 423900 training acc: 0.34375
Global Iter: 424000 training loss: 1.9019
Global Iter: 424000 training acc: 0.25
Global Iter: 424100 training loss: 1.99259
Global Iter: 424100 training acc: 0.125
Global Iter: 424200 training loss: 1.87961
Global Iter: 424200 training acc: 0.3125
Global Iter: 424300 training loss: 1.96442
Global Iter: 424300 training acc: 0.21875
Global Iter: 424400 training loss: 1.92137
Global Iter: 424400 training acc: 0.125
Global Iter: 424500 training loss: 2.01014
Global Iter: 424500 training acc: 0.21875
Global Iter: 424600 training loss: 2.18058
Global Iter: 424600 training acc: 0.15625
Global Iter: 424700 training loss: 2.0616
Global Iter: 424700 training acc: 0.15625
Global Iter: 424800 training loss: 2.00198
Global Iter: 424800 training acc: 0.21875
Global Iter: 424900 training loss: 2.05478
Global Iter: 424900 training acc: 0.125
Global Iter: 425000 training loss: 2.05369
Global Iter: 425000 training acc: 0.1875
Global Iter: 425100 training loss: 2.00025
Global Iter: 425100 training acc: 0.3125
Global Iter: 425200 training loss: 1.93289
Global Iter: 425200 training acc: 0.125
Global Iter: 425300 training loss: 1.96328
Global Iter: 425300 training acc: 0.15625
Global Iter: 425400 trai2017-06-21 17:01:35.637187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-426843
ning loss: 1.96658
Global Iter: 425400 training acc: 0.21875
Global Iter: 425500 training loss: 1.90501
Global Iter: 425500 training acc: 0.15625
Global Iter: 425600 training loss: 1.95342
Global Iter: 425600 training acc: 0.28125
Global Iter: 425700 training loss: 2.02177
Global Iter: 425700 training acc: 0.21875
Global Iter: 425800 training loss: 2.02721
Global Iter: 425800 training acc: 0.125
Global Iter: 425900 training loss: 1.97407
Global Iter: 425900 training acc: 0.09375
Global Iter: 426000 training loss: 2.02691
Global Iter: 426000 training acc: 0.3125
Global Iter: 426100 training loss: 1.9574
Global Iter: 426100 training acc: 0.1875
Global Iter: 426200 training loss: 1.99299
Global Iter: 426200 training acc: 0.09375
Global Iter: 426300 training loss: 1.99113
Global Iter: 426300 training acc: 0.25
Global Iter: 426400 training loss: 2.02963
Global Iter: 426400 training acc: 0.15625
Global Iter: 426500 training loss: 1.95153
Global Iter: 426500 training acc: 0.25
Global Iter: 426600 training loss: 2.01309
Global Iter: 426600 training acc: 0.09375
Global Iter: 426700 training loss: 2.01179
Global Iter: 426700 training acc: 0.09375
Global Iter: 426800 training loss: 2.08473
Global Iter: 426800 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-426843
Number of Patches: 239092
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-426843
Global Iter: 426900 training loss: 1.91756
Global Iter: 426900 training acc: 0.15625
Global Iter: 427000 training loss: 1.99317
Global Iter: 427000 training acc: 0.28125
Global Iter: 427100 training loss: 1.85992
Global Iter: 427100 training acc: 0.34375
Global Iter: 427200 training loss: 1.95888
Global Iter: 427200 training acc: 0.28125
Global Iter: 427300 training loss: 1.92878
Global Iter: 427300 training acc: 0.15625
Global Iter: 427400 training loss: 1.93576
Global Iter: 427400 training acc: 0.25
Global Iter: 427500 training loss: 1.96321
Global Iter: 427500 training acc: 0.21875
Global Iter: 427600 training loss: 1.8935
Global Iter: 427600 training acc: 0.3125
Global Iter: 427700 training loss: 1.99444
Global Iter: 427700 training acc: 0.25
Global Iter: 427800 training loss: 1.9115
Global Iter: 427800 training acc: 0.25
Global Iter: 427900 training loss: 1.92319
Global Iter: 427900 training acc: 0.28125
Global Iter: 428000 training loss: 1.93421
Global Iter: 428000 training acc: 0.28125
Global Iter: 428100 training loss: 1.99875
Global Iter: 428100 training acc: 0.21875
Global Iter: 428200 training loss: 2.01535
Global Iter: 428200 training acc: 0.125
Global Iter: 428300 training loss: 1.91585
Global Iter: 428300 training acc: 0.21875
Global Iter: 428400 training loss: 1.997
Global Iter: 428400 training acc: 0.09375
Global Iter: 428500 training loss: 1.96074
Global Iter: 428500 training acc: 0.3125
Global Iter: 428600 training loss: 2.09476
Global Iter: 428600 training acc: 0.21875
Global Iter: 428700 training loss: 1.91458
Global Iter: 428700 training acc: 0.09375
Global Iter: 428800 training loss: 2.03176
Global Iter: 428800 training acc: 0.125
Global Iter: 428900 training loss: 1.95577
Global Iter: 428900 training acc: 0.09375
Global Iter: 429000 training loss: 1.96868
Global Iter: 429000 training acc: 0.21875
Global Iter: 429100 training loss: 2.10307
Global Iter: 429100 training acc: 0.1875
Global Iter: 429200 training loss: 1.95452
Global Iter: 429200 training acc: 0.1875
Global Iter: 429300 training loss: 2.00722
Global Iter: 429300 training acc: 0.15625
Global Iter: 429400 training loss: 2.11784
Global Iter: 429400 training acc: 0.0625
Global Iter: 429500 training loss: 2.01553
Global Iter: 429500 training acc: 0.25
Global Iter: 429600 training loss: 2.01053
Global Iter: 429600 training acc: 0.1875
Global Iter: 429700 training loss: 2.01606
Global Iter: 429700 training acc: 0.21875
Global Iter: 429800 training loss: 2.00574
Global Iter: 429800 training acc: 0.15625
Global Iter: 429900 training loss: 1.97236
Global Iter: 429900 training acc: 0.1875
Global Iter: 430000 training loss: 1.97352
Global Iter: 430000 training acc: 0.125
Global Iter: 430100 training loss: 2.01861
Global Iter: 430100 training acc: 0.15625
Global Iter: 430200 training loss: 2.0075
Global Iter: 430200 training acc: 0.09375
Global Iter: 430300 training loss: 1.97456
Global Iter: 430300 training acc: 0.15625
Global Iter: 430400 training loss: 1.91053
Global Iter: 430400 training acc: 0.25
Global Iter: 430500 training loss: 1.98436
Global Iter: 430500 training acc: 0.1875
Global Iter: 430600 training loss: 2.01257
Global Iter: 430600 training acc: 0.1875
Global Iter: 430700 training loss: 2.09795
Global Iter: 430700 training acc: 0.03125
Global Iter: 430800 training loss: 1.93877
Global Iter: 430800 training acc: 0.25
Global Iter: 430900 training loss: 2.04583
Global Iter: 430900 training acc: 0.1875
Global Iter: 431000 training loss: 1.94132
Global Iter: 431000 training acc: 0.25
Global Iter: 431100 training loss: 1.9485
Global Iter: 431100 training acc: 0.21875
Global Iter: 431200 training loss: 1.87528
Global Iter: 431200 training acc: 0.28125
Global Iter: 431300 training loss: 1.90221
Global Iter: 431300 training acc: 0.375
Global Iter: 431400 training loss: 2.05705
Global Iter: 431400 training acc: 0.09375
Global Iter: 431500 training loss: 2.04821
Global Iter: 431500 training acc: 0.1875
Global Iter: 431600 training loss: 2.05303
Global Iter: 431600 training acc: 0.1875
Global Iter: 431700 training loss: 2.0164
Global Iter: 431700 training acc: 0.09375
Global Iter: 431800 training loss: 1.92511
Global Iter: 431800 training acc: 0.15625
Global Iter: 431900 training loss: 1.89195
Global Iter: 431900 training acc: 0.25
Global Iter: 432000 training loss: 1.99076
Global Iter: 432000 training acc: 0.21875
Global Iter: 432100 training loss: 2.12202
Global Iter: 432100 training acc: 0.125
Global Iter: 432200 training loss: 1.99283
Global Iter: 432200 training acc: 0.09375
Global Iter: 432300 training loss: 1.86469
Global Iter: 432300 training acc: 0.1875
Global Iter: 432400 training loss: 1.96212
Global Iter: 432400 training acc: 0.15625
Global Iter: 432500 training loss: 2.04779
Global Iter: 432500 training acc: 0.15625
Global Iter: 432600 training loss: 1.98404
Global Iter: 432600 training acc: 0.1875
Global Iter: 432700 training loss: 1.92023
Global Iter: 432700 training acc: 0.25
Global Iter: 432800 training loss: 2.07524
Global Iter: 432800 training acc: 0.1875
Global Iter: 432900 training loss: 1.98783
Global Iter: 432900 training acc: 0.15625
Global Iter: 433000 training loss: 1.98569
Global Iter: 433000 training acc: 0.15625
Global Iter: 433100 training loss: 1.92165
Global Iter: 433100 training acc: 0.28125
Global Iter: 433200 training loss: 2.0036
Global Iter: 433200 training acc: 0.1875
Global Iter: 433300 training loss: 2.04119
Global Iter: 433300 training acc: 0.125
Global Iter: 433400 training loss: 2.0219
Global Iter: 433400 training acc: 0.125
Global Iter: 433500 training loss: 1.98626
Global Iter: 433500 training acc: 0.25
Global Iter: 433600 training loss: 1.88606
Global Iter: 433600 training acc: 0.21875
Global Iter: 433700 training loss: 1.98211
Global Iter: 433700 training acc: 0.25
Global Iter: 433800 training loss: 1.97365
Global Iter: 433800 training acc: 0.21875
Global Iter: 433900 training loss: 1.94311
Global Iter: 433900 training acc: 0.1875
Global Iter: 434000 training loss: 1.9499
Global Iter: 434000 training acc: 0.1875
Global Iter: 434100 training loss: 1.88025
Global Iter: 434100 training acc: 0.34375
Global Iter: 434200 training loss: 2.0063
Global Iter: 434200 training acc: 0.21875
Global Iter: 434300 training loss: 1.91323
Global Iter: 434300 training acc: 0.25
Global Iter: 434400 training loss: 1.90596
Global Iter: 434400 training acc: 0.25
Global Iter: 434500 training loss: 1.98608
Global Iter: 434500 training acc: 0.25
Global Iter: 434600 training loss: 2.02244
Global Iter: 434600 training acc: 0.21875
Global Iter: 434700 training loss: 1.87494
Global Iter: 434700 training acc: 0.28125
Global Iter: 434800 training loss: 1.99494
Global Iter: 434800 training acc: 0.28125
Global Iter: 434900 training loss: 2.00842
Global Iter: 434900 training acc: 0.15625
Global Iter: 435000 training loss: 2.00346
Global Iter: 435000 training acc: 0.125
Global Iter: 435100 training loss: 1.96745
Global Iter: 435100 training acc: 0.125
Global Iter: 435200 training loss: 2.04285
Global Iter: 435200 training acc: 0.15625
Global Iter: 435300 training loss: 2.06739
Global Iter: 435300 training acc: 0.0
Global Iter: 435400 training loss: 2.10057
Global Iter: 435400 training acc: 0.09375
Global Iter: 435500 training loss: 1.94913
Global Iter: 435500 training acc: 0.25
Global Iter: 435600 training loss: 1.94184
Global Iter: 435600 training acc: 0.125
Global Iter: 435700 training loss: 2.00459
Global Iter: 435700 training acc: 0.21875
Global Iter: 435800 training loss: 1.99668
Global Iter: 435800 training acc: 0.1875
Global Iter: 435900 training loss: 1.91603
Global Iter: 435900 training acc: 0.21875
Global Iter: 436000 training loss: 1.85894
Global Iter: 436000 training acc: 0.25
Global Iter: 436100 training loss: 1.94724
Global Iter: 436100 training acc: 0.125
Global Iter: 436200 training loss: 2.05434
Global Iter: 436200 training acc: 0.125
Global Iter: 436300 training loss: 1.96813
Global Iter: 436300 training acc: 0.1875
Global Iter: 436400 training loss: 2.09842
Global Iter: 436400 training acc: 0.125
Global Iter: 436500 training loss: 1.93194
Global Iter: 436500 training acc: 0.25
Global Iter: 436600 training loss: 2.07639
Global Iter: 436600 training acc: 0.15625
Global Iter: 436700 training loss: 1.99317
Global Iter: 436700 training acc: 0.125
Global Iter: 436800 training loss: 1.94673
Global Iter: 436800 training acc: 0.21875
Global Iter: 436900 training loss: 2.07664
Global Iter: 436900 training acc: 0.1875
Global Iter: 437000 training loss: 2.03073
Global Iter: 437000 training acc: 0.15625
Global Iter: 437100 training loss: 1.87668
Global Iter: 437100 training acc: 0.3125
Global Iter: 437200 training loss: 1.94975
Global Iter: 437200 training acc: 0.1875
Global Iter: 437300 training loss: 1.90185
Global Iter: 437300 training acc: 0.28125
Global Iter: 437400 training loss: 1.97057
Global Iter: 437400 training acc: 0.25
Global Iter: 437500 training loss: 1.89503
Global Iter: 437500 training acc: 0.28125
Global Iter: 437600 training loss: 2.01427
Global Iter: 437600 training acc: 0.15625
Global Iter: 437700 training loss: 1.97244
Global Iter: 437700 training acc: 0.21875
Global Iter: 437800 training loss: 1.9317
Global Iter: 437800 training acc: 0.28125
Global Iter: 437900 training loss: 2.07438
Global Iter: 437900 training acc: 0.1875
Global Iter: 438000 training loss: 2.0663
Global Iter: 438000 training acc: 0.0625
Global Iter: 438100 training loss: 1.9216
Global Iter: 438100 training acc: 0.25
Global Iter: 438200 training loss: 2.00086
Global Iter: 438200 training acc: 0.125
Global Iter: 438300 training loss: 1.96729
Global Iter: 438300 training acc: 0.125
Global Iter: 438400 training loss: 1.91023
Global Iter: 438400 training acc: 0.25
Global Iter: 438500 training loss: 2.0477
Global Iter: 438500 training acc: 0.25
Global Iter: 438600 training loss: 2.05988
Global Iter: 438600 training acc: 0.15625
Global Iter: 438700 training loss: 1.97504
Global Iter: 438700 training acc: 0.25
Global Iter: 438800 training loss: 2.06337
Global Iter: 438800 training acc: 0.21875
Global Iter: 438900 training loss: 2.1568
Global Iter: 438900 training acc: 0.125
Global Iter: 439000 training loss: 1.95674
Global Iter: 439000 training acc: 0.15625
Global Iter: 439100 training loss: 1.99555
Global Iter: 439100 training acc: 0.125
Global Iter: 439200 training loss: 1.9148
Global Iter: 439200 training acc: 0.28125
Global Iter: 439300 training loss: 1.95314
Global Iter: 439300 training acc: 0.21875
Global Iter: 439400 training loss: 1.90508
Global Iter: 439400 training acc: 0.375
Global Iter: 439500 training loss: 2.13134
Global Iter: 439500 training acc: 0.03125
Global Iter: 439600 training loss: 1.91611
Global Iter: 439600 training acc: 0.15625
Global Iter: 439700 training loss: 1.98629
Global Iter: 439700 training acc: 0.25
Global Iter: 2017-06-21 17:27:24.922449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-441787
439800 training loss: 2.06579
Global Iter: 439800 training acc: 0.125
Global Iter: 439900 training loss: 1.98339
Global Iter: 439900 training acc: 0.25
Global Iter: 440000 training loss: 1.95603
Global Iter: 440000 training acc: 0.15625
Global Iter: 440100 training loss: 1.96557
Global Iter: 440100 training acc: 0.15625
Global Iter: 440200 training loss: 1.909
Global Iter: 440200 training acc: 0.21875
Global Iter: 440300 training loss: 1.9098
Global Iter: 440300 training acc: 0.21875
Global Iter: 440400 training loss: 1.94842
Global Iter: 440400 training acc: 0.1875
Global Iter: 440500 training loss: 1.96036
Global Iter: 440500 training acc: 0.21875
Global Iter: 440600 training loss: 1.95895
Global Iter: 440600 training acc: 0.1875
Global Iter: 440700 training loss: 2.1026
Global Iter: 440700 training acc: 0.0625
Global Iter: 440800 training loss: 1.91905
Global Iter: 440800 training acc: 0.34375
Global Iter: 440900 training loss: 2.03834
Global Iter: 440900 training acc: 0.09375
Global Iter: 441000 training loss: 1.98464
Global Iter: 441000 training acc: 0.1875
Global Iter: 441100 training loss: 1.91055
Global Iter: 441100 training acc: 0.21875
Global Iter: 441200 training loss: 2.0153
Global Iter: 441200 training acc: 0.3125
Global Iter: 441300 training loss: 1.93942
Global Iter: 441300 training acc: 0.15625
Global Iter: 441400 training loss: 1.8763
Global Iter: 441400 training acc: 0.34375
Global Iter: 441500 training loss: 2.07724
Global Iter: 441500 training acc: 0.1875
Global Iter: 441600 training loss: 1.98297
Global Iter: 441600 training acc: 0.125
Global Iter: 441700 training loss: 1.92976
Global Iter: 441700 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-441787
Number of Patches: 236702
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-441787
Global Iter: 441800 training loss: 1.96285
Global Iter: 441800 training acc: 0.3125
Global Iter: 441900 training loss: 2.0348
Global Iter: 441900 training acc: 0.0625
Global Iter: 442000 training loss: 1.99155
Global Iter: 442000 training acc: 0.125
Global Iter: 442100 training loss: 1.93935
Global Iter: 442100 training acc: 0.28125
Global Iter: 442200 training loss: 2.00488
Global Iter: 442200 training acc: 0.1875
Global Iter: 442300 training loss: 2.01973
Global Iter: 442300 training acc: 0.21875
Global Iter: 442400 training loss: 1.99897
Global Iter: 442400 training acc: 0.1875
Global Iter: 442500 training loss: 2.00242
Global Iter: 442500 training acc: 0.15625
Global Iter: 442600 training loss: 1.90689
Global Iter: 442600 training acc: 0.1875
Global Iter: 442700 training loss: 1.99931
Global Iter: 442700 training acc: 0.15625
Global Iter: 442800 training loss: 2.08101
Global Iter: 442800 training acc: 0.1875
Global Iter: 442900 training loss: 1.97366
Global Iter: 442900 training acc: 0.15625
Global Iter: 443000 training loss: 2.10041
Global Iter: 443000 training acc: 0.21875
Global Iter: 443100 training loss: 1.92249
Global Iter: 443100 training acc: 0.28125
Global Iter: 443200 training loss: 1.96119
Global Iter: 443200 training acc: 0.1875
Global Iter: 443300 training loss: 1.981
Global Iter: 443300 training acc: 0.25
Global Iter: 443400 training loss: 1.99642
Global Iter: 443400 training acc: 0.15625
Global Iter: 443500 training loss: 2.00321
Global Iter: 443500 training acc: 0.21875
Global Iter: 443600 training loss: 1.91332
Global Iter: 443600 training acc: 0.1875
Global Iter: 443700 training loss: 1.95958
Global Iter: 443700 training acc: 0.1875
Global Iter: 443800 training loss: 1.98415
Global Iter: 443800 training acc: 0.125
Global Iter: 443900 training loss: 1.94491
Global Iter: 443900 training acc: 0.15625
Global Iter: 444000 training loss: 2.00965
Global Iter: 444000 training acc: 0.25
Global Iter: 444100 training loss: 1.99993
Global Iter: 444100 training acc: 0.34375
Global Iter: 444200 training loss: 1.98761
Global Iter: 444200 training acc: 0.1875
Global Iter: 444300 training loss: 2.30787
Global Iter: 444300 training acc: 0.09375
Global Iter: 444400 training loss: 1.97802
Global Iter: 444400 training acc: 0.25
Global Iter: 444500 training loss: 2.02061
Global Iter: 444500 training acc: 0.15625
Global Iter: 444600 training loss: 2.07473
Global Iter: 444600 training acc: 0.28125
Global Iter: 444700 training loss: 2.00052
Global Iter: 444700 training acc: 0.28125
Global Iter: 444800 training loss: 1.98693
Global Iter: 444800 training acc: 0.25
Global Iter: 444900 training loss: 1.91116
Global Iter: 444900 training acc: 0.28125
Global Iter: 445000 training loss: 1.90124
Global Iter: 445000 training acc: 0.25
Global Iter: 445100 training loss: 2.0056
Global Iter: 445100 training acc: 0.21875
Global Iter: 445200 training loss: 1.96415
Global Iter: 445200 training acc: 0.21875
Global Iter: 445300 training loss: 2.01338
Global Iter: 445300 training acc: 0.15625
Global Iter: 445400 training loss: 1.94034
Global Iter: 445400 training acc: 0.28125
Global Iter: 445500 training loss: 2.04746
Global Iter: 445500 training acc: 0.125
Global Iter: 445600 training loss: 2.00072
Global Iter: 445600 training acc: 0.1875
Global Iter: 445700 training loss: 1.95251
Global Iter: 445700 training acc: 0.28125
Global Iter: 445800 training loss: 1.95975
Global Iter: 445800 training acc: 0.21875
Global Iter: 445900 training loss: 1.93139
Global Iter: 445900 training acc: 0.1875
Global Iter: 446000 training loss: 2.02273
Global Iter: 446000 training acc: 0.21875
Global Iter: 446100 training loss: 1.8991
Global Iter: 446100 training acc: 0.15625
Global Iter: 446200 training loss: 2.0244
Global Iter: 446200 training acc: 0.125
Global Iter: 446300 training loss: 2.02133
Global Iter: 446300 training acc: 0.125
Global Iter: 446400 training loss: 1.92011
Global Iter: 446400 training acc: 0.3125
Global Iter: 446500 training loss: 1.93565
Global Iter: 446500 training acc: 0.1875
Global Iter: 446600 training loss: 1.97168
Global Iter: 446600 training acc: 0.21875
Global Iter: 446700 training loss: 2.09202
Global Iter: 446700 training acc: 0.15625
Global Iter: 446800 training loss: 2.04883
Global Iter: 446800 training acc: 0.1875
Global Iter: 446900 training loss: 2.06066
Global Iter: 446900 training acc: 0.0625
Global Iter: 447000 training loss: 1.96901
Global Iter: 447000 training acc: 0.15625
Global Iter: 447100 training loss: 2.00543
Global Iter: 447100 training acc: 0.21875
Global Iter: 447200 training loss: 2.00953
Global Iter: 447200 training acc: 0.15625
Global Iter: 447300 training loss: 1.96236
Global Iter: 447300 training acc: 0.3125
Global Iter: 447400 training loss: 1.981
Global Iter: 447400 training acc: 0.1875
Global Iter: 447500 training loss: 2.02112
Global Iter: 447500 training acc: 0.21875
Global Iter: 447600 training loss: 1.8808
Global Iter: 447600 training acc: 0.34375
Global Iter: 447700 training loss: 1.99983
Global Iter: 447700 training acc: 0.21875
Global Iter: 447800 training loss: 1.97907
Global Iter: 447800 training acc: 0.09375
Global Iter: 447900 training loss: 1.96582
Global Iter: 447900 training acc: 0.09375
Global Iter: 448000 training loss: 1.97508
Global Iter: 448000 training acc: 0.15625
Global Iter: 448100 training loss: 1.88574
Global Iter: 448100 training acc: 0.46875
Global Iter: 448200 training loss: 1.98321
Global Iter: 448200 training acc: 0.21875
Global Iter: 448300 training loss: 1.92627
Global Iter: 448300 training acc: 0.25
Global Iter: 448400 training loss: 1.94605
Global Iter: 448400 training acc: 0.28125
Global Iter: 448500 training loss: 2.07877
Global Iter: 448500 training acc: 0.3125
Global Iter: 448600 training loss: 2.06468
Global Iter: 448600 training acc: 0.3125
Global Iter: 448700 training loss: 1.97385
Global Iter: 448700 training acc: 0.15625
Global Iter: 448800 training loss: 2.05019
Global Iter: 448800 training acc: 0.125
Global Iter: 448900 training loss: 2.05373
Global Iter: 448900 training acc: 0.125
Global Iter: 449000 training loss: 2.08268
Global Iter: 449000 training acc: 0.125
Global Iter: 449100 training loss: 1.97077
Global Iter: 449100 training acc: 0.1875
Global Iter: 449200 training loss: 1.94703
Global Iter: 449200 training acc: 0.21875
Global Iter: 449300 training loss: 1.94837
Global Iter: 449300 training acc: 0.21875
Global Iter: 449400 training loss: 1.9911
Global Iter: 449400 training acc: 0.125
Global Iter: 449500 training loss: 1.9917
Global Iter: 449500 training acc: 0.0625
Global Iter: 449600 training loss: 1.98725
Global Iter: 449600 training acc: 0.21875
Global Iter: 449700 training loss: 2.02146
Global Iter: 449700 training acc: 0.125
Global Iter: 449800 training loss: 1.98941
Global Iter: 449800 training acc: 0.21875
Global Iter: 449900 training loss: 1.99837
Global Iter: 449900 training acc: 0.09375
Global Iter: 450000 training loss: 1.96919
Global Iter: 450000 training acc: 0.21875
Global Iter: 450100 training loss: 1.92571
Global Iter: 450100 training acc: 0.21875
Global Iter: 450200 training loss: 1.93032
Global Iter: 450200 training acc: 0.25
Global Iter: 450300 training loss: 1.96618
Global Iter: 450300 training acc: 0.21875
Global Iter: 450400 training loss: 2.04102
Global Iter: 450400 training acc: 0.25
Global Iter: 450500 training loss: 2.00384
Global Iter: 450500 training acc: 0.1875
Global Iter: 450600 training loss: 1.96615
Global Iter: 450600 training acc: 0.1875
Global Iter: 450700 training loss: 1.96581
Global Iter: 450700 training acc: 0.1875
Global Iter: 450800 training loss: 1.96291
Global Iter: 450800 training acc: 0.21875
Global Iter: 450900 training loss: 1.97856
Global Iter: 450900 training acc: 0.3125
Global Iter: 451000 training loss: 2.02876
Global Iter: 451000 training acc: 0.1875
Global Iter: 451100 training loss: 1.96887
Global Iter: 451100 training acc: 0.21875
Global Iter: 451200 training loss: 1.91379
Global Iter: 451200 training acc: 0.25
Global Iter: 451300 training loss: 2.11329
Global Iter: 451300 training acc: 0.0625
Global Iter: 451400 training loss: 1.9516
Global Iter: 451400 training acc: 0.25
Global Iter: 451500 training loss: 1.95398
Global Iter: 451500 training acc: 0.15625
Global Iter: 451600 training loss: 1.93467
Global Iter: 451600 training acc: 0.21875
Global Iter: 451700 training loss: 2.04486
Global Iter: 451700 training acc: 0.1875
Global Iter: 451800 training loss: 2.06031
Global Iter: 451800 training acc: 0.125
Global Iter: 451900 training loss: 1.98874
Global Iter: 451900 training acc: 0.15625
Global Iter: 452000 training loss: 2.11085
Global Iter: 452000 training acc: 0.125
Global Iter: 452100 training loss: 2.04114
Global Iter: 452100 training acc: 0.1875
Global Iter: 452200 training loss: 1.92115
Global Iter: 452200 training acc: 0.25
Global Iter: 452300 training loss: 1.92804
Global Iter: 452300 training acc: 0.21875
Global Iter: 452400 training loss: 1.92746
Global Iter: 452400 training acc: 0.3125
Global Iter: 452500 training loss: 2.05182
Global Iter: 452500 training acc: 0.15625
Global Iter: 452600 training loss: 1.99844
Global Iter: 452600 training acc: 0.125
Global Iter: 452700 training loss: 2.0151
Global Iter: 452700 training acc: 0.25
Global Iter: 452800 training loss: 1.99552
Global Iter: 452800 training acc: 0.1875
Global Iter: 452900 training loss: 1.9146
Global Iter: 452900 training acc: 0.1875
Global Iter: 453000 training loss: 1.91781
Global Iter: 453000 training acc: 0.21875
Global Iter: 453100 training loss: 2.07708
Global Iter: 453100 training acc: 0.0625
Global Iter: 453200 training loss: 1.97814
Global Iter: 453200 training acc: 0.21875
Global Iter: 453300 training loss: 2.0134
Global Iter: 453300 training acc: 0.1875
Global Iter: 453400 training loss: 2.03322
Global Iter: 453400 training acc: 0.15625
Global Iter: 453500 training loss: 1.92878
Global Iter: 453500 training acc: 0.1875
Global Iter: 453600 training loss: 1.96939
Global Iter: 453600 training acc: 0.25
Global Iter: 453700 training loss: 2.11562
Global Iter: 453700 training acc: 0.1875
Global Iter: 453800 training loss: 1.89062
Global Iter: 453800 training acc: 0.28125
Global Iter: 453900 training loss: 1.96635
Global Iter: 453900 training acc: 0.25
Global Iter: 454000 training loss: 1.97114
Global Iter: 454000 training acc: 0.1875
Global Iter: 454100 training loss: 1.95505
Global Iter: 454100 2017-06-21 17:53:28.164752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-456581
training acc: 0.21875
Global Iter: 454200 training loss: 1.94285
Global Iter: 454200 training acc: 0.25
Global Iter: 454300 training loss: 1.96937
Global Iter: 454300 training acc: 0.21875
Global Iter: 454400 training loss: 2.1469
Global Iter: 454400 training acc: 0.0625
Global Iter: 454500 training loss: 2.04881
Global Iter: 454500 training acc: 0.15625
Global Iter: 454600 training loss: 2.07318
Global Iter: 454600 training acc: 0.15625
Global Iter: 454700 training loss: 1.96214
Global Iter: 454700 training acc: 0.09375
Global Iter: 454800 training loss: 2.01417
Global Iter: 454800 training acc: 0.1875
Global Iter: 454900 training loss: 1.91252
Global Iter: 454900 training acc: 0.1875
Global Iter: 455000 training loss: 2.028
Global Iter: 455000 training acc: 0.125
Global Iter: 455100 training loss: 2.10207
Global Iter: 455100 training acc: 0.09375
Global Iter: 455200 training loss: 2.047
Global Iter: 455200 training acc: 0.09375
Global Iter: 455300 training loss: 2.07922
Global Iter: 455300 training acc: 0.15625
Global Iter: 455400 training loss: 1.93569
Global Iter: 455400 training acc: 0.15625
Global Iter: 455500 training loss: 1.92461
Global Iter: 455500 training acc: 0.25
Global Iter: 455600 training loss: 1.89492
Global Iter: 455600 training acc: 0.3125
Global Iter: 455700 training loss: 1.98594
Global Iter: 455700 training acc: 0.1875
Global Iter: 455800 training loss: 1.95439
Global Iter: 455800 training acc: 0.15625
Global Iter: 455900 training loss: 1.98244
Global Iter: 455900 training acc: 0.125
Global Iter: 456000 training loss: 1.91961
Global Iter: 456000 training acc: 0.25
Global Iter: 456100 training loss: 1.92546
Global Iter: 456100 training acc: 0.28125
Global Iter: 456200 training loss: 1.94523
Global Iter: 456200 training acc: 0.21875
Global Iter: 456300 training loss: 1.96666
Global Iter: 456300 training acc: 0.1875
Global Iter: 456400 training loss: 1.96005
Global Iter: 456400 training acc: 0.21875
Global Iter: 456500 training loss: 2.03039
Global Iter: 456500 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-456581
Number of Patches: 234335
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-456581
Global Iter: 456600 training loss: 2.0166
Global Iter: 456600 training acc: 0.09375
Global Iter: 456700 training loss: 2.00966
Global Iter: 456700 training acc: 0.28125
Global Iter: 456800 training loss: 1.94909
Global Iter: 456800 training acc: 0.125
Global Iter: 456900 training loss: 1.90062
Global Iter: 456900 training acc: 0.28125
Global Iter: 457000 training loss: 1.94401
Global Iter: 457000 training acc: 0.15625
Global Iter: 457100 training loss: 2.03156
Global Iter: 457100 training acc: 0.25
Global Iter: 457200 training loss: 1.94003
Global Iter: 457200 training acc: 0.3125
Global Iter: 457300 training loss: 1.98475
Global Iter: 457300 training acc: 0.28125
Global Iter: 457400 training loss: 2.06529
Global Iter: 457400 training acc: 0.1875
Global Iter: 457500 training loss: 2.04068
Global Iter: 457500 training acc: 0.125
Global Iter: 457600 training loss: 2.05634
Global Iter: 457600 training acc: 0.1875
Global Iter: 457700 training loss: 2.02095
Global Iter: 457700 training acc: 0.09375
Global Iter: 457800 training loss: 2.06049
Global Iter: 457800 training acc: 0.21875
Global Iter: 457900 training loss: 1.92604
Global Iter: 457900 training acc: 0.25
Global Iter: 458000 training loss: 2.07615
Global Iter: 458000 training acc: 0.125
Global Iter: 458100 training loss: 1.9196
Global Iter: 458100 training acc: 0.21875
Global Iter: 458200 training loss: 2.03474
Global Iter: 458200 training acc: 0.15625
Global Iter: 458300 training loss: 2.01979
Global Iter: 458300 training acc: 0.25
Global Iter: 458400 training loss: 2.05505
Global Iter: 458400 training acc: 0.09375
Global Iter: 458500 training loss: 2.0162
Global Iter: 458500 training acc: 0.09375
Global Iter: 458600 training loss: 1.98204
Global Iter: 458600 training acc: 0.09375
Global Iter: 458700 training loss: 1.91489
Global Iter: 458700 training acc: 0.25
Global Iter: 458800 training loss: 1.86893
Global Iter: 458800 training acc: 0.28125
Global Iter: 458900 training loss: 2.005
Global Iter: 458900 training acc: 0.125
Global Iter: 459000 training loss: 1.96742
Global Iter: 459000 training acc: 0.1875
Global Iter: 459100 training loss: 1.9363
Global Iter: 459100 training acc: 0.1875
Global Iter: 459200 training loss: 1.99432
Global Iter: 459200 training acc: 0.21875
Global Iter: 459300 training loss: 2.03324
Global Iter: 459300 training acc: 0.25
Global Iter: 459400 training loss: 2.1619
Global Iter: 459400 training acc: 0.21875
Global Iter: 459500 training loss: 2.00821
Global Iter: 459500 training acc: 0.03125
Global Iter: 459600 training loss: 2.02208
Global Iter: 459600 training acc: 0.15625
Global Iter: 459700 training loss: 2.06292
Global Iter: 459700 training acc: 0.15625
Global Iter: 459800 training loss: 2.07983
Global Iter: 459800 training acc: 0.1875
Global Iter: 459900 training loss: 1.98847
Global Iter: 459900 training acc: 0.03125
Global Iter: 460000 training loss: 1.96205
Global Iter: 460000 training acc: 0.125
Global Iter: 460100 training loss: 2.01635
Global Iter: 460100 training acc: 0.15625
Global Iter: 460200 training loss: 1.9897
Global Iter: 460200 training acc: 0.09375
Global Iter: 460300 training loss: 2.15154
Global Iter: 460300 training acc: 0.09375
Global Iter: 460400 training loss: 2.0024
Global Iter: 460400 training acc: 0.0625
Global Iter: 460500 training loss: 1.93048
Global Iter: 460500 training acc: 0.21875
Global Iter: 460600 training loss: 2.06437
Global Iter: 460600 training acc: 0.0625
Global Iter: 460700 training loss: 1.97681
Global Iter: 460700 training acc: 0.125
Global Iter: 460800 training loss: 2.02135
Global Iter: 460800 training acc: 0.28125
Global Iter: 460900 training loss: 1.98699
Global Iter: 460900 training acc: 0.15625
Global Iter: 461000 training loss: 2.04539
Global Iter: 461000 training acc: 0.15625
Global Iter: 461100 training loss: 1.95406
Global Iter: 461100 training acc: 0.21875
Global Iter: 461200 training loss: 1.95616
Global Iter: 461200 training acc: 0.3125
Global Iter: 461300 training loss: 1.97208
Global Iter: 461300 training acc: 0.1875
Global Iter: 461400 training loss: 1.97652
Global Iter: 461400 training acc: 0.3125
Global Iter: 461500 training loss: 2.08382
Global Iter: 461500 training acc: 0.25
Global Iter: 461600 training loss: 2.03952
Global Iter: 461600 training acc: 0.125
Global Iter: 461700 training loss: 1.9673
Global Iter: 461700 training acc: 0.21875
Global Iter: 461800 training loss: 1.93523
Global Iter: 461800 training acc: 0.21875
Global Iter: 461900 training loss: 2.02555
Global Iter: 461900 training acc: 0.15625
Global Iter: 462000 training loss: 1.89329
Global Iter: 462000 training acc: 0.21875
Global Iter: 462100 training loss: 1.99862
Global Iter: 462100 training acc: 0.375
Global Iter: 462200 training loss: 2.07323
Global Iter: 462200 training acc: 0.09375
Global Iter: 462300 training loss: 1.91013
Global Iter: 462300 training acc: 0.375
Global Iter: 462400 training loss: 1.96374
Global Iter: 462400 training acc: 0.125
Global Iter: 462500 training loss: 1.96105
Global Iter: 462500 training acc: 0.0625
Global Iter: 462600 training loss: 2.01342
Global Iter: 462600 training acc: 0.15625
Global Iter: 462700 training loss: 1.95399
Global Iter: 462700 training acc: 0.25
Global Iter: 462800 training loss: 2.01764
Global Iter: 462800 training acc: 0.0625
Global Iter: 462900 training loss: 1.99376
Global Iter: 462900 training acc: 0.15625
Global Iter: 463000 training loss: 1.96249
Global Iter: 463000 training acc: 0.3125
Global Iter: 463100 training loss: 1.89558
Global Iter: 463100 training acc: 0.21875
Global Iter: 463200 training loss: 1.94058
Global Iter: 463200 training acc: 0.21875
Global Iter: 463300 training loss: 2.01532
Global Iter: 463300 training acc: 0.15625
Global Iter: 463400 training loss: 2.13092
Global Iter: 463400 training acc: 0.03125
Global Iter: 463500 training loss: 1.88268
Global Iter: 463500 training acc: 0.3125
Global Iter: 463600 training loss: 2.11999
Global Iter: 463600 training acc: 0.03125
Global Iter: 463700 training loss: 1.79606
Global Iter: 463700 training acc: 0.40625
Global Iter: 463800 training loss: 1.93828
Global Iter: 463800 training acc: 0.1875
Global Iter: 463900 training loss: 2.08163
Global Iter: 463900 training acc: 0.125
Global Iter: 464000 training loss: 1.92426
Global Iter: 464000 training acc: 0.21875
Global Iter: 464100 training loss: 1.93969
Global Iter: 464100 training acc: 0.1875
Global Iter: 464200 training loss: 2.00974
Global Iter: 464200 training acc: 0.0625
Global Iter: 464300 training loss: 1.95986
Global Iter: 464300 training acc: 0.09375
Global Iter: 464400 training loss: 1.96176
Global Iter: 464400 training acc: 0.1875
Global Iter: 464500 training loss: 2.01927
Global Iter: 464500 training acc: 0.1875
Global Iter: 464600 training loss: 1.89252
Global Iter: 464600 training acc: 0.1875
Global Iter: 464700 training loss: 1.98116
Global Iter: 464700 training acc: 0.28125
Global Iter: 464800 training loss: 2.07116
Global Iter: 464800 training acc: 0.15625
Global Iter: 464900 training loss: 1.95449
Global Iter: 464900 training acc: 0.1875
Global Iter: 465000 training loss: 1.99486
Global Iter: 465000 training acc: 0.1875
Global Iter: 465100 training loss: 1.93453
Global Iter: 465100 training acc: 0.09375
Global Iter: 465200 training loss: 1.9017
Global Iter: 465200 training acc: 0.3125
Global Iter: 465300 training loss: 2.07591
Global Iter: 465300 training acc: 0.125
Global Iter: 465400 training loss: 1.87601
Global Iter: 465400 training acc: 0.21875
Global Iter: 465500 training loss: 1.98082
Global Iter: 465500 training acc: 0.15625
Global Iter: 465600 training loss: 2.04889
Global Iter: 465600 training acc: 0.125
Global Iter: 465700 training loss: 1.97063
Global Iter: 465700 training acc: 0.1875
Global Iter: 465800 training loss: 1.88653
Global Iter: 465800 training acc: 0.28125
Global Iter: 465900 training loss: 2.10267
Global Iter: 465900 training acc: 0.21875
Global Iter: 466000 training loss: 1.92272
Global Iter: 466000 training acc: 0.28125
Global Iter: 466100 training loss: 2.02727
Global Iter: 466100 training acc: 0.125
Global Iter: 466200 training loss: 1.96137
Global Iter: 466200 training acc: 0.1875
Global Iter: 466300 training loss: 1.94243
Global Iter: 466300 training acc: 0.1875
Global Iter: 466400 training loss: 1.95556
Global Iter: 466400 training acc: 0.21875
Global Iter: 466500 training loss: 2.07721
Global Iter: 466500 training acc: 0.125
Global Iter: 466600 training loss: 2.02012
Global Iter: 466600 training acc: 0.21875
Global Iter: 466700 training loss: 1.92968
Global Iter: 466700 training acc: 0.1875
Global Iter: 466800 training loss: 1.93267
Global Iter: 466800 training acc: 0.1875
Global Iter: 466900 training loss: 1.9629
Global Iter: 466900 training acc: 0.3125
Global Iter: 467000 training loss: 1.92479
Global Iter: 467000 training acc: 0.1875
Global Iter: 467100 training loss: 2.06119
Global Iter: 467100 training acc: 0.125
Global Iter: 467200 training loss: 1.91365
Global Iter: 467200 training acc: 0.15625
Global Iter: 467300 training loss: 1.89821
Global Iter: 467300 training acc: 0.25
Global Iter: 467400 training loss: 2.04118
Global Iter: 467400 training acc: 0.15625
Global Iter: 467500 training loss: 2.12651
Global Iter: 467500 training acc: 0.1875
Global Iter: 467600 training loss: 2.02485
Global Iter: 467600 training acc: 0.1875
Global Iter: 467700 training loss: 2.08117
Global Iter: 467700 training acc: 0.1875
Global Iter: 467800 training loss: 2.01811
Global Iter: 467800 training acc: 0.09375
Global Iter: 467900 training loss: 1.85251
Global Iter: 467900 training acc: 0.4375
Global Iter: 468000 training loss: 1.93341
Global Iter: 468000 training acc: 0.125
Global Iter: 468100 training loss: 1.97113
Global Iter: 468100 training acc: 0.1875
Global Iter: 468200 training loss: 1.90383
Global Iter: 468200 training acc: 0.21875
Global Iter: 468300 training loss: 2.04556
Global Iter: 468300 training acc: 0.25
Global Iter: 468400 training loss: 2.00232
Global Iter: 468400 training acc: 0.25
Global Iter: 468500 trai2017-06-21 18:18:54.267535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-471227
ning loss: 1.97996
Global Iter: 468500 training acc: 0.21875
Global Iter: 468600 training loss: 1.95967
Global Iter: 468600 training acc: 0.1875
Global Iter: 468700 training loss: 2.02885
Global Iter: 468700 training acc: 0.125
Global Iter: 468800 training loss: 2.04354
Global Iter: 468800 training acc: 0.125
Global Iter: 468900 training loss: 1.87859
Global Iter: 468900 training acc: 0.28125
Global Iter: 469000 training loss: 1.92506
Global Iter: 469000 training acc: 0.34375
Global Iter: 469100 training loss: 1.96813
Global Iter: 469100 training acc: 0.125
Global Iter: 469200 training loss: 1.968
Global Iter: 469200 training acc: 0.25
Global Iter: 469300 training loss: 1.86255
Global Iter: 469300 training acc: 0.25
Global Iter: 469400 training loss: 1.9834
Global Iter: 469400 training acc: 0.0625
Global Iter: 469500 training loss: 2.01289
Global Iter: 469500 training acc: 0.21875
Global Iter: 469600 training loss: 1.95281
Global Iter: 469600 training acc: 0.1875
Global Iter: 469700 training loss: 2.06652
Global Iter: 469700 training acc: 0.15625
Global Iter: 469800 training loss: 2.01838
Global Iter: 469800 training acc: 0.0625
Global Iter: 469900 training loss: 2.04425
Global Iter: 469900 training acc: 0.09375
Global Iter: 470000 training loss: 1.96758
Global Iter: 470000 training acc: 0.25
Global Iter: 470100 training loss: 1.93178
Global Iter: 470100 training acc: 0.3125
Global Iter: 470200 training loss: 1.94405
Global Iter: 470200 training acc: 0.125
Global Iter: 470300 training loss: 2.02159
Global Iter: 470300 training acc: 0.15625
Global Iter: 470400 training loss: 2.11168
Global Iter: 470400 training acc: 0.0625
Global Iter: 470500 training loss: 1.92897
Global Iter: 470500 training acc: 0.21875
Global Iter: 470600 training loss: 2.10398
Global Iter: 470600 training acc: 0.1875
Global Iter: 470700 training loss: 1.97631
Global Iter: 470700 training acc: 0.03125
Global Iter: 470800 training loss: 1.9574
Global Iter: 470800 training acc: 0.21875
Global Iter: 470900 training loss: 2.03213
Global Iter: 470900 training acc: 0.15625
Global Iter: 471000 training loss: 2.13751
Global Iter: 471000 training acc: 0.125
Global Iter: 471100 training loss: 1.93541
Global Iter: 471100 training acc: 0.21875
Global Iter: 471200 training loss: 1.95158
Global Iter: 471200 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-471227
Number of Patches: 231992
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-471227
Global Iter: 471300 training loss: 2.00648
Global Iter: 471300 training acc: 0.1875
Global Iter: 471400 training loss: 2.13406
Global Iter: 471400 training acc: 0.125
Global Iter: 471500 training loss: 2.00806
Global Iter: 471500 training acc: 0.3125
Global Iter: 471600 training loss: 2.11554
Global Iter: 471600 training acc: 0.1875
Global Iter: 471700 training loss: 1.98983
Global Iter: 471700 training acc: 0.125
Global Iter: 471800 training loss: 2.09173
Global Iter: 471800 training acc: 0.15625
Global Iter: 471900 training loss: 1.91387
Global Iter: 471900 training acc: 0.15625
Global Iter: 472000 training loss: 1.97405
Global Iter: 472000 training acc: 0.15625
Global Iter: 472100 training loss: 1.94526
Global Iter: 472100 training acc: 0.21875
Global Iter: 472200 training loss: 1.98768
Global Iter: 472200 training acc: 0.1875
Global Iter: 472300 training loss: 1.96207
Global Iter: 472300 training acc: 0.15625
Global Iter: 472400 training loss: 1.93353
Global Iter: 472400 training acc: 0.21875
Global Iter: 472500 training loss: 1.94816
Global Iter: 472500 training acc: 0.1875
Global Iter: 472600 training loss: 2.08022
Global Iter: 472600 training acc: 0.09375
Global Iter: 472700 training loss: 2.05084
Global Iter: 472700 training acc: 0.15625
Global Iter: 472800 training loss: 2.03125
Global Iter: 472800 training acc: 0.15625
Global Iter: 472900 training loss: 1.99471
Global Iter: 472900 training acc: 0.25
Global Iter: 473000 training loss: 1.89673
Global Iter: 473000 training acc: 0.21875
Global Iter: 473100 training loss: 1.9284
Global Iter: 473100 training acc: 0.0625
Global Iter: 473200 training loss: 1.99774
Global Iter: 473200 training acc: 0.1875
Global Iter: 473300 training loss: 1.97838
Global Iter: 473300 training acc: 0.1875
Global Iter: 473400 training loss: 1.93264
Global Iter: 473400 training acc: 0.28125
Global Iter: 473500 training loss: 2.01154
Global Iter: 473500 training acc: 0.125
Global Iter: 473600 training loss: 1.94707
Global Iter: 473600 training acc: 0.09375
Global Iter: 473700 training loss: 1.92774
Global Iter: 473700 training acc: 0.25
Global Iter: 473800 training loss: 1.98982
Global Iter: 473800 training acc: 0.0625
Global Iter: 473900 training loss: 2.02243
Global Iter: 473900 training acc: 0.21875
Global Iter: 474000 training loss: 1.90962
Global Iter: 474000 training acc: 0.1875
Global Iter: 474100 training loss: 1.93182
Global Iter: 474100 training acc: 0.25
Global Iter: 474200 training loss: 1.93826
Global Iter: 474200 training acc: 0.25
Global Iter: 474300 training loss: 1.97077
Global Iter: 474300 training acc: 0.25
Global Iter: 474400 training loss: 1.99543
Global Iter: 474400 training acc: 0.1875
Global Iter: 474500 training loss: 1.94151
Global Iter: 474500 training acc: 0.25
Global Iter: 474600 training loss: 1.93692
Global Iter: 474600 training acc: 0.25
Global Iter: 474700 training loss: 1.90583
Global Iter: 474700 training acc: 0.25
Global Iter: 474800 training loss: 1.94951
Global Iter: 474800 training acc: 0.125
Global Iter: 474900 training loss: 2.04144
Global Iter: 474900 training acc: 0.1875
Global Iter: 475000 training loss: 2.02778
Global Iter: 475000 training acc: 0.15625
Global Iter: 475100 training loss: 1.98282
Global Iter: 475100 training acc: 0.15625
Global Iter: 475200 training loss: 2.06545
Global Iter: 475200 training acc: 0.09375
Global Iter: 475300 training loss: 1.9522
Global Iter: 475300 training acc: 0.09375
Global Iter: 475400 training loss: 2.01712
Global Iter: 475400 training acc: 0.21875
Global Iter: 475500 training loss: 1.9302
Global Iter: 475500 training acc: 0.21875
Global Iter: 475600 training loss: 1.90467
Global Iter: 475600 training acc: 0.21875
Global Iter: 475700 training loss: 1.99371
Global Iter: 475700 training acc: 0.1875
Global Iter: 475800 training loss: 2.06476
Global Iter: 475800 training acc: 0.21875
Global Iter: 475900 training loss: 2.1247
Global Iter: 475900 training acc: 0.1875
Global Iter: 476000 training loss: 2.15937
Global Iter: 476000 training acc: 0.1875
Global Iter: 476100 training loss: 1.96666
Global Iter: 476100 training acc: 0.15625
Global Iter: 476200 training loss: 1.91048
Global Iter: 476200 training acc: 0.125
Global Iter: 476300 training loss: 1.86495
Global Iter: 476300 training acc: 0.28125
Global Iter: 476400 training loss: 1.91262
Global Iter: 476400 training acc: 0.15625
Global Iter: 476500 training loss: 2.00368
Global Iter: 476500 training acc: 0.125
Global Iter: 476600 training loss: 2.03001
Global Iter: 476600 training acc: 0.125
Global Iter: 476700 training loss: 1.9903
Global Iter: 476700 training acc: 0.15625
Global Iter: 476800 training loss: 2.03414
Global Iter: 476800 training acc: 0.28125
Global Iter: 476900 training loss: 2.02818
Global Iter: 476900 training acc: 0.21875
Global Iter: 477000 training loss: 1.95876
Global Iter: 477000 training acc: 0.1875
Global Iter: 477100 training loss: 1.92097
Global Iter: 477100 training acc: 0.1875
Global Iter: 477200 training loss: 1.95387
Global Iter: 477200 training acc: 0.1875
Global Iter: 477300 training loss: 1.93906
Global Iter: 477300 training acc: 0.15625
Global Iter: 477400 training loss: 2.06014
Global Iter: 477400 training acc: 0.0625
Global Iter: 477500 training loss: 1.98742
Global Iter: 477500 training acc: 0.21875
Global Iter: 477600 training loss: 1.9807
Global Iter: 477600 training acc: 0.1875
Global Iter: 477700 training loss: 1.89671
Global Iter: 477700 training acc: 0.125
Global Iter: 477800 training loss: 2.05514
Global Iter: 477800 training acc: 0.09375
Global Iter: 477900 training loss: 1.95299
Global Iter: 477900 training acc: 0.1875
Global Iter: 478000 training loss: 1.90572
Global Iter: 478000 training acc: 0.125
Global Iter: 478100 training loss: 2.07358
Global Iter: 478100 training acc: 0.15625
Global Iter: 478200 training loss: 1.9785
Global Iter: 478200 training acc: 0.21875
Global Iter: 478300 training loss: 2.03577
Global Iter: 478300 training acc: 0.25
Global Iter: 478400 training loss: 2.0512
Global Iter: 478400 training acc: 0.34375
Global Iter: 478500 training loss: 2.07708
Global Iter: 478500 training acc: 0.09375
Global Iter: 478600 training loss: 1.93311
Global Iter: 478600 training acc: 0.1875
Global Iter: 478700 training loss: 1.95946
Global Iter: 478700 training acc: 0.15625
Global Iter: 478800 training loss: 1.8651
Global Iter: 478800 training acc: 0.15625
Global Iter: 478900 training loss: 1.95988
Global Iter: 478900 training acc: 0.28125
Global Iter: 479000 training loss: 2.16233
Global Iter: 479000 training acc: 0.09375
Global Iter: 479100 training loss: 1.98424
Global Iter: 479100 training acc: 0.1875
Global Iter: 479200 training loss: 2.01324
Global Iter: 479200 training acc: 0.09375
Global Iter: 479300 training loss: 1.92496
Global Iter: 479300 training acc: 0.21875
Global Iter: 479400 training loss: 2.04698
Global Iter: 479400 training acc: 0.0625
Global Iter: 479500 training loss: 2.054
Global Iter: 479500 training acc: 0.25
Global Iter: 479600 training loss: 1.97165
Global Iter: 479600 training acc: 0.1875
Global Iter: 479700 training loss: 2.08294
Global Iter: 479700 training acc: 0.1875
Global Iter: 479800 training loss: 2.00803
Global Iter: 479800 training acc: 0.125
Global Iter: 479900 training loss: 1.98521
Global Iter: 479900 training acc: 0.15625
Global Iter: 480000 training loss: 1.95287
Global Iter: 480000 training acc: 0.15625
Global Iter: 480100 training loss: 2.02145
Global Iter: 480100 training acc: 0.15625
Global Iter: 480200 training loss: 1.90515
Global Iter: 480200 training acc: 0.3125
Global Iter: 480300 training loss: 1.93002
Global Iter: 480300 training acc: 0.375
Global Iter: 480400 training loss: 1.99509
Global Iter: 480400 training acc: 0.15625
Global Iter: 480500 training loss: 2.03235
Global Iter: 480500 training acc: 0.25
Global Iter: 480600 training loss: 2.07022
Global Iter: 480600 training acc: 0.21875
Global Iter: 480700 training loss: 1.92395
Global Iter: 480700 training acc: 0.25
Global Iter: 480800 training loss: 2.01616
Global Iter: 480800 training acc: 0.1875
Global Iter: 480900 training loss: 1.95692
Global Iter: 480900 training acc: 0.1875
Global Iter: 481000 training loss: 2.03237
Global Iter: 481000 training acc: 0.125
Global Iter: 481100 training loss: 1.99335
Global Iter: 481100 training acc: 0.25
Global Iter: 481200 training loss: 1.87037
Global Iter: 481200 training acc: 0.3125
Global Iter: 481300 training loss: 1.91289
Global Iter: 481300 training acc: 0.21875
Global Iter: 481400 training loss: 1.93741
Global Iter: 481400 training acc: 0.1875
Global Iter: 481500 training loss: 2.06521
Global Iter: 481500 training acc: 0.15625
Global Iter: 481600 training loss: 1.95193
Global Iter: 481600 training acc: 0.25
Global Iter: 481700 training loss: 2.01724
Global Iter: 481700 training acc: 0.21875
Global Iter: 481800 training loss: 1.94585
Global Iter: 481800 training acc: 0.15625
Global Iter: 481900 training loss: 1.91699
Global Iter: 481900 training acc: 0.375
Global Iter: 482000 training loss: 1.94158
Global Iter: 482000 training acc: 0.25
Global Iter: 482100 training loss: 2.01233
Global Iter: 482100 training acc: 0.1875
Global Iter: 482200 training loss: 2.07208
Global Iter: 482200 training acc: 0.15625
Global Iter: 482300 training loss: 1.94421
Global Iter: 482300 training acc: 0.21875
Global Iter: 482400 training loss: 1.99931
Global Iter: 482400 training acc: 0.15625
Global Iter: 482500 training loss: 1.98171
Global Iter: 482500 training acc: 0.1875
Global Iter: 482600 training loss: 1.97637
Global Iter: 482600 training acc: 0.21875
Global Iter: 482700 training loss: 1.88649
Global Iter: 482700 training acc: 0.28125
Global Iter: 482800 training loss: 1.96256
Global Iter: 482800 training acc: 0.2812017-06-21 18:44:03.584201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-485727
25
Global Iter: 482900 training loss: 2.01364
Global Iter: 482900 training acc: 0.21875
Global Iter: 483000 training loss: 2.03266
Global Iter: 483000 training acc: 0.1875
Global Iter: 483100 training loss: 2.01938
Global Iter: 483100 training acc: 0.125
Global Iter: 483200 training loss: 2.04167
Global Iter: 483200 training acc: 0.15625
Global Iter: 483300 training loss: 2.04847
Global Iter: 483300 training acc: 0.09375
Global Iter: 483400 training loss: 1.91844
Global Iter: 483400 training acc: 0.28125
Global Iter: 483500 training loss: 1.95299
Global Iter: 483500 training acc: 0.15625
Global Iter: 483600 training loss: 1.95811
Global Iter: 483600 training acc: 0.15625
Global Iter: 483700 training loss: 1.93668
Global Iter: 483700 training acc: 0.25
Global Iter: 483800 training loss: 2.00837
Global Iter: 483800 training acc: 0.3125
Global Iter: 483900 training loss: 2.01996
Global Iter: 483900 training acc: 0.21875
Global Iter: 484000 training loss: 2.04036
Global Iter: 484000 training acc: 0.1875
Global Iter: 484100 training loss: 2.03563
Global Iter: 484100 training acc: 0.15625
Global Iter: 484200 training loss: 2.01715
Global Iter: 484200 training acc: 0.1875
Global Iter: 484300 training loss: 2.00047
Global Iter: 484300 training acc: 0.09375
Global Iter: 484400 training loss: 1.89995
Global Iter: 484400 training acc: 0.15625
Global Iter: 484500 training loss: 1.9168
Global Iter: 484500 training acc: 0.25
Global Iter: 484600 training loss: 1.92508
Global Iter: 484600 training acc: 0.125
Global Iter: 484700 training loss: 2.00057
Global Iter: 484700 training acc: 0.125
Global Iter: 484800 training loss: 2.04294
Global Iter: 484800 training acc: 0.21875
Global Iter: 484900 training loss: 1.98739
Global Iter: 484900 training acc: 0.03125
Global Iter: 485000 training loss: 1.88257
Global Iter: 485000 training acc: 0.15625
Global Iter: 485100 training loss: 2.00892
Global Iter: 485100 training acc: 0.21875
Global Iter: 485200 training loss: 2.01313
Global Iter: 485200 training acc: 0.21875
Global Iter: 485300 training loss: 2.01358
Global Iter: 485300 training acc: 0.09375
Global Iter: 485400 training loss: 1.92523
Global Iter: 485400 training acc: 0.1875
Global Iter: 485500 training loss: 1.95192
Global Iter: 485500 training acc: 0.21875
Global Iter: 485600 training loss: 1.96582
Global Iter: 485600 training acc: 0.15625
Global Iter: 485700 training loss: 1.95146
Global Iter: 485700 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-485727
Number of Patches: 229673
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-485727
Global Iter: 485800 training loss: 2.00399
Global Iter: 485800 training acc: 0.21875
Global Iter: 485900 training loss: 2.09153
Global Iter: 485900 training acc: 0.1875
Global Iter: 486000 training loss: 1.99464
Global Iter: 486000 training acc: 0.15625
Global Iter: 486100 training loss: 1.91244
Global Iter: 486100 training acc: 0.25
Global Iter: 486200 training loss: 1.95576
Global Iter: 486200 training acc: 0.125
Global Iter: 486300 training loss: 1.92383
Global Iter: 486300 training acc: 0.1875
Global Iter: 486400 training loss: 1.927
Global Iter: 486400 training acc: 0.25
Global Iter: 486500 training loss: 1.98396
Global Iter: 486500 training acc: 0.1875
Global Iter: 486600 training loss: 1.98897
Global Iter: 486600 training acc: 0.125
Global Iter: 486700 training loss: 2.00978
Global Iter: 486700 training acc: 0.09375
Global Iter: 486800 training loss: 2.12323
Global Iter: 486800 training acc: 0.125
Global Iter: 486900 training loss: 1.99637
Global Iter: 486900 training acc: 0.21875
Global Iter: 487000 training loss: 1.91485
Global Iter: 487000 training acc: 0.1875
Global Iter: 487100 training loss: 1.96927
Global Iter: 487100 training acc: 0.15625
Global Iter: 487200 training loss: 2.02664
Global Iter: 487200 training acc: 0.15625
Global Iter: 487300 training loss: 2.00606
Global Iter: 487300 training acc: 0.15625
Global Iter: 487400 training loss: 2.09014
Global Iter: 487400 training acc: 0.34375
Global Iter: 487500 training loss: 1.99599
Global Iter: 487500 training acc: 0.125
Global Iter: 487600 training loss: 1.94977
Global Iter: 487600 training acc: 0.1875
Global Iter: 487700 training loss: 2.06399
Global Iter: 487700 training acc: 0.0625
Global Iter: 487800 training loss: 1.9559
Global Iter: 487800 training acc: 0.09375
Global Iter: 487900 training loss: 1.96241
Global Iter: 487900 training acc: 0.28125
Global Iter: 488000 training loss: 1.98109
Global Iter: 488000 training acc: 0.21875
Global Iter: 488100 training loss: 2.02097
Global Iter: 488100 training acc: 0.09375
Global Iter: 488200 training loss: 2.0163
Global Iter: 488200 training acc: 0.09375
Global Iter: 488300 training loss: 1.95651
Global Iter: 488300 training acc: 0.1875
Global Iter: 488400 training loss: 1.95226
Global Iter: 488400 training acc: 0.21875
Global Iter: 488500 training loss: 2.15174
Global Iter: 488500 training acc: 0.21875
Global Iter: 488600 training loss: 1.94169
Global Iter: 488600 training acc: 0.1875
Global Iter: 488700 training loss: 2.23075
Global Iter: 488700 training acc: 0.0625
Global Iter: 488800 training loss: 1.93627
Global Iter: 488800 training acc: 0.125
Global Iter: 488900 training loss: 1.9999
Global Iter: 488900 training acc: 0.1875
Global Iter: 489000 training loss: 1.93158
Global Iter: 489000 training acc: 0.21875
Global Iter: 489100 training loss: 1.8875
Global Iter: 489100 training acc: 0.28125
Global Iter: 489200 training loss: 1.93009
Global Iter: 489200 training acc: 0.34375
Global Iter: 489300 training loss: 2.03456
Global Iter: 489300 training acc: 0.1875
Global Iter: 489400 training loss: 2.00061
Global Iter: 489400 training acc: 0.375
Global Iter: 489500 training loss: 2.04744
Global Iter: 489500 training acc: 0.0625
Global Iter: 489600 training loss: 1.96882
Global Iter: 489600 training acc: 0.1875
Global Iter: 489700 training loss: 2.02868
Global Iter: 489700 training acc: 0.125
Global Iter: 489800 training loss: 2.0135
Global Iter: 489800 training acc: 0.25
Global Iter: 489900 training loss: 2.0991
Global Iter: 489900 training acc: 0.15625
Global Iter: 490000 training loss: 1.96699
Global Iter: 490000 training acc: 0.1875
Global Iter: 490100 training loss: 1.91256
Global Iter: 490100 training acc: 0.1875
Global Iter: 490200 training loss: 2.08394
Global Iter: 490200 training acc: 0.15625
Global Iter: 490300 training loss: 1.9307
Global Iter: 490300 training acc: 0.21875
Global Iter: 490400 training loss: 2.01748
Global Iter: 490400 training acc: 0.25
Global Iter: 490500 training loss: 2.00692
Global Iter: 490500 training acc: 0.25
Global Iter: 490600 training loss: 2.03616
Global Iter: 490600 training acc: 0.1875
Global Iter: 490700 training loss: 2.08009
Global Iter: 490700 training acc: 0.21875
Global Iter: 490800 training loss: 1.88897
Global Iter: 490800 training acc: 0.21875
Global Iter: 490900 training loss: 2.06123
Global Iter: 490900 training acc: 0.25
Global Iter: 491000 training loss: 2.01359
Global Iter: 491000 training acc: 0.15625
Global Iter: 491100 training loss: 2.05539
Global Iter: 491100 training acc: 0.125
Global Iter: 491200 training loss: 1.9789
Global Iter: 491200 training acc: 0.21875
Global Iter: 491300 training loss: 1.94553
Global Iter: 491300 training acc: 0.28125
Global Iter: 491400 training loss: 2.03125
Global Iter: 491400 training acc: 0.15625
Global Iter: 491500 training loss: 1.96961
Global Iter: 491500 training acc: 0.03125
Global Iter: 491600 training loss: 1.91672
Global Iter: 491600 training acc: 0.34375
Global Iter: 491700 training loss: 1.94871
Global Iter: 491700 training acc: 0.1875
Global Iter: 491800 training loss: 2.00609
Global Iter: 491800 training acc: 0.15625
Global Iter: 491900 training loss: 1.89765
Global Iter: 491900 training acc: 0.1875
Global Iter: 492000 training loss: 1.90313
Global Iter: 492000 training acc: 0.28125
Global Iter: 492100 training loss: 1.91737
Global Iter: 492100 training acc: 0.1875
Global Iter: 492200 training loss: 1.86137
Global Iter: 492200 training acc: 0.28125
Global Iter: 492300 training loss: 1.90789
Global Iter: 492300 training acc: 0.1875
Global Iter: 492400 training loss: 1.86042
Global Iter: 492400 training acc: 0.3125
Global Iter: 492500 training loss: 1.97659
Global Iter: 492500 training acc: 0.21875
Global Iter: 492600 training loss: 1.96964
Global Iter: 492600 training acc: 0.21875
Global Iter: 492700 training loss: 2.13381
Global Iter: 492700 training acc: 0.125
Global Iter: 492800 training loss: 1.88896
Global Iter: 492800 training acc: 0.15625
Global Iter: 492900 training loss: 1.98611
Global Iter: 492900 training acc: 0.09375
Global Iter: 493000 training loss: 2.06073
Global Iter: 493000 training acc: 0.125
Global Iter: 493100 training loss: 1.88193
Global Iter: 493100 training acc: 0.28125
Global Iter: 493200 training loss: 1.96537
Global Iter: 493200 training acc: 0.15625
Global Iter: 493300 training loss: 1.91288
Global Iter: 493300 training acc: 0.15625
Global Iter: 493400 training loss: 2.0298
Global Iter: 493400 training acc: 0.15625
Global Iter: 493500 training loss: 1.91234
Global Iter: 493500 training acc: 0.25
Global Iter: 493600 training loss: 1.98096
Global Iter: 493600 training acc: 0.09375
Global Iter: 493700 training loss: 2.01316
Global Iter: 493700 training acc: 0.21875
Global Iter: 493800 training loss: 1.97625
Global Iter: 493800 training acc: 0.34375
Global Iter: 493900 training loss: 2.01109
Global Iter: 493900 training acc: 0.25
Global Iter: 494000 training loss: 2.08551
Global Iter: 494000 training acc: 0.21875
Global Iter: 494100 training loss: 1.99518
Global Iter: 494100 training acc: 0.125
Global Iter: 494200 training loss: 1.95495
Global Iter: 494200 training acc: 0.28125
Global Iter: 494300 training loss: 1.89819
Global Iter: 494300 training acc: 0.28125
Global Iter: 494400 training loss: 2.01134
Global Iter: 494400 training acc: 0.28125
Global Iter: 494500 training loss: 2.07837
Global Iter: 494500 training acc: 0.1875
Global Iter: 494600 training loss: 1.99542
Global Iter: 494600 training acc: 0.1875
Global Iter: 494700 training loss: 1.94151
Global Iter: 494700 training acc: 0.1875
Global Iter: 494800 training loss: 1.93681
Global Iter: 494800 training acc: 0.21875
Global Iter: 494900 training loss: 2.10575
Global Iter: 494900 training acc: 0.0625
Global Iter: 495000 training loss: 1.9846
Global Iter: 495000 training acc: 0.1875
Global Iter: 495100 training loss: 1.96453
Global Iter: 495100 training acc: 0.125
Global Iter: 495200 training loss: 1.9675
Global Iter: 495200 training acc: 0.34375
Global Iter: 495300 training loss: 1.90844
Global Iter: 495300 training acc: 0.375
Global Iter: 495400 training loss: 1.97467
Global Iter: 495400 training acc: 0.21875
Global Iter: 495500 training loss: 2.02474
Global Iter: 495500 training acc: 0.15625
Global Iter: 495600 training loss: 2.0378
Global Iter: 495600 training acc: 0.21875
Global Iter: 495700 training loss: 1.92539
Global Iter: 495700 training acc: 0.21875
Global Iter: 495800 training loss: 1.90207
Global Iter: 495800 training acc: 0.25
Global Iter: 495900 training loss: 2.02376
Global Iter: 495900 training acc: 0.25
Global Iter: 496000 training loss: 1.99599
Global Iter: 496000 training acc: 0.1875
Global Iter: 496100 training loss: 1.99817
Global Iter: 496100 training acc: 0.3125
Global Iter: 496200 training loss: 2.01552
Global Iter: 496200 training acc: 0.1875
Global Iter: 496300 training loss: 2.06129
Global Iter: 496300 training acc: 0.21875
Global Iter: 496400 training loss: 2.00954
Global Iter: 496400 training acc: 0.34375
Global Iter: 496500 training loss: 1.96011
Global Iter: 496500 training acc: 0.25
Global Iter: 496600 training loss: 1.95291
Global Iter: 496600 training acc: 0.1875
Global Iter: 496700 training loss: 2.01687
Global Iter: 496700 training acc: 0.1875
Global Iter: 496800 training loss: 2.03128
Global Iter: 496800 training acc: 0.15625
Global Iter: 496900 training loss: 2.01648
Global Iter: 496900 training acc: 0.25
Global Iter: 497000 training loss: 2.01915
Global Iter: 497000 training acc: 0.0625
Global Iter: 497100 training loss: 2.0476
Global Iter: 497100 training acc: 0.15625
Global Iter: 497200 training loss:2017-06-21 19:09:35.661962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-500082
 1.99171
Global Iter: 497200 training acc: 0.1875
Global Iter: 497300 training loss: 1.96265
Global Iter: 497300 training acc: 0.15625
Global Iter: 497400 training loss: 2.08353
Global Iter: 497400 training acc: 0.09375
Global Iter: 497500 training loss: 1.93745
Global Iter: 497500 training acc: 0.1875
Global Iter: 497600 training loss: 1.99264
Global Iter: 497600 training acc: 0.21875
Global Iter: 497700 training loss: 2.02285
Global Iter: 497700 training acc: 0.125
Global Iter: 497800 training loss: 1.97451
Global Iter: 497800 training acc: 0.25
Global Iter: 497900 training loss: 1.89546
Global Iter: 497900 training acc: 0.28125
Global Iter: 498000 training loss: 1.90348
Global Iter: 498000 training acc: 0.25
Global Iter: 498100 training loss: 1.90626
Global Iter: 498100 training acc: 0.1875
Global Iter: 498200 training loss: 1.96188
Global Iter: 498200 training acc: 0.28125
Global Iter: 498300 training loss: 1.8633
Global Iter: 498300 training acc: 0.1875
Global Iter: 498400 training loss: 2.01821
Global Iter: 498400 training acc: 0.21875
Global Iter: 498500 training loss: 1.94498
Global Iter: 498500 training acc: 0.21875
Global Iter: 498600 training loss: 2.08907
Global Iter: 498600 training acc: 0.09375
Global Iter: 498700 training loss: 1.92497
Global Iter: 498700 training acc: 0.21875
Global Iter: 498800 training loss: 2.02827
Global Iter: 498800 training acc: 0.125
Global Iter: 498900 training loss: 2.06239
Global Iter: 498900 training acc: 0.1875
Global Iter: 499000 training loss: 1.94545
Global Iter: 499000 training acc: 0.1875
Global Iter: 499100 training loss: 2.00815
Global Iter: 499100 training acc: 0.1875
Global Iter: 499200 training loss: 1.94409
Global Iter: 499200 training acc: 0.21875
Global Iter: 499300 training loss: 1.97729
Global Iter: 499300 training acc: 0.1875
Global Iter: 499400 training loss: 1.92508
Global Iter: 499400 training acc: 0.21875
Global Iter: 499500 training loss: 1.91964
Global Iter: 499500 training acc: 0.25
Global Iter: 499600 training loss: 2.03363
Global Iter: 499600 training acc: 0.1875
Global Iter: 499700 training loss: 2.00986
Global Iter: 499700 training acc: 0.28125
Global Iter: 499800 training loss: 2.04329
Global Iter: 499800 training acc: 0.1875
Global Iter: 499900 training loss: 2.03817
Global Iter: 499900 training acc: 0.125
Global Iter: 500000 training loss: 2.04711
Global Iter: 500000 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-500082
Number of Patches: 227377
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-500082
Global Iter: 500100 training loss: 2.02802
Global Iter: 500100 training acc: 0.1875
Global Iter: 500200 training loss: 2.03198
Global Iter: 500200 training acc: 0.1875
Global Iter: 500300 training loss: 1.96695
Global Iter: 500300 training acc: 0.21875
Global Iter: 500400 training loss: 1.9079
Global Iter: 500400 training acc: 0.1875
Global Iter: 500500 training loss: 2.09224
Global Iter: 500500 training acc: 0.125
Global Iter: 500600 training loss: 1.95381
Global Iter: 500600 training acc: 0.25
Global Iter: 500700 training loss: 1.93057
Global Iter: 500700 training acc: 0.25
Global Iter: 500800 training loss: 2.05415
Global Iter: 500800 training acc: 0.1875
Global Iter: 500900 training loss: 1.89503
Global Iter: 500900 training acc: 0.25
Global Iter: 501000 training loss: 2.0767
Global Iter: 501000 training acc: 0.15625
Global Iter: 501100 training loss: 2.04976
Global Iter: 501100 training acc: 0.15625
Global Iter: 501200 training loss: 2.01633
Global Iter: 501200 training acc: 0.25
Global Iter: 501300 training loss: 1.96053
Global Iter: 501300 training acc: 0.21875
Global Iter: 501400 training loss: 1.97036
Global Iter: 501400 training acc: 0.34375
Global Iter: 501500 training loss: 2.018
Global Iter: 501500 training acc: 0.0625
Global Iter: 501600 training loss: 1.97069
Global Iter: 501600 training acc: 0.21875
Global Iter: 501700 training loss: 1.90546
Global Iter: 501700 training acc: 0.1875
Global Iter: 501800 training loss: 1.95729
Global Iter: 501800 training acc: 0.03125
Global Iter: 501900 training loss: 2.01917
Global Iter: 501900 training acc: 0.0625
Global Iter: 502000 training loss: 1.91793
Global Iter: 502000 training acc: 0.34375
Global Iter: 502100 training loss: 1.9944
Global Iter: 502100 training acc: 0.1875
Global Iter: 502200 training loss: 2.02521
Global Iter: 502200 training acc: 0.15625
Global Iter: 502300 training loss: 1.99404
Global Iter: 502300 training acc: 0.1875
Global Iter: 502400 training loss: 1.98472
Global Iter: 502400 training acc: 0.125
Global Iter: 502500 training loss: 2.03605
Global Iter: 502500 training acc: 0.1875
Global Iter: 502600 training loss: 1.97532
Global Iter: 502600 training acc: 0.0625
Global Iter: 502700 training loss: 2.05802
Global Iter: 502700 training acc: 0.125
Global Iter: 502800 training loss: 1.95563
Global Iter: 502800 training acc: 0.25
Global Iter: 502900 training loss: 2.01114
Global Iter: 502900 training acc: 0.125
Global Iter: 503000 training loss: 2.03377
Global Iter: 503000 training acc: 0.09375
Global Iter: 503100 training loss: 2.00706
Global Iter: 503100 training acc: 0.15625
Global Iter: 503200 training loss: 1.98739
Global Iter: 503200 training acc: 0.125
Global Iter: 503300 training loss: 1.92304
Global Iter: 503300 training acc: 0.1875
Global Iter: 503400 training loss: 2.00821
Global Iter: 503400 training acc: 0.0625
Global Iter: 503500 training loss: 1.94989
Global Iter: 503500 training acc: 0.21875
Global Iter: 503600 training loss: 1.97297
Global Iter: 503600 training acc: 0.21875
Global Iter: 503700 training loss: 1.99394
Global Iter: 503700 training acc: 0.125
Global Iter: 503800 training loss: 2.04851
Global Iter: 503800 training acc: 0.1875
Global Iter: 503900 training loss: 1.9778
Global Iter: 503900 training acc: 0.15625
Global Iter: 504000 training loss: 2.01301
Global Iter: 504000 training acc: 0.25
Global Iter: 504100 training loss: 1.90898
Global Iter: 504100 training acc: 0.25
Global Iter: 504200 training loss: 2.0712
Global Iter: 504200 training acc: 0.15625
Global Iter: 504300 training loss: 1.94327
Global Iter: 504300 training acc: 0.28125
Global Iter: 504400 training loss: 1.99076
Global Iter: 504400 training acc: 0.15625
Global Iter: 504500 training loss: 1.96974
Global Iter: 504500 training acc: 0.125
Global Iter: 504600 training loss: 2.11361
Global Iter: 504600 training acc: 0.25
Global Iter: 504700 training loss: 2.08097
Global Iter: 504700 training acc: 0.15625
Global Iter: 504800 training loss: 2.03456
Global Iter: 504800 training acc: 0.21875
Global Iter: 504900 training loss: 1.85794
Global Iter: 504900 training acc: 0.3125
Global Iter: 505000 training loss: 1.98486
Global Iter: 505000 training acc: 0.21875
Global Iter: 505100 training loss: 1.94976
Global Iter: 505100 training acc: 0.1875
Global Iter: 505200 training loss: 1.96265
Global Iter: 505200 training acc: 0.1875
Global Iter: 505300 training loss: 1.97497
Global Iter: 505300 training acc: 0.1875
Global Iter: 505400 training loss: 1.96894
Global Iter: 505400 training acc: 0.25
Global Iter: 505500 training loss: 2.1521
Global Iter: 505500 training acc: 0.15625
Global Iter: 505600 training loss: 1.9536
Global Iter: 505600 training acc: 0.1875
Global Iter: 505700 training loss: 2.07998
Global Iter: 505700 training acc: 0.0625
Global Iter: 505800 training loss: 2.01751
Global Iter: 505800 training acc: 0.1875
Global Iter: 505900 training loss: 2.05447
Global Iter: 505900 training acc: 0.28125
Global Iter: 506000 training loss: 2.00046
Global Iter: 506000 training acc: 0.125
Global Iter: 506100 training loss: 1.9733
Global Iter: 506100 training acc: 0.28125
Global Iter: 506200 training loss: 1.95581
Global Iter: 506200 training acc: 0.15625
Global Iter: 506300 training loss: 2.00446
Global Iter: 506300 training acc: 0.125
Global Iter: 506400 training loss: 1.99099
Global Iter: 506400 training acc: 0.21875
Global Iter: 506500 training loss: 1.95695
Global Iter: 506500 training acc: 0.15625
Global Iter: 506600 training loss: 1.93303
Global Iter: 506600 training acc: 0.21875
Global Iter: 506700 training loss: 1.95667
Global Iter: 506700 training acc: 0.28125
Global Iter: 506800 training loss: 1.94269
Global Iter: 506800 training acc: 0.28125
Global Iter: 506900 training loss: 1.99232
Global Iter: 506900 training acc: 0.15625
Global Iter: 507000 training loss: 1.92132
Global Iter: 507000 training acc: 0.3125
Global Iter: 507100 training loss: 2.07476
Global Iter: 507100 training acc: 0.15625
Global Iter: 507200 training loss: 2.00814
Global Iter: 507200 training acc: 0.1875
Global Iter: 507300 training loss: 1.95441
Global Iter: 507300 training acc: 0.1875
Global Iter: 507400 training loss: 1.93836
Global Iter: 507400 training acc: 0.125
Global Iter: 507500 training loss: 2.17038
Global Iter: 507500 training acc: 0.15625
Global Iter: 507600 training loss: 1.97529
Global Iter: 507600 training acc: 0.03125
Global Iter: 507700 training loss: 2.04493
Global Iter: 507700 training acc: 0.0625
Global Iter: 507800 training loss: 2.01243
Global Iter: 507800 training acc: 0.1875
Global Iter: 507900 training loss: 2.01296
Global Iter: 507900 training acc: 0.1875
Global Iter: 508000 training loss: 1.93985
Global Iter: 508000 training acc: 0.15625
Global Iter: 508100 training loss: 2.00026
Global Iter: 508100 training acc: 0.3125
Global Iter: 508200 training loss: 1.9905
Global Iter: 508200 training acc: 0.21875
Global Iter: 508300 training loss: 2.03006
Global Iter: 508300 training acc: 0.21875
Global Iter: 508400 training loss: 2.02382
Global Iter: 508400 training acc: 0.0625
Global Iter: 508500 training loss: 1.95096
Global Iter: 508500 training acc: 0.28125
Global Iter: 508600 training loss: 1.97872
Global Iter: 508600 training acc: 0.15625
Global Iter: 508700 training loss: 2.02193
Global Iter: 508700 training acc: 0.15625
Global Iter: 508800 training loss: 2.0354
Global Iter: 508800 training acc: 0.15625
Global Iter: 508900 training loss: 1.96545
Global Iter: 508900 training acc: 0.1875
Global Iter: 509000 training loss: 1.94372
Global Iter: 509000 training acc: 0.21875
Global Iter: 509100 training loss: 2.07305
Global Iter: 509100 training acc: 0.15625
Global Iter: 509200 training loss: 1.9291
Global Iter: 509200 training acc: 0.125
Global Iter: 509300 training loss: 2.12555
Global Iter: 509300 training acc: 0.03125
Global Iter: 509400 training loss: 1.98671
Global Iter: 509400 training acc: 0.1875
Global Iter: 509500 training loss: 1.88834
Global Iter: 509500 training acc: 0.28125
Global Iter: 509600 training loss: 1.99651
Global Iter: 509600 training acc: 0.28125
Global Iter: 509700 training loss: 1.98451
Global Iter: 509700 training acc: 0.21875
Global Iter: 509800 training loss: 1.95551
Global Iter: 509800 training acc: 0.1875
Global Iter: 509900 training loss: 2.01381
Global Iter: 509900 training acc: 0.1875
Global Iter: 510000 training loss: 1.97088
Global Iter: 510000 training acc: 0.09375
Global Iter: 510100 training loss: 1.96129
Global Iter: 510100 training acc: 0.1875
Global Iter: 510200 training loss: 1.96243
Global Iter: 510200 training acc: 0.25
Global Iter: 510300 training loss: 2.01225
Global Iter: 510300 training acc: 0.1875
Global Iter: 510400 training loss: 1.974
Global Iter: 510400 training acc: 0.21875
Global Iter: 510500 training loss: 1.90877
Global Iter: 510500 training acc: 0.28125
Global Iter: 510600 training loss: 1.9152
Global Iter: 510600 training acc: 0.25
Global Iter: 510700 training loss: 2.06316
Global Iter: 510700 training acc: 0.1875
Global Iter: 510800 training loss: 2.04242
Global Iter: 510800 training acc: 0.125
Global Iter: 510900 training loss: 2.04486
Global Iter: 510900 training acc: 0.03125
Global Iter: 511000 training loss: 2.10986
Global Iter: 511000 training acc: 0.15625
Global Iter: 511100 training loss: 2.04626
Global Iter: 511100 training acc: 0.1875
Global Iter: 511200 training loss: 1.93439
Global Iter: 511200 training acc: 0.25
Global Iter: 511300 training loss: 1.96158
Global Iter: 511300 training acc: 0.25
Global Iter: 511400 training loss: 1.92933
Global Iter: 511400 training acc: 0.25
Global Iter: 511500 training loss: 1.975
Global Iter: 511500 training acc: 0.1875
Global It2017-06-21 19:34:27.391983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-514294
er: 511600 training loss: 2.06777
Global Iter: 511600 training acc: 0.1875
Global Iter: 511700 training loss: 2.11978
Global Iter: 511700 training acc: 0.1875
Global Iter: 511800 training loss: 1.94748
Global Iter: 511800 training acc: 0.125
Global Iter: 511900 training loss: 1.90888
Global Iter: 511900 training acc: 0.1875
Global Iter: 512000 training loss: 1.98489
Global Iter: 512000 training acc: 0.125
Global Iter: 512100 training loss: 2.03702
Global Iter: 512100 training acc: 0.1875
Global Iter: 512200 training loss: 2.01734
Global Iter: 512200 training acc: 0.125
Global Iter: 512300 training loss: 2.08419
Global Iter: 512300 training acc: 0.25
Global Iter: 512400 training loss: 1.91558
Global Iter: 512400 training acc: 0.25
Global Iter: 512500 training loss: 2.00573
Global Iter: 512500 training acc: 0.28125
Global Iter: 512600 training loss: 2.03491
Global Iter: 512600 training acc: 0.15625
Global Iter: 512700 training loss: 1.99726
Global Iter: 512700 training acc: 0.125
Global Iter: 512800 training loss: 1.94425
Global Iter: 512800 training acc: 0.25
Global Iter: 512900 training loss: 2.10467
Global Iter: 512900 training acc: 0.25
Global Iter: 513000 training loss: 2.01541
Global Iter: 513000 training acc: 0.125
Global Iter: 513100 training loss: 1.91085
Global Iter: 513100 training acc: 0.21875
Global Iter: 513200 training loss: 2.11873
Global Iter: 513200 training acc: 0.21875
Global Iter: 513300 training loss: 2.04231
Global Iter: 513300 training acc: 0.125
Global Iter: 513400 training loss: 1.91397
Global Iter: 513400 training acc: 0.15625
Global Iter: 513500 training loss: 1.8965
Global Iter: 513500 training acc: 0.125
Global Iter: 513600 training loss: 1.95358
Global Iter: 513600 training acc: 0.21875
Global Iter: 513700 training loss: 1.93159
Global Iter: 513700 training acc: 0.21875
Global Iter: 513800 training loss: 1.91113
Global Iter: 513800 training acc: 0.25
Global Iter: 513900 training loss: 1.86708
Global Iter: 513900 training acc: 0.34375
Global Iter: 514000 training loss: 2.0267
Global Iter: 514000 training acc: 0.1875
Global Iter: 514100 training loss: 1.95622
Global Iter: 514100 training acc: 0.15625
Global Iter: 514200 training loss: 2.03297
Global Iter: 514200 training acc: 0.03125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-514294
Number of Patches: 225104
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-514294
Global Iter: 514300 training loss: 1.93548
Global Iter: 514300 training acc: 0.375
Global Iter: 514400 training loss: 2.00702
Global Iter: 514400 training acc: 0.1875
Global Iter: 514500 training loss: 2.01814
Global Iter: 514500 training acc: 0.25
Global Iter: 514600 training loss: 2.01599
Global Iter: 514600 training acc: 0.1875
Global Iter: 514700 training loss: 1.94357
Global Iter: 514700 training acc: 0.3125
Global Iter: 514800 training loss: 1.98721
Global Iter: 514800 training acc: 0.28125
Global Iter: 514900 training loss: 1.94727
Global Iter: 514900 training acc: 0.125
Global Iter: 515000 training loss: 1.90033
Global Iter: 515000 training acc: 0.34375
Global Iter: 515100 training loss: 2.06533
Global Iter: 515100 training acc: 0.1875
Global Iter: 515200 training loss: 2.02238
Global Iter: 515200 training acc: 0.125
Global Iter: 515300 training loss: 2.07912
Global Iter: 515300 training acc: 0.1875
Global Iter: 515400 training loss: 2.05193
Global Iter: 515400 training acc: 0.21875
Global Iter: 515500 training loss: 1.91976
Global Iter: 515500 training acc: 0.21875
Global Iter: 515600 training loss: 1.99206
Global Iter: 515600 training acc: 0.21875
Global Iter: 515700 training loss: 2.01836
Global Iter: 515700 training acc: 0.1875
Global Iter: 515800 training loss: 2.00723
Global Iter: 515800 training acc: 0.0625
Global Iter: 515900 training loss: 1.9818
Global Iter: 515900 training acc: 0.125
Global Iter: 516000 training loss: 1.95319
Global Iter: 516000 training acc: 0.21875
Global Iter: 516100 training loss: 1.99628
Global Iter: 516100 training acc: 0.21875
Global Iter: 516200 training loss: 2.05976
Global Iter: 516200 training acc: 0.15625
Global Iter: 516300 training loss: 2.0634
Global Iter: 516300 training acc: 0.0625
Global Iter: 516400 training loss: 1.98449
Global Iter: 516400 training acc: 0.125
Global Iter: 516500 training loss: 1.96935
Global Iter: 516500 training acc: 0.25
Global Iter: 516600 training loss: 2.08817
Global Iter: 516600 training acc: 0.3125
Global Iter: 516700 training loss: 1.99939
Global Iter: 516700 training acc: 0.21875
Global Iter: 516800 training loss: 2.11088
Global Iter: 516800 training acc: 0.125
Global Iter: 516900 training loss: 1.97616
Global Iter: 516900 training acc: 0.1875
Global Iter: 517000 training loss: 2.07299
Global Iter: 517000 training acc: 0.03125
Global Iter: 517100 training loss: 1.96528
Global Iter: 517100 training acc: 0.1875
Global Iter: 517200 training loss: 1.90244
Global Iter: 517200 training acc: 0.21875
Global Iter: 517300 training loss: 2.09571
Global Iter: 517300 training acc: 0.28125
Global Iter: 517400 training loss: 1.95129
Global Iter: 517400 training acc: 0.03125
Global Iter: 517500 training loss: 1.96683
Global Iter: 517500 training acc: 0.1875
Global Iter: 517600 training loss: 2.01991
Global Iter: 517600 training acc: 0.3125
Global Iter: 517700 training loss: 1.8907
Global Iter: 517700 training acc: 0.1875
Global Iter: 517800 training loss: 1.9639
Global Iter: 517800 training acc: 0.125
Global Iter: 517900 training loss: 1.99269
Global Iter: 517900 training acc: 0.21875
Global Iter: 518000 training loss: 1.93243
Global Iter: 518000 training acc: 0.1875
Global Iter: 518100 training loss: 1.99684
Global Iter: 518100 training acc: 0.15625
Global Iter: 518200 training loss: 1.99326
Global Iter: 518200 training acc: 0.21875
Global Iter: 518300 training loss: 1.95326
Global Iter: 518300 training acc: 0.25
Global Iter: 518400 training loss: 1.87826
Global Iter: 518400 training acc: 0.3125
Global Iter: 518500 training loss: 1.97513
Global Iter: 518500 training acc: 0.15625
Global Iter: 518600 training loss: 2.00635
Global Iter: 518600 training acc: 0.21875
Global Iter: 518700 training loss: 1.95203
Global Iter: 518700 training acc: 0.21875
Global Iter: 518800 training loss: 2.00345
Global Iter: 518800 training acc: 0.28125
Global Iter: 518900 training loss: 2.03142
Global Iter: 518900 training acc: 0.21875
Global Iter: 519000 training loss: 2.00844
Global Iter: 519000 training acc: 0.28125
Global Iter: 519100 training loss: 2.04404
Global Iter: 519100 training acc: 0.15625
Global Iter: 519200 training loss: 2.03674
Global Iter: 519200 training acc: 0.1875
Global Iter: 519300 training loss: 1.96248
Global Iter: 519300 training acc: 0.28125
Global Iter: 519400 training loss: 2.0229
Global Iter: 519400 training acc: 0.25
Global Iter: 519500 training loss: 2.12583
Global Iter: 519500 training acc: 0.125
Global Iter: 519600 training loss: 1.99782
Global Iter: 519600 training acc: 0.15625
Global Iter: 519700 training loss: 1.98181
Global Iter: 519700 training acc: 0.1875
Global Iter: 519800 training loss: 1.89535
Global Iter: 519800 training acc: 0.28125
Global Iter: 519900 training loss: 1.9347
Global Iter: 519900 training acc: 0.25
Global Iter: 520000 training loss: 2.07824
Global Iter: 520000 training acc: 0.25
Global Iter: 520100 training loss: 1.98881
Global Iter: 520100 training acc: 0.25
Global Iter: 520200 training loss: 2.05214
Global Iter: 520200 training acc: 0.09375
Global Iter: 520300 training loss: 2.06332
Global Iter: 520300 training acc: 0.15625
Global Iter: 520400 training loss: 1.8768
Global Iter: 520400 training acc: 0.21875
Global Iter: 520500 training loss: 1.94214
Global Iter: 520500 training acc: 0.15625
Global Iter: 520600 training loss: 2.02081
Global Iter: 520600 training acc: 0.09375
Global Iter: 520700 training loss: 1.97624
Global Iter: 520700 training acc: 0.1875
Global Iter: 520800 training loss: 2.00188
Global Iter: 520800 training acc: 0.1875
Global Iter: 520900 training loss: 1.88688
Global Iter: 520900 training acc: 0.28125
Global Iter: 521000 training loss: 2.07122
Global Iter: 521000 training acc: 0.125
Global Iter: 521100 training loss: 1.90464
Global Iter: 521100 training acc: 0.1875
Global Iter: 521200 training loss: 1.94028
Global Iter: 521200 training acc: 0.21875
Global Iter: 521300 training loss: 2.02064
Global Iter: 521300 training acc: 0.125
Global Iter: 521400 training loss: 2.01302
Global Iter: 521400 training acc: 0.1875
Global Iter: 521500 training loss: 1.91458
Global Iter: 521500 training acc: 0.375
Global Iter: 521600 training loss: 1.87435
Global Iter: 521600 training acc: 0.375
Global Iter: 521700 training loss: 1.90712
Global Iter: 521700 training acc: 0.21875
Global Iter: 521800 training loss: 1.99126
Global Iter: 521800 training acc: 0.0625
Global Iter: 521900 training loss: 1.96672
Global Iter: 521900 training acc: 0.28125
Global Iter: 522000 training loss: 2.0113
Global Iter: 522000 training acc: 0.25
Global Iter: 522100 training loss: 1.88377
Global Iter: 522100 training acc: 0.21875
Global Iter: 522200 training loss: 2.01539
Global Iter: 522200 training acc: 0.125
Global Iter: 522300 training loss: 1.9797
Global Iter: 522300 training acc: 0.1875
Global Iter: 522400 training loss: 2.02399
Global Iter: 522400 training acc: 0.03125
Global Iter: 522500 training loss: 1.95242
Global Iter: 522500 training acc: 0.15625
Global Iter: 522600 training loss: 2.03546
Global Iter: 522600 training acc: 0.25
Global Iter: 522700 training loss: 1.96225
Global Iter: 522700 training acc: 0.125
Global Iter: 522800 training loss: 1.98929
Global Iter: 522800 training acc: 0.125
Global Iter: 522900 training loss: 2.06556
Global Iter: 522900 training acc: 0.125
Global Iter: 523000 training loss: 1.95824
Global Iter: 523000 training acc: 0.3125
Global Iter: 523100 training loss: 2.00504
Global Iter: 523100 training acc: 0.21875
Global Iter: 523200 training loss: 1.93322
Global Iter: 523200 training acc: 0.1875
Global Iter: 523300 training loss: 1.90222
Global Iter: 523300 training acc: 0.28125
Global Iter: 523400 training loss: 2.11217
Global Iter: 523400 training acc: 0.125
Global Iter: 523500 training loss: 1.93204
Global Iter: 523500 training acc: 0.0625
Global Iter: 523600 training loss: 1.92041
Global Iter: 523600 training acc: 0.15625
Global Iter: 523700 training loss: 2.00969
Global Iter: 523700 training acc: 0.125
Global Iter: 523800 training loss: 1.98873
Global Iter: 523800 training acc: 0.1875
Global Iter: 523900 training loss: 2.05508
Global Iter: 523900 training acc: 0.21875
Global Iter: 524000 training loss: 1.95434
Global Iter: 524000 training acc: 0.15625
Global Iter: 524100 training loss: 2.03632
Global Iter: 524100 training acc: 0.1875
Global Iter: 524200 training loss: 2.0054
Global Iter: 524200 training acc: 0.1875
Global Iter: 524300 training loss: 2.01677
Global Iter: 524300 training acc: 0.1875
Global Iter: 524400 training loss: 1.95564
Global Iter: 524400 training acc: 0.125
Global Iter: 524500 training loss: 1.96008
Global Iter: 524500 training acc: 0.3125
Global Iter: 524600 training loss: 1.88061
Global Iter: 524600 training acc: 0.40625
Global Iter: 524700 training loss: 1.91362
Global Iter: 524700 training acc: 0.34375
Global Iter: 524800 training loss: 2.03318
Global Iter: 524800 training acc: 0.09375
Global Iter: 524900 training loss: 2.01639
Global Iter: 524900 training acc: 0.1875
Global Iter: 525000 training loss: 2.09156
Global Iter: 525000 training acc: 0.15625
Global Iter: 525100 training loss: 1.93634
Global Iter: 525100 training acc: 0.1875
Global Iter: 525200 training loss: 1.98905
Global Iter: 525200 training acc: 0.21875
Global Iter: 525300 training loss: 2.15603
Global Iter: 525300 training acc: 0.125
Global Iter: 525400 training loss: 1.97703
Global Iter: 525400 training acc: 0.28125
Global Iter: 525500 training loss: 1.98396
Global Iter: 525500 training acc: 0.21875
Global Iter: 525600 training loss: 2.06946
Global Iter: 525600 training acc: 0.1875
Global Iter: 525700 training loss: 1.97845
Global Iter: 525700 training acc: 0.21875
Global Iter: 525800 training loss: 2.05983
Global Iter: 525800 training acc: 0.21875
Global Iter: 525900 training loss: 1.96861
Global Iter: 525900 trai2017-06-21 19:59:17.080311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-528363
ning acc: 0.15625
Global Iter: 526000 training loss: 1.97942
Global Iter: 526000 training acc: 0.15625
Global Iter: 526100 training loss: 1.92343
Global Iter: 526100 training acc: 0.125
Global Iter: 526200 training loss: 2.11334
Global Iter: 526200 training acc: 0.0625
Global Iter: 526300 training loss: 2.00043
Global Iter: 526300 training acc: 0.21875
Global Iter: 526400 training loss: 2.00038
Global Iter: 526400 training acc: 0.25
Global Iter: 526500 training loss: 2.00388
Global Iter: 526500 training acc: 0.09375
Global Iter: 526600 training loss: 2.00838
Global Iter: 526600 training acc: 0.125
Global Iter: 526700 training loss: 2.01273
Global Iter: 526700 training acc: 0.125
Global Iter: 526800 training loss: 1.99886
Global Iter: 526800 training acc: 0.15625
Global Iter: 526900 training loss: 1.9583
Global Iter: 526900 training acc: 0.1875
Global Iter: 527000 training loss: 1.94707
Global Iter: 527000 training acc: 0.15625
Global Iter: 527100 training loss: 1.96066
Global Iter: 527100 training acc: 0.3125
Global Iter: 527200 training loss: 2.02589
Global Iter: 527200 training acc: 0.21875
Global Iter: 527300 training loss: 1.91714
Global Iter: 527300 training acc: 0.15625
Global Iter: 527400 training loss: 2.00028
Global Iter: 527400 training acc: 0.1875
Global Iter: 527500 training loss: 1.92918
Global Iter: 527500 training acc: 0.3125
Global Iter: 527600 training loss: 1.94973
Global Iter: 527600 training acc: 0.21875
Global Iter: 527700 training loss: 1.92891
Global Iter: 527700 training acc: 0.28125
Global Iter: 527800 training loss: 2.06407
Global Iter: 527800 training acc: 0.15625
Global Iter: 527900 training loss: 2.01914
Global Iter: 527900 training acc: 0.34375
Global Iter: 528000 training loss: 1.99258
Global Iter: 528000 training acc: 0.3125
Global Iter: 528100 training loss: 1.98602
Global Iter: 528100 training acc: 0.1875
Global Iter: 528200 training loss: 1.88557
Global Iter: 528200 training acc: 0.3125
Global Iter: 528300 training loss: 1.97616
Global Iter: 528300 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-528363
Number of Patches: 222853
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-528363
Global Iter: 528400 training loss: 1.98727
Global Iter: 528400 training acc: 0.21875
Global Iter: 528500 training loss: 1.94563
Global Iter: 528500 training acc: 0.25
Global Iter: 528600 training loss: 2.09541
Global Iter: 528600 training acc: 0.1875
Global Iter: 528700 training loss: 2.09148
Global Iter: 528700 training acc: 0.1875
Global Iter: 528800 training loss: 2.07407
Global Iter: 528800 training acc: 0.25
Global Iter: 528900 training loss: 2.01995
Global Iter: 528900 training acc: 0.09375
Global Iter: 529000 training loss: 1.89868
Global Iter: 529000 training acc: 0.28125
Global Iter: 529100 training loss: 1.87406
Global Iter: 529100 training acc: 0.375
Global Iter: 529200 training loss: 1.98539
Global Iter: 529200 training acc: 0.1875
Global Iter: 529300 training loss: 1.95993
Global Iter: 529300 training acc: 0.125
Global Iter: 529400 training loss: 2.1037
Global Iter: 529400 training acc: 0.15625
Global Iter: 529500 training loss: 1.92399
Global Iter: 529500 training acc: 0.25
Global Iter: 529600 training loss: 1.96931
Global Iter: 529600 training acc: 0.21875
Global Iter: 529700 training loss: 2.07321
Global Iter: 529700 training acc: 0.1875
Global Iter: 529800 training loss: 1.9188
Global Iter: 529800 training acc: 0.15625
Global Iter: 529900 training loss: 2.07068
Global Iter: 529900 training acc: 0.15625
Global Iter: 530000 training loss: 2.00748
Global Iter: 530000 training acc: 0.1875
Global Iter: 530100 training loss: 1.941
Global Iter: 530100 training acc: 0.25
Global Iter: 530200 training loss: 2.1427
Global Iter: 530200 training acc: 0.0625
Global Iter: 530300 training loss: 2.06753
Global Iter: 530300 training acc: 0.09375
Global Iter: 530400 training loss: 1.93488
Global Iter: 530400 training acc: 0.21875
Global Iter: 530500 training loss: 1.99765
Global Iter: 530500 training acc: 0.3125
Global Iter: 530600 training loss: 2.03112
Global Iter: 530600 training acc: 0.09375
Global Iter: 530700 training loss: 2.03926
Global Iter: 530700 training acc: 0.21875
Global Iter: 530800 training loss: 1.94465
Global Iter: 530800 training acc: 0.21875
Global Iter: 530900 training loss: 2.02535
Global Iter: 530900 training acc: 0.25
Global Iter: 531000 training loss: 1.87177
Global Iter: 531000 training acc: 0.28125
Global Iter: 531100 training loss: 1.99832
Global Iter: 531100 training acc: 0.25
Global Iter: 531200 training loss: 2.07503
Global Iter: 531200 training acc: 0.25
Global Iter: 531300 training loss: 2.06539
Global Iter: 531300 training acc: 0.0625
Global Iter: 531400 training loss: 1.95489
Global Iter: 531400 training acc: 0.25
Global Iter: 531500 training loss: 2.14418
Global Iter: 531500 training acc: 0.1875
Global Iter: 531600 training loss: 1.94109
Global Iter: 531600 training acc: 0.21875
Global Iter: 531700 training loss: 2.04669
Global Iter: 531700 training acc: 0.28125
Global Iter: 531800 training loss: 1.98203
Global Iter: 531800 training acc: 0.15625
Global Iter: 531900 training loss: 2.07747
Global Iter: 531900 training acc: 0.25
Global Iter: 532000 training loss: 2.0247
Global Iter: 532000 training acc: 0.28125
Global Iter: 532100 training loss: 1.92358
Global Iter: 532100 training acc: 0.25
Global Iter: 532200 training loss: 1.9155
Global Iter: 532200 training acc: 0.25
Global Iter: 532300 training loss: 1.89883
Global Iter: 532300 training acc: 0.28125
Global Iter: 532400 training loss: 1.94327
Global Iter: 532400 training acc: 0.21875
Global Iter: 532500 training loss: 1.91756
Global Iter: 532500 training acc: 0.21875
Global Iter: 532600 training loss: 2.00018
Global Iter: 532600 training acc: 0.09375
Global Iter: 532700 training loss: 1.94241
Global Iter: 532700 training acc: 0.15625
Global Iter: 532800 training loss: 2.02927
Global Iter: 532800 training acc: 0.125
Global Iter: 532900 training loss: 1.97147
Global Iter: 532900 training acc: 0.15625
Global Iter: 533000 training loss: 1.99981
Global Iter: 533000 training acc: 0.15625
Global Iter: 533100 training loss: 1.94688
Global Iter: 533100 training acc: 0.125
Global Iter: 533200 training loss: 1.94516
Global Iter: 533200 training acc: 0.21875
Global Iter: 533300 training loss: 1.95342
Global Iter: 533300 training acc: 0.21875
Global Iter: 533400 training loss: 1.97667
Global Iter: 533400 training acc: 0.1875
Global Iter: 533500 training loss: 2.05275
Global Iter: 533500 training acc: 0.21875
Global Iter: 533600 training loss: 1.94967
Global Iter: 533600 training acc: 0.28125
Global Iter: 533700 training loss: 2.04441
Global Iter: 533700 training acc: 0.21875
Global Iter: 533800 training loss: 1.93549
Global Iter: 533800 training acc: 0.25
Global Iter: 533900 training loss: 1.99567
Global Iter: 533900 training acc: 0.15625
Global Iter: 534000 training loss: 2.12448
Global Iter: 534000 training acc: 0.1875
Global Iter: 534100 training loss: 1.95653
Global Iter: 534100 training acc: 0.25
Global Iter: 534200 training loss: 2.07054
Global Iter: 534200 training acc: 0.09375
Global Iter: 534300 training loss: 1.95703
Global Iter: 534300 training acc: 0.25
Global Iter: 534400 training loss: 1.94309
Global Iter: 534400 training acc: 0.1875
Global Iter: 534500 training loss: 1.87739
Global Iter: 534500 training acc: 0.1875
Global Iter: 534600 training loss: 1.9557
Global Iter: 534600 training acc: 0.15625
Global Iter: 534700 training loss: 2.07825
Global Iter: 534700 training acc: 0.0625
Global Iter: 534800 training loss: 1.93557
Global Iter: 534800 training acc: 0.125
Global Iter: 534900 training loss: 1.97464
Global Iter: 534900 training acc: 0.09375
Global Iter: 535000 training loss: 2.13526
Global Iter: 535000 training acc: 0.1875
Global Iter: 535100 training loss: 1.98803
Global Iter: 535100 training acc: 0.15625
Global Iter: 535200 training loss: 2.02528
Global Iter: 535200 training acc: 0.1875
Global Iter: 535300 training loss: 2.0504
Global Iter: 535300 training acc: 0.125
Global Iter: 535400 training loss: 1.94073
Global Iter: 535400 training acc: 0.21875
Global Iter: 535500 training loss: 2.02754
Global Iter: 535500 training acc: 0.125
Global Iter: 535600 training loss: 1.93655
Global Iter: 535600 training acc: 0.1875
Global Iter: 535700 training loss: 1.98731
Global Iter: 535700 training acc: 0.25
Global Iter: 535800 training loss: 2.03191
Global Iter: 535800 training acc: 0.25
Global Iter: 535900 training loss: 2.0549
Global Iter: 535900 training acc: 0.09375
Global Iter: 536000 training loss: 1.96427
Global Iter: 536000 training acc: 0.21875
Global Iter: 536100 training loss: 2.07065
Global Iter: 536100 training acc: 0.09375
Global Iter: 536200 training loss: 1.98987
Global Iter: 536200 training acc: 0.125
Global Iter: 536300 training loss: 1.95227
Global Iter: 536300 training acc: 0.28125
Global Iter: 536400 training loss: 1.8991
Global Iter: 536400 training acc: 0.28125
Global Iter: 536500 training loss: 2.08903
Global Iter: 536500 training acc: 0.0625
Global Iter: 536600 training loss: 1.99394
Global Iter: 536600 training acc: 0.1875
Global Iter: 536700 training loss: 1.98973
Global Iter: 536700 training acc: 0.15625
Global Iter: 536800 training loss: 1.897
Global Iter: 536800 training acc: 0.21875
Global Iter: 536900 training loss: 1.85808
Global Iter: 536900 training acc: 0.25
Global Iter: 537000 training loss: 1.90805
Global Iter: 537000 training acc: 0.1875
Global Iter: 537100 training loss: 1.94852
Global Iter: 537100 training acc: 0.28125
Global Iter: 537200 training loss: 2.09135
Global Iter: 537200 training acc: 0.15625
Global Iter: 537300 training loss: 2.00096
Global Iter: 537300 training acc: 0.1875
Global Iter: 537400 training loss: 1.87212
Global Iter: 537400 training acc: 0.3125
Global Iter: 537500 training loss: 1.9764
Global Iter: 537500 training acc: 0.1875
Global Iter: 537600 training loss: 1.97396
Global Iter: 537600 training acc: 0.15625
Global Iter: 537700 training loss: 1.99391
Global Iter: 537700 training acc: 0.125
Global Iter: 537800 training loss: 2.08265
Global Iter: 537800 training acc: 0.09375
Global Iter: 537900 training loss: 1.9498
Global Iter: 537900 training acc: 0.125
Global Iter: 538000 training loss: 1.9136
Global Iter: 538000 training acc: 0.09375
Global Iter: 538100 training loss: 2.03592
Global Iter: 538100 training acc: 0.25
Global Iter: 538200 training loss: 1.9765
Global Iter: 538200 training acc: 0.1875
Global Iter: 538300 training loss: 2.00263
Global Iter: 538300 training acc: 0.15625
Global Iter: 538400 training loss: 1.97681
Global Iter: 538400 training acc: 0.3125
Global Iter: 538500 training loss: 1.99326
Global Iter: 538500 training acc: 0.15625
Global Iter: 538600 training loss: 1.88946
Global Iter: 538600 training acc: 0.125
Global Iter: 538700 training loss: 1.97831
Global Iter: 538700 training acc: 0.125
Global Iter: 538800 training loss: 1.9186
Global Iter: 538800 training acc: 0.15625
Global Iter: 538900 training loss: 1.9567
Global Iter: 538900 training acc: 0.0625
Global Iter: 539000 training loss: 2.02673
Global Iter: 539000 training acc: 0.15625
Global Iter: 539100 training loss: 1.88264
Global Iter: 539100 training acc: 0.25
Global Iter: 539200 training loss: 2.03008
Global Iter: 539200 training acc: 0.25
Global Iter: 539300 training loss: 2.06056
Global Iter: 539300 training acc: 0.0625
Global Iter: 539400 training loss: 2.04186
Global Iter: 539400 training acc: 0.125
Global Iter: 539500 training loss: 1.98341
Global Iter: 539500 training acc: 0.09375
Global Iter: 539600 training loss: 1.99061
Global Iter: 539600 training acc: 0.15625
Global Iter: 539700 training loss: 2.1413
Global Iter: 539700 training acc: 0.09375
Global Iter: 539800 training loss: 2.03566
Global Iter: 539800 training acc: 0.1875
Global Iter: 539900 training loss: 1.93498
Global Iter: 539900 training acc: 0.1875
Global Iter: 540000 training loss: 2.06663
Global Iter: 540000 training acc: 0.15625
Global Iter: 540100 training loss: 1.99447
Global Iter: 540100 training acc: 0.21875
Global Iter: 540200 training loss: 1.94704
Global Iter: 540200 training acc: 0.125
Global Iter: 540300 training loss: 1.9831
Global2017-06-21 20:23:34.030717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-542292
 Iter: 540300 training acc: 0.125
Global Iter: 540400 training loss: 1.97404
Global Iter: 540400 training acc: 0.0625
Global Iter: 540500 training loss: 1.98932
Global Iter: 540500 training acc: 0.28125
Global Iter: 540600 training loss: 1.94731
Global Iter: 540600 training acc: 0.21875
Global Iter: 540700 training loss: 1.9309
Global Iter: 540700 training acc: 0.25
Global Iter: 540800 training loss: 1.92855
Global Iter: 540800 training acc: 0.15625
Global Iter: 540900 training loss: 2.00417
Global Iter: 540900 training acc: 0.1875
Global Iter: 541000 training loss: 1.89483
Global Iter: 541000 training acc: 0.3125
Global Iter: 541100 training loss: 2.04289
Global Iter: 541100 training acc: 0.15625
Global Iter: 541200 training loss: 1.91958
Global Iter: 541200 training acc: 0.25
Global Iter: 541300 training loss: 1.97765
Global Iter: 541300 training acc: 0.125
Global Iter: 541400 training loss: 1.9289
Global Iter: 541400 training acc: 0.25
Global Iter: 541500 training loss: 2.06527
Global Iter: 541500 training acc: 0.1875
Global Iter: 541600 training loss: 2.1015
Global Iter: 541600 training acc: 0.1875
Global Iter: 541700 training loss: 1.97621
Global Iter: 541700 training acc: 0.21875
Global Iter: 541800 training loss: 2.04787
Global Iter: 541800 training acc: 0.1875
Global Iter: 541900 training loss: 1.90606
Global Iter: 541900 training acc: 0.28125
Global Iter: 542000 training loss: 1.96229
Global Iter: 542000 training acc: 0.15625
Global Iter: 542100 training loss: 2.00864
Global Iter: 542100 training acc: 0.15625
Global Iter: 542200 training loss: 1.97325
Global Iter: 542200 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-542292
Number of Patches: 220625
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-542292
Global Iter: 542300 training loss: 2.01024
Global Iter: 542300 training acc: 0.1875
Global Iter: 542400 training loss: 1.97911
Global Iter: 542400 training acc: 0.09375
Global Iter: 542500 training loss: 1.99041
Global Iter: 542500 training acc: 0.09375
Global Iter: 542600 training loss: 1.89631
Global Iter: 542600 training acc: 0.28125
Global Iter: 542700 training loss: 2.0478
Global Iter: 542700 training acc: 0.1875
Global Iter: 542800 training loss: 1.9275
Global Iter: 542800 training acc: 0.1875
Global Iter: 542900 training loss: 1.98684
Global Iter: 542900 training acc: 0.21875
Global Iter: 543000 training loss: 2.02576
Global Iter: 543000 training acc: 0.1875
Global Iter: 543100 training loss: 1.98651
Global Iter: 543100 training acc: 0.21875
Global Iter: 543200 training loss: 2.04061
Global Iter: 543200 training acc: 0.1875
Global Iter: 543300 training loss: 2.07406
Global Iter: 543300 training acc: 0.125
Global Iter: 543400 training loss: 1.99072
Global Iter: 543400 training acc: 0.15625
Global Iter: 543500 training loss: 2.01314
Global Iter: 543500 training acc: 0.15625
Global Iter: 543600 training loss: 1.87342
Global Iter: 543600 training acc: 0.21875
Global Iter: 543700 training loss: 1.93533
Global Iter: 543700 training acc: 0.1875
Global Iter: 543800 training loss: 1.96008
Global Iter: 543800 training acc: 0.25
Global Iter: 543900 training loss: 1.98849
Global Iter: 543900 training acc: 0.15625
Global Iter: 544000 training loss: 2.0014
Global Iter: 544000 training acc: 0.1875
Global Iter: 544100 training loss: 2.01212
Global Iter: 544100 training acc: 0.0625
Global Iter: 544200 training loss: 1.90194
Global Iter: 544200 training acc: 0.25
Global Iter: 544300 training loss: 2.04023
Global Iter: 544300 training acc: 0.21875
Global Iter: 544400 training loss: 2.04575
Global Iter: 544400 training acc: 0.1875
Global Iter: 544500 training loss: 2.07051
Global Iter: 544500 training acc: 0.15625
Global Iter: 544600 training loss: 2.00947
Global Iter: 544600 training acc: 0.1875
Global Iter: 544700 training loss: 2.08263
Global Iter: 544700 training acc: 0.28125
Global Iter: 544800 training loss: 1.94543
Global Iter: 544800 training acc: 0.1875
Global Iter: 544900 training loss: 1.95351
Global Iter: 544900 training acc: 0.21875
Global Iter: 545000 training loss: 2.00626
Global Iter: 545000 training acc: 0.15625
Global Iter: 545100 training loss: 2.03169
Global Iter: 545100 training acc: 0.28125
Global Iter: 545200 training loss: 2.0417
Global Iter: 545200 training acc: 0.28125
Global Iter: 545300 training loss: 1.89026
Global Iter: 545300 training acc: 0.28125
Global Iter: 545400 training loss: 1.92989
Global Iter: 545400 training acc: 0.21875
Global Iter: 545500 training loss: 1.89157
Global Iter: 545500 training acc: 0.28125
Global Iter: 545600 training loss: 1.99375
Global Iter: 545600 training acc: 0.25
Global Iter: 545700 training loss: 2.03655
Global Iter: 545700 training acc: 0.09375
Global Iter: 545800 training loss: 2.06525
Global Iter: 545800 training acc: 0.1875
Global Iter: 545900 training loss: 2.05105
Global Iter: 545900 training acc: 0.15625
Global Iter: 546000 training loss: 1.96146
Global Iter: 546000 training acc: 0.21875
Global Iter: 546100 training loss: 1.99123
Global Iter: 546100 training acc: 0.125
Global Iter: 546200 training loss: 2.087
Global Iter: 546200 training acc: 0.1875
Global Iter: 546300 training loss: 1.94391
Global Iter: 546300 training acc: 0.25
Global Iter: 546400 training loss: 1.92966
Global Iter: 546400 training acc: 0.28125
Global Iter: 546500 training loss: 1.88854
Global Iter: 546500 training acc: 0.21875
Global Iter: 546600 training loss: 1.88562
Global Iter: 546600 training acc: 0.21875
Global Iter: 546700 training loss: 1.98422
Global Iter: 546700 training acc: 0.15625
Global Iter: 546800 training loss: 1.96347
Global Iter: 546800 training acc: 0.125
Global Iter: 546900 training loss: 1.99945
Global Iter: 546900 training acc: 0.3125
Global Iter: 547000 training loss: 1.92709
Global Iter: 547000 training acc: 0.21875
Global Iter: 547100 training loss: 1.98018
Global Iter: 547100 training acc: 0.15625
Global Iter: 547200 training loss: 1.93711
Global Iter: 547200 training acc: 0.21875
Global Iter: 547300 training loss: 2.0143
Global Iter: 547300 training acc: 0.09375
Global Iter: 547400 training loss: 2.08974
Global Iter: 547400 training acc: 0.125
Global Iter: 547500 training loss: 1.90524
Global Iter: 547500 training acc: 0.3125
Global Iter: 547600 training loss: 2.00167
Global Iter: 547600 training acc: 0.25
Global Iter: 547700 training loss: 1.98132
Global Iter: 547700 training acc: 0.21875
Global Iter: 547800 training loss: 1.90591
Global Iter: 547800 training acc: 0.21875
Global Iter: 547900 training loss: 1.90869
Global Iter: 547900 training acc: 0.21875
Global Iter: 548000 training loss: 1.91242
Global Iter: 548000 training acc: 0.25
Global Iter: 548100 training loss: 1.97442
Global Iter: 548100 training acc: 0.21875
Global Iter: 548200 training loss: 1.95725
Global Iter: 548200 training acc: 0.125
Global Iter: 548300 training loss: 2.07657
Global Iter: 548300 training acc: 0.09375
Global Iter: 548400 training loss: 2.04467
Global Iter: 548400 training acc: 0.15625
Global Iter: 548500 training loss: 1.98233
Global Iter: 548500 training acc: 0.15625
Global Iter: 548600 training loss: 1.99761
Global Iter: 548600 training acc: 0.125
Global Iter: 548700 training loss: 1.96069
Global Iter: 548700 training acc: 0.15625
Global Iter: 548800 training loss: 2.02422
Global Iter: 548800 training acc: 0.15625
Global Iter: 548900 training loss: 1.99015
Global Iter: 548900 training acc: 0.15625
Global Iter: 549000 training loss: 1.9428
Global Iter: 549000 training acc: 0.25
Global Iter: 549100 training loss: 1.99187
Global Iter: 549100 training acc: 0.125
Global Iter: 549200 training loss: 1.96493
Global Iter: 549200 training acc: 0.21875
Global Iter: 549300 training loss: 1.90752
Global Iter: 549300 training acc: 0.21875
Global Iter: 549400 training loss: 1.96066
Global Iter: 549400 training acc: 0.1875
Global Iter: 549500 training loss: 1.91513
Global Iter: 549500 training acc: 0.21875
Global Iter: 549600 training loss: 2.03987
Global Iter: 549600 training acc: 0.15625
Global Iter: 549700 training loss: 1.95161
Global Iter: 549700 training acc: 0.25
Global Iter: 549800 training loss: 1.8932
Global Iter: 549800 training acc: 0.25
Global Iter: 549900 training loss: 1.96101
Global Iter: 549900 training acc: 0.21875
Global Iter: 550000 training loss: 1.97864
Global Iter: 550000 training acc: 0.21875
Global Iter: 550100 training loss: 1.91557
Global Iter: 550100 training acc: 0.1875
Global Iter: 550200 training loss: 1.94429
Global Iter: 550200 training acc: 0.3125
Global Iter: 550300 training loss: 1.9537
Global Iter: 550300 training acc: 0.15625
Global Iter: 550400 training loss: 2.06156
Global Iter: 550400 training acc: 0.15625
Global Iter: 550500 training loss: 1.99626
Global Iter: 550500 training acc: 0.09375
Global Iter: 550600 training loss: 1.98719
Global Iter: 550600 training acc: 0.28125
Global Iter: 550700 training loss: 1.89249
Global Iter: 550700 training acc: 0.28125
Global Iter: 550800 training loss: 1.94017
Global Iter: 550800 training acc: 0.1875
Global Iter: 550900 training loss: 1.96004
Global Iter: 550900 training acc: 0.15625
Global Iter: 551000 training loss: 2.17789
Global Iter: 551000 training acc: 0.1875
Global Iter: 551100 training loss: 1.92277
Global Iter: 551100 training acc: 0.21875
Global Iter: 551200 training loss: 1.94538
Global Iter: 551200 training acc: 0.40625
Global Iter: 551300 training loss: 2.01154
Global Iter: 551300 training acc: 0.15625
Global Iter: 551400 training loss: 1.9712
Global Iter: 551400 training acc: 0.25
Global Iter: 551500 training loss: 2.00674
Global Iter: 551500 training acc: 0.21875
Global Iter: 551600 training loss: 1.98444
Global Iter: 551600 training acc: 0.125
Global Iter: 551700 training loss: 1.99842
Global Iter: 551700 training acc: 0.09375
Global Iter: 551800 training loss: 2.05207
Global Iter: 551800 training acc: 0.125
Global Iter: 551900 training loss: 1.96994
Global Iter: 551900 training acc: 0.21875
Global Iter: 552000 training loss: 1.94721
Global Iter: 552000 training acc: 0.15625
Global Iter: 552100 training loss: 1.96971
Global Iter: 552100 training acc: 0.28125
Global Iter: 552200 training loss: 2.00364
Global Iter: 552200 training acc: 0.21875
Global Iter: 552300 training loss: 1.96183
Global Iter: 552300 training acc: 0.125
Global Iter: 552400 training loss: 1.96554
Global Iter: 552400 training acc: 0.21875
Global Iter: 552500 training loss: 2.10243
Global Iter: 552500 training acc: 0.125
Global Iter: 552600 training loss: 2.06195
Global Iter: 552600 training acc: 0.15625
Global Iter: 552700 training loss: 1.94557
Global Iter: 552700 training acc: 0.21875
Global Iter: 552800 training loss: 2.03813
Global Iter: 552800 training acc: 0.1875
Global Iter: 552900 training loss: 1.99964
Global Iter: 552900 training acc: 0.09375
Global Iter: 553000 training loss: 1.92494
Global Iter: 553000 training acc: 0.09375
Global Iter: 553100 training loss: 2.00495
Global Iter: 553100 training acc: 0.1875
Global Iter: 553200 training loss: 2.01101
Global Iter: 553200 training acc: 0.28125
Global Iter: 553300 training loss: 1.85336
Global Iter: 553300 training acc: 0.28125
Global Iter: 553400 training loss: 1.9236
Global Iter: 553400 training acc: 0.1875
Global Iter: 553500 training loss: 2.03256
Global Iter: 553500 training acc: 0.125
Global Iter: 553600 training loss: 2.14385
Global Iter: 553600 training acc: 0.09375
Global Iter: 553700 training loss: 1.97404
Global Iter: 553700 training acc: 0.21875
Global Iter: 553800 training loss: 2.11934
Global Iter: 553800 training acc: 0.15625
Global Iter: 553900 training loss: 2.00144
Global Iter: 553900 training acc: 0.21875
Global Iter: 554000 training loss: 2.04317
Global Iter: 554000 training acc: 0.1875
Global Iter: 554100 training loss: 2.04548
Global Iter: 554100 training acc: 0.125
Global Iter: 554200 training loss: 2.06326
Global Iter: 554200 training acc: 0.3125
Global Iter: 554300 training loss: 1.95058
Global Iter: 554300 training acc: 0.1875
Global Iter: 554400 training loss: 1.96766
Global Iter: 554400 training acc: 0.28125
Global Iter: 554500 training loss: 1.92603
Global Iter: 554500 training acc: 0.1875
Global Iter: 554600 training loss: 2.03924
Global Iter: 554600 training acc: 02017-06-21 20:47:37.517059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-556082
.125
Global Iter: 554700 training loss: 2.07402
Global Iter: 554700 training acc: 0.125
Global Iter: 554800 training loss: 2.02854
Global Iter: 554800 training acc: 0.125
Global Iter: 554900 training loss: 1.98241
Global Iter: 554900 training acc: 0.1875
Global Iter: 555000 training loss: 2.03393
Global Iter: 555000 training acc: 0.25
Global Iter: 555100 training loss: 2.02143
Global Iter: 555100 training acc: 0.15625
Global Iter: 555200 training loss: 1.90611
Global Iter: 555200 training acc: 0.28125
Global Iter: 555300 training loss: 1.974
Global Iter: 555300 training acc: 0.15625
Global Iter: 555400 training loss: 1.91398
Global Iter: 555400 training acc: 0.3125
Global Iter: 555500 training loss: 1.99548
Global Iter: 555500 training acc: 0.25
Global Iter: 555600 training loss: 1.95341
Global Iter: 555600 training acc: 0.21875
Global Iter: 555700 training loss: 2.04849
Global Iter: 555700 training acc: 0.21875
Global Iter: 555800 training loss: 1.96418
Global Iter: 555800 training acc: 0.125
Global Iter: 555900 training loss: 2.00722
Global Iter: 555900 training acc: 0.15625
Global Iter: 556000 training loss: 1.87584
Global Iter: 556000 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-556082
Number of Patches: 218419
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-556082
Global Iter: 556100 training loss: 1.90124
Global Iter: 556100 training acc: 0.25
Global Iter: 556200 training loss: 1.97414
Global Iter: 556200 training acc: 0.28125
Global Iter: 556300 training loss: 1.93005
Global Iter: 556300 training acc: 0.15625
Global Iter: 556400 training loss: 2.00915
Global Iter: 556400 training acc: 0.125
Global Iter: 556500 training loss: 1.9696
Global Iter: 556500 training acc: 0.21875
Global Iter: 556600 training loss: 1.9953
Global Iter: 556600 training acc: 0.09375
Global Iter: 556700 training loss: 1.87706
Global Iter: 556700 training acc: 0.25
Global Iter: 556800 training loss: 1.94393
Global Iter: 556800 training acc: 0.15625
Global Iter: 556900 training loss: 1.88711
Global Iter: 556900 training acc: 0.1875
Global Iter: 557000 training loss: 1.93646
Global Iter: 557000 training acc: 0.21875
Global Iter: 557100 training loss: 2.04256
Global Iter: 557100 training acc: 0.1875
Global Iter: 557200 training loss: 1.94803
Global Iter: 557200 training acc: 0.1875
Global Iter: 557300 training loss: 2.02579
Global Iter: 557300 training acc: 0.25
Global Iter: 557400 training loss: 1.92995
Global Iter: 557400 training acc: 0.28125
Global Iter: 557500 training loss: 1.96749
Global Iter: 557500 training acc: 0.21875
Global Iter: 557600 training loss: 1.96192
Global Iter: 557600 training acc: 0.1875
Global Iter: 557700 training loss: 1.90082
Global Iter: 557700 training acc: 0.25
Global Iter: 557800 training loss: 1.94218
Global Iter: 557800 training acc: 0.15625
Global Iter: 557900 training loss: 2.04546
Global Iter: 557900 training acc: 0.1875
Global Iter: 558000 training loss: 1.99224
Global Iter: 558000 training acc: 0.28125
Global Iter: 558100 training loss: 2.00456
Global Iter: 558100 training acc: 0.28125
Global Iter: 558200 training loss: 1.99018
Global Iter: 558200 training acc: 0.0625
Global Iter: 558300 training loss: 2.02987
Global Iter: 558300 training acc: 0.1875
Global Iter: 558400 training loss: 2.06925
Global Iter: 558400 training acc: 0.125
Global Iter: 558500 training loss: 1.90566
Global Iter: 558500 training acc: 0.21875
Global Iter: 558600 training loss: 1.96324
Global Iter: 558600 training acc: 0.125
Global Iter: 558700 training loss: 2.05729
Global Iter: 558700 training acc: 0.21875
Global Iter: 558800 training loss: 2.00592
Global Iter: 558800 training acc: 0.125
Global Iter: 558900 training loss: 1.94178
Global Iter: 558900 training acc: 0.25
Global Iter: 559000 training loss: 1.99358
Global Iter: 559000 training acc: 0.0625
Global Iter: 559100 training loss: 1.8481
Global Iter: 559100 training acc: 0.28125
Global Iter: 559200 training loss: 1.96291
Global Iter: 559200 training acc: 0.0625
Global Iter: 559300 training loss: 2.10867
Global Iter: 559300 training acc: 0.15625
Global Iter: 559400 training loss: 2.03629
Global Iter: 559400 training acc: 0.1875
Global Iter: 559500 training loss: 2.03907
Global Iter: 559500 training acc: 0.125
Global Iter: 559600 training loss: 1.91507
Global Iter: 559600 training acc: 0.21875
Global Iter: 559700 training loss: 2.02178
Global Iter: 559700 training acc: 0.15625
Global Iter: 559800 training loss: 2.00486
Global Iter: 559800 training acc: 0.1875
Global Iter: 559900 training loss: 2.00423
Global Iter: 559900 training acc: 0.125
Global Iter: 560000 training loss: 1.9626
Global Iter: 560000 training acc: 0.28125
Global Iter: 560100 training loss: 1.93868
Global Iter: 560100 training acc: 0.1875
Global Iter: 560200 training loss: 1.91884
Global Iter: 560200 training acc: 0.09375
Global Iter: 560300 training loss: 1.89769
Global Iter: 560300 training acc: 0.25
Global Iter: 560400 training loss: 2.03963
Global Iter: 560400 training acc: 0.125
Global Iter: 560500 training loss: 1.95625
Global Iter: 560500 training acc: 0.125
Global Iter: 560600 training loss: 1.93128
Global Iter: 560600 training acc: 0.21875
Global Iter: 560700 training loss: 1.90141
Global Iter: 560700 training acc: 0.21875
Global Iter: 560800 training loss: 1.91865
Global Iter: 560800 training acc: 0.21875
Global Iter: 560900 training loss: 1.96
Global Iter: 560900 training acc: 0.15625
Global Iter: 561000 training loss: 2.04335
Global Iter: 561000 training acc: 0.125
Global Iter: 561100 training loss: 2.04264
Global Iter: 561100 training acc: 0.15625
Global Iter: 561200 training loss: 2.03294
Global Iter: 561200 training acc: 0.28125
Global Iter: 561300 training loss: 1.95619
Global Iter: 561300 training acc: 0.25
Global Iter: 561400 training loss: 1.93647
Global Iter: 561400 training acc: 0.21875
Global Iter: 561500 training loss: 1.99777
Global Iter: 561500 training acc: 0.09375
Global Iter: 561600 training loss: 1.95795
Global Iter: 561600 training acc: 0.15625
Global Iter: 561700 training loss: 1.99436
Global Iter: 561700 training acc: 0.28125
Global Iter: 561800 training loss: 1.94464
Global Iter: 561800 training acc: 0.21875
Global Iter: 561900 training loss: 1.95769
Global Iter: 561900 training acc: 0.25
Global Iter: 562000 training loss: 2.0418
Global Iter: 562000 training acc: 0.125
Global Iter: 562100 training loss: 1.98015
Global Iter: 562100 training acc: 0.1875
Global Iter: 562200 training loss: 1.94439
Global Iter: 562200 training acc: 0.21875
Global Iter: 562300 training loss: 2.06979
Global Iter: 562300 training acc: 0.1875
Global Iter: 562400 training loss: 1.91406
Global Iter: 562400 training acc: 0.1875
Global Iter: 562500 training loss: 1.97644
Global Iter: 562500 training acc: 0.09375
Global Iter: 562600 training loss: 2.04368
Global Iter: 562600 training acc: 0.09375
Global Iter: 562700 training loss: 1.94324
Global Iter: 562700 training acc: 0.15625
Global Iter: 562800 training loss: 1.99487
Global Iter: 562800 training acc: 0.125
Global Iter: 562900 training loss: 2.03947
Global Iter: 562900 training acc: 0.21875
Global Iter: 563000 training loss: 2.06536
Global Iter: 563000 training acc: 0.21875
Global Iter: 563100 training loss: 1.97696
Global Iter: 563100 training acc: 0.125
Global Iter: 563200 training loss: 2.00427
Global Iter: 563200 training acc: 0.15625
Global Iter: 563300 training loss: 2.00627
Global Iter: 563300 training acc: 0.21875
Global Iter: 563400 training loss: 2.02289
Global Iter: 563400 training acc: 0.125
Global Iter: 563500 training loss: 2.03238
Global Iter: 563500 training acc: 0.15625
Global Iter: 563600 training loss: 1.94224
Global Iter: 563600 training acc: 0.1875
Global Iter: 563700 training loss: 1.99572
Global Iter: 563700 training acc: 0.28125
Global Iter: 563800 training loss: 2.11239
Global Iter: 563800 training acc: 0.09375
Global Iter: 563900 training loss: 1.96862
Global Iter: 563900 training acc: 0.15625
Global Iter: 564000 training loss: 2.12916
Global Iter: 564000 training acc: 0.0625
Global Iter: 564100 training loss: 1.98959
Global Iter: 564100 training acc: 0.21875
Global Iter: 564200 training loss: 1.88469
Global Iter: 564200 training acc: 0.28125
Global Iter: 564300 training loss: 2.05782
Global Iter: 564300 training acc: 0.1875
Global Iter: 564400 training loss: 1.97245
Global Iter: 564400 training acc: 0.3125
Global Iter: 564500 training loss: 2.03041
Global Iter: 564500 training acc: 0.125
Global Iter: 564600 training loss: 1.94094
Global Iter: 564600 training acc: 0.25
Global Iter: 564700 training loss: 1.95423
Global Iter: 564700 training acc: 0.125
Global Iter: 564800 training loss: 1.95045
Global Iter: 564800 training acc: 0.3125
Global Iter: 564900 training loss: 1.97534
Global Iter: 564900 training acc: 0.09375
Global Iter: 565000 training loss: 1.93812
Global Iter: 565000 training acc: 0.21875
Global Iter: 565100 training loss: 1.95239
Global Iter: 565100 training acc: 0.15625
Global Iter: 565200 training loss: 2.00631
Global Iter: 565200 training acc: 0.15625
Global Iter: 565300 training loss: 1.98077
Global Iter: 565300 training acc: 0.15625
Global Iter: 565400 training loss: 1.94359
Global Iter: 565400 training acc: 0.125
Global Iter: 565500 training loss: 2.03321
Global Iter: 565500 training acc: 0.15625
Global Iter: 565600 training loss: 2.37503
Global Iter: 565600 training acc: 0.09375
Global Iter: 565700 training loss: 2.02305
Global Iter: 565700 training acc: 0.1875
Global Iter: 565800 training loss: 2.03071
Global Iter: 565800 training acc: 0.15625
Global Iter: 565900 training loss: 1.92804
Global Iter: 565900 training acc: 0.1875
Global Iter: 566000 training loss: 2.03995
Global Iter: 566000 training acc: 0.15625
Global Iter: 566100 training loss: 1.93381
Global Iter: 566100 training acc: 0.25
Global Iter: 566200 training loss: 2.08989
Global Iter: 566200 training acc: 0.1875
Global Iter: 566300 training loss: 1.93973
Global Iter: 566300 training acc: 0.09375
Global Iter: 566400 training loss: 1.88081
Global Iter: 566400 training acc: 0.25
Global Iter: 566500 training loss: 1.95458
Global Iter: 566500 training acc: 0.21875
Global Iter: 566600 training loss: 1.92777
Global Iter: 566600 training acc: 0.21875
Global Iter: 566700 training loss: 1.92479
Global Iter: 566700 training acc: 0.15625
Global Iter: 566800 training loss: 1.95687
Global Iter: 566800 training acc: 0.09375
Global Iter: 566900 training loss: 1.93035
Global Iter: 566900 training acc: 0.28125
Global Iter: 567000 training loss: 2.05292
Global Iter: 567000 training acc: 0.21875
Global Iter: 567100 training loss: 2.00166
Global Iter: 567100 training acc: 0.1875
Global Iter: 567200 training loss: 1.97549
Global Iter: 567200 training acc: 0.1875
Global Iter: 567300 training loss: 2.04416
Global Iter: 567300 training acc: 0.15625
Global Iter: 567400 training loss: 2.07501
Global Iter: 567400 training acc: 0.125
Global Iter: 567500 training loss: 2.13307
Global Iter: 567500 training acc: 0.21875
Global Iter: 567600 training loss: 1.9328
Global Iter: 567600 training acc: 0.28125
Global Iter: 567700 training loss: 2.04905
Global Iter: 567700 training acc: 0.125
Global Iter: 567800 training loss: 1.85555
Global Iter: 567800 training acc: 0.375
Global Iter: 567900 training loss: 2.0435
Global Iter: 567900 training acc: 0.125
Global Iter: 568000 training loss: 2.08218
Global Iter: 568000 training acc: 0.15625
Global Iter: 568100 training loss: 2.00453
Global Iter: 568100 training acc: 0.1875
Global Iter: 568200 training loss: 1.99647
Global Iter: 568200 training acc: 0.15625
Global Iter: 568300 training loss: 1.91189
Global Iter: 568300 training acc: 0.21875
Global Iter: 568400 training loss: 1.97488
Global Iter: 568400 training acc: 0.09375
Global Iter: 568500 training loss: 2.01591
Global Iter: 568500 training acc: 0.125
Global Iter: 568600 training loss: 1.99326
Global Iter: 568600 training acc: 0.25
Global Iter: 568700 training loss: 1.98222
Global Iter: 568700 training acc: 0.21875
Global Iter: 568800 training loss: 2.05182
Global Iter: 568800 training acc: 0.125
Global Iter: 568900 training loss: 2.08866
Global Iter: 568900 training acc: 0.1875
Global Iter: 569000 training loss: 2.032017-06-21 21:11:14.327581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-569734
865
Global Iter: 569000 training acc: 0.28125
Global Iter: 569100 training loss: 1.92544
Global Iter: 569100 training acc: 0.21875
Global Iter: 569200 training loss: 1.89464
Global Iter: 569200 training acc: 0.34375
Global Iter: 569300 training loss: 1.91487
Global Iter: 569300 training acc: 0.25
Global Iter: 569400 training loss: 2.02872
Global Iter: 569400 training acc: 0.25
Global Iter: 569500 training loss: 1.88312
Global Iter: 569500 training acc: 0.25
Global Iter: 569600 training loss: 1.99172
Global Iter: 569600 training acc: 0.21875
Global Iter: 569700 training loss: 1.97786
Global Iter: 569700 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-569734
Number of Patches: 216235
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-569734
Global Iter: 569800 training loss: 2.0752
Global Iter: 569800 training acc: 0.15625
Global Iter: 569900 training loss: 2.08773
Global Iter: 569900 training acc: 0.21875
Global Iter: 570000 training loss: 1.99053
Global Iter: 570000 training acc: 0.0625
Global Iter: 570100 training loss: 2.00235
Global Iter: 570100 training acc: 0.1875
Global Iter: 570200 training loss: 1.98443
Global Iter: 570200 training acc: 0.15625
Global Iter: 570300 training loss: 2.00382
Global Iter: 570300 training acc: 0.1875
Global Iter: 570400 training loss: 1.93974
Global Iter: 570400 training acc: 0.1875
Global Iter: 570500 training loss: 1.90772
Global Iter: 570500 training acc: 0.21875
Global Iter: 570600 training loss: 1.95673
Global Iter: 570600 training acc: 0.15625
Global Iter: 570700 training loss: 1.93396
Global Iter: 570700 training acc: 0.28125
Global Iter: 570800 training loss: 2.01314
Global Iter: 570800 training acc: 0.15625
Global Iter: 570900 training loss: 1.87403
Global Iter: 570900 training acc: 0.3125
Global Iter: 571000 training loss: 1.92989
Global Iter: 571000 training acc: 0.125
Global Iter: 571100 training loss: 1.97576
Global Iter: 571100 training acc: 0.28125
Global Iter: 571200 training loss: 2.04475
Global Iter: 571200 training acc: 0.15625
Global Iter: 571300 training loss: 2.09361
Global Iter: 571300 training acc: 0.09375
Global Iter: 571400 training loss: 2.01685
Global Iter: 571400 training acc: 0.1875
Global Iter: 571500 training loss: 2.08384
Global Iter: 571500 training acc: 0.1875
Global Iter: 571600 training loss: 2.10497
Global Iter: 571600 training acc: 0.125
Global Iter: 571700 training loss: 1.93712
Global Iter: 571700 training acc: 0.21875
Global Iter: 571800 training loss: 1.96918
Global Iter: 571800 training acc: 0.1875
Global Iter: 571900 training loss: 1.99379
Global Iter: 571900 training acc: 0.15625
Global Iter: 572000 training loss: 1.99807
Global Iter: 572000 training acc: 0.0625
Global Iter: 572100 training loss: 1.94685
Global Iter: 572100 training acc: 0.21875
Global Iter: 572200 training loss: 2.08479
Global Iter: 572200 training acc: 0.09375
Global Iter: 572300 training loss: 1.97728
Global Iter: 572300 training acc: 0.25
Global Iter: 572400 training loss: 2.08622
Global Iter: 572400 training acc: 0.1875
Global Iter: 572500 training loss: 1.93759
Global Iter: 572500 training acc: 0.25
Global Iter: 572600 training loss: 1.98301
Global Iter: 572600 training acc: 0.125
Global Iter: 572700 training loss: 2.00384
Global Iter: 572700 training acc: 0.1875
Global Iter: 572800 training loss: 1.87883
Global Iter: 572800 training acc: 0.28125
Global Iter: 572900 training loss: 2.00226
Global Iter: 572900 training acc: 0.1875
Global Iter: 573000 training loss: 1.96888
Global Iter: 573000 training acc: 0.1875
Global Iter: 573100 training loss: 1.90181
Global Iter: 573100 training acc: 0.28125
Global Iter: 573200 training loss: 1.86765
Global Iter: 573200 training acc: 0.28125
Global Iter: 573300 training loss: 1.96754
Global Iter: 573300 training acc: 0.1875
Global Iter: 573400 training loss: 2.10244
Global Iter: 573400 training acc: 0.03125
Global Iter: 573500 training loss: 1.94915
Global Iter: 573500 training acc: 0.28125
Global Iter: 573600 training loss: 1.96614
Global Iter: 573600 training acc: 0.21875
Global Iter: 573700 training loss: 2.00936
Global Iter: 573700 training acc: 0.09375
Global Iter: 573800 training loss: 1.92967
Global Iter: 573800 training acc: 0.3125
Global Iter: 573900 training loss: 1.98157
Global Iter: 573900 training acc: 0.25
Global Iter: 574000 training loss: 1.92171
Global Iter: 574000 training acc: 0.28125
Global Iter: 574100 training loss: 2.03059
Global Iter: 574100 training acc: 0.125
Global Iter: 574200 training loss: 2.04952
Global Iter: 574200 training acc: 0.125
Global Iter: 574300 training loss: 1.99135
Global Iter: 574300 training acc: 0.21875
Global Iter: 574400 training loss: 1.9568
Global Iter: 574400 training acc: 0.1875
Global Iter: 574500 training loss: 2.01951
Global Iter: 574500 training acc: 0.1875
Global Iter: 574600 training loss: 1.95296
Global Iter: 574600 training acc: 0.34375
Global Iter: 574700 training loss: 1.99844
Global Iter: 574700 training acc: 0.15625
Global Iter: 574800 training loss: 1.97488
Global Iter: 574800 training acc: 0.1875
Global Iter: 574900 training loss: 1.98582
Global Iter: 574900 training acc: 0.1875
Global Iter: 575000 training loss: 1.94657
Global Iter: 575000 training acc: 0.3125
Global Iter: 575100 training loss: 2.04979
Global Iter: 575100 training acc: 0.15625
Global Iter: 575200 training loss: 1.94215
Global Iter: 575200 training acc: 0.25
Global Iter: 575300 training loss: 2.03277
Global Iter: 575300 training acc: 0.21875
Global Iter: 575400 training loss: 1.95263
Global Iter: 575400 training acc: 0.1875
Global Iter: 575500 training loss: 2.01026
Global Iter: 575500 training acc: 0.1875
Global Iter: 575600 training loss: 2.06843
Global Iter: 575600 training acc: 0.125
Global Iter: 575700 training loss: 2.024
Global Iter: 575700 training acc: 0.25
Global Iter: 575800 training loss: 2.00652
Global Iter: 575800 training acc: 0.09375
Global Iter: 575900 training loss: 1.92873
Global Iter: 575900 training acc: 0.125
Global Iter: 576000 training loss: 2.01243
Global Iter: 576000 training acc: 0.21875
Global Iter: 576100 training loss: 2.00361
Global Iter: 576100 training acc: 0.21875
Global Iter: 576200 training loss: 2.00462
Global Iter: 576200 training acc: 0.34375
Global Iter: 576300 training loss: 1.88229
Global Iter: 576300 training acc: 0.125
Global Iter: 576400 training loss: 1.9395
Global Iter: 576400 training acc: 0.21875
Global Iter: 576500 training loss: 1.94293
Global Iter: 576500 training acc: 0.15625
Global Iter: 576600 training loss: 1.93816
Global Iter: 576600 training acc: 0.125
Global Iter: 576700 training loss: 1.93841
Global Iter: 576700 training acc: 0.1875
Global Iter: 576800 training loss: 1.92905
Global Iter: 576800 training acc: 0.125
Global Iter: 576900 training loss: 1.87472
Global Iter: 576900 training acc: 0.21875
Global Iter: 577000 training loss: 1.97517
Global Iter: 577000 training acc: 0.21875
Global Iter: 577100 training loss: 2.03248
Global Iter: 577100 training acc: 0.21875
Global Iter: 577200 training loss: 1.94393
Global Iter: 577200 training acc: 0.28125
Global Iter: 577300 training loss: 2.00779
Global Iter: 577300 training acc: 0.15625
Global Iter: 577400 training loss: 2.07274
Global Iter: 577400 training acc: 0.15625
Global Iter: 577500 training loss: 1.95813
Global Iter: 577500 training acc: 0.25
Global Iter: 577600 training loss: 2.09788
Global Iter: 577600 training acc: 0.21875
Global Iter: 577700 training loss: 1.98635
Global Iter: 577700 training acc: 0.15625
Global Iter: 577800 training loss: 1.97171
Global Iter: 577800 training acc: 0.28125
Global Iter: 577900 training loss: 1.92563
Global Iter: 577900 training acc: 0.125
Global Iter: 578000 training loss: 1.87938
Global Iter: 578000 training acc: 0.4375
Global Iter: 578100 training loss: 1.98841
Global Iter: 578100 training acc: 0.1875
Global Iter: 578200 training loss: 1.98435
Global Iter: 578200 training acc: 0.21875
Global Iter: 578300 training loss: 1.89086
Global Iter: 578300 training acc: 0.25
Global Iter: 578400 training loss: 1.9372
Global Iter: 578400 training acc: 0.3125
Global Iter: 578500 training loss: 2.09664
Global Iter: 578500 training acc: 0.125
Global Iter: 578600 training loss: 2.09039
Global Iter: 578600 training acc: 0.0625
Global Iter: 578700 training loss: 2.00262
Global Iter: 578700 training acc: 0.15625
Global Iter: 578800 training loss: 2.06257
Global Iter: 578800 training acc: 0.125
Global Iter: 578900 training loss: 1.93435
Global Iter: 578900 training acc: 0.1875
Global Iter: 579000 training loss: 2.02771
Global Iter: 579000 training acc: 0.15625
Global Iter: 579100 training loss: 2.05203
Global Iter: 579100 training acc: 0.21875
Global Iter: 579200 training loss: 2.11964
Global Iter: 579200 training acc: 0.09375
Global Iter: 579300 training loss: 1.87158
Global Iter: 579300 training acc: 0.28125
Global Iter: 579400 training loss: 1.93173
Global Iter: 579400 training acc: 0.25
Global Iter: 579500 training loss: 2.08208
Global Iter: 579500 training acc: 0.15625
Global Iter: 579600 training loss: 2.04357
Global Iter: 579600 training acc: 0.125
Global Iter: 579700 training loss: 2.00133
Global Iter: 579700 training acc: 0.125
Global Iter: 579800 training loss: 2.07117
Global Iter: 579800 training acc: 0.09375
Global Iter: 579900 training loss: 2.07525
Global Iter: 579900 training acc: 0.0625
Global Iter: 580000 training loss: 2.02678
Global Iter: 580000 training acc: 0.15625
Global Iter: 580100 training loss: 1.98241
Global Iter: 580100 training acc: 0.09375
Global Iter: 580200 training loss: 2.0095
Global Iter: 580200 training acc: 0.25
Global Iter: 580300 training loss: 1.97889
Global Iter: 580300 training acc: 0.125
Global Iter: 580400 training loss: 1.98753
Global Iter: 580400 training acc: 0.1875
Global Iter: 580500 training loss: 1.99423
Global Iter: 580500 training acc: 0.1875
Global Iter: 580600 training loss: 2.0197
Global Iter: 580600 training acc: 0.1875
Global Iter: 580700 training loss: 2.05595
Global Iter: 580700 training acc: 0.25
Global Iter: 580800 training loss: 1.94641
Global Iter: 580800 training acc: 0.1875
Global Iter: 580900 training loss: 2.0586
Global Iter: 580900 training acc: 0.125
Global Iter: 581000 training loss: 1.95201
Global Iter: 581000 training acc: 0.15625
Global Iter: 581100 training loss: 1.9435
Global Iter: 581100 training acc: 0.15625
Global Iter: 581200 training loss: 2.03364
Global Iter: 581200 training acc: 0.09375
Global Iter: 581300 training loss: 1.90232
Global Iter: 581300 training acc: 0.21875
Global Iter: 581400 training loss: 2.05981
Global Iter: 581400 training acc: 0.21875
Global Iter: 581500 training loss: 1.95969
Global Iter: 581500 training acc: 0.1875
Global Iter: 581600 training loss: 2.1049
Global Iter: 581600 training acc: 0.125
Global Iter: 581700 training loss: 2.03105
Global Iter: 581700 training acc: 0.09375
Global Iter: 581800 training loss: 1.93472
Global Iter: 581800 training acc: 0.21875
Global Iter: 581900 training loss: 2.09253
Global Iter: 581900 training acc: 0.09375
Global Iter: 582000 training loss: 1.98774
Global Iter: 582000 training acc: 0.15625
Global Iter: 582100 training loss: 1.96208
Global Iter: 582100 training acc: 0.1875
Global Iter: 582200 training loss: 1.87192
Global Iter: 582200 training acc: 0.3125
Global Iter: 582300 training loss: 1.98712
Global Iter: 582300 training acc: 0.125
Global Iter: 582400 training loss: 1.97078
Global Iter: 582400 training acc: 0.15625
Global Iter: 582500 training loss: 2.05734
Global Iter: 582500 training acc: 0.1875
Global Iter: 582600 training loss: 1.99757
Global Iter: 582600 training acc: 0.3125
Global Iter: 582700 training loss: 2.00374
Global Iter: 582700 training acc: 0.1875
Global Iter: 582800 training loss: 2.05457
Global Iter: 582800 training acc: 0.15625
Global Iter: 582900 training loss: 2.05039
Global Iter: 582900 training acc: 0.125
Global Iter: 583000 training loss: 2.05287
Global Iter: 583000 training acc: 0.09375
Global Iter: 583100 training loss: 1.92094
Global Iter: 583100 training acc: 0.34375
Global Iter: 583200 training loss: 1.93514
Global Iter: 583200 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr02017-06-21 21:34:38.278824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-583249
005/model.ckpt-583249
Number of Patches: 214073
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-583249
Global Iter: 583300 training loss: 1.99117
Global Iter: 583300 training acc: 0.1875
Global Iter: 583400 training loss: 1.99863
Global Iter: 583400 training acc: 0.1875
Global Iter: 583500 training loss: 1.99736
Global Iter: 583500 training acc: 0.28125
Global Iter: 583600 training loss: 1.92156
Global Iter: 583600 training acc: 0.1875
Global Iter: 583700 training loss: 1.87047
Global Iter: 583700 training acc: 0.34375
Global Iter: 583800 training loss: 2.06584
Global Iter: 583800 training acc: 0.0625
Global Iter: 583900 training loss: 1.93865
Global Iter: 583900 training acc: 0.15625
Global Iter: 584000 training loss: 1.9677
Global Iter: 584000 training acc: 0.3125
Global Iter: 584100 training loss: 1.91924
Global Iter: 584100 training acc: 0.21875
Global Iter: 584200 training loss: 1.86301
Global Iter: 584200 training acc: 0.25
Global Iter: 584300 training loss: 1.99131
Global Iter: 584300 training acc: 0.1875
Global Iter: 584400 training loss: 1.93971
Global Iter: 584400 training acc: 0.1875
Global Iter: 584500 training loss: 1.96419
Global Iter: 584500 training acc: 0.1875
Global Iter: 584600 training loss: 1.96864
Global Iter: 584600 training acc: 0.25
Global Iter: 584700 training loss: 1.89216
Global Iter: 584700 training acc: 0.1875
Global Iter: 584800 training loss: 1.92872
Global Iter: 584800 training acc: 0.25
Global Iter: 584900 training loss: 1.90901
Global Iter: 584900 training acc: 0.1875
Global Iter: 585000 training loss: 1.92638
Global Iter: 585000 training acc: 0.1875
Global Iter: 585100 training loss: 2.00843
Global Iter: 585100 training acc: 0.09375
Global Iter: 585200 training loss: 1.91327
Global Iter: 585200 training acc: 0.1875
Global Iter: 585300 training loss: 1.96039
Global Iter: 585300 training acc: 0.125
Global Iter: 585400 training loss: 2.01297
Global Iter: 585400 training acc: 0.15625
Global Iter: 585500 training loss: 1.94835
Global Iter: 585500 training acc: 0.1875
Global Iter: 585600 training loss: 1.89906
Global Iter: 585600 training acc: 0.34375
Global Iter: 585700 training loss: 1.96846
Global Iter: 585700 training acc: 0.1875
Global Iter: 585800 training loss: 1.92299
Global Iter: 585800 training acc: 0.21875
Global Iter: 585900 training loss: 2.05028
Global Iter: 585900 training acc: 0.1875
Global Iter: 586000 training loss: 1.97496
Global Iter: 586000 training acc: 0.15625
Global Iter: 586100 training loss: 1.88791
Global Iter: 586100 training acc: 0.1875
Global Iter: 586200 training loss: 2.06311
Global Iter: 586200 training acc: 0.1875
Global Iter: 586300 training loss: 1.94529
Global Iter: 586300 training acc: 0.1875
Global Iter: 586400 training loss: 2.09752
Global Iter: 586400 training acc: 0.09375
Global Iter: 586500 training loss: 2.06941
Global Iter: 586500 training acc: 0.0625
Global Iter: 586600 training loss: 1.95503
Global Iter: 586600 training acc: 0.125
Global Iter: 586700 training loss: 1.94429
Global Iter: 586700 training acc: 0.21875
Global Iter: 586800 training loss: 1.9675
Global Iter: 586800 training acc: 0.25
Global Iter: 586900 training loss: 1.95376
Global Iter: 586900 training acc: 0.25
Global Iter: 587000 training loss: 2.01511
Global Iter: 587000 training acc: 0.25
Global Iter: 587100 training loss: 1.84611
Global Iter: 587100 training acc: 0.34375
Global Iter: 587200 training loss: 2.00871
Global Iter: 587200 training acc: 0.125
Global Iter: 587300 training loss: 1.96601
Global Iter: 587300 training acc: 0.1875
Global Iter: 587400 training loss: 2.07288
Global Iter: 587400 training acc: 0.125
Global Iter: 587500 training loss: 1.93303
Global Iter: 587500 training acc: 0.28125
Global Iter: 587600 training loss: 2.04422
Global Iter: 587600 training acc: 0.15625
Global Iter: 587700 training loss: 2.08593
Global Iter: 587700 training acc: 0.125
Global Iter: 587800 training loss: 1.98324
Global Iter: 587800 training acc: 0.1875
Global Iter: 587900 training loss: 1.91273
Global Iter: 587900 training acc: 0.21875
Global Iter: 588000 training loss: 1.91712
Global Iter: 588000 training acc: 0.21875
Global Iter: 588100 training loss: 1.98091
Global Iter: 588100 training acc: 0.21875
Global Iter: 588200 training loss: 2.10645
Global Iter: 588200 training acc: 0.25
Global Iter: 588300 training loss: 2.03655
Global Iter: 588300 training acc: 0.28125
Global Iter: 588400 training loss: 1.89635
Global Iter: 588400 training acc: 0.3125
Global Iter: 588500 training loss: 1.97857
Global Iter: 588500 training acc: 0.15625
Global Iter: 588600 training loss: 1.98851
Global Iter: 588600 training acc: 0.1875
Global Iter: 588700 training loss: 1.94459
Global Iter: 588700 training acc: 0.1875
Global Iter: 588800 training loss: 1.94203
Global Iter: 588800 training acc: 0.125
Global Iter: 588900 training loss: 1.98992
Global Iter: 588900 training acc: 0.21875
Global Iter: 589000 training loss: 1.95988
Global Iter: 589000 training acc: 0.21875
Global Iter: 589100 training loss: 1.99144
Global Iter: 589100 training acc: 0.09375
Global Iter: 589200 training loss: 1.93005
Global Iter: 589200 training acc: 0.25
Global Iter: 589300 training loss: 1.90811
Global Iter: 589300 training acc: 0.1875
Global Iter: 589400 training loss: 1.95645
Global Iter: 589400 training acc: 0.125
Global Iter: 589500 training loss: 2.11144
Global Iter: 589500 training acc: 0.09375
Global Iter: 589600 training loss: 1.99148
Global Iter: 589600 training acc: 0.25
Global Iter: 589700 training loss: 2.04078
Global Iter: 589700 training acc: 0.15625
Global Iter: 589800 training loss: 2.04801
Global Iter: 589800 training acc: 0.3125
Global Iter: 589900 training loss: 1.88018
Global Iter: 589900 training acc: 0.34375
Global Iter: 590000 training loss: 1.95902
Global Iter: 590000 training acc: 0.28125
Global Iter: 590100 training loss: 2.01029
Global Iter: 590100 training acc: 0.21875
Global Iter: 590200 training loss: 1.97236
Global Iter: 590200 training acc: 0.28125
Global Iter: 590300 training loss: 2.07798
Global Iter: 590300 training acc: 0.125
Global Iter: 590400 training loss: 2.04662
Global Iter: 590400 training acc: 0.1875
Global Iter: 590500 training loss: 1.95035
Global Iter: 590500 training acc: 0.21875
Global Iter: 590600 training loss: 2.03032
Global Iter: 590600 training acc: 0.3125
Global Iter: 590700 training loss: 1.95922
Global Iter: 590700 training acc: 0.1875
Global Iter: 590800 training loss: 1.94088
Global Iter: 590800 training acc: 0.15625
Global Iter: 590900 training loss: 1.90985
Global Iter: 590900 training acc: 0.40625
Global Iter: 591000 training loss: 1.94389
Global Iter: 591000 training acc: 0.125
Global Iter: 591100 training loss: 1.96927
Global Iter: 591100 training acc: 0.21875
Global Iter: 591200 training loss: 2.12346
Global Iter: 591200 training acc: 0.1875
Global Iter: 591300 training loss: 1.96324
Global Iter: 591300 training acc: 0.1875
Global Iter: 591400 training loss: 2.02083
Global Iter: 591400 training acc: 0.09375
Global Iter: 591500 training loss: 1.90469
Global Iter: 591500 training acc: 0.1875
Global Iter: 591600 training loss: 1.93645
Global Iter: 591600 training acc: 0.25
Global Iter: 591700 training loss: 1.98694
Global Iter: 591700 training acc: 0.21875
Global Iter: 591800 training loss: 1.92167
Global Iter: 591800 training acc: 0.34375
Global Iter: 591900 training loss: 2.05182
Global Iter: 591900 training acc: 0.1875
Global Iter: 592000 training loss: 1.92879
Global Iter: 592000 training acc: 0.25
Global Iter: 592100 training loss: 1.87061
Global Iter: 592100 training acc: 0.21875
Global Iter: 592200 training loss: 2.05034
Global Iter: 592200 training acc: 0.1875
Global Iter: 592300 training loss: 2.00976
Global Iter: 592300 training acc: 0.125
Global Iter: 592400 training loss: 1.94997
Global Iter: 592400 training acc: 0.15625
Global Iter: 592500 training loss: 2.00015
Global Iter: 592500 training acc: 0.15625
Global Iter: 592600 training loss: 2.04363
Global Iter: 592600 training acc: 0.21875
Global Iter: 592700 training loss: 1.9807
Global Iter: 592700 training acc: 0.03125
Global Iter: 592800 training loss: 1.93896
Global Iter: 5928002017-06-21 21:58:09.675020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-596629
 training acc: 0.21875
Global Iter: 592900 training loss: 2.0271
Global Iter: 592900 training acc: 0.15625
Global Iter: 593000 training loss: 2.00257
Global Iter: 593000 training acc: 0.21875
Global Iter: 593100 training loss: 2.06052
Global Iter: 593100 training acc: 0.03125
Global Iter: 593200 training loss: 1.93209
Global Iter: 593200 training acc: 0.28125
Global Iter: 593300 training loss: 2.03243
Global Iter: 593300 training acc: 0.21875
Global Iter: 593400 training loss: 1.92445
Global Iter: 593400 training acc: 0.28125
Global Iter: 593500 training loss: 2.04014
Global Iter: 593500 training acc: 0.21875
Global Iter: 593600 training loss: 2.03866
Global Iter: 593600 training acc: 0.09375
Global Iter: 593700 training loss: 2.00935
Global Iter: 593700 training acc: 0.21875
Global Iter: 593800 training loss: 1.96492
Global Iter: 593800 training acc: 0.09375
Global Iter: 593900 training loss: 1.95851
Global Iter: 593900 training acc: 0.21875
Global Iter: 594000 training loss: 1.98245
Global Iter: 594000 training acc: 0.25
Global Iter: 594100 training loss: 1.89501
Global Iter: 594100 training acc: 0.25
Global Iter: 594200 training loss: 1.9048
Global Iter: 594200 training acc: 0.21875
Global Iter: 594300 training loss: 2.0466
Global Iter: 594300 training acc: 0.1875
Global Iter: 594400 training loss: 2.00787
Global Iter: 594400 training acc: 0.09375
Global Iter: 594500 training loss: 2.0363
Global Iter: 594500 training acc: 0.03125
Global Iter: 594600 training loss: 1.93361
Global Iter: 594600 training acc: 0.25
Global Iter: 594700 training loss: 1.93277
Global Iter: 594700 training acc: 0.25
Global Iter: 594800 training loss: 2.0059
Global Iter: 594800 training acc: 0.25
Global Iter: 594900 training loss: 2.02848
Global Iter: 594900 training acc: 0.21875
Global Iter: 595000 training loss: 1.91413
Global Iter: 595000 training acc: 0.25
Global Iter: 595100 training loss: 2.06287
Global Iter: 595100 training acc: 0.09375
Global Iter: 595200 training loss: 2.02295
Global Iter: 595200 training acc: 0.28125
Global Iter: 595300 training loss: 1.97606
Global Iter: 595300 training acc: 0.125
Global Iter: 595400 training loss: 2.1027
Global Iter: 595400 training acc: 0.125
Global Iter: 595500 training loss: 2.00175
Global Iter: 595500 training acc: 0.0625
Global Iter: 595600 training loss: 2.03825
Global Iter: 595600 training acc: 0.21875
Global Iter: 595700 training loss: 1.88496
Global Iter: 595700 training acc: 0.25
Global Iter: 595800 training loss: 2.06629
Global Iter: 595800 training acc: 0.0625
Global Iter: 595900 training loss: 1.99271
Global Iter: 595900 training acc: 0.25
Global Iter: 596000 training loss: 1.93992
Global Iter: 596000 training acc: 0.1875
Global Iter: 596100 training loss: 1.93275
Global Iter: 596100 training acc: 0.15625
Global Iter: 596200 training loss: 1.92039
Global Iter: 596200 training acc: 0.28125
Global Iter: 596300 training loss: 1.98235
Global Iter: 596300 training acc: 0.15625
Global Iter: 596400 training loss: 1.9675
Global Iter: 596400 training acc: 0.21875
Global Iter: 596500 training loss: 1.99999
Global Iter: 596500 training acc: 0.1875
Global Iter: 596600 training loss: 2.06203
Global Iter: 596600 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-596629
Number of Patches: 211933
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-596629
Global Iter: 596700 training loss: 1.98077
Global Iter: 596700 training acc: 0.125
Global Iter: 596800 training loss: 2.02078
Global Iter: 596800 training acc: 0.09375
Global Iter: 596900 training loss: 1.97304
Global Iter: 596900 training acc: 0.15625
Global Iter: 597000 training loss: 2.07722
Global Iter: 597000 training acc: 0.1875
Global Iter: 597100 training loss: 1.92541
Global Iter: 597100 training acc: 0.25
Global Iter: 597200 training loss: 2.00179
Global Iter: 597200 training acc: 0.125
Global Iter: 597300 training loss: 1.95051
Global Iter: 597300 training acc: 0.15625
Global Iter: 597400 training loss: 1.90437
Global Iter: 597400 training acc: 0.15625
Global Iter: 597500 training loss: 2.00251
Global Iter: 597500 training acc: 0.15625
Global Iter: 597600 training loss: 2.0011
Global Iter: 597600 training acc: 0.15625
Global Iter: 597700 training loss: 1.94312
Global Iter: 597700 training acc: 0.125
Global Iter: 597800 training loss: 1.91585
Global Iter: 597800 training acc: 0.1875
Global Iter: 597900 training loss: 2.0096
Global Iter: 597900 training acc: 0.125
Global Iter: 598000 training loss: 1.91014
Global Iter: 598000 training acc: 0.3125
Global Iter: 598100 training loss: 1.96012
Global Iter: 598100 training acc: 0.40625
Global Iter: 598200 training loss: 1.95385
Global Iter: 598200 training acc: 0.28125
Global Iter: 598300 training loss: 1.97627
Global Iter: 598300 training acc: 0.1875
Global Iter: 598400 training loss: 1.87603
Global Iter: 598400 training acc: 0.21875
Global Iter: 598500 training loss: 2.06735
Global Iter: 598500 training acc: 0.25
Global Iter: 598600 training loss: 1.96556
Global Iter: 598600 training acc: 0.1875
Global Iter: 598700 training loss: 2.05034
Global Iter: 598700 training acc: 0.125
Global Iter: 598800 training loss: 2.09172
Global Iter: 598800 training acc: 0.15625
Global Iter: 598900 training loss: 1.96217
Global Iter: 598900 training acc: 0.1875
Global Iter: 599000 training loss: 1.89704
Global Iter: 599000 training acc: 0.34375
Global Iter: 599100 training loss: 2.05045
Global Iter: 599100 training acc: 0.125
Global Iter: 599200 training loss: 1.98359
Global Iter: 599200 training acc: 0.15625
Global Iter: 599300 training loss: 1.88879
Global Iter: 599300 training acc: 0.34375
Global Iter: 599400 training loss: 2.16496
Global Iter: 599400 training acc: 0.03125
Global Iter: 599500 training loss: 1.96657
Global Iter: 599500 training acc: 0.25
Global Iter: 599600 training loss: 1.925
Global Iter: 599600 training acc: 0.09375
Global Iter: 599700 training loss: 1.97085
Global Iter: 599700 training acc: 0.1875
Global Iter: 599800 training loss: 1.99891
Global Iter: 599800 training acc: 0.1875
Global Iter: 599900 training loss: 1.96355
Global Iter: 599900 training acc: 0.21875
Global Iter: 600000 training loss: 1.9762
Global Iter: 600000 training acc: 0.15625
Global Iter: 600100 training loss: 1.98123
Global Iter: 600100 training acc: 0.21875
Global Iter: 600200 training loss: 1.9394
Global Iter: 600200 training acc: 0.15625
Global Iter: 600300 training loss: 2.03445
Global Iter: 600300 training acc: 0.0625
Global Iter: 600400 training loss: 1.99714
Global Iter: 600400 training acc: 0.15625
Global Iter: 600500 training loss: 1.95443
Global Iter: 600500 training acc: 0.125
Global Iter: 600600 training loss: 1.9968
Global Iter: 600600 training acc: 0.15625
Global Iter: 600700 training loss: 2.04626
Global Iter: 600700 training acc: 0.28125
Global Iter: 600800 training loss: 2.13531
Global Iter: 600800 training acc: 0.15625
Global Iter: 600900 training loss: 1.90732
Global Iter: 600900 training acc: 0.1875
Global Iter: 601000 training loss: 1.97656
Global Iter: 601000 training acc: 0.15625
Global Iter: 601100 training loss: 1.93522
Global Iter: 601100 training acc: 0.3125
Global Iter: 601200 training loss: 2.09184
Global Iter: 601200 training acc: 0.1875
Global Iter: 601300 training loss: 1.87831
Global Iter: 601300 training acc: 0.34375
Global Iter: 601400 training loss: 1.94819
Global Iter: 601400 training acc: 0.21875
Global Iter: 601500 training loss: 2.11289
Global Iter: 601500 training acc: 0.03125
Global Iter: 601600 training loss: 1.8921
Global Iter: 601600 training acc: 0.15625
Global Iter: 601700 training loss: 2.04108
Global Iter: 601700 training acc: 0.1875
Global Iter: 601800 training loss: 2.02635
Global Iter: 601800 training acc: 0.25
Global Iter: 601900 training loss: 1.96353
Global Iter: 601900 training acc: 0.34375
Global Iter: 602000 training loss: 2.06595
Global Iter: 602000 training acc: 0.15625
Global Iter: 602100 training loss: 1.92778
Global Iter: 602100 training acc: 0.3125
Global Iter: 602200 training loss: 2.10928
Global Iter: 602200 training acc: 0.09375
Global Iter: 602300 training loss: 1.93314
Global Iter: 602300 training acc: 0.375
Global Iter: 602400 training loss: 2.00144
Global Iter: 602400 training acc: 0.125
Global Iter: 602500 training loss: 2.06111
Global Iter: 602500 training acc: 0.125
Global Iter: 602600 training loss: 1.94812
Global Iter: 602600 training acc: 0.125
Global Iter: 602700 training loss: 1.9772
Global Iter: 602700 training acc: 0.25
Global Iter: 602800 training loss: 2.02551
Global Iter: 602800 training acc: 0.1875
Global Iter: 602900 training loss: 1.9978
Global Iter: 602900 training acc: 0.15625
Global Iter: 603000 training loss: 1.90339
Global Iter: 603000 training acc: 0.28125
Global Iter: 603100 training loss: 1.92597
Global Iter: 603100 training acc: 0.09375
Global Iter: 603200 training loss: 2.09448
Global Iter: 603200 training acc: 0.1875
Global Iter: 603300 training loss: 2.06975
Global Iter: 603300 training acc: 0.21875
Global Iter: 603400 training loss: 1.93495
Global Iter: 603400 training acc: 0.25
Global Iter: 603500 training loss: 2.00952
Global Iter: 603500 training acc: 0.125
Global Iter: 603600 training loss: 2.05569
Global Iter: 603600 training acc: 0.25
Global Iter: 603700 training loss: 1.98544
Global Iter: 603700 training acc: 0.09375
Global Iter: 603800 training loss: 2.09365
Global Iter: 603800 training acc: 0.125
Global Iter: 603900 training loss: 1.94783
Global Iter: 603900 training acc: 0.21875
Global Iter: 604000 training loss: 2.17405
Global Iter: 604000 training acc: 0.25
Global Iter: 604100 training loss: 1.913
Global Iter: 604100 training acc: 0.125
Global Iter: 604200 training loss: 2.08823
Global Iter: 604200 training acc: 0.21875
Global Iter: 604300 training loss: 2.17993
Global Iter: 604300 training acc: 0.09375
Global Iter: 604400 training loss: 2.03875
Global Iter: 604400 training acc: 0.125
Global Iter: 604500 training loss: 1.9271
Global Iter: 604500 training acc: 0.0625
Global Iter: 604600 training loss: 2.01969
Global Iter: 604600 training acc: 0.25
Global Iter: 604700 training loss: 1.97181
Global Iter: 604700 training acc: 0.21875
Global Iter: 604800 training loss: 1.96808
Global Iter: 604800 training acc: 0.21875
Global Iter: 604900 training loss: 2.01552
Global Iter: 604900 training acc: 0.28125
Global Iter: 605000 training loss: 1.8996
Global Iter: 605000 training acc: 0.25
Global Iter: 605100 training loss: 2.10644
Global Iter: 605100 training acc: 0.125
Global Iter: 605200 training loss: 1.96077
Global Iter: 605200 training acc: 0.09375
Global Iter: 605300 training loss: 2.02762
Global Iter: 605300 training acc: 0.28125
Global Iter: 605400 training loss: 2.05981
Global Iter: 605400 training acc: 0.21875
Global Iter: 605500 training loss: 1.99158
Global Iter: 605500 training acc: 0.28125
Global Iter: 605600 training loss: 1.95237
Global Iter: 605600 training acc: 0.15625
Global Iter: 605700 training loss: 1.93717
Global Iter: 605700 training acc: 0.21875
Global Iter: 605800 training loss: 2.07913
Global Iter: 605800 training acc: 0.15625
Global Iter: 605900 training loss: 1.91307
Global Iter: 605900 training acc: 0.0625
Global Iter: 606000 training loss: 2.05682
Global Iter: 606000 training acc: 0.09375
Global Iter: 606100 training loss: 1.98607
Global Iter: 606100 training acc: 0.25
Global Iter: 606200 training loss: 1.91754
Global Iter: 606200 training acc: 0.3125
Global Iter: 606300 training loss: 1.92334
Global Iter: 606300 training acc: 0.28125
Global Iter: 606400 training loss: 2.00936
Global Iter: 606400 training acc: 0.125
Global Iter: 606500 training loss: 1.97287
Global Iter: 606500 training acc: 0.3125
Global Iter: 606600 training loss: 2.01589
Global Iter: 606600 training acc: 0.125
Global Iter: 606700 training loss: 2.03126
Global Iter: 606700 training acc: 0.3125
Global Iter: 606800 training loss: 2.13908
Global Iter: 606800 training acc: 0.09375
Global Iter: 606900 training loss: 1.99797
Global Iter: 606900 training acc: 0.15625
Global Iter: 607000 training loss: 2.01234
Global Iter: 607000 training acc: 0.21875
Global Iter: 607100 training loss: 2.04495
Global Iter: 607100 training acc: 0.3125
Global Iter: 607200 training loss2017-06-21 22:21:39.045673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-609875
: 2.02306
Global Iter: 607200 training acc: 0.15625
Global Iter: 607300 training loss: 1.96669
Global Iter: 607300 training acc: 0.25
Global Iter: 607400 training loss: 2.05294
Global Iter: 607400 training acc: 0.21875
Global Iter: 607500 training loss: 1.97094
Global Iter: 607500 training acc: 0.15625
Global Iter: 607600 training loss: 2.04115
Global Iter: 607600 training acc: 0.15625
Global Iter: 607700 training loss: 1.94905
Global Iter: 607700 training acc: 0.3125
Global Iter: 607800 training loss: 2.01827
Global Iter: 607800 training acc: 0.1875
Global Iter: 607900 training loss: 1.89238
Global Iter: 607900 training acc: 0.375
Global Iter: 608000 training loss: 1.92902
Global Iter: 608000 training acc: 0.09375
Global Iter: 608100 training loss: 1.96781
Global Iter: 608100 training acc: 0.40625
Global Iter: 608200 training loss: 1.98993
Global Iter: 608200 training acc: 0.125
Global Iter: 608300 training loss: 1.87514
Global Iter: 608300 training acc: 0.34375
Global Iter: 608400 training loss: 1.92169
Global Iter: 608400 training acc: 0.1875
Global Iter: 608500 training loss: 1.93569
Global Iter: 608500 training acc: 0.3125
Global Iter: 608600 training loss: 1.95294
Global Iter: 608600 training acc: 0.28125
Global Iter: 608700 training loss: 2.02562
Global Iter: 608700 training acc: 0.09375
Global Iter: 608800 training loss: 1.97207
Global Iter: 608800 training acc: 0.21875
Global Iter: 608900 training loss: 2.01842
Global Iter: 608900 training acc: 0.25
Global Iter: 609000 training loss: 2.08543
Global Iter: 609000 training acc: 0.0625
Global Iter: 609100 training loss: 2.07059
Global Iter: 609100 training acc: 0.1875
Global Iter: 609200 training loss: 2.01498
Global Iter: 609200 training acc: 0.21875
Global Iter: 609300 training loss: 1.93579
Global Iter: 609300 training acc: 0.25
Global Iter: 609400 training loss: 1.97278
Global Iter: 609400 training acc: 0.125
Global Iter: 609500 training loss: 2.02094
Global Iter: 609500 training acc: 0.15625
Global Iter: 609600 training loss: 1.94959
Global Iter: 609600 training acc: 0.3125
Global Iter: 609700 training loss: 1.9791
Global Iter: 609700 training acc: 0.28125
Global Iter: 609800 training loss: 1.95837
Global Iter: 609800 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-609875
Number of Patches: 209814
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-609875
Global Iter: 609900 training loss: 2.00804
Global Iter: 609900 training acc: 0.1875
Global Iter: 610000 training loss: 1.96325
Global Iter: 610000 training acc: 0.1875
Global Iter: 610100 training loss: 1.97028
Global Iter: 610100 training acc: 0.1875
Global Iter: 610200 training loss: 1.99664
Global Iter: 610200 training acc: 0.25
Global Iter: 610300 training loss: 1.96061
Global Iter: 610300 training acc: 0.21875
Global Iter: 610400 training loss: 2.03854
Global Iter: 610400 training acc: 0.09375
Global Iter: 610500 training loss: 2.0346
Global Iter: 610500 training acc: 0.34375
Global Iter: 610600 training loss: 1.91002
Global Iter: 610600 training acc: 0.25
Global Iter: 610700 training loss: 1.98956
Global Iter: 610700 training acc: 0.25
Global Iter: 610800 training loss: 2.02465
Global Iter: 610800 training acc: 0.0625
Global Iter: 610900 training loss: 2.00673
Global Iter: 610900 training acc: 0.15625
Global Iter: 611000 training loss: 2.00488
Global Iter: 611000 training acc: 0.25
Global Iter: 611100 training loss: 1.94127
Global Iter: 611100 training acc: 0.15625
Global Iter: 611200 training loss: 2.02856
Global Iter: 611200 training acc: 0.15625
Global Iter: 611300 training loss: 1.96577
Global Iter: 611300 training acc: 0.125
Global Iter: 611400 training loss: 1.94849
Global Iter: 611400 training acc: 0.25
Global Iter: 611500 training loss: 1.95287
Global Iter: 611500 training acc: 0.25
Global Iter: 611600 training loss: 2.10647
Global Iter: 611600 training acc: 0.21875
Global Iter: 611700 training loss: 1.99945
Global Iter: 611700 training acc: 0.1875
Global Iter: 611800 training loss: 2.0298
Global Iter: 611800 training acc: 0.21875
Global Iter: 611900 training loss: 1.96969
Global Iter: 611900 training acc: 0.21875
Global Iter: 612000 training loss: 2.02334
Global Iter: 612000 training acc: 0.09375
Global Iter: 612100 training loss: 1.95764
Global Iter: 612100 training acc: 0.09375
Global Iter: 612200 training loss: 2.02684
Global Iter: 612200 training acc: 0.21875
Global Iter: 612300 training loss: 1.88066
Global Iter: 612300 training acc: 0.3125
Global Iter: 612400 training loss: 2.11805
Global Iter: 612400 training acc: 0.125
Global Iter: 612500 training loss: 1.93979
Global Iter: 612500 training acc: 0.125
Global Iter: 612600 training loss: 2.01886
Global Iter: 612600 training acc: 0.09375
Global Iter: 612700 training loss: 1.92652
Global Iter: 612700 training acc: 0.1875
Global Iter: 612800 training loss: 2.0258
Global Iter: 612800 training acc: 0.1875
Global Iter: 612900 training loss: 1.96248
Global Iter: 612900 training acc: 0.15625
Global Iter: 613000 training loss: 2.0392
Global Iter: 613000 training acc: 0.15625
Global Iter: 613100 training loss: 2.04963
Global Iter: 613100 training acc: 0.15625
Global Iter: 613200 training loss: 2.02196
Global Iter: 613200 training acc: 0.125
Global Iter: 613300 training loss: 2.07263
Global Iter: 613300 training acc: 0.0625
Global Iter: 613400 training loss: 2.01143
Global Iter: 613400 training acc: 0.15625
Global Iter: 613500 training loss: 1.95964
Global Iter: 613500 training acc: 0.1875
Global Iter: 613600 training loss: 1.99423
Global Iter: 613600 training acc: 0.0625
Global Iter: 613700 training loss: 1.94023
Global Iter: 613700 training acc: 0.09375
Global Iter: 613800 training loss: 1.95315
Global Iter: 613800 training acc: 0.3125
Global Iter: 613900 training loss: 1.96851
Global Iter: 613900 training acc: 0.25
Global Iter: 614000 training loss: 1.94578
Global Iter: 614000 training acc: 0.15625
Global Iter: 614100 training loss: 1.92537
Global Iter: 614100 training acc: 0.125
Global Iter: 614200 training loss: 1.94255
Global Iter: 614200 training acc: 0.3125
Global Iter: 614300 training loss: 1.99059
Global Iter: 614300 training acc: 0.15625
Global Iter: 614400 training loss: 2.06867
Global Iter: 614400 training acc: 0.25
Global Iter: 614500 training loss: 2.09991
Global Iter: 614500 training acc: 0.125
Global Iter: 614600 training loss: 1.88805
Global Iter: 614600 training acc: 0.3125
Global Iter: 614700 training loss: 1.96927
Global Iter: 614700 training acc: 0.15625
Global Iter: 614800 training loss: 1.95097
Global Iter: 614800 training acc: 0.21875
Global Iter: 614900 training loss: 2.04567
Global Iter: 614900 training acc: 0.125
Global Iter: 615000 training loss: 1.96252
Global Iter: 615000 training acc: 0.1875
Global Iter: 615100 training loss: 1.95355
Global Iter: 615100 training acc: 0.21875
Global Iter: 615200 training loss: 2.04987
Global Iter: 615200 training acc: 0.15625
Global Iter: 615300 training loss: 1.93437
Global Iter: 615300 training acc: 0.25
Global Iter: 615400 training loss: 2.0447
Global Iter: 615400 training acc: 0.1875
Global Iter: 615500 training loss: 2.06073
Global Iter: 615500 training acc: 0.1875
Global Iter: 615600 training loss: 2.09631
Global Iter: 615600 training acc: 0.0625
Global Iter: 615700 training loss: 2.00506
Global Iter: 615700 training acc: 0.0625
Global Iter: 615800 training loss: 2.01231
Global Iter: 615800 training acc: 0.125
Global Iter: 615900 training loss: 2.08881
Global Iter: 615900 training acc: 0.0625
Global Iter: 616000 training loss: 1.99177
Global Iter: 616000 training acc: 0.125
Global Iter: 616100 training loss: 1.9483
Global Iter: 616100 training acc: 0.25
Global Iter: 616200 training loss: 1.96754
Global Iter: 616200 training acc: 0.21875
Global Iter: 616300 training loss: 2.0285
Global Iter: 616300 training acc: 0.125
Global Iter: 616400 training loss: 2.03998
Global Iter: 616400 training acc: 0.15625
Global Iter: 616500 training loss: 2.05902
Global Iter: 616500 training acc: 0.1875
Global Iter: 616600 training loss: 1.89124
Global Iter: 616600 training acc: 0.21875
Global Iter: 616700 training loss: 1.97394
Global Iter: 616700 training acc: 0.1875
Global Iter: 616800 training loss: 2.0251
Global Iter: 616800 training acc: 0.25
Global Iter: 616900 training loss: 1.94252
Global Iter: 616900 training acc: 0.15625
Global Iter: 617000 training loss: 2.04537
Global Iter: 617000 training acc: 0.125
Global Iter: 617100 training loss: 2.08731
Global Iter: 617100 training acc: 0.1875
Global Iter: 617200 training loss: 1.93555
Global Iter: 617200 training acc: 0.1875
Global Iter: 617300 training loss: 1.9497
Global Iter: 617300 training acc: 0.1875
Global Iter: 617400 training loss: 1.97831
Global Iter: 617400 training acc: 0.09375
Global Iter: 617500 training loss: 2.00347
Global Iter: 617500 training acc: 0.1875
Global Iter: 617600 training loss: 1.98381
Global Iter: 617600 training acc: 0.21875
Global Iter: 617700 training loss: 2.01044
Global Iter: 617700 training acc: 0.34375
Global Iter: 617800 training loss: 1.90456
Global Iter: 617800 training acc: 0.1875
Global Iter: 617900 training loss: 2.02103
Global Iter: 617900 training acc: 0.21875
Global Iter: 618000 training loss: 2.06555
Global Iter: 618000 training acc: 0.15625
Global Iter: 618100 training loss: 2.05058
Global Iter: 618100 training acc: 0.0625
Global Iter: 618200 training loss: 1.90768
Global Iter: 618200 training acc: 0.3125
Global Iter: 618300 training loss: 1.97854
Global Iter: 618300 training acc: 0.15625
Global Iter: 618400 training loss: 1.98267
Global Iter: 618400 training acc: 0.15625
Global Iter: 618500 training loss: 1.95225
Global Iter: 618500 training acc: 0.1875
Global Iter: 618600 training loss: 1.9283
Global Iter: 618600 training acc: 0.28125
Global Iter: 618700 training loss: 1.94975
Global Iter: 618700 training acc: 0.21875
Global Iter: 618800 training loss: 2.0699
Global Iter: 618800 training acc: 0.09375
Global Iter: 618900 training loss: 1.96949
Global Iter: 618900 training acc: 0.28125
Global Iter: 619000 training loss: 1.93925
Global Iter: 619000 training acc: 0.34375
Global Iter: 619100 training loss: 1.99099
Global Iter: 619100 training acc: 0.125
Global Iter: 619200 training loss: 2.10169
Global Iter: 619200 training acc: 0.09375
Global Iter: 619300 training loss: 2.03767
Global Iter: 619300 training acc: 0.1875
Global Iter: 619400 training loss: 1.9661
Global Iter: 619400 training acc: 0.1875
Global Iter: 619500 training loss: 2.07683
Global Iter: 619500 training acc: 0.09375
Global Iter: 619600 training loss: 1.93121
Global Iter: 619600 training acc: 0.1875
Global Iter: 619700 training loss: 2.01643
Global Iter: 619700 training acc: 0.21875
Global Iter: 619800 training loss: 1.82201
Global Iter: 619800 training acc: 0.34375
Global Iter: 619900 training loss: 2.09415
Global Iter: 619900 training acc: 0.03125
Global Iter: 620000 training loss: 1.96743
Global Iter: 620000 training acc: 0.125
Global Iter: 620100 training loss: 2.04014
Global Iter: 620100 training acc: 0.1875
Global Iter: 620200 training loss: 1.96866
Global Iter: 620200 training acc: 0.21875
Global Iter: 620300 training loss: 2.03535
Global Iter: 620300 training acc: 0.1875
Global Iter: 620400 training loss: 2.26944
Global Iter: 620400 training acc: 0.0625
Global Iter: 620500 training loss: 1.99245
Global Iter: 620500 training acc: 0.125
Global Iter: 620600 training loss: 1.91494
Global Iter: 620600 training acc: 0.34375
Global Iter: 620700 training loss: 1.97963
Global Iter: 620700 training acc: 0.125
Global Iter: 620800 training loss: 1.96981
Global Iter: 620800 training acc: 0.1875
Global Iter: 620900 training loss: 2.06274
Global Iter: 620900 training acc: 0.1875
Global Iter: 621000 training loss: 2.06885
Global Iter: 621000 training acc: 0.21875
Global Iter: 621100 training loss: 1.97058
Global Iter: 621100 training acc: 0.125
Global Iter: 621200 training loss: 1.93817
Global Iter: 621200 training acc: 0.1875
Global Iter: 621300 training loss: 1.96291
Global Iter: 621300 training acc: 0.09375
Global Iter: 621400 training loss: 2.01423
Global Iter: 621400 training acc: 0.1875
Global Iter: 621500 training loss: 2.02058
Global Iter: 621500 training acc: 0.18752017-06-21 22:44:28.280090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-622989

Global Iter: 621600 training loss: 2.04779
Global Iter: 621600 training acc: 0.15625
Global Iter: 621700 training loss: 2.08601
Global Iter: 621700 training acc: 0.15625
Global Iter: 621800 training loss: 1.89048
Global Iter: 621800 training acc: 0.25
Global Iter: 621900 training loss: 1.93964
Global Iter: 621900 training acc: 0.25
Global Iter: 622000 training loss: 1.98284
Global Iter: 622000 training acc: 0.125
Global Iter: 622100 training loss: 2.03515
Global Iter: 622100 training acc: 0.1875
Global Iter: 622200 training loss: 1.96998
Global Iter: 622200 training acc: 0.375
Global Iter: 622300 training loss: 2.05174
Global Iter: 622300 training acc: 0.21875
Global Iter: 622400 training loss: 1.94125
Global Iter: 622400 training acc: 0.25
Global Iter: 622500 training loss: 1.94028
Global Iter: 622500 training acc: 0.25
Global Iter: 622600 training loss: 1.97036
Global Iter: 622600 training acc: 0.21875
Global Iter: 622700 training loss: 2.00746
Global Iter: 622700 training acc: 0.25
Global Iter: 622800 training loss: 1.95131
Global Iter: 622800 training acc: 0.125
Global Iter: 622900 training loss: 2.16634
Global Iter: 622900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-622989
Number of Patches: 207716
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-622989
Global Iter: 623000 training loss: 1.98974
Global Iter: 623000 training acc: 0.125
Global Iter: 623100 training loss: 1.95623
Global Iter: 623100 training acc: 0.15625
Global Iter: 623200 training loss: 1.98554
Global Iter: 623200 training acc: 0.15625
Global Iter: 623300 training loss: 2.03529
Global Iter: 623300 training acc: 0.1875
Global Iter: 623400 training loss: 2.11287
Global Iter: 623400 training acc: 0.09375
Global Iter: 623500 training loss: 1.99348
Global Iter: 623500 training acc: 0.1875
Global Iter: 623600 training loss: 2.0706
Global Iter: 623600 training acc: 0.15625
Global Iter: 623700 training loss: 2.02553
Global Iter: 623700 training acc: 0.21875
Global Iter: 623800 training loss: 2.02071
Global Iter: 623800 training acc: 0.21875
Global Iter: 623900 training loss: 1.94936
Global Iter: 623900 training acc: 0.1875
Global Iter: 624000 training loss: 2.03055
Global Iter: 624000 training acc: 0.09375
Global Iter: 624100 training loss: 2.02551
Global Iter: 624100 training acc: 0.3125
Global Iter: 624200 training loss: 1.96672
Global Iter: 624200 training acc: 0.25
Global Iter: 624300 training loss: 1.96349
Global Iter: 624300 training acc: 0.1875
Global Iter: 624400 training loss: 1.98853
Global Iter: 624400 training acc: 0.1875
Global Iter: 624500 training loss: 2.01419
Global Iter: 624500 training acc: 0.15625
Global Iter: 624600 training loss: 1.88584
Global Iter: 624600 training acc: 0.3125
Global Iter: 624700 training loss: 2.06122
Global Iter: 624700 training acc: 0.1875
Global Iter: 624800 training loss: 1.99559
Global Iter: 624800 training acc: 0.1875
Global Iter: 624900 training loss: 2.04833
Global Iter: 624900 training acc: 0.1875
Global Iter: 625000 training loss: 1.91567
Global Iter: 625000 training acc: 0.25
Global Iter: 625100 training loss: 1.95966
Global Iter: 625100 training acc: 0.25
Global Iter: 625200 training loss: 1.95956
Global Iter: 625200 training acc: 0.1875
Global Iter: 625300 training loss: 1.87621
Global Iter: 625300 training acc: 0.3125
Global Iter: 625400 training loss: 2.11251
Global Iter: 625400 training acc: 0.125
Global Iter: 625500 training loss: 2.01226
Global Iter: 625500 training acc: 0.1875
Global Iter: 625600 training loss: 2.02883
Global Iter: 625600 training acc: 0.15625
Global Iter: 625700 training loss: 2.05614
Global Iter: 625700 training acc: 0.21875
Global Iter: 625800 training loss: 2.02503
Global Iter: 625800 training acc: 0.1875
Global Iter: 625900 training loss: 2.01947
Global Iter: 625900 training acc: 0.25
Global Iter: 626000 training loss: 1.95482
Global Iter: 626000 training acc: 0.21875
Global Iter: 626100 training loss: 2.02711
Global Iter: 626100 training acc: 0.15625
Global Iter: 626200 training loss: 2.00976
Global Iter: 626200 training acc: 0.0625
Global Iter: 626300 training loss: 2.01595
Global Iter: 626300 training acc: 0.21875
Global Iter: 626400 training loss: 1.85884
Global Iter: 626400 training acc: 0.25
Global Iter: 626500 training loss: 1.99974
Global Iter: 626500 training acc: 0.125
Global Iter: 626600 training loss: 2.00571
Global Iter: 626600 training acc: 0.125
Global Iter: 626700 training loss: 1.91956
Global Iter: 626700 training acc: 0.21875
Global Iter: 626800 training loss: 2.05924
Global Iter: 626800 training acc: 0.09375
Global Iter: 626900 training loss: 2.18679
Global Iter: 626900 training acc: 0.09375
Global Iter: 627000 training loss: 2.042
Global Iter: 627000 training acc: 0.09375
Global Iter: 627100 training loss: 1.9166
Global Iter: 627100 training acc: 0.25
Global Iter: 627200 training loss: 1.99437
Global Iter: 627200 training acc: 0.25
Global Iter: 627300 training loss: 1.86688
Global Iter: 627300 training acc: 0.21875
Global Iter: 627400 training loss: 2.05986
Global Iter: 627400 training acc: 0.28125
Global Iter: 627500 training loss: 1.8145
Global Iter: 627500 training acc: 0.375
Global Iter: 627600 training loss: 2.03336
Global Iter: 627600 training acc: 0.1875
Global Iter: 627700 training loss: 1.99553
Global Iter: 627700 training acc: 0.15625
Global Iter: 627800 training loss: 1.94806
Global Iter: 627800 training acc: 0.09375
Global Iter: 627900 training loss: 2.03033
Global Iter: 627900 training acc: 0.1875
Global Iter: 628000 training loss: 2.0361
Global Iter: 628000 training acc: 0.15625
Global Iter: 628100 training loss: 1.94934
Global Iter: 628100 training acc: 0.25
Global Iter: 628200 training loss: 2.00916
Global Iter: 628200 training acc: 0.15625
Global Iter: 628300 training loss: 2.01736
Global Iter: 628300 training acc: 0.09375
Global Iter: 628400 training loss: 1.9669
Global Iter: 628400 training acc: 0.09375
Global Iter: 628500 training loss: 2.09483
Global Iter: 628500 training acc: 0.25
Global Iter: 628600 training loss: 2.01446
Global Iter: 628600 training acc: 0.125
Global Iter: 628700 training loss: 1.96028
Global Iter: 628700 training acc: 0.15625
Global Iter: 628800 training loss: 1.96149
Global Iter: 628800 training acc: 0.25
Global Iter: 628900 training loss: 2.00898
Global Iter: 628900 training acc: 0.0625
Global Iter: 629000 training loss: 1.92865
Global Iter: 629000 training acc: 0.28125
Global Iter: 629100 training loss: 1.97778
Global Iter: 629100 training acc: 0.15625
Global Iter: 629200 training loss: 2.00969
Global Iter: 629200 training acc: 0.3125
Global Iter: 629300 training loss: 1.98761
Global Iter: 629300 training acc: 0.15625
Global Iter: 629400 training loss: 2.06501
Global Iter: 629400 training acc: 0.09375
Global Iter: 629500 training loss: 1.92129
Global Iter: 629500 training acc: 0.28125
Global Iter: 629600 training loss: 1.98911
Global Iter: 629600 training acc: 0.1875
Global Iter: 629700 training loss: 1.94155
Global Iter: 629700 training acc: 0.28125
Global Iter: 629800 training loss: 1.96157
Global Iter: 629800 training acc: 0.15625
Global Iter: 629900 training loss: 1.98162
Global Iter: 629900 training acc: 0.15625
Global Iter: 630000 training loss: 1.97698
Global Iter: 630000 training acc: 0.28125
Global Iter: 630100 training loss: 1.93388
Global Iter: 630100 training acc: 0.15625
Global Iter: 630200 training loss: 1.91813
Global Iter: 630200 training acc: 0.21875
Global Iter: 630300 training loss: 1.93302
Global Iter: 630300 training acc: 0.25
Global Iter: 630400 training loss: 1.91563
Global Iter: 630400 training acc: 0.15625
Global Iter: 630500 training loss: 1.93013
Global Iter: 630500 training acc: 0.1875
Global Iter: 630600 training loss: 1.90368
Global Iter: 630600 training acc: 0.40625
Global Iter: 630700 training loss: 1.98277
Global Iter: 630700 training acc: 0.1875
Global Iter: 630800 training loss: 1.9308
Global Iter: 630800 training acc: 0.1875
Global Iter: 630900 training loss: 1.90197
Global Iter: 630900 training acc: 0.1875
Global Iter: 631000 training loss: 1.92175
Global Iter: 631000 training acc: 0.125
Global Iter: 631100 training loss: 2.06659
Global Iter: 631100 training acc: 0.15625
Global Iter: 631200 training loss: 1.89218
Global Iter: 631200 training acc: 0.25
Global Iter: 631300 training loss: 1.90818
Global Iter: 631300 training acc: 0.28125
Global Iter: 631400 training loss: 2.11496
Global Iter: 631400 training acc: 0.0
Global Iter: 631500 training loss: 1.87692
Global Iter: 631500 training acc: 0.375
Global Iter: 631600 training loss: 1.92547
Global Iter: 631600 training acc: 0.25
Global Iter: 631700 training loss: 1.89232
Global Iter: 631700 training acc: 0.3125
Global Iter: 631800 training loss: 2.02868
Global Iter: 631800 training acc: 0.09375
Global Iter: 631900 training loss: 1.96064
Global Iter: 631900 training acc: 0.3125
Global Iter: 632000 training loss: 1.94164
Global Iter: 632000 training acc: 0.3125
Global Iter: 632100 training loss: 2.0201
Global Iter: 632100 training acc: 0.1875
Global Iter: 632200 training loss: 1.99748
Global Iter: 632200 training acc: 0.25
Global Iter: 632300 training loss: 1.9608
Global Iter: 632300 training acc: 0.1875
Global Iter: 632400 training loss: 1.92228
Global Iter: 632400 training acc: 0.21875
Global Iter: 632500 training loss: 1.94712
Global Iter: 632500 training acc: 0.28125
Global Iter: 632600 training loss: 1.91555
Global Iter: 632600 training acc: 0.1875
Global Iter: 632700 training loss: 1.96959
Global Iter: 632700 training acc: 0.1875
Global Iter: 632800 training loss: 2.07969
Global Iter: 632800 training acc: 0.125
Global Iter: 632900 training loss: 2.01588
Global Iter: 632900 training acc: 0.09375
Global Iter: 633000 training loss: 2.05755
Global Iter: 633000 training acc: 0.28125
Global Iter: 633100 training loss: 1.95349
Global Iter: 633100 training acc: 0.25
Global Iter: 633200 training loss: 2.00811
Global Iter: 633200 training acc: 0.21875
Global Iter: 633300 training loss: 1.91803
Global Iter: 633300 training acc: 0.15625
Global Iter: 633400 training loss: 2.09794
Global Iter: 633400 training acc: 0.09375
Global Iter: 633500 training loss: 1.99403
Global Iter: 633500 training acc: 0.25
Global Iter: 633600 training loss: 2.03309
Global Iter: 633600 training acc: 0.21875
Global Iter: 633700 training loss: 1.87607
Global Iter: 633700 training acc: 0.1875
Global Iter: 633800 training loss: 1.93974
Global Iter: 633800 training acc: 0.25
Global Iter: 633900 training loss: 2.03982
Global Iter: 633900 training acc: 0.25
Global Iter: 634000 training loss: 2.03668
Global Iter: 634000 training acc: 0.1875
Global Iter: 634100 training loss: 1.99175
Global Iter: 634100 training acc: 0.1875
Global Iter: 634200 training loss: 2.01042
Global Iter: 634200 training acc: 0.21875
Global Iter: 634300 training loss: 2.05975
Global Iter: 634300 training acc: 0.25
Global Iter: 634400 training loss: 1.95483
Global Iter: 634400 training acc: 0.125
Global Iter: 634500 training loss: 1.96862
Global Iter: 634500 training acc: 0.1875
Global Iter: 634600 training loss: 2.02942
Global Iter: 634600 training acc: 0.15625
Global Iter: 634700 training loss: 1.99389
Global Iter: 634700 training acc: 0.15625
Global Iter: 634800 training loss: 2.02098
Global Iter: 634800 training acc: 0.21875
Global Iter: 634900 training loss: 2.00228
Global Iter: 634900 training acc: 0.21875
Global Iter: 635000 training loss: 2.07106
Global Iter: 635000 training acc: 0.1875
Global Iter: 635100 training loss: 1.99351
Global Iter: 635100 training acc: 0.15625
Global Iter: 635200 training loss: 2.07396
Global Iter: 635200 training acc: 0.09375
Global Iter: 635300 training loss: 2.05917
Global Iter: 635300 training acc: 0.125
Global Iter: 635400 training loss: 1.99354
Global Iter: 635400 training acc: 0.15625
Global Iter: 635500 training loss: 2.04672
Global Iter: 635500 training acc: 0.15625
Global Iter: 635600 training loss: 2.00556
Global Iter: 635600 training acc: 0.15625
Global Iter: 635700 training loss: 2.05649
Global Iter: 635700 training acc: 0.09375
Global Iter: 635800 training loss: 2.09251
Global Iter: 635800 training acc: 0.09375
Global Iter: 635900 training loss: 1.968
Global Iter: 635900 t2017-06-21 23:07:13.226939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-635972
raining acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-635972
Number of Patches: 205639
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-635972
Global Iter: 636000 training loss: 1.97349
Global Iter: 636000 training acc: 0.1875
Global Iter: 636100 training loss: 1.89658
Global Iter: 636100 training acc: 0.3125
Global Iter: 636200 training loss: 1.93543
Global Iter: 636200 training acc: 0.25
Global Iter: 636300 training loss: 1.98551
Global Iter: 636300 training acc: 0.15625
Global Iter: 636400 training loss: 1.99101
Global Iter: 636400 training acc: 0.125
Global Iter: 636500 training loss: 1.94881
Global Iter: 636500 training acc: 0.125
Global Iter: 636600 training loss: 2.0878
Global Iter: 636600 training acc: 0.1875
Global Iter: 636700 training loss: 2.07416
Global Iter: 636700 training acc: 0.125
Global Iter: 636800 training loss: 1.98156
Global Iter: 636800 training acc: 0.125
Global Iter: 636900 training loss: 1.99343
Global Iter: 636900 training acc: 0.15625
Global Iter: 637000 training loss: 1.9555
Global Iter: 637000 training acc: 0.28125
Global Iter: 637100 training loss: 2.01872
Global Iter: 637100 training acc: 0.09375
Global Iter: 637200 training loss: 1.97485
Global Iter: 637200 training acc: 0.1875
Global Iter: 637300 training loss: 2.01212
Global Iter: 637300 training acc: 0.15625
Global Iter: 637400 training loss: 2.00554
Global Iter: 637400 training acc: 0.21875
Global Iter: 637500 training loss: 2.01135
Global Iter: 637500 training acc: 0.15625
Global Iter: 637600 training loss: 1.90705
Global Iter: 637600 training acc: 0.21875
Global Iter: 637700 training loss: 2.02862
Global Iter: 637700 training acc: 0.15625
Global Iter: 637800 training loss: 2.03842
Global Iter: 637800 training acc: 0.21875
Global Iter: 637900 training loss: 2.04014
Global Iter: 637900 training acc: 0.25
Global Iter: 638000 training loss: 2.03949
Global Iter: 638000 training acc: 0.1875
Global Iter: 638100 training loss: 1.9419
Global Iter: 638100 training acc: 0.3125
Global Iter: 638200 training loss: 1.94192
Global Iter: 638200 training acc: 0.1875
Global Iter: 638300 training loss: 1.99218
Global Iter: 638300 training acc: 0.28125
Global Iter: 638400 training loss: 2.02153
Global Iter: 638400 training acc: 0.1875
Global Iter: 638500 training loss: 1.88531
Global Iter: 638500 training acc: 0.21875
Global Iter: 638600 training loss: 1.96042
Global Iter: 638600 training acc: 0.09375
Global Iter: 638700 training loss: 1.88759
Global Iter: 638700 training acc: 0.125
Global Iter: 638800 training loss: 1.93404
Global Iter: 638800 training acc: 0.15625
Global Iter: 638900 training loss: 1.96677
Global Iter: 638900 training acc: 0.125
Global Iter: 639000 training loss: 2.02482
Global Iter: 639000 training acc: 0.28125
Global Iter: 639100 training loss: 2.10301
Global Iter: 639100 training acc: 0.09375
Global Iter: 639200 training loss: 1.94043
Global Iter: 639200 training acc: 0.21875
Global Iter: 639300 training loss: 1.93695
Global Iter: 639300 training acc: 0.21875
Global Iter: 639400 training loss: 2.15485
Global Iter: 639400 training acc: 0.125
Global Iter: 639500 training loss: 1.92293
Global Iter: 639500 training acc: 0.21875
Global Iter: 639600 training loss: 1.95518
Global Iter: 639600 training acc: 0.1875
Global Iter: 639700 training loss: 1.91331
Global Iter: 639700 training acc: 0.1875
Global Iter: 639800 training loss: 2.03865
Global Iter: 639800 training acc: 0.0625
Global Iter: 639900 training loss: 1.91091
Global Iter: 639900 training acc: 0.25
Global Iter: 640000 training loss: 2.03662
Global Iter: 640000 training acc: 0.21875
Global Iter: 640100 training loss: 2.02328
Global Iter: 640100 training acc: 0.25
Global Iter: 640200 training loss: 2.0286
Global Iter: 640200 training acc: 0.1875
Global Iter: 640300 training loss: 1.96574
Global Iter: 640300 training acc: 0.25
Global Iter: 640400 training loss: 2.0205
Global Iter: 640400 training acc: 0.1875
Global Iter: 640500 training loss: 1.98093
Global Iter: 640500 training acc: 0.1875
Global Iter: 640600 training loss: 2.03704
Global Iter: 640600 training acc: 0.09375
Global Iter: 640700 training loss: 1.94389
Global Iter: 640700 training acc: 0.15625
Global Iter: 640800 training loss: 1.99779
Global Iter: 640800 training acc: 0.09375
Global Iter: 640900 training loss: 1.87919
Global Iter: 640900 training acc: 0.4375
Global Iter: 641000 training loss: 2.02133
Global Iter: 641000 training acc: 0.09375
Global Iter: 641100 training loss: 1.96659
Global Iter: 641100 training acc: 0.28125
Global Iter: 641200 training loss: 1.95374
Global Iter: 641200 training acc: 0.15625
Global Iter: 641300 training loss: 2.0217
Global Iter: 641300 training acc: 0.125
Global Iter: 641400 training loss: 2.0073
Global Iter: 641400 training acc: 0.09375
Global Iter: 641500 training loss: 1.88433
Global Iter: 641500 training acc: 0.3125
Global Iter: 641600 training loss: 1.96568
Global Iter: 641600 training acc: 0.15625
Global Iter: 641700 training loss: 1.95536
Global Iter: 641700 training acc: 0.3125
Global Iter: 641800 training loss: 1.98172
Global Iter: 641800 training acc: 0.1875
Global Iter: 641900 training loss: 1.92419
Global Iter: 641900 training acc: 0.1875
Global Iter: 642000 training loss: 1.91513
Global Iter: 642000 training acc: 0.21875
Global Iter: 642100 training loss: 2.08464
Global Iter: 642100 training acc: 0.0625
Global Iter: 642200 training loss: 1.91891
Global Iter: 642200 training acc: 0.1875
Global Iter: 642300 training loss: 1.95339
Global Iter: 642300 training acc: 0.15625
Global Iter: 642400 training loss: 1.98238
Global Iter: 642400 training acc: 0.21875
Global Iter: 642500 training loss: 1.97346
Global Iter: 642500 training acc: 0.15625
Global Iter: 642600 training loss: 2.02076
Global Iter: 642600 training acc: 0.1875
Global Iter: 642700 training loss: 1.91359
Global Iter: 642700 training acc: 0.28125
Global Iter: 642800 training loss: 1.93259
Global Iter: 642800 training acc: 0.21875
Global Iter: 642900 training loss: 2.11174
Global Iter: 642900 training acc: 0.15625
Global Iter: 643000 training loss: 2.0259
Global Iter: 643000 training acc: 0.0625
Global Iter: 643100 training loss: 2.06445
Global Iter: 643100 training acc: 0.21875
Global Iter: 643200 training loss: 1.95035
Global Iter: 643200 training acc: 0.15625
Global Iter: 643300 training loss: 1.98716
Global Iter: 643300 training acc: 0.09375
Global Iter: 643400 training loss: 2.0401
Global Iter: 643400 training acc: 0.0625
Global Iter: 643500 training loss: 2.11922
Global Iter: 643500 training acc: 0.125
Global Iter: 643600 training loss: 2.04183
Global Iter: 643600 training acc: 0.25
Global Iter: 643700 training loss: 1.89934
Global Iter: 643700 training acc: 0.21875
Global Iter: 643800 training loss: 2.02134
Global Iter: 643800 training acc: 0.125
Global Iter: 643900 training loss: 1.99079
Global Iter: 643900 training acc: 0.28125
Global Iter: 644000 training loss: 2.02976
Global Iter: 644000 training acc: 0.21875
Global Iter: 644100 training loss: 2.03386
Global Iter: 644100 training acc: 0.25
Global Iter: 644200 training loss: 1.99913
Global Iter: 644200 training acc: 0.15625
Global Iter: 644300 training loss: 1.94668
Global Iter: 644300 training acc: 0.21875
Global Iter: 644400 training loss: 2.06211
Global Iter: 644400 training acc: 0.15625
Global Iter: 644500 training loss: 1.95067
Global Iter: 644500 training acc: 0.125
Global Iter: 644600 training loss: 1.98467
Global Iter: 644600 training acc: 0.21875
Global Iter: 644700 training loss: 1.9232
Global Iter: 644700 training acc: 0.28125
Global Iter: 644800 training loss: 2.13769
Global Iter: 644800 training acc: 0.09375
Global Iter: 644900 training loss: 1.9381
Global Iter: 644900 training acc: 0.15625
Global Iter: 645000 training loss: 1.89707
Global Iter: 645000 training acc: 0.125
Global Iter: 645100 training loss: 1.9552
Global Iter: 645100 training acc: 0.15625
Global Iter: 645200 training loss: 2.10708
Global Iter: 645200 training acc: 0.15625
Global Iter: 645300 training loss: 1.9786
Global Iter: 645300 training acc: 0.21875
Global Iter: 645400 training loss: 1.9442017-06-21 23:29:58.035029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-648825
93
Global Iter: 645400 training acc: 0.15625
Global Iter: 645500 training loss: 2.06584
Global Iter: 645500 training acc: 0.125
Global Iter: 645600 training loss: 2.01089
Global Iter: 645600 training acc: 0.1875
Global Iter: 645700 training loss: 2.03741
Global Iter: 645700 training acc: 0.28125
Global Iter: 645800 training loss: 1.93527
Global Iter: 645800 training acc: 0.25
Global Iter: 645900 training loss: 2.18429
Global Iter: 645900 training acc: 0.0625
Global Iter: 646000 training loss: 1.93762
Global Iter: 646000 training acc: 0.1875
Global Iter: 646100 training loss: 1.96418
Global Iter: 646100 training acc: 0.21875
Global Iter: 646200 training loss: 1.97523
Global Iter: 646200 training acc: 0.125
Global Iter: 646300 training loss: 1.95701
Global Iter: 646300 training acc: 0.1875
Global Iter: 646400 training loss: 1.9844
Global Iter: 646400 training acc: 0.3125
Global Iter: 646500 training loss: 2.04988
Global Iter: 646500 training acc: 0.15625
Global Iter: 646600 training loss: 2.11054
Global Iter: 646600 training acc: 0.1875
Global Iter: 646700 training loss: 1.97263
Global Iter: 646700 training acc: 0.1875
Global Iter: 646800 training loss: 1.97098
Global Iter: 646800 training acc: 0.1875
Global Iter: 646900 training loss: 2.02507
Global Iter: 646900 training acc: 0.125
Global Iter: 647000 training loss: 2.02825
Global Iter: 647000 training acc: 0.21875
Global Iter: 647100 training loss: 1.95051
Global Iter: 647100 training acc: 0.25
Global Iter: 647200 training loss: 1.98023
Global Iter: 647200 training acc: 0.25
Global Iter: 647300 training loss: 1.97702
Global Iter: 647300 training acc: 0.09375
Global Iter: 647400 training loss: 1.97081
Global Iter: 647400 training acc: 0.09375
Global Iter: 647500 training loss: 1.99588
Global Iter: 647500 training acc: 0.125
Global Iter: 647600 training loss: 2.04928
Global Iter: 647600 training acc: 0.0625
Global Iter: 647700 training loss: 1.94449
Global Iter: 647700 training acc: 0.28125
Global Iter: 647800 training loss: 2.05828
Global Iter: 647800 training acc: 0.1875
Global Iter: 647900 training loss: 2.03666
Global Iter: 647900 training acc: 0.15625
Global Iter: 648000 training loss: 2.01498
Global Iter: 648000 training acc: 0.3125
Global Iter: 648100 training loss: 2.09909
Global Iter: 648100 training acc: 0.03125
Global Iter: 648200 training loss: 1.89036
Global Iter: 648200 training acc: 0.25
Global Iter: 648300 training loss: 2.02013
Global Iter: 648300 training acc: 0.25
Global Iter: 648400 training loss: 1.862
Global Iter: 648400 training acc: 0.28125
Global Iter: 648500 training loss: 1.93249
Global Iter: 648500 training acc: 0.125
Global Iter: 648600 training loss: 1.98716
Global Iter: 648600 training acc: 0.28125
Global Iter: 648700 training loss: 1.89062
Global Iter: 648700 training acc: 0.375
Global Iter: 648800 training loss: 2.01426
Global Iter: 648800 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-648825
Number of Patches: 203583
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-648825
Global Iter: 648900 training loss: 2.0349
Global Iter: 648900 training acc: 0.25
Global Iter: 649000 training loss: 2.12643
Global Iter: 649000 training acc: 0.125
Global Iter: 649100 training loss: 2.05109
Global Iter: 649100 training acc: 0.09375
Global Iter: 649200 training loss: 1.97004
Global Iter: 649200 training acc: 0.21875
Global Iter: 649300 training loss: 1.9945
Global Iter: 649300 training acc: 0.21875
Global Iter: 649400 training loss: 1.98969
Global Iter: 649400 training acc: 0.15625
Global Iter: 649500 training loss: 2.06832
Global Iter: 649500 training acc: 0.125
Global Iter: 649600 training loss: 2.00125
Global Iter: 649600 training acc: 0.0625
Global Iter: 649700 training loss: 2.05182
Global Iter: 649700 training acc: 0.15625
Global Iter: 649800 training loss: 2.02928
Global Iter: 649800 training acc: 0.125
Global Iter: 649900 training loss: 1.99491
Global Iter: 649900 training acc: 0.1875
Global Iter: 650000 training loss: 1.97532
Global Iter: 650000 training acc: 0.125
Global Iter: 650100 training loss: 1.97327
Global Iter: 650100 training acc: 0.21875
Global Iter: 650200 training loss: 1.92138
Global Iter: 650200 training acc: 0.3125
Global Iter: 650300 training loss: 1.99671
Global Iter: 650300 training acc: 0.1875
Global Iter: 650400 training loss: 2.01659
Global Iter: 650400 training acc: 0.15625
Global Iter: 650500 training loss: 1.99349
Global Iter: 650500 training acc: 0.3125
Global Iter: 650600 training loss: 1.97368
Global Iter: 650600 training acc: 0.25
Global Iter: 650700 training loss: 1.96546
Global Iter: 650700 training acc: 0.21875
Global Iter: 650800 training loss: 1.96692
Global Iter: 650800 training acc: 0.21875
Global Iter: 650900 training loss: 1.93806
Global Iter: 650900 training acc: 0.21875
Global Iter: 651000 training loss: 2.03011
Global Iter: 651000 training acc: 0.21875
Global Iter: 651100 training loss: 2.02242
Global Iter: 651100 training acc: 0.28125
Global Iter: 651200 training loss: 1.95299
Global Iter: 651200 training acc: 0.3125
Global Iter: 651300 training loss: 1.87664
Global Iter: 651300 training acc: 0.28125
Global Iter: 651400 training loss: 1.91561
Global Iter: 651400 training acc: 0.21875
Global Iter: 651500 training loss: 1.96589
Global Iter: 651500 training acc: 0.0625
Global Iter: 651600 training loss: 1.96396
Global Iter: 651600 training acc: 0.21875
Global Iter: 651700 training loss: 2.21539
Global Iter: 651700 training acc: 0.0625
Global Iter: 651800 training loss: 2.07085
Global Iter: 651800 training acc: 0.125
Global Iter: 651900 training loss: 2.1108
Global Iter: 651900 training acc: 0.21875
Global Iter: 652000 training loss: 1.8773
Global Iter: 652000 training acc: 0.21875
Global Iter: 652100 training loss: 2.00214
Global Iter: 652100 training acc: 0.21875
Global Iter: 652200 training loss: 1.97806
Global Iter: 652200 training acc: 0.0625
Global Iter: 652300 training loss: 1.95278
Global Iter: 652300 training acc: 0.0625
Global Iter: 652400 training loss: 1.98385
Global Iter: 652400 training acc: 0.15625
Global Iter: 652500 training loss: 1.92639
Global Iter: 652500 training acc: 0.25
Global Iter: 652600 training loss: 1.96716
Global Iter: 652600 training acc: 0.25
Global Iter: 652700 training loss: 1.99971
Global Iter: 652700 training acc: 0.21875
Global Iter: 652800 training loss: 2.03889
Global Iter: 652800 training acc: 0.125
Global Iter: 652900 training loss: 2.00419
Global Iter: 652900 training acc: 0.1875
Global Iter: 653000 training loss: 2.06882
Global Iter: 653000 training acc: 0.15625
Global Iter: 653100 training loss: 1.97318
Global Iter: 653100 training acc: 0.28125
Global Iter: 653200 training loss: 2.00717
Global Iter: 653200 training acc: 0.125
Global Iter: 653300 training loss: 1.90278
Global Iter: 653300 training acc: 0.28125
Global Iter: 653400 training loss: 1.96097
Global Iter: 653400 training acc: 0.15625
Global Iter: 653500 training loss: 2.07467
Global Iter: 653500 training acc: 0.0625
Global Iter: 653600 training loss: 2.04604
Global Iter: 653600 training acc: 0.03125
Global Iter: 653700 training loss: 1.88443
Global Iter: 653700 training acc: 0.28125
Global Iter: 653800 training loss: 1.87706
Global Iter: 653800 training acc: 0.21875
Global Iter: 653900 training loss: 2.00418
Global Iter: 653900 training acc: 0.25
Global Iter: 654000 training loss: 1.97162
Global Iter: 654000 training acc: 0.25
Global Iter: 654100 training loss: 2.00654
Global Iter: 654100 training acc: 0.1875
Global Iter: 654200 training loss: 2.00635
Global Iter: 654200 training acc: 0.125
Global Iter: 654300 training loss: 2.1788
Global Iter: 654300 training acc: 0.15625
Global Iter: 654400 training loss: 1.92504
Global Iter: 654400 training acc: 0.3125
Global Iter: 654500 training loss: 1.98368
Global Iter: 654500 training acc: 0.21875
Global Iter: 654600 training loss: 1.97908
Global Iter: 654600 training acc: 0.1875
Global Iter: 654700 training loss: 1.98599
Global Iter: 654700 training acc: 0.21875
Global Iter: 654800 training loss: 1.9605
Global Iter: 654800 training acc: 0.1875
Global Iter: 654900 training loss: 2.07725
Global Iter: 654900 training acc: 0.28125
Global Iter: 655000 training loss: 1.97922
Global Iter: 655000 training acc: 0.1875
Global Iter: 655100 training loss: 1.99963
Global Iter: 655100 training acc: 0.09375
Global Iter: 655200 training loss: 2.01503
Global Iter: 655200 training acc: 0.1875
Global Iter: 655300 training loss: 1.95619
Global Iter: 655300 training acc: 0.15625
Global Iter: 655400 training loss: 1.98133
Global Iter: 655400 training acc: 0.09375
Global Iter: 655500 training loss: 1.9691
Global Iter: 655500 training acc: 0.1875
Global Iter: 655600 training loss: 2.08085
Global Iter: 655600 training acc: 0.15625
Global Iter: 655700 training loss: 1.94581
Global Iter: 655700 training acc: 0.125
Global Iter: 655800 training loss: 2.12618
Global Iter: 655800 training acc: 0.09375
Global Iter: 655900 training loss: 2.02201
Global Iter: 655900 training acc: 0.1875
Global Iter: 656000 training loss: 1.99203
Global Iter: 656000 training acc: 0.15625
Global Iter: 656100 training loss: 2.18586
Global Iter: 656100 training acc: 0.125
Global Iter: 656200 training loss: 1.93144
Global Iter: 656200 training acc: 0.21875
Global Iter: 656300 training loss: 1.93912
Global Iter: 656300 training acc: 0.34375
Global Iter: 656400 training loss: 1.9861
Global Iter: 656400 training acc: 0.1875
Global Iter: 656500 training loss: 2.00251
Global Iter: 656500 training acc: 0.28125
Global Iter: 656600 training loss: 2.03815
Global Iter: 656600 training acc: 0.21875
Global Iter: 656700 training loss: 1.88126
Global Iter: 656700 training acc: 0.25
Global Iter: 656800 training loss: 1.9886
Global Iter: 656800 training acc: 0.21875
Global Iter: 656900 training loss: 1.9236
Global Iter: 656900 training acc: 0.28125
Global Iter: 657000 training loss: 1.8906
Global Iter: 657000 training acc: 0.1875
Global Iter: 657100 training loss: 1.9511
Global Iter: 657100 training acc: 0.25
Global Iter: 657200 training loss: 2.05406
Global Iter: 657200 training acc: 0.125
Global Iter: 657300 training loss: 1.9736
Global Iter: 657300 training acc: 0.125
Global Iter: 657400 training loss: 2.00767
Global Iter: 657400 training acc: 0.09375
Global Iter: 657500 training loss: 2.01448
Global Iter: 657500 training acc: 0.15625
Global Iter: 657600 training loss: 1.95937
Global Iter: 657600 training acc: 0.15625
Global Iter: 657700 training loss: 2.00824
Global Iter: 657700 training acc: 0.21875
Global Iter: 657800 training loss: 1.92847
Global Iter: 657800 training acc: 0.1875
Global Iter: 657900 training loss: 2.04988
Global Iter: 657900 training acc: 0.09375
Global Iter: 658000 training loss: 2.01863
Global Iter: 658000 training acc: 0.125
Global Iter: 658100 training loss: 1.93949
Global Iter: 658100 training acc: 0.28125
Global Iter: 658200 training loss: 1.91937
Global Iter: 658200 training acc: 0.375
Global Iter: 658300 training loss: 1.91939
Global Iter: 658300 training acc: 0.21875
Global Iter: 658400 training loss: 1.91883
Global Iter: 658400 training acc: 0.3125
Global Iter: 658500 training loss: 1.95183
Global Iter: 658500 training acc: 0.25
Global Iter: 658600 training loss: 1.9342
Global Iter: 658600 training acc: 0.21875
Global Iter: 658700 training loss: 1.97899
Global Iter: 658700 training acc: 0.1875
Global Iter: 658800 training loss: 2.02037
Global Iter: 658800 training acc: 0.21875
Global Iter: 658900 training loss: 1.94657
Global Iter: 658900 training acc: 0.21875
Global Iter: 659000 training loss: 1.82651
Global Iter: 659000 training acc: 0.34375
Global Iter: 659100 training loss: 1.98153
Global Iter: 659100 training acc: 0.1875
Global Iter: 659200 training loss: 1.97981
Global Iter: 659200 training acc: 0.15625
Global Iter: 659300 training loss: 2.14525
Global Iter: 659300 training acc: 0.03125
Global Iter: 659400 training loss: 1.8431
Global Iter: 659400 training acc: 0.40625
Global Iter: 659500 training loss: 1.90181
Global Iter: 659500 training acc: 0.28125
Global Iter: 659600 training loss: 1.989
Global Iter: 659600 training acc: 0.1875
Global Iter: 659700 training loss: 1.91074
Global Iter: 659700 training acc: 0.3125
Global2017-06-21 23:52:14.839188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-661549
 Iter: 659800 training loss: 1.90132
Global Iter: 659800 training acc: 0.28125
Global Iter: 659900 training loss: 2.05713
Global Iter: 659900 training acc: 0.21875
Global Iter: 660000 training loss: 2.0494
Global Iter: 660000 training acc: 0.28125
Global Iter: 660100 training loss: 2.01496
Global Iter: 660100 training acc: 0.125
Global Iter: 660200 training loss: 1.99169
Global Iter: 660200 training acc: 0.125
Global Iter: 660300 training loss: 1.94582
Global Iter: 660300 training acc: 0.25
Global Iter: 660400 training loss: 2.05449
Global Iter: 660400 training acc: 0.15625
Global Iter: 660500 training loss: 2.0032
Global Iter: 660500 training acc: 0.21875
Global Iter: 660600 training loss: 1.94889
Global Iter: 660600 training acc: 0.09375
Global Iter: 660700 training loss: 2.04853
Global Iter: 660700 training acc: 0.15625
Global Iter: 660800 training loss: 1.99646
Global Iter: 660800 training acc: 0.21875
Global Iter: 660900 training loss: 1.94124
Global Iter: 660900 training acc: 0.25
Global Iter: 661000 training loss: 1.9446
Global Iter: 661000 training acc: 0.3125
Global Iter: 661100 training loss: 2.04451
Global Iter: 661100 training acc: 0.1875
Global Iter: 661200 training loss: 1.98275
Global Iter: 661200 training acc: 0.21875
Global Iter: 661300 training loss: 1.98988
Global Iter: 661300 training acc: 0.28125
Global Iter: 661400 training loss: 2.04458
Global Iter: 661400 training acc: 0.125
Global Iter: 661500 training loss: 1.95549
Global Iter: 661500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-661549
Number of Patches: 201548
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-661549
Global Iter: 661600 training loss: 2.03532
Global Iter: 661600 training acc: 0.15625
Global Iter: 661700 training loss: 1.97236
Global Iter: 661700 training acc: 0.25
Global Iter: 661800 training loss: 1.9411
Global Iter: 661800 training acc: 0.1875
Global Iter: 661900 training loss: 2.00136
Global Iter: 661900 training acc: 0.15625
Global Iter: 662000 training loss: 1.95264
Global Iter: 662000 training acc: 0.15625
Global Iter: 662100 training loss: 1.94831
Global Iter: 662100 training acc: 0.125
Global Iter: 662200 training loss: 1.92578
Global Iter: 662200 training acc: 0.28125
Global Iter: 662300 training loss: 2.11191
Global Iter: 662300 training acc: 0.0625
Global Iter: 662400 training loss: 1.95186
Global Iter: 662400 training acc: 0.21875
Global Iter: 662500 training loss: 1.97884
Global Iter: 662500 training acc: 0.28125
Global Iter: 662600 training loss: 1.9218
Global Iter: 662600 training acc: 0.21875
Global Iter: 662700 training loss: 1.97106
Global Iter: 662700 training acc: 0.1875
Global Iter: 662800 training loss: 2.0085
Global Iter: 662800 training acc: 0.125
Global Iter: 662900 training loss: 1.99634
Global Iter: 662900 training acc: 0.15625
Global Iter: 663000 training loss: 1.90503
Global Iter: 663000 training acc: 0.28125
Global Iter: 663100 training loss: 2.05908
Global Iter: 663100 training acc: 0.1875
Global Iter: 663200 training loss: 2.04985
Global Iter: 663200 training acc: 0.125
Global Iter: 663300 training loss: 1.87847
Global Iter: 663300 training acc: 0.28125
Global Iter: 663400 training loss: 1.90617
Global Iter: 663400 training acc: 0.21875
Global Iter: 663500 training loss: 1.94606
Global Iter: 663500 training acc: 0.21875
Global Iter: 663600 training loss: 2.07076
Global Iter: 663600 training acc: 0.1875
Global Iter: 663700 training loss: 2.03243
Global Iter: 663700 training acc: 0.125
Global Iter: 663800 training loss: 1.97776
Global Iter: 663800 training acc: 0.0625
Global Iter: 663900 training loss: 1.95965
Global Iter: 663900 training acc: 0.25
Global Iter: 664000 training loss: 2.01356
Global Iter: 664000 training acc: 0.15625
Global Iter: 664100 training loss: 1.97007
Global Iter: 664100 training acc: 0.1875
Global Iter: 664200 training loss: 1.89689
Global Iter: 664200 training acc: 0.1875
Global Iter: 664300 training loss: 1.99168
Global Iter: 664300 training acc: 0.1875
Global Iter: 664400 training loss: 2.00352
Global Iter: 664400 training acc: 0.25
Global Iter: 664500 training loss: 2.08829
Global Iter: 664500 training acc: 0.21875
Global Iter: 664600 training loss: 2.00764
Global Iter: 664600 training acc: 0.0625
Global Iter: 664700 training loss: 2.01237
Global Iter: 664700 training acc: 0.1875
Global Iter: 664800 training loss: 1.94657
Global Iter: 664800 training acc: 0.15625
Global Iter: 664900 training loss: 2.08458
Global Iter: 664900 training acc: 0.125
Global Iter: 665000 training loss: 1.89824
Global Iter: 665000 training acc: 0.28125
Global Iter: 665100 training loss: 1.94821
Global Iter: 665100 training acc: 0.1875
Global Iter: 665200 training loss: 2.03163
Global Iter: 665200 training acc: 0.1875
Global Iter: 665300 training loss: 1.93314
Global Iter: 665300 training acc: 0.1875
Global Iter: 665400 training loss: 2.03044
Global Iter: 665400 training acc: 0.21875
Global Iter: 665500 training loss: 2.02246
Global Iter: 665500 training acc: 0.09375
Global Iter: 665600 training loss: 1.97267
Global Iter: 665600 training acc: 0.1875
Global Iter: 665700 training loss: 1.95507
Global Iter: 665700 training acc: 0.0625
Global Iter: 665800 training loss: 2.01005
Global Iter: 665800 training acc: 0.21875
Global Iter: 665900 training loss: 2.06284
Global Iter: 665900 training acc: 0.125
Global Iter: 666000 training loss: 1.97378
Global Iter: 666000 training acc: 0.28125
Global Iter: 666100 training loss: 2.0445
Global Iter: 666100 training acc: 0.21875
Global Iter: 666200 training loss: 1.95767
Global Iter: 666200 training acc: 0.21875
Global Iter: 666300 training loss: 1.90579
Global Iter: 666300 training acc: 0.21875
Global Iter: 666400 training loss: 2.0057
Global Iter: 666400 training acc: 0.0625
Global Iter: 666500 training loss: 1.94091
Global Iter: 666500 training acc: 0.0625
Global Iter: 666600 training loss: 1.95704
Global Iter: 666600 training acc: 0.21875
Global Iter: 666700 training loss: 2.00743
Global Iter: 666700 training acc: 0.25
Global Iter: 666800 training loss: 1.89317
Global Iter: 666800 training acc: 0.3125
Global Iter: 666900 training loss: 2.01267
Global Iter: 666900 training acc: 0.125
Global Iter: 667000 training loss: 1.97718
Global Iter: 667000 training acc: 0.25
Global Iter: 667100 training loss: 2.00018
Global Iter: 667100 training acc: 0.28125
Global Iter: 667200 training loss: 1.98213
Global Iter: 667200 training acc: 0.15625
Global Iter: 667300 training loss: 1.96472
Global Iter: 667300 training acc: 0.1875
Global Iter: 667400 training loss: 1.95123
Global Iter: 667400 training acc: 0.21875
Global Iter: 667500 training loss: 1.97259
Global Iter: 667500 training acc: 0.15625
Global Iter: 667600 training loss: 1.95293
Global Iter: 667600 training acc: 0.1875
Global Iter: 667700 training loss: 1.94852
Global Iter: 667700 training acc: 0.21875
Global Iter: 667800 training loss: 1.92755
Global Iter: 667800 training acc: 0.21875
Global Iter: 667900 training loss: 1.99828
Global Iter: 667900 training acc: 0.25
Global Iter: 668000 training loss: 2.00254
Global Iter: 668000 training acc: 0.28125
Global Iter: 668100 training loss: 2.03207
Global Iter: 668100 training acc: 0.15625
Global Iter: 668200 training loss: 1.95144
Global Iter: 668200 training acc: 0.1875
Global Iter: 668300 training loss: 1.99675
Global Iter: 668300 training acc: 0.0625
Global Iter: 668400 training loss: 2.04105
Global Iter: 668400 training acc: 0.21875
Global Iter: 668500 training loss: 1.88055
Global Iter: 668500 training acc: 0.15625
Global Iter: 668600 training loss: 1.94049
Global Iter: 668600 training acc: 0.34375
Global Iter: 668700 training loss: 2.05731
Global Iter: 668700 training acc: 0.1875
Global Iter: 668800 training loss: 2.00067
Global Iter: 668800 training acc: 0.15625
Global Iter: 668900 training loss: 2.01605
Global Iter: 668900 training acc: 0.125
Global Iter: 669000 training loss: 2.01879
Global Iter: 669000 training acc: 0.25
Global Iter: 669100 training loss: 2.14485
Global Iter: 669100 training acc: 0.15625
Global Iter: 669200 training loss: 1.94988
Global Iter: 669200 training acc: 0.34375
Global Iter: 669300 training loss: 1.97078
Global Iter: 669300 training acc: 0.125
Global Iter: 669400 training loss: 1.94537
Global Iter: 669400 training acc: 0.1875
Global Iter: 669500 training loss: 2.02773
Global Iter: 669500 training acc: 0.1875
Global Iter: 669600 training loss: 2.01649
Global Iter: 669600 training acc: 0.1875
Global Iter: 669700 training loss: 2.0633
Global Iter: 669700 training acc: 0.25
Global Iter: 669800 training loss: 2.05614
Global Iter: 669800 training acc: 0.1875
Global Iter: 669900 training loss: 1.97041
Global Iter: 669900 training acc: 0.25
Global Iter: 670000 training loss: 2.0721
Global Iter: 670000 training acc: 0.125
Global Iter: 670100 training loss: 1.99809
Global Iter: 670100 training acc: 0.125
Global Iter: 670200 training loss: 1.96953
Global Iter: 670200 training acc: 0.125
Global Iter: 670300 training loss: 1.93214
Global Iter: 670300 training acc: 0.15625
Global Iter: 670400 training loss: 2.16095
Global Iter: 670400 training acc: 0.09375
Global Iter: 670500 training loss: 1.96351
Global Iter: 670500 training acc: 0.28125
Global Iter: 670600 training loss: 1.91083
Global Iter: 670600 training acc: 0.15625
Global Iter: 670700 training loss: 1.91062
Global Iter: 670700 training acc: 0.25
Global Iter: 670800 training loss: 2.10834
Global Iter: 670800 training acc: 0.15625
Global Iter: 670900 training loss: 1.89434
Global Iter: 670900 training acc: 0.1875
Global Iter: 671000 training loss: 1.92751
Global Iter: 671000 training acc: 0.375
Global Iter: 671100 training loss: 1.95217
Global Iter: 671100 training acc: 0.21875
Global Iter: 671200 training loss: 2.05468
Global Iter: 671200 training acc: 0.09375
Global Iter: 671300 training loss: 1.94397
Global Iter: 671300 training acc: 0.15625
Global Iter: 671400 training loss: 2.0226
Global Iter: 671400 training acc: 0.25
Global Iter: 671500 training loss: 2.05258
Global Iter: 671500 training acc: 0.125
Global Iter: 671600 training loss: 1.95159
Global Iter: 671600 training acc: 0.28125
Global Iter: 671700 training loss: 2.04045
Global Iter: 671700 training acc: 0.15625
Global Iter: 671800 training loss: 1.86034
Global Iter: 671800 training acc: 0.3125
Global Iter: 671900 training loss: 2.02006
Global Iter: 671900 training acc: 0.0625
Global Iter: 672000 training loss: 1.98884
Global Iter: 672000 training acc: 0.125
Global Iter: 672100 training loss: 2.0021
Global Iter: 672100 training acc: 0.1875
Global Iter: 672200 training loss: 2.03016
Global Iter: 672200 training acc: 0.25
Global Iter: 672300 training loss: 2.04314
Global Iter: 672300 training acc: 0.21875
Global Iter: 672400 training loss: 1.90348
Global Iter: 672400 training acc: 0.3125
Global Iter: 672500 training loss: 1.97613
Global Iter: 672500 training acc: 0.0
Global Iter: 672600 training loss: 1.97095
Global Iter: 672600 training acc: 0.28125
Global Iter: 672700 training loss: 1.94185
Global Iter: 672700 training acc: 0.1875
Global Iter: 672800 training loss: 1.9973
Global Iter: 672800 training acc: 0.15625
Global Iter: 672900 training loss: 2.11622
Global Iter: 672900 training acc: 0.1875
Global Iter: 673000 training loss: 2.08619
Global Iter: 673000 training acc: 0.125
Global Iter: 673100 training loss: 1.94769
Global Iter: 673100 training acc: 0.25
Global Iter: 673200 training loss: 1.91667
Global Iter: 673200 training acc: 0.21875
Global Iter: 673300 training loss: 1.90717
Global Iter: 673300 training acc: 0.1875
Global Iter: 673400 training loss: 1.96513
Global Iter: 673400 training acc: 0.1875
Global Iter: 673500 training loss: 2.09754
Global Iter: 673500 training acc: 0.1875
Global Iter: 673600 training loss: 2.0426
Global Iter: 673600 training acc: 0.1875
Global Iter: 673700 training loss: 1.96959
Global Iter: 673700 training acc: 0.125
Global Iter: 673800 training loss: 1.93066
Global Iter: 673800 training acc: 0.09375
Global Iter: 673900 training loss: 2.01717
Global Iter: 673900 training acc: 0.1875
Global Iter: 674000 training loss: 2.0977
Global Iter: 674000 training acc: 0.1875
Global Iter: 674100 training loss: 2.0216
Global Iter: 674100 t2017-06-22 00:14:28.616017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-674146
raining acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-674146
Number of Patches: 199533
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-674146
Global Iter: 674200 training loss: 2.06438
Global Iter: 674200 training acc: 0.15625
Global Iter: 674300 training loss: 1.92771
Global Iter: 674300 training acc: 0.375
Global Iter: 674400 training loss: 1.89712
Global Iter: 674400 training acc: 0.25
Global Iter: 674500 training loss: 1.97118
Global Iter: 674500 training acc: 0.15625
Global Iter: 674600 training loss: 2.03971
Global Iter: 674600 training acc: 0.15625
Global Iter: 674700 training loss: 1.99632
Global Iter: 674700 training acc: 0.21875
Global Iter: 674800 training loss: 2.05836
Global Iter: 674800 training acc: 0.25
Global Iter: 674900 training loss: 1.93454
Global Iter: 674900 training acc: 0.25
Global Iter: 675000 training loss: 2.13468
Global Iter: 675000 training acc: 0.1875
Global Iter: 675100 training loss: 2.0094
Global Iter: 675100 training acc: 0.1875
Global Iter: 675200 training loss: 1.94891
Global Iter: 675200 training acc: 0.1875
Global Iter: 675300 training loss: 1.88958
Global Iter: 675300 training acc: 0.1875
Global Iter: 675400 training loss: 2.06723
Global Iter: 675400 training acc: 0.15625
Global Iter: 675500 training loss: 2.04145
Global Iter: 675500 training acc: 0.15625
Global Iter: 675600 training loss: 1.91534
Global Iter: 675600 training acc: 0.34375
Global Iter: 675700 training loss: 1.96292
Global Iter: 675700 training acc: 0.15625
Global Iter: 675800 training loss: 2.03447
Global Iter: 675800 training acc: 0.1875
Global Iter: 675900 training loss: 1.94865
Global Iter: 675900 training acc: 0.15625
Global Iter: 676000 training loss: 1.90986
Global Iter: 676000 training acc: 0.25
Global Iter: 676100 training loss: 2.18117
Global Iter: 676100 training acc: 0.15625
Global Iter: 676200 training loss: 1.97536
Global Iter: 676200 training acc: 0.21875
Global Iter: 676300 training loss: 1.98769
Global Iter: 676300 training acc: 0.28125
Global Iter: 676400 training loss: 1.97308
Global Iter: 676400 training acc: 0.25
Global Iter: 676500 training loss: 2.09302
Global Iter: 676500 training acc: 0.1875
Global Iter: 676600 training loss: 1.91405
Global Iter: 676600 training acc: 0.1875
Global Iter: 676700 training loss: 2.08421
Global Iter: 676700 training acc: 0.09375
Global Iter: 676800 training loss: 2.04395
Global Iter: 676800 training acc: 0.25
Global Iter: 676900 training loss: 2.06618
Global Iter: 676900 training acc: 0.15625
Global Iter: 677000 training loss: 1.97531
Global Iter: 677000 training acc: 0.1875
Global Iter: 677100 training loss: 1.94711
Global Iter: 677100 training acc: 0.25
Global Iter: 677200 training loss: 1.95449
Global Iter: 677200 training acc: 0.28125
Global Iter: 677300 training loss: 1.94607
Global Iter: 677300 training acc: 0.125
Global Iter: 677400 training loss: 2.0059
Global Iter: 677400 training acc: 0.03125
Global Iter: 677500 training loss: 1.88975
Global Iter: 677500 training acc: 0.28125
Global Iter: 677600 training loss: 2.17072
Global Iter: 677600 training acc: 0.125
Global Iter: 677700 training loss: 1.96899
Global Iter: 677700 training acc: 0.21875
Global Iter: 677800 training loss: 1.96904
Global Iter: 677800 training acc: 0.15625
Global Iter: 677900 training loss: 1.92863
Global Iter: 677900 training acc: 0.1875
Global Iter: 678000 training loss: 2.07422
Global Iter: 678000 training acc: 0.1875
Global Iter: 678100 training loss: 1.91798
Global Iter: 678100 training acc: 0.25
Global Iter: 678200 training loss: 2.03816
Global Iter: 678200 training acc: 0.125
Global Iter: 678300 training loss: 1.92788
Global Iter: 678300 training acc: 0.25
Global Iter: 678400 training loss: 2.00828
Global Iter: 678400 training acc: 0.3125
Global Iter: 678500 training loss: 2.00665
Global Iter: 678500 training acc: 0.21875
Global Iter: 678600 training loss: 2.0131
Global Iter: 678600 training acc: 0.09375
Global Iter: 678700 training loss: 1.93082
Global Iter: 678700 training acc: 0.21875
Global Iter: 678800 training loss: 2.04341
Global Iter: 678800 training acc: 0.28125
Global Iter: 678900 training loss: 2.04599
Global Iter: 678900 training acc: 0.25
Global Iter: 679000 training loss: 2.03136
Global Iter: 679000 training acc: 0.0625
Global Iter: 679100 training loss: 1.96008
Global Iter: 679100 training acc: 0.15625
Global Iter: 679200 training loss: 2.04824
Global Iter: 679200 training acc: 0.21875
Global Iter: 679300 training loss: 2.06422
Global Iter: 679300 training acc: 0.15625
Global Iter: 679400 training loss: 1.94058
Global Iter: 679400 training acc: 0.21875
Global Iter: 679500 training loss: 2.12138
Global Iter: 679500 training acc: 0.09375
Global Iter: 679600 training loss: 1.9542
Global Iter: 679600 training acc: 0.15625
Global Iter: 679700 training loss: 2.07502
Global Iter: 679700 training acc: 0.15625
Global Iter: 679800 training loss: 1.93741
Global Iter: 679800 training acc: 0.21875
Global Iter: 679900 training loss: 1.98879
Global Iter: 679900 training acc: 0.125
Global Iter: 680000 training loss: 1.9425
Global Iter: 680000 training acc: 0.3125
Global Iter: 680100 training loss: 1.88332
Global Iter: 680100 training acc: 0.15625
Global Iter: 680200 training loss: 1.98635
Global Iter: 680200 training acc: 0.1875
Global Iter: 680300 training loss: 2.01173
Global Iter: 680300 training acc: 0.0625
Global Iter: 680400 training loss: 2.19067
Global Iter: 680400 training acc: 0.25
Global Iter: 680500 training loss: 1.97443
Global Iter: 680500 training acc: 0.15625
Global Iter: 680600 training loss: 2.09873
Global Iter: 680600 training acc: 0.09375
Global Iter: 680700 training loss: 2.00112
Global Iter: 680700 training acc: 0.25
Global Iter: 680800 training loss: 1.96853
Global Iter: 680800 training acc: 0.09375
Global Iter: 680900 training loss: 2.03263
Global Iter: 680900 training acc: 0.125
Global Iter: 681000 training loss: 1.98031
Global Iter: 681000 training acc: 0.0625
Global Iter: 681100 training loss: 2.03317
Global Iter: 681100 training acc: 0.0
Global Iter: 681200 training loss: 1.94439
Global Iter: 681200 training acc: 0.34375
Global Iter: 681300 training loss: 1.95289
Global Iter: 681300 training acc: 0.125
Global Iter: 681400 training loss: 2.03205
Global Iter: 681400 training acc: 0.1875
Global Iter: 681500 training loss: 1.91577
Global Iter: 681500 training acc: 0.25
Global Iter: 681600 training loss: 2.02473
Global Iter: 681600 training acc: 0.15625
Global Iter: 681700 training loss: 2.05724
Global Iter: 681700 training acc: 0.15625
Global Iter: 681800 training loss: 1.90375
Global Iter: 681800 training acc: 0.21875
Global Iter: 681900 training loss: 2.08743
Global Iter: 681900 training acc: 0.1875
Global Iter: 682000 training loss: 1.97545
Global Iter: 682000 training acc: 0.1875
Global Iter: 682100 training loss: 2.00966
Global Iter: 682100 training acc: 0.09375
Global Iter: 682200 training loss: 1.94194
Global Iter: 682200 training acc: 0.28125
Global Iter: 682300 training loss: 2.007
Global Iter: 682300 training acc: 0.125
Global Iter: 682400 training loss: 1.9907
Global Iter: 682400 training acc: 0.21875
Global Iter: 682500 training loss: 1.98683
Global Iter: 682500 training acc: 0.0625
Global Iter: 682600 training loss: 1.99531
Global Iter: 682600 training acc: 0.1875
Global Iter: 682700 training loss: 1.98395
Global Iter: 682700 training acc: 0.15625
Global Iter: 682800 training loss: 2.09566
Global Iter: 682800 training acc: 0.125
Global Iter: 682900 training loss: 1.95367
Global Iter: 682900 training acc: 0.15625
Global Iter: 683000 training loss: 2.15952
Global Iter: 683000 training acc: 0.21875
Global Iter: 683100 training loss: 2.02011
Global Iter: 683100 training acc: 0.1875
Global Iter: 683200 training loss: 2.02462
Global Iter: 683200 training acc: 0.1875
Global Iter: 683300 training loss: 1.97587
Global Iter: 683300 training acc: 0.1875
Global Iter: 683400 training loss: 2.141
Global Iter: 683400 training acc: 0.25
Global Iter: 683500 training loss: 1.90277
Global Iter: 683500 training acc: 0.09375
Global Iter: 683600 training loss: 1.89483
Global Iter: 6832017-06-22 00:36:40.027448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-686617
600 training acc: 0.15625
Global Iter: 683700 training loss: 1.98208
Global Iter: 683700 training acc: 0.15625
Global Iter: 683800 training loss: 2.06944
Global Iter: 683800 training acc: 0.09375
Global Iter: 683900 training loss: 2.13567
Global Iter: 683900 training acc: 0.25
Global Iter: 684000 training loss: 2.02443
Global Iter: 684000 training acc: 0.09375
Global Iter: 684100 training loss: 2.02619
Global Iter: 684100 training acc: 0.15625
Global Iter: 684200 training loss: 1.9103
Global Iter: 684200 training acc: 0.21875
Global Iter: 684300 training loss: 1.90406
Global Iter: 684300 training acc: 0.1875
Global Iter: 684400 training loss: 2.11122
Global Iter: 684400 training acc: 0.09375
Global Iter: 684500 training loss: 1.83263
Global Iter: 684500 training acc: 0.34375
Global Iter: 684600 training loss: 2.15979
Global Iter: 684600 training acc: 0.125
Global Iter: 684700 training loss: 1.94447
Global Iter: 684700 training acc: 0.1875
Global Iter: 684800 training loss: 2.01181
Global Iter: 684800 training acc: 0.21875
Global Iter: 684900 training loss: 1.92789
Global Iter: 684900 training acc: 0.15625
Global Iter: 685000 training loss: 1.95481
Global Iter: 685000 training acc: 0.25
Global Iter: 685100 training loss: 1.97455
Global Iter: 685100 training acc: 0.1875
Global Iter: 685200 training loss: 1.9681
Global Iter: 685200 training acc: 0.1875
Global Iter: 685300 training loss: 1.98872
Global Iter: 685300 training acc: 0.15625
Global Iter: 685400 training loss: 1.9274
Global Iter: 685400 training acc: 0.21875
Global Iter: 685500 training loss: 2.00689
Global Iter: 685500 training acc: 0.125
Global Iter: 685600 training loss: 2.05409
Global Iter: 685600 training acc: 0.0625
Global Iter: 685700 training loss: 1.94409
Global Iter: 685700 training acc: 0.09375
Global Iter: 685800 training loss: 1.95473
Global Iter: 685800 training acc: 0.28125
Global Iter: 685900 training loss: 2.06714
Global Iter: 685900 training acc: 0.1875
Global Iter: 686000 training loss: 1.93515
Global Iter: 686000 training acc: 0.3125
Global Iter: 686100 training loss: 1.93715
Global Iter: 686100 training acc: 0.15625
Global Iter: 686200 training loss: 1.94589
Global Iter: 686200 training acc: 0.1875
Global Iter: 686300 training loss: 1.97015
Global Iter: 686300 training acc: 0.1875
Global Iter: 686400 training loss: 1.88865
Global Iter: 686400 training acc: 0.15625
Global Iter: 686500 training loss: 1.98335
Global Iter: 686500 training acc: 0.125
Global Iter: 686600 training loss: 1.95388
Global Iter: 686600 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-686617
Number of Patches: 197538
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-686617
Global Iter: 686700 training loss: 2.1092
Global Iter: 686700 training acc: 0.0625
Global Iter: 686800 training loss: 2.01976
Global Iter: 686800 training acc: 0.1875
Global Iter: 686900 training loss: 1.95912
Global Iter: 686900 training acc: 0.09375
Global Iter: 687000 training loss: 1.97977
Global Iter: 687000 training acc: 0.15625
Global Iter: 687100 training loss: 1.94715
Global Iter: 687100 training acc: 0.1875
Global Iter: 687200 training loss: 1.96864
Global Iter: 687200 training acc: 0.125
Global Iter: 687300 training loss: 1.88938
Global Iter: 687300 training acc: 0.28125
Global Iter: 687400 training loss: 2.0584
Global Iter: 687400 training acc: 0.0625
Global Iter: 687500 training loss: 1.99231
Global Iter: 687500 training acc: 0.3125
Global Iter: 687600 training loss: 1.86383
Global Iter: 687600 training acc: 0.21875
Global Iter: 687700 training loss: 2.0429
Global Iter: 687700 training acc: 0.125
Global Iter: 687800 training loss: 1.99823
Global Iter: 687800 training acc: 0.1875
Global Iter: 687900 training loss: 2.00007
Global Iter: 687900 training acc: 0.15625
Global Iter: 688000 training loss: 1.95707
Global Iter: 688000 training acc: 0.1875
Global Iter: 688100 training loss: 1.95561
Global Iter: 688100 training acc: 0.125
Global Iter: 688200 training loss: 2.06139
Global Iter: 688200 training acc: 0.1875
Global Iter: 688300 training loss: 2.02893
Global Iter: 688300 training acc: 0.15625
Global Iter: 688400 training loss: 1.94649
Global Iter: 688400 training acc: 0.21875
Global Iter: 688500 training loss: 2.04908
Global Iter: 688500 training acc: 0.15625
Global Iter: 688600 training loss: 2.04955
Global Iter: 688600 training acc: 0.15625
Global Iter: 688700 training loss: 1.91521
Global Iter: 688700 training acc: 0.15625
Global Iter: 688800 training loss: 2.0348
Global Iter: 688800 training acc: 0.15625
Global Iter: 688900 training loss: 1.90775
Global Iter: 688900 training acc: 0.1875
Global Iter: 689000 training loss: 1.90288
Global Iter: 689000 training acc: 0.125
Global Iter: 689100 training loss: 2.0123
Global Iter: 689100 training acc: 0.125
Global Iter: 689200 training loss: 2.08793
Global Iter: 689200 training acc: 0.1875
Global Iter: 689300 training loss: 1.98618
Global Iter: 689300 training acc: 0.25
Global Iter: 689400 training loss: 1.99782
Global Iter: 689400 training acc: 0.1875
Global Iter: 689500 training loss: 1.98669
Global Iter: 689500 training acc: 0.15625
Global Iter: 689600 training loss: 1.9536
Global Iter: 689600 training acc: 0.125
Global Iter: 689700 training loss: 2.06385
Global Iter: 689700 training acc: 0.09375
Global Iter: 689800 training loss: 2.00264
Global Iter: 689800 training acc: 0.25
Global Iter: 689900 training loss: 1.99195
Global Iter: 689900 training acc: 0.1875
Global Iter: 690000 training loss: 1.92322
Global Iter: 690000 training acc: 0.28125
Global Iter: 690100 training loss: 2.10714
Global Iter: 690100 training acc: 0.1875
Global Iter: 690200 training loss: 2.00127
Global Iter: 690200 training acc: 0.25
Global Iter: 690300 training loss: 1.96994
Global Iter: 690300 training acc: 0.25
Global Iter: 690400 training loss: 2.02859
Global Iter: 690400 training acc: 0.125
Global Iter: 690500 training loss: 1.99225
Global Iter: 690500 training acc: 0.3125
Global Iter: 690600 training loss: 1.91444
Global Iter: 690600 training acc: 0.1875
Global Iter: 690700 training loss: 1.91796
Global Iter: 690700 training acc: 0.28125
Global Iter: 690800 training loss: 1.99357
Global Iter: 690800 training acc: 0.21875
Global Iter: 690900 training loss: 1.99754
Global Iter: 690900 training acc: 0.15625
Global Iter: 691000 training loss: 1.91665
Global Iter: 691000 training acc: 0.40625
Global Iter: 691100 training loss: 1.93898
Global Iter: 691100 training acc: 0.21875
Global Iter: 691200 training loss: 1.98145
Global Iter: 691200 training acc: 0.09375
Global Iter: 691300 training loss: 2.02753
Global Iter: 691300 training acc: 0.1875
Global Iter: 691400 training loss: 1.96586
Global Iter: 691400 training acc: 0.1875
Global Iter: 691500 training loss: 1.94444
Global Iter: 691500 training acc: 0.25
Global Iter: 691600 training loss: 1.91226
Global Iter: 691600 training acc: 0.3125
Global Iter: 691700 training loss: 2.00829
Global Iter: 691700 training acc: 0.1875
Global Iter: 691800 training loss: 1.98177
Global Iter: 691800 training acc: 0.25
Global Iter: 691900 training loss: 2.08924
Global Iter: 691900 training acc: 0.125
Global Iter: 692000 training loss: 2.02063
Global Iter: 692000 training acc: 0.125
Global Iter: 692100 training loss: 2.07479
Global Iter: 692100 training acc: 0.15625
Global Iter: 692200 training loss: 2.25611
Global Iter: 692200 training acc: 0.125
Global Iter: 692300 training loss: 1.98957
Global Iter: 692300 training acc: 0.09375
Global Iter: 692400 training loss: 1.98563
Global Iter: 692400 training acc: 0.125
Global Iter: 692500 training loss: 1.87282
Global Iter: 692500 training acc: 0.28125
Global Iter: 692600 training loss: 1.91015
Global Iter: 692600 training acc: 0.3125
Global Iter: 692700 training loss: 1.9256
Global Iter: 692700 training acc: 0.25
Global Iter: 692800 training loss: 1.95904
Global Iter: 692800 training acc: 0.15625
Global Iter: 692900 training loss: 1.93576
Global Iter: 692900 training acc: 0.21875
Global Iter: 693000 training loss: 1.90739
Global Iter: 693000 training acc: 0.25
Global Iter: 693100 training loss: 1.93706
Global Iter: 693100 training acc: 0.1875
Global Iter: 693200 training loss: 1.94106
Global Iter: 693200 training acc: 0.3125
Global Iter: 693300 training loss: 1.88179
Global Iter: 693300 training acc: 0.15625
Global Iter: 693400 training loss: 2.13503
Global Iter: 693400 training acc: 0.1875
Global Iter: 693500 training loss: 1.89634
Global Iter: 693500 training acc: 0.25
Global Iter: 693600 training loss: 2.08817
Global Iter: 693600 training acc: 0.25
Global Iter: 693700 training loss: 1.9686
Global Iter: 693700 training acc: 0.25
Global Iter: 693800 training loss: 1.92481
Global Iter: 693800 training acc: 0.1875
Global Iter: 693900 training loss: 1.99023
Global Iter: 693900 training acc: 0.21875
Global Iter: 694000 training loss: 2.02303
Global Iter: 694000 training acc: 0.1875
Global Iter: 694100 training loss: 2.02676
Global Iter: 694100 training acc: 0.21875
Global Iter: 694200 training loss: 1.98462
Global Iter: 694200 training acc: 0.15625
Global Iter: 694300 training loss: 1.94059
Global Iter: 694300 training acc: 0.1875
Global Iter: 694400 training loss: 2.04829
Global Iter: 694400 training acc: 0.09375
Global Iter: 694500 training loss: 2.00545
Global Iter: 694500 training acc: 0.0625
Global Iter: 694600 training loss: 1.90596
Global Iter: 694600 training acc: 0.28125
Global Iter: 694700 training loss: 2.00051
Global Iter: 694700 training acc: 0.15625
Global Iter: 694800 training loss: 1.91977
Global Iter: 694800 training acc: 0.21875
Global Iter: 694900 training loss: 2.07743
Global Iter: 694900 training acc: 0.03125
Global Iter: 695000 training loss: 2.01993
Global Iter: 695000 training acc: 0.3125
Global Iter: 695100 training loss: 2.0062
Global Iter: 695100 training acc: 0.28125
Global Iter: 695200 training loss: 2.0655
Global Iter: 695200 training acc: 0.1875
Global Iter: 695300 training loss: 2.02586
Global Iter: 695300 training acc: 0.21875
Global Iter: 695400 training loss: 1.91414
Global Iter: 695400 training acc: 0.15625
Global Iter: 695500 training loss: 2.02284
Global Iter: 695500 training acc: 0.21875
Global Iter: 695600 training loss: 1.9688
Global Iter: 695600 training acc: 0.15625
Global Iter: 695700 training loss: 1.99882
Global Iter: 695700 training acc: 0.125
Global Iter: 695800 training loss: 1.92154
Global Iter: 695800 training acc: 0.28125
Global Iter: 695900 training loss: 2.09782
Global Iter: 695900 training acc: 0.15625
Global Iter: 696000 training loss: 2.15184
Global Iter: 696000 training acc: 0.1875
Global Iter: 696100 training loss: 1.95812
Global Iter: 696100 training acc: 0.1875
Global Iter: 696200 training loss: 1.93075
Global Iter: 696200 training acc: 0.1875
Global Iter: 696300 training loss: 2.01331
Global Iter: 696300 training acc: 0.15625
Global Iter: 696400 training loss: 2.05601
Global Iter: 696400 training acc: 0.09375
Global Iter: 696500 training loss: 1.93763
Global Iter: 696500 training acc: 0.1875
Global Iter: 696600 training loss: 1.92533
Global Iter: 696600 training acc: 0.1875
Global Iter: 696700 training loss: 1.9823
Global Iter: 696700 training acc: 0.1875
Global Iter: 696800 training loss: 1.93563
Global Iter: 696800 training acc: 0.1875
Global Iter: 696900 training loss: 1.96406
Global Iter: 696900 training acc: 0.1875
Global Iter: 697000 training loss: 1.99924
Global Iter: 697000 training acc: 0.09375
Global Iter: 697100 training loss: 2.13124
Global Iter: 697100 training acc: 0.0625
Global Iter: 697200 training loss: 2.03617
Global Iter: 697200 training acc: 0.1875
Global Iter: 697300 training loss: 1.91955
Global Iter: 697300 training acc: 0.21875
Global Iter: 697400 training loss: 1.91304
Global Iter: 697400 training acc: 0.125
Global Iter: 697500 training loss: 1.92102
Global Iter: 697500 training acc: 0.1875
Global Iter: 697600 training loss: 2.03355
Global Iter: 697600 training acc: 0.15625
Global Iter: 697700 training loss: 1.92547
Global Iter: 697700 training acc: 0.15625
Global Iter: 697800 training loss: 1.96449
Global Iter: 697800 training acc: 0.21875
Global Iter: 697900 training loss: 2.06472
Global Iter: 697900 training acc: 0.0625
Global Iter: 698000 2017-06-22 00:58:03.670694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-698964
training loss: 1.96683
Global Iter: 698000 training acc: 0.21875
Global Iter: 698100 training loss: 1.95911
Global Iter: 698100 training acc: 0.1875
Global Iter: 698200 training loss: 1.95026
Global Iter: 698200 training acc: 0.21875
Global Iter: 698300 training loss: 2.06862
Global Iter: 698300 training acc: 0.15625
Global Iter: 698400 training loss: 2.04398
Global Iter: 698400 training acc: 0.15625
Global Iter: 698500 training loss: 2.05521
Global Iter: 698500 training acc: 0.25
Global Iter: 698600 training loss: 2.03648
Global Iter: 698600 training acc: 0.15625
Global Iter: 698700 training loss: 1.89034
Global Iter: 698700 training acc: 0.3125
Global Iter: 698800 training loss: 1.94387
Global Iter: 698800 training acc: 0.125
Global Iter: 698900 training loss: 2.05775
Global Iter: 698900 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-698964
Number of Patches: 195563
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-698964
Global Iter: 699000 training loss: 1.91637
Global Iter: 699000 training acc: 0.21875
Global Iter: 699100 training loss: 1.92756
Global Iter: 699100 training acc: 0.25
Global Iter: 699200 training loss: 2.01727
Global Iter: 699200 training acc: 0.25
Global Iter: 699300 training loss: 2.04366
Global Iter: 699300 training acc: 0.15625
Global Iter: 699400 training loss: 2.02353
Global Iter: 699400 training acc: 0.0625
Global Iter: 699500 training loss: 2.05794
Global Iter: 699500 training acc: 0.21875
Global Iter: 699600 training loss: 1.99757
Global Iter: 699600 training acc: 0.21875
Global Iter: 699700 training loss: 1.99129
Global Iter: 699700 training acc: 0.1875
Global Iter: 699800 training loss: 1.96596
Global Iter: 699800 training acc: 0.1875
Global Iter: 699900 training loss: 2.07045
Global Iter: 699900 training acc: 0.1875
Global Iter: 700000 training loss: 2.04728
Global Iter: 700000 training acc: 0.15625
Global Iter: 700100 training loss: 1.94074
Global Iter: 700100 training acc: 0.1875
Global Iter: 700200 training loss: 2.10422
Global Iter: 700200 training acc: 0.0625
Global Iter: 700300 training loss: 1.95607
Global Iter: 700300 training acc: 0.15625
Global Iter: 700400 training loss: 1.9978
Global Iter: 700400 training acc: 0.125
Global Iter: 700500 training loss: 1.98286
Global Iter: 700500 training acc: 0.21875
Global Iter: 700600 training loss: 2.0488
Global Iter: 700600 training acc: 0.15625
Global Iter: 700700 training loss: 1.97644
Global Iter: 700700 training acc: 0.21875
Global Iter: 700800 training loss: 1.94734
Global Iter: 700800 training acc: 0.1875
Global Iter: 700900 training loss: 1.92921
Global Iter: 700900 training acc: 0.25
Global Iter: 701000 training loss: 2.07966
Global Iter: 701000 training acc: 0.15625
Global Iter: 701100 training loss: 2.10517
Global Iter: 701100 training acc: 0.15625
Global Iter: 701200 training loss: 1.97527
Global Iter: 701200 training acc: 0.15625
Global Iter: 701300 training loss: 1.90961
Global Iter: 701300 training acc: 0.09375
Global Iter: 701400 training loss: 2.01499
Global Iter: 701400 training acc: 0.125
Global Iter: 701500 training loss: 2.00576
Global Iter: 701500 training acc: 0.125
Global Iter: 701600 training loss: 2.04852
Global Iter: 701600 training acc: 0.15625
Global Iter: 701700 training loss: 1.90448
Global Iter: 701700 training acc: 0.21875
Global Iter: 701800 training loss: 1.99625
Global Iter: 701800 training acc: 0.21875
Global Iter: 701900 training loss: 1.92991
Global Iter: 701900 training acc: 0.09375
Global Iter: 702000 training loss: 1.97034
Global Iter: 702000 training acc: 0.125
Global Iter: 702100 training loss: 2.06733
Global Iter: 702100 training acc: 0.09375
Global Iter: 702200 training loss: 1.98946
Global Iter: 702200 training acc: 0.1875
Global Iter: 702300 training loss: 1.94801
Global Iter: 702300 training acc: 0.25
Global Iter: 702400 training loss: 2.15451
Global Iter: 702400 training acc: 0.15625
Global Iter: 702500 training loss: 1.93731
Global Iter: 702500 training acc: 0.21875
Global Iter: 702600 training loss: 1.88324
Global Iter: 702600 training acc: 0.25
Global Iter: 702700 training loss: 2.04182
Global Iter: 702700 training acc: 0.125
Global Iter: 702800 training loss: 1.97314
Global Iter: 702800 training acc: 0.1875
Global Iter: 702900 training loss: 2.03749
Global Iter: 702900 training acc: 0.15625
Global Iter: 703000 training loss: 2.0202
Global Iter: 703000 training acc: 0.125
Global Iter: 703100 training loss: 2.02616
Global Iter: 703100 training acc: 0.25
Global Iter: 703200 training loss: 2.08913
Global Iter: 703200 training acc: 0.125
Global Iter: 703300 training loss: 1.89768
Global Iter: 703300 training acc: 0.28125
Global Iter: 703400 training loss: 2.09158
Global Iter: 703400 training acc: 0.09375
Global Iter: 703500 training loss: 2.10093
Global Iter: 703500 training acc: 0.15625
Global Iter: 703600 training loss: 1.99325
Global Iter: 703600 training acc: 0.09375
Global Iter: 703700 training loss: 2.00607
Global Iter: 703700 training acc: 0.125
Global Iter: 703800 training loss: 2.056
Global Iter: 703800 training acc: 0.15625
Global Iter: 703900 training loss: 1.97243
Global Iter: 703900 training acc: 0.3125
Global Iter: 704000 training loss: 2.0615
Global Iter: 704000 training acc: 0.15625
Global Iter: 704100 training loss: 1.99954
Global Iter: 704100 training acc: 0.25
Global Iter: 704200 training loss: 1.9684
Global Iter: 704200 training acc: 0.1875
Global Iter: 704300 training loss: 2.08469
Global Iter: 704300 training acc: 0.1875
Global Iter: 704400 training loss: 1.93108
Global Iter: 704400 training acc: 0.21875
Global Iter: 704500 training loss: 2.03658
Global Iter: 704500 training acc: 0.21875
Global Iter: 704600 training loss: 2.01762
Global Iter: 704600 training acc: 0.1875
Global Iter: 704700 training loss: 1.85654
Global Iter: 704700 training acc: 0.1875
Global Iter: 704800 training loss: 2.05852
Global Iter: 704800 training acc: 0.1875
Global Iter: 704900 training loss: 1.9668
Global Iter: 704900 training acc: 0.1875
Global Iter: 705000 training loss: 2.02388
Global Iter: 705000 training acc: 0.125
Global Iter: 705100 training loss: 2.06156
Global Iter: 705100 training acc: 0.1875
Global Iter: 705200 training loss: 2.05277
Global Iter: 705200 training acc: 0.15625
Global Iter: 705300 training loss: 2.00046
Global Iter: 705300 training acc: 0.09375
Global Iter: 705400 training loss: 1.90643
Global Iter: 705400 training acc: 0.28125
Global Iter: 705500 training loss: 1.90004
Global Iter: 705500 training acc: 0.25
Global Iter: 705600 training loss: 1.9803
Global Iter: 705600 training acc: 0.1875
Global Iter: 705700 training loss: 1.90143
Global Iter: 705700 training acc: 0.34375
Global Iter: 705800 training loss: 1.9621
Global Iter: 705800 training acc: 0.25
Global Iter: 705900 training loss: 1.84038
Global Iter: 705900 training acc: 0.34375
Global Iter: 706000 training loss: 1.96511
Global Iter: 706000 training acc: 0.25
Global Iter: 706100 training loss: 1.95596
Global Iter: 706100 training acc: 0.28125
Global Iter: 706200 training loss: 1.9658
Global Iter: 706200 training acc: 0.25
Global Iter: 706300 training loss: 1.96733
Global Iter: 706300 training acc: 0.0625
Global Iter: 706400 training loss: 2.02446
Global Iter: 706400 training acc: 0.1875
Global Iter: 706500 training loss: 1.96368
Global Iter: 706500 training acc: 0.21875
Global Iter: 706600 training loss: 1.96016
Global Iter: 706600 training acc: 0.21875
Global Iter: 706700 training loss: 1.92099
Global Iter: 706700 training acc: 0.21875
Global Iter: 706800 training loss: 2.02209
Global Iter: 706800 training acc: 0.125
Global Iter: 706900 training loss: 2.07228
Global Iter: 706900 training acc: 0.125
Global Iter: 707000 training loss: 2.09128
Global Iter: 707000 training acc: 0.125
Global Iter: 707100 training loss: 2.0296
Global Iter: 707100 training acc: 0.21875
Global Iter: 707200 training loss: 1.97463
Global Iter: 707200 training acc: 0.09375
Global Iter: 707300 training loss: 2.01633
Global Iter: 707300 training acc: 0.125
Global Iter: 707400 training loss: 1.84694
Global Iter: 707400 training acc: 0.34375
Global Iter: 2017-06-22 01:19:32.540825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-711187
707500 training loss: 2.00707
Global Iter: 707500 training acc: 0.21875
Global Iter: 707600 training loss: 1.97744
Global Iter: 707600 training acc: 0.25
Global Iter: 707700 training loss: 2.05255
Global Iter: 707700 training acc: 0.3125
Global Iter: 707800 training loss: 1.88855
Global Iter: 707800 training acc: 0.28125
Global Iter: 707900 training loss: 1.96679
Global Iter: 707900 training acc: 0.28125
Global Iter: 708000 training loss: 2.04378
Global Iter: 708000 training acc: 0.21875
Global Iter: 708100 training loss: 1.9759
Global Iter: 708100 training acc: 0.21875
Global Iter: 708200 training loss: 1.97935
Global Iter: 708200 training acc: 0.15625
Global Iter: 708300 training loss: 2.03017
Global Iter: 708300 training acc: 0.125
Global Iter: 708400 training loss: 2.04404
Global Iter: 708400 training acc: 0.09375
Global Iter: 708500 training loss: 2.02339
Global Iter: 708500 training acc: 0.21875
Global Iter: 708600 training loss: 1.91432
Global Iter: 708600 training acc: 0.15625
Global Iter: 708700 training loss: 1.98032
Global Iter: 708700 training acc: 0.25
Global Iter: 708800 training loss: 2.00838
Global Iter: 708800 training acc: 0.1875
Global Iter: 708900 training loss: 1.91761
Global Iter: 708900 training acc: 0.3125
Global Iter: 709000 training loss: 1.95693
Global Iter: 709000 training acc: 0.21875
Global Iter: 709100 training loss: 2.05715
Global Iter: 709100 training acc: 0.21875
Global Iter: 709200 training loss: 1.94057
Global Iter: 709200 training acc: 0.125
Global Iter: 709300 training loss: 1.91963
Global Iter: 709300 training acc: 0.21875
Global Iter: 709400 training loss: 1.93655
Global Iter: 709400 training acc: 0.1875
Global Iter: 709500 training loss: 2.13213
Global Iter: 709500 training acc: 0.09375
Global Iter: 709600 training loss: 1.94741
Global Iter: 709600 training acc: 0.1875
Global Iter: 709700 training loss: 1.9562
Global Iter: 709700 training acc: 0.25
Global Iter: 709800 training loss: 1.97678
Global Iter: 709800 training acc: 0.34375
Global Iter: 709900 training loss: 2.05152
Global Iter: 709900 training acc: 0.125
Global Iter: 710000 training loss: 2.15767
Global Iter: 710000 training acc: 0.28125
Global Iter: 710100 training loss: 1.9313
Global Iter: 710100 training acc: 0.21875
Global Iter: 710200 training loss: 1.99571
Global Iter: 710200 training acc: 0.15625
Global Iter: 710300 training loss: 1.9933
Global Iter: 710300 training acc: 0.21875
Global Iter: 710400 training loss: 1.90018
Global Iter: 710400 training acc: 0.21875
Global Iter: 710500 training loss: 2.03697
Global Iter: 710500 training acc: 0.125
Global Iter: 710600 training loss: 1.94197
Global Iter: 710600 training acc: 0.1875
Global Iter: 710700 training loss: 1.94947
Global Iter: 710700 training acc: 0.21875
Global Iter: 710800 training loss: 1.93425
Global Iter: 710800 training acc: 0.125
Global Iter: 710900 training loss: 2.08422
Global Iter: 710900 training acc: 0.15625
Global Iter: 711000 training loss: 2.01661
Global Iter: 711000 training acc: 0.21875
Global Iter: 711100 training loss: 2.03241
Global Iter: 711100 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-711187
Number of Patches: 193608
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-711187
Global Iter: 711200 training loss: 1.99963
Global Iter: 711200 training acc: 0.125
Global Iter: 711300 training loss: 1.95835
Global Iter: 711300 training acc: 0.09375
Global Iter: 711400 training loss: 1.99102
Global Iter: 711400 training acc: 0.28125
Global Iter: 711500 training loss: 1.98151
Global Iter: 711500 training acc: 0.28125
Global Iter: 711600 training loss: 2.09635
Global Iter: 711600 training acc: 0.21875
Global Iter: 711700 training loss: 1.99426
Global Iter: 711700 training acc: 0.1875
Global Iter: 711800 training loss: 1.95941
Global Iter: 711800 training acc: 0.21875
Global Iter: 711900 training loss: 1.99971
Global Iter: 711900 training acc: 0.1875
Global Iter: 712000 training loss: 2.02277
Global Iter: 712000 training acc: 0.125
Global Iter: 712100 training loss: 1.88475
Global Iter: 712100 training acc: 0.25
Global Iter: 712200 training loss: 1.91244
Global Iter: 712200 training acc: 0.21875
Global Iter: 712300 training loss: 1.96895
Global Iter: 712300 training acc: 0.15625
Global Iter: 712400 training loss: 1.94483
Global Iter: 712400 training acc: 0.25
Global Iter: 712500 training loss: 2.0113
Global Iter: 712500 training acc: 0.28125
Global Iter: 712600 training loss: 1.99055
Global Iter: 712600 training acc: 0.125
Global Iter: 712700 training loss: 1.88251
Global Iter: 712700 training acc: 0.21875
Global Iter: 712800 training loss: 2.09835
Global Iter: 712800 training acc: 0.15625
Global Iter: 712900 training loss: 2.02562
Global Iter: 712900 training acc: 0.15625
Global Iter: 713000 training loss: 1.90057
Global Iter: 713000 training acc: 0.125
Global Iter: 713100 training loss: 1.9843
Global Iter: 713100 training acc: 0.125
Global Iter: 713200 training loss: 1.91792
Global Iter: 713200 training acc: 0.25
Global Iter: 713300 training loss: 1.92854
Global Iter: 713300 training acc: 0.15625
Global Iter: 713400 training loss: 1.95744
Global Iter: 713400 training acc: 0.1875
Global Iter: 713500 training loss: 1.99225
Global Iter: 713500 training acc: 0.125
Global Iter: 713600 training loss: 2.04134
Global Iter: 713600 training acc: 0.25
Global Iter: 713700 training loss: 2.04039
Global Iter: 713700 training acc: 0.15625
Global Iter: 713800 training loss: 1.92495
Global Iter: 713800 training acc: 0.125
Global Iter: 713900 training loss: 1.95075
Global Iter: 713900 training acc: 0.25
Global Iter: 714000 training loss: 1.88819
Global Iter: 714000 training acc: 0.15625
Global Iter: 714100 training loss: 2.09364
Global Iter: 714100 training acc: 0.21875
Global Iter: 714200 training loss: 2.11122
Global Iter: 714200 training acc: 0.125
Global Iter: 714300 training loss: 1.99511
Global Iter: 714300 training acc: 0.1875
Global Iter: 714400 training loss: 2.04462
Global Iter: 714400 training acc: 0.09375
Global Iter: 714500 training loss: 2.10718
Global Iter: 714500 training acc: 0.125
Global Iter: 714600 training loss: 2.12121
Global Iter: 714600 training acc: 0.09375
Global Iter: 714700 training loss: 2.02305
Global Iter: 714700 training acc: 0.1875
Global Iter: 714800 training loss: 1.9323
Global Iter: 714800 training acc: 0.15625
Global Iter: 714900 training loss: 2.06818
Global Iter: 714900 training acc: 0.125
Global Iter: 715000 training loss: 2.01584
Global Iter: 715000 training acc: 0.1875
Global Iter: 715100 training loss: 1.92629
Global Iter: 715100 training acc: 0.25
Global Iter: 715200 training loss: 2.00261
Global Iter: 715200 training acc: 0.125
Global Iter: 715300 training loss: 1.93651
Global Iter: 715300 training acc: 0.15625
Global Iter: 715400 training loss: 2.04617
Global Iter: 715400 training acc: 0.25
Global Iter: 715500 training loss: 1.97609
Global Iter: 715500 training acc: 0.1875
Global Iter: 715600 training loss: 1.92179
Global Iter: 715600 training acc: 0.1875
Global Iter: 715700 training loss: 2.02115
Global Iter: 715700 training acc: 0.15625
Global Iter: 715800 training loss: 1.94147
Global Iter: 715800 training acc: 0.3125
Global Iter: 715900 training loss: 1.93806
Global Iter: 715900 training acc: 0.125
Global Iter: 716000 training loss: 2.16505
Global Iter: 716000 training acc: 0.125
Global Iter: 716100 training loss: 1.96784
Global Iter: 716100 training acc: 0.21875
Global Iter: 716200 training loss: 2.04701
Global Iter: 716200 training acc: 0.125
Global Iter: 716300 training loss: 1.97014
Global Iter: 716300 training acc: 0.3125
Global Iter: 716400 training loss: 1.96765
Global Iter: 716400 training acc: 0.1875
Global Iter: 716500 training loss: 1.93842
Global Iter: 716500 training acc: 0.1875
Global Iter: 716600 training loss: 1.9235
Global Iter: 716600 training acc: 0.28125
Global Iter: 716700 training loss: 2.00095
Global Iter: 716700 training acc: 0.1875
Global Iter: 716800 training loss: 2.02431
Global Iter: 716800 training acc: 0.15625
Global Iter: 716900 training loss: 2.07035
Global Iter: 716900 training acc: 0.15625
Global Iter: 717000 training loss: 2.02278
Global Iter: 717000 training acc: 0.0625
Global Iter: 717100 training loss: 2.08136
Global Iter: 717100 training acc: 0.21875
Global Iter: 717200 training loss: 1.93843
Global Iter: 717200 training acc: 0.1875
Global Iter: 717300 training loss: 2.12407
Global Iter: 717300 training acc: 0.21875
Global Iter: 717400 training loss: 2.03786
Global Iter: 717400 training acc: 0.1875
Global Iter: 717500 training loss: 1.98421
Global Iter: 717500 training acc: 0.21875
Global Iter: 717600 training loss: 2.04687
Global Iter: 717600 training acc: 0.1875
Global Iter: 717700 training loss: 1.95113
Global Iter: 717700 training acc: 0.3125
Global Iter: 717800 training loss: 1.94218
Global Iter: 717800 training acc: 0.28125
Global Iter: 717900 training loss: 1.98725
Global Iter: 717900 training acc: 0.21875
Global Iter: 718000 training loss: 2.00237
Global Iter: 718000 training acc: 0.15625
Global Iter: 718100 training loss: 1.86894
Global Iter: 718100 training acc: 0.25
Global Iter: 718200 training loss: 2.09208
Global Iter: 718200 training acc: 0.21875
Global Iter: 718300 training loss: 1.9375
Global Iter: 718300 training acc: 0.1875
Global Iter: 718400 training loss: 1.9642
Global Iter: 718400 training acc: 0.25
Global Iter: 718500 training loss: 1.93644
Global Iter: 718500 training acc: 0.21875
Global Iter: 718600 training loss: 1.88969
Global Iter: 718600 training acc: 0.3125
Global Iter: 718700 training loss: 2.075
Global Iter: 718700 training acc: 0.09375
Global Iter: 718800 training loss: 2.0188
Global Iter: 718800 training acc: 0.1875
Global Iter: 718900 training loss: 2.0557
Global Iter: 718900 training acc: 0.125
Global Iter: 719000 training loss: 1.96169
Global Iter: 719000 training acc: 0.15625
Global Iter: 719100 training loss: 1.93671
Global Iter: 719100 training acc: 0.25
Global Iter: 719200 training loss: 1.92487
Global Iter: 719200 training acc: 0.3125
Global Iter: 719300 training loss: 2.0226
Global Iter: 719300 training acc: 0.15625
Global Iter: 719400 training loss: 1.95436
Global Iter: 719400 training acc: 0.09375
Global Iter: 719500 training loss: 1.99889
Global Iter: 719500 training acc: 0.34375
Global Iter: 719600 training loss: 1.99693
Global Iter: 719600 training acc: 0.25
Global Iter: 719700 training loss: 1.99231
Global Iter: 719700 training acc: 0.15625
Global Iter: 719800 training loss: 1.97316
Global Iter: 719800 training acc: 0.21875
Global Iter: 719900 training loss: 1.93499
Global Iter: 719900 training acc: 0.15625
Global Iter: 720000 training loss: 1.89101
Global Iter: 720000 training acc: 0.25
Global Iter: 720100 training loss: 1.99424
Global Iter: 720100 training acc: 0.25
Global Iter: 720200 training loss: 2.01573
Global Iter: 720200 training acc: 0.09375
Global Iter: 720300 training loss: 1.95867
Global Iter: 720300 training acc: 0.21875
Global Iter: 720400 training loss: 1.90848
Global Iter: 720400 training acc: 0.28125
Global Iter: 720500 training loss: 1.98002
Global Iter: 720500 training acc: 0.1875
Global Iter: 720600 training loss: 1.9818
Global Iter: 720600 training acc: 0.25
Global Iter: 720700 training loss: 1.98942
Global Iter: 720700 training acc: 0.1875
Global Iter: 720800 training loss: 1.99905
Global Iter: 720800 training acc: 0.15625
Global Iter: 720900 training loss: 1.95291
Global Iter: 720900 training acc: 0.1875
Global Iter: 721000 training loss: 1.98828
Global Iter: 721000 training acc: 0.25
Global Iter: 721100 training loss: 1.99297
Global Iter: 721100 training acc: 0.21875
Global Iter: 721200 training loss: 1.90934
Global Iter: 721200 training acc: 0.28125
Global Iter: 721300 training loss: 1.94658
Global Iter: 721300 training acc: 0.15625
Global Iter: 721400 training loss: 2.06712
Global Iter: 721400 training acc: 0.28125
Global Iter: 721500 training loss: 1.97591
Global Iter: 721500 training acc: 0.3125
Global Iter: 721600 training loss: 1.94538
Global Iter: 721600 training acc: 0.21875
Global Iter: 721700 training loss: 1.9382
Global Iter: 721700 training acc: 0.3125
Global Iter: 721800 training loss: 1.97697
Global Iter: 721800 trai2017-06-22 01:40:59.339298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-723288
ning acc: 0.375
Global Iter: 721900 training loss: 1.95116
Global Iter: 721900 training acc: 0.25
Global Iter: 722000 training loss: 1.93887
Global Iter: 722000 training acc: 0.15625
Global Iter: 722100 training loss: 1.95255
Global Iter: 722100 training acc: 0.21875
Global Iter: 722200 training loss: 1.95592
Global Iter: 722200 training acc: 0.21875
Global Iter: 722300 training loss: 2.00625
Global Iter: 722300 training acc: 0.21875
Global Iter: 722400 training loss: 2.07405
Global Iter: 722400 training acc: 0.1875
Global Iter: 722500 training loss: 2.01132
Global Iter: 722500 training acc: 0.15625
Global Iter: 722600 training loss: 2.08466
Global Iter: 722600 training acc: 0.03125
Global Iter: 722700 training loss: 1.9862
Global Iter: 722700 training acc: 0.125
Global Iter: 722800 training loss: 1.96707
Global Iter: 722800 training acc: 0.1875
Global Iter: 722900 training loss: 2.00815
Global Iter: 722900 training acc: 0.15625
Global Iter: 723000 training loss: 1.89623
Global Iter: 723000 training acc: 0.1875
Global Iter: 723100 training loss: 1.93828
Global Iter: 723100 training acc: 0.28125
Global Iter: 723200 training loss: 2.00214
Global Iter: 723200 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-723288
Number of Patches: 191672
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-723288
Global Iter: 723300 training loss: 2.00421
Global Iter: 723300 training acc: 0.125
Global Iter: 723400 training loss: 1.9238
Global Iter: 723400 training acc: 0.21875
Global Iter: 723500 training loss: 1.9467
Global Iter: 723500 training acc: 0.1875
Global Iter: 723600 training loss: 2.1019
Global Iter: 723600 training acc: 0.09375
Global Iter: 723700 training loss: 1.85181
Global Iter: 723700 training acc: 0.34375
Global Iter: 723800 training loss: 2.02982
Global Iter: 723800 training acc: 0.125
Global Iter: 723900 training loss: 1.96479
Global Iter: 723900 training acc: 0.1875
Global Iter: 724000 training loss: 2.01752
Global Iter: 724000 training acc: 0.09375
Global Iter: 724100 training loss: 2.06009
Global Iter: 724100 training acc: 0.09375
Global Iter: 724200 training loss: 2.02159
Global Iter: 724200 training acc: 0.125
Global Iter: 724300 training loss: 1.98398
Global Iter: 724300 training acc: 0.3125
Global Iter: 724400 training loss: 2.06662
Global Iter: 724400 training acc: 0.15625
Global Iter: 724500 training loss: 1.9129
Global Iter: 724500 training acc: 0.21875
Global Iter: 724600 training loss: 2.00549
Global Iter: 724600 training acc: 0.15625
Global Iter: 724700 training loss: 2.03222
Global Iter: 724700 training acc: 0.125
Global Iter: 724800 training loss: 2.02967
Global Iter: 724800 training acc: 0.0
Global Iter: 724900 training loss: 1.95649
Global Iter: 724900 training acc: 0.15625
Global Iter: 725000 training loss: 2.02418
Global Iter: 725000 training acc: 0.15625
Global Iter: 725100 training loss: 1.976
Global Iter: 725100 training acc: 0.34375
Global Iter: 725200 training loss: 2.06649
Global Iter: 725200 training acc: 0.15625
Global Iter: 725300 training loss: 1.92012
Global Iter: 725300 training acc: 0.15625
Global Iter: 725400 training loss: 1.94818
Global Iter: 725400 training acc: 0.3125
Global Iter: 725500 training loss: 1.9969
Global Iter: 725500 training acc: 0.1875
Global Iter: 725600 training loss: 2.06759
Global Iter: 725600 training acc: 0.09375
Global Iter: 725700 training loss: 2.00136
Global Iter: 725700 training acc: 0.21875
Global Iter: 725800 training loss: 2.07615
Global Iter: 725800 training acc: 0.1875
Global Iter: 725900 training loss: 2.06627
Global Iter: 725900 training acc: 0.3125
Global Iter: 726000 training loss: 2.06757
Global Iter: 726000 training acc: 0.15625
Global Iter: 726100 training loss: 1.98283
Global Iter: 726100 training acc: 0.15625
Global Iter: 726200 training loss: 1.94076
Global Iter: 726200 training acc: 0.25
Global Iter: 726300 training loss: 2.07636
Global Iter: 726300 training acc: 0.21875
Global Iter: 726400 training loss: 1.93202
Global Iter: 726400 training acc: 0.3125
Global Iter: 726500 training loss: 2.11737
Global Iter: 726500 training acc: 0.21875
Global Iter: 726600 training loss: 2.00138
Global Iter: 726600 training acc: 0.25
Global Iter: 726700 training loss: 1.97384
Global Iter: 726700 training acc: 0.0625
Global Iter: 726800 training loss: 2.03458
Global Iter: 726800 training acc: 0.1875
Global Iter: 726900 training loss: 1.92747
Global Iter: 726900 training acc: 0.1875
Global Iter: 727000 training loss: 1.90809
Global Iter: 727000 training acc: 0.25
Global Iter: 727100 training loss: 2.03402
Global Iter: 727100 training acc: 0.15625
Global Iter: 727200 training loss: 1.95987
Global Iter: 727200 training acc: 0.1875
Global Iter: 727300 training loss: 1.89021
Global Iter: 727300 training acc: 0.1875
Global Iter: 727400 training loss: 2.02153
Global Iter: 727400 training acc: 0.0625
Global Iter: 727500 training loss: 1.94348
Global Iter: 727500 training acc: 0.0625
Global Iter: 727600 training loss: 1.95385
Global Iter: 727600 training acc: 0.15625
Global Iter: 727700 training loss: 1.99757
Global Iter: 727700 training acc: 0.25
Global Iter: 727800 training loss: 2.16769
Global Iter: 727800 training acc: 0.0625
Global Iter: 727900 training loss: 1.97493
Global Iter: 727900 training acc: 0.21875
Global Iter: 728000 training loss: 2.11079
Global Iter: 728000 training acc: 0.125
Global Iter: 728100 training loss: 2.09307
Global Iter: 728100 training acc: 0.21875
Global Iter: 728200 training loss: 1.89455
Global Iter: 728200 training acc: 0.28125
Global Iter: 728300 training loss: 1.93794
Global Iter: 728300 training acc: 0.25
Global Iter: 728400 training loss: 1.95727
Global Iter: 728400 training acc: 0.25
Global Iter: 728500 training loss: 2.04721
Global Iter: 728500 training acc: 0.1875
Global Iter: 728600 training loss: 1.98735
Global Iter: 728600 training acc: 0.375
Global Iter: 728700 training loss: 2.00123
Global Iter: 728700 training acc: 0.25
Global Iter: 728800 training loss: 2.0556
Global Iter: 728800 training acc: 0.21875
Global Iter: 728900 training loss: 2.06729
Global Iter: 728900 training acc: 0.1875
Global Iter: 729000 training loss: 1.91228
Global Iter: 729000 training acc: 0.21875
Global Iter: 729100 training loss: 2.00977
Global Iter: 729100 training acc: 0.125
Global Iter: 729200 training loss: 1.93788
Global Iter: 729200 training acc: 0.21875
Global Iter: 729300 training loss: 1.93878
Global Iter: 729300 training acc: 0.21875
Global Iter: 729400 training loss: 2.04416
Global Iter: 729400 training acc: 0.25
Global Iter: 729500 training loss: 2.035
Global Iter: 729500 training acc: 0.21875
Global Iter: 729600 training loss: 1.99604
Global Iter: 729600 training acc: 0.25
Global Iter: 729700 training loss: 1.97902
Global Iter: 729700 training acc: 0.1875
Global Iter: 729800 training loss: 2.0275
Global Iter: 729800 training acc: 0.15625
Global Iter: 729900 training loss: 1.97335
Global Iter: 729900 training acc: 0.21875
Global Iter: 730000 training loss: 1.97055
Global Iter: 730000 training acc: 0.125
Global Iter: 730100 training loss: 1.96761
Global Iter: 730100 training acc: 0.09375
Global Iter: 730200 training loss: 1.95339
Global Iter: 730200 training acc: 0.15625
Global Iter: 730300 training loss: 2.03533
Global Iter: 730300 training acc: 0.1875
Global Iter: 730400 training loss: 2.00165
Global Iter: 730400 training acc: 0.25
Global Iter: 730500 training loss: 1.97889
Global Iter: 730500 training acc: 0.34375
Global Iter: 730600 training loss: 2.03281
Global Iter: 730600 training acc: 0.15625
Global Iter: 730700 training loss: 1.90971
Global Iter: 730700 training acc: 0.1875
Global Iter: 730800 training loss: 2.06656
Global Iter: 730800 training acc: 0.09375
Global Iter: 730900 training loss: 1.97789
Global Iter: 730900 training acc: 0.28125
Global Iter: 731000 training loss: 1.99723
Global Iter: 731000 training acc: 0.3125
Global Iter: 731100 training loss: 1.96518
Global Iter: 731100 training acc: 0.375
Global Iter: 731200 training loss: 1.92736
Global Iter: 731200 training acc: 0.15625
Global Iter: 731300 training loss: 2.02511
Global Iter: 72017-06-22 02:02:01.881071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-735268
31300 training acc: 0.125
Global Iter: 731400 training loss: 2.07643
Global Iter: 731400 training acc: 0.1875
Global Iter: 731500 training loss: 1.91202
Global Iter: 731500 training acc: 0.1875
Global Iter: 731600 training loss: 1.93088
Global Iter: 731600 training acc: 0.15625
Global Iter: 731700 training loss: 1.9991
Global Iter: 731700 training acc: 0.125
Global Iter: 731800 training loss: 2.04837
Global Iter: 731800 training acc: 0.15625
Global Iter: 731900 training loss: 1.96682
Global Iter: 731900 training acc: 0.1875
Global Iter: 732000 training loss: 2.04657
Global Iter: 732000 training acc: 0.15625
Global Iter: 732100 training loss: 2.05047
Global Iter: 732100 training acc: 0.28125
Global Iter: 732200 training loss: 1.9946
Global Iter: 732200 training acc: 0.28125
Global Iter: 732300 training loss: 2.02494
Global Iter: 732300 training acc: 0.1875
Global Iter: 732400 training loss: 1.99666
Global Iter: 732400 training acc: 0.1875
Global Iter: 732500 training loss: 1.97903
Global Iter: 732500 training acc: 0.09375
Global Iter: 732600 training loss: 2.01902
Global Iter: 732600 training acc: 0.125
Global Iter: 732700 training loss: 1.89513
Global Iter: 732700 training acc: 0.34375
Global Iter: 732800 training loss: 1.97738
Global Iter: 732800 training acc: 0.21875
Global Iter: 732900 training loss: 1.96386
Global Iter: 732900 training acc: 0.25
Global Iter: 733000 training loss: 1.96672
Global Iter: 733000 training acc: 0.09375
Global Iter: 733100 training loss: 2.05132
Global Iter: 733100 training acc: 0.125
Global Iter: 733200 training loss: 2.08902
Global Iter: 733200 training acc: 0.09375
Global Iter: 733300 training loss: 2.04565
Global Iter: 733300 training acc: 0.21875
Global Iter: 733400 training loss: 1.95644
Global Iter: 733400 training acc: 0.1875
Global Iter: 733500 training loss: 2.00895
Global Iter: 733500 training acc: 0.21875
Global Iter: 733600 training loss: 1.99699
Global Iter: 733600 training acc: 0.09375
Global Iter: 733700 training loss: 1.90745
Global Iter: 733700 training acc: 0.25
Global Iter: 733800 training loss: 2.02434
Global Iter: 733800 training acc: 0.15625
Global Iter: 733900 training loss: 1.99455
Global Iter: 733900 training acc: 0.09375
Global Iter: 734000 training loss: 1.96931
Global Iter: 734000 training acc: 0.125
Global Iter: 734100 training loss: 1.96916
Global Iter: 734100 training acc: 0.21875
Global Iter: 734200 training loss: 2.05311
Global Iter: 734200 training acc: 0.1875
Global Iter: 734300 training loss: 1.95518
Global Iter: 734300 training acc: 0.15625
Global Iter: 734400 training loss: 1.97386
Global Iter: 734400 training acc: 0.09375
Global Iter: 734500 training loss: 2.03134
Global Iter: 734500 training acc: 0.1875
Global Iter: 734600 training loss: 1.84769
Global Iter: 734600 training acc: 0.28125
Global Iter: 734700 training loss: 2.06041
Global Iter: 734700 training acc: 0.125
Global Iter: 734800 training loss: 1.97065
Global Iter: 734800 training acc: 0.15625
Global Iter: 734900 training loss: 1.8706
Global Iter: 734900 training acc: 0.375
Global Iter: 735000 training loss: 2.04788
Global Iter: 735000 training acc: 0.09375
Global Iter: 735100 training loss: 2.05893
Global Iter: 735100 training acc: 0.0625
Global Iter: 735200 training loss: 2.02904
Global Iter: 735200 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-735268
Number of Patches: 189756
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-735268
Global Iter: 735300 training loss: 1.92684
Global Iter: 735300 training acc: 0.1875
Global Iter: 735400 training loss: 2.02284
Global Iter: 735400 training acc: 0.0625
Global Iter: 735500 training loss: 1.87925
Global Iter: 735500 training acc: 0.34375
Global Iter: 735600 training loss: 2.02986
Global Iter: 735600 training acc: 0.21875
Global Iter: 735700 training loss: 2.04734
Global Iter: 735700 training acc: 0.15625
Global Iter: 735800 training loss: 1.99661
Global Iter: 735800 training acc: 0.125
Global Iter: 735900 training loss: 1.95037
Global Iter: 735900 training acc: 0.21875
Global Iter: 736000 training loss: 2.11151
Global Iter: 736000 training acc: 0.125
Global Iter: 736100 training loss: 1.98881
Global Iter: 736100 training acc: 0.15625
Global Iter: 736200 training loss: 1.97286
Global Iter: 736200 training acc: 0.21875
Global Iter: 736300 training loss: 1.98944
Global Iter: 736300 training acc: 0.09375
Global Iter: 736400 training loss: 1.95842
Global Iter: 736400 training acc: 0.25
Global Iter: 736500 training loss: 2.02273
Global Iter: 736500 training acc: 0.25
Global Iter: 736600 training loss: 1.94249
Global Iter: 736600 training acc: 0.15625
Global Iter: 736700 training loss: 2.02643
Global Iter: 736700 training acc: 0.1875
Global Iter: 736800 training loss: 1.97205
Global Iter: 736800 training acc: 0.09375
Global Iter: 736900 training loss: 1.91112
Global Iter: 736900 training acc: 0.21875
Global Iter: 737000 training loss: 1.96729
Global Iter: 737000 training acc: 0.09375
Global Iter: 737100 training loss: 2.10403
Global Iter: 737100 training acc: 0.125
Global Iter: 737200 training loss: 1.95722
Global Iter: 737200 training acc: 0.25
Global Iter: 737300 training loss: 1.96984
Global Iter: 737300 training acc: 0.15625
Global Iter: 737400 training loss: 1.98251
Global Iter: 737400 training acc: 0.21875
Global Iter: 737500 training loss: 1.91793
Global Iter: 737500 training acc: 0.28125
Global Iter: 737600 training loss: 1.9844
Global Iter: 737600 training acc: 0.28125
Global Iter: 737700 training loss: 2.06136
Global Iter: 737700 training acc: 0.15625
Global Iter: 737800 training loss: 1.98436
Global Iter: 737800 training acc: 0.15625
Global Iter: 737900 training loss: 2.05186
Global Iter: 737900 training acc: 0.15625
Global Iter: 738000 training loss: 1.96931
Global Iter: 738000 training acc: 0.1875
Global Iter: 738100 training loss: 1.87215
Global Iter: 738100 training acc: 0.21875
Global Iter: 738200 training loss: 2.0376
Global Iter: 738200 training acc: 0.25
Global Iter: 738300 training loss: 1.99647
Global Iter: 738300 training acc: 0.125
Global Iter: 738400 training loss: 1.92509
Global Iter: 738400 training acc: 0.3125
Global Iter: 738500 training loss: 1.9194
Global Iter: 738500 training acc: 0.25
Global Iter: 738600 training loss: 2.01466
Global Iter: 738600 training acc: 0.09375
Global Iter: 738700 training loss: 1.98232
Global Iter: 738700 training acc: 0.21875
Global Iter: 738800 training loss: 2.04765
Global Iter: 738800 training acc: 0.125
Global Iter: 738900 training loss: 1.93276
Global Iter: 738900 training acc: 0.3125
Global Iter: 739000 training loss: 1.99007
Global Iter: 739000 training acc: 0.25
Global Iter: 739100 training loss: 1.96481
Global Iter: 739100 training acc: 0.28125
Global Iter: 739200 training loss: 2.07219
Global Iter: 739200 training acc: 0.21875
Global Iter: 739300 training loss: 1.92338
Global Iter: 739300 training acc: 0.21875
Global Iter: 739400 training loss: 1.94672
Global Iter: 739400 training acc: 0.21875
Global Iter: 739500 training loss: 2.14274
Global Iter: 739500 training acc: 0.125
Global Iter: 739600 training loss: 1.93264
Global Iter: 739600 training acc: 0.15625
Global Iter: 739700 training loss: 1.95961
Global Iter: 739700 training acc: 0.125
Global Iter: 739800 training loss: 2.01434
Global Iter: 739800 training acc: 0.25
Global Iter: 739900 training loss: 1.92772
Global Iter: 739900 training acc: 0.15625
Global Iter: 740000 training loss: 1.95915
Global Iter: 740000 training acc: 0.21875
Global Iter: 740100 training loss: 2.04653
Global Iter: 740100 training acc: 0.09375
Global Iter: 740200 training loss: 2.00374
Global Iter: 740200 training acc: 0.09375
Global Iter: 740300 training loss: 1.99871
Global Iter: 740300 training acc: 0.125
Global Iter: 740400 training loss: 1.96299
Global Iter: 740400 training acc: 0.21875
Global Iter: 740500 training loss: 2.05901
Global Iter: 740500 training acc: 0.25
Global Iter: 740600 training loss: 1.98818
Global Iter: 740600 training acc: 0.125
Global Iter: 740700 training loss: 1.95843
Global Iter: 740700 training acc: 0.15625
Global Iter: 740800 training loss: 2.0394
Global Iter: 740800 training acc: 0.15625
Global Iter: 740900 training loss: 1.95138
Global Iter: 740900 training acc: 0.21875
Global Iter: 741000 training loss: 1.98936
Global Iter: 741000 training acc: 0.1875
Global Iter: 741100 training loss: 2.05317
Global Iter: 741100 training acc: 0.1875
Global Iter: 741200 training loss: 2.04959
Global Iter: 741200 training acc: 0.21875
Global Iter: 741300 training loss: 2.10244
Global Iter: 741300 training acc: 0.15625
Global Iter: 741400 training loss: 2.10568
Global Iter: 741400 training acc: 0.03125
Global Iter: 741500 training loss: 1.99243
Global Iter: 741500 training acc: 0.125
Global Iter: 741600 training loss: 2.00199
Global Iter: 741600 training acc: 0.125
Global Iter: 741700 training loss: 1.90276
Global Iter: 741700 training acc: 0.15625
Global Iter: 741800 training loss: 2.0075
Global Iter: 741800 training acc: 0.15625
Global Iter: 741900 training loss: 2.00392
Global Iter: 741900 training acc: 0.125
Global Iter: 742000 training loss: 1.94618
Global Iter: 742000 training acc: 0.25
Global Iter: 742100 training loss: 2.03573
Global Iter: 742100 training acc: 0.125
Global Iter: 742200 training loss: 2.02644
Global Iter: 742200 training acc: 0.21875
Global Iter: 742300 training loss: 2.02531
Global Iter: 742300 training acc: 0.28125
Global Iter: 742400 training loss: 1.98254
Global Iter: 742400 training acc: 0.21875
Global Iter: 742500 training loss: 1.9573
Global Iter: 742500 training acc: 0.125
Global Iter: 742600 training loss: 1.90268
Global Iter: 742600 training acc: 0.1875
Global Iter: 742700 training loss: 1.95268
Global Iter: 742700 training acc: 0.1875
Global Iter: 742800 training loss: 2.00501
Global Iter: 742800 training acc: 0.15625
Global Iter: 742900 training loss: 2.04816
Global Iter: 742900 training acc: 0.15625
Global Iter: 743000 training loss: 1.97934
Global Iter: 743000 training acc: 0.3125
Global Iter: 743100 training loss: 2.03221
Global Iter: 743100 training acc: 0.21875
Global Iter: 743200 training loss: 1.97338
Global Iter: 743200 training acc: 0.1875
Global Iter: 743300 training loss: 2.05782
Global Iter: 743300 training acc: 0.09375
Global Iter: 743400 training loss: 2.07046
Global Iter: 743400 training acc: 0.09375
Global Iter: 743500 training loss: 1.98949
Global Iter: 743500 training acc: 0.21875
Global Iter: 743600 training loss: 1.95827
Global Iter: 743600 training acc: 0.3125
Global Iter: 743700 training loss: 2.03058
Global Iter: 743700 training acc: 0.125
Global Iter: 743800 training loss: 1.97527
Global Iter: 743800 training acc: 0.21875
Global Iter: 743900 training loss: 2.00456
Global Iter: 743900 training acc: 0.25
Global Iter: 744000 training loss: 1.91406
Global Iter: 744000 training acc: 0.15625
Global Iter: 744100 training loss: 1.92177
Global Iter: 744100 training acc: 0.3125
Global Iter: 744200 training loss: 2.01943
Global Iter: 744200 training acc: 0.125
Global Iter: 744300 training loss: 1.95718
Global Iter: 744300 training acc: 0.21875
Global Iter: 744400 training loss: 1.87439
Global Iter: 744400 training acc: 0.28125
Global Iter: 744500 training loss: 1.99146
Global Iter: 744500 training acc: 0.1875
Global Iter: 744600 training loss: 1.90676
Global Iter: 744600 training acc: 0.375
Global Iter: 744700 training loss: 2.12241
Global Iter: 744700 training acc: 0.21875
Global Iter: 744800 training loss: 1.98452
Global Iter: 744800 training acc: 0.125
Global Iter: 744900 training loss: 1.93248
Global Iter: 744900 training acc: 0.3125
Global Iter: 745000 training loss: 1.93993
Global Iter: 745000 training acc: 0.125
Global Iter: 745100 training loss: 2.00411
Global Iter: 745100 training acc: 0.1875
Global Iter: 745200 training loss: 1.94499
Global Iter: 745200 training acc: 0.1875
Global Iter: 745300 training loss: 1.99948
Global Iter: 745300 training acc: 0.25
Global Iter: 745400 training loss: 1.93344
Global Iter: 745400 training acc: 0.21875
Global Iter: 745500 training loss: 2.00511
Global Iter: 745500 training acc: 0.125
Global Iter: 745600 training loss: 1.98971
Global Iter: 745600 training acc: 0.15625
Global Iter: 2017-06-22 02:22:49.127909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-747128
745700 training loss: 1.93343
Global Iter: 745700 training acc: 0.21875
Global Iter: 745800 training loss: 1.97453
Global Iter: 745800 training acc: 0.15625
Global Iter: 745900 training loss: 1.99102
Global Iter: 745900 training acc: 0.125
Global Iter: 746000 training loss: 1.97345
Global Iter: 746000 training acc: 0.1875
Global Iter: 746100 training loss: 1.97051
Global Iter: 746100 training acc: 0.3125
Global Iter: 746200 training loss: 1.95434
Global Iter: 746200 training acc: 0.125
Global Iter: 746300 training loss: 1.97729
Global Iter: 746300 training acc: 0.25
Global Iter: 746400 training loss: 1.90492
Global Iter: 746400 training acc: 0.1875
Global Iter: 746500 training loss: 2.01365
Global Iter: 746500 training acc: 0.34375
Global Iter: 746600 training loss: 1.92821
Global Iter: 746600 training acc: 0.125
Global Iter: 746700 training loss: 1.95227
Global Iter: 746700 training acc: 0.25
Global Iter: 746800 training loss: 1.84196
Global Iter: 746800 training acc: 0.3125
Global Iter: 746900 training loss: 2.04569
Global Iter: 746900 training acc: 0.15625
Global Iter: 747000 training loss: 2.0176
Global Iter: 747000 training acc: 0.21875
Global Iter: 747100 training loss: 1.92466
Global Iter: 747100 training acc: 0.3125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-747128
Number of Patches: 187859
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-747128
Global Iter: 747200 training loss: 2.09849
Global Iter: 747200 training acc: 0.1875
Global Iter: 747300 training loss: 2.08803
Global Iter: 747300 training acc: 0.125
Global Iter: 747400 training loss: 2.05453
Global Iter: 747400 training acc: 0.15625
Global Iter: 747500 training loss: 1.92461
Global Iter: 747500 training acc: 0.28125
Global Iter: 747600 training loss: 1.92505
Global Iter: 747600 training acc: 0.21875
Global Iter: 747700 training loss: 1.98988
Global Iter: 747700 training acc: 0.1875
Global Iter: 747800 training loss: 1.90603
Global Iter: 747800 training acc: 0.21875
Global Iter: 747900 training loss: 1.93631
Global Iter: 747900 training acc: 0.25
Global Iter: 748000 training loss: 2.02211
Global Iter: 748000 training acc: 0.15625
Global Iter: 748100 training loss: 1.99842
Global Iter: 748100 training acc: 0.1875
Global Iter: 748200 training loss: 2.00318
Global Iter: 748200 training acc: 0.0625
Global Iter: 748300 training loss: 1.99303
Global Iter: 748300 training acc: 0.125
Global Iter: 748400 training loss: 2.14261
Global Iter: 748400 training acc: 0.1875
Global Iter: 748500 training loss: 2.09394
Global Iter: 748500 training acc: 0.125
Global Iter: 748600 training loss: 1.8694
Global Iter: 748600 training acc: 0.25
Global Iter: 748700 training loss: 1.97611
Global Iter: 748700 training acc: 0.125
Global Iter: 748800 training loss: 2.00874
Global Iter: 748800 training acc: 0.09375
Global Iter: 748900 training loss: 1.92315
Global Iter: 748900 training acc: 0.1875
Global Iter: 749000 training loss: 2.03215
Global Iter: 749000 training acc: 0.21875
Global Iter: 749100 training loss: 1.95673
Global Iter: 749100 training acc: 0.1875
Global Iter: 749200 training loss: 1.86684
Global Iter: 749200 training acc: 0.34375
Global Iter: 749300 training loss: 1.94158
Global Iter: 749300 training acc: 0.15625
Global Iter: 749400 training loss: 1.98667
Global Iter: 749400 training acc: 0.3125
Global Iter: 749500 training loss: 1.933
Global Iter: 749500 training acc: 0.34375
Global Iter: 749600 training loss: 2.09945
Global Iter: 749600 training acc: 0.15625
Global Iter: 749700 training loss: 2.05485
Global Iter: 749700 training acc: 0.15625
Global Iter: 749800 training loss: 1.98502
Global Iter: 749800 training acc: 0.21875
Global Iter: 749900 training loss: 1.90071
Global Iter: 749900 training acc: 0.21875
Global Iter: 750000 training loss: 1.95394
Global Iter: 750000 training acc: 0.25
Global Iter: 750100 training loss: 1.96993
Global Iter: 750100 training acc: 0.15625
Global Iter: 750200 training loss: 1.94169
Global Iter: 750200 training acc: 0.25
Global Iter: 750300 training loss: 2.11179
Global Iter: 750300 training acc: 0.0625
Global Iter: 750400 training loss: 2.02278
Global Iter: 750400 training acc: 0.25
Global Iter: 750500 training loss: 1.98646
Global Iter: 750500 training acc: 0.09375
Global Iter: 750600 training loss: 1.97254
Global Iter: 750600 training acc: 0.15625
Global Iter: 750700 training loss: 2.05895
Global Iter: 750700 training acc: 0.125
Global Iter: 750800 training loss: 1.95066
Global Iter: 750800 training acc: 0.09375
Global Iter: 750900 training loss: 1.95072
Global Iter: 750900 training acc: 0.25
Global Iter: 751000 training loss: 2.03472
Global Iter: 751000 training acc: 0.1875
Global Iter: 751100 training loss: 2.04329
Global Iter: 751100 training acc: 0.15625
Global Iter: 751200 training loss: 1.98392
Global Iter: 751200 training acc: 0.21875
Global Iter: 751300 training loss: 1.95697
Global Iter: 751300 training acc: 0.28125
Global Iter: 751400 training loss: 2.12352
Global Iter: 751400 training acc: 0.15625
Global Iter: 751500 training loss: 1.9616
Global Iter: 751500 training acc: 0.15625
Global Iter: 751600 training loss: 1.96496
Global Iter: 751600 training acc: 0.125
Global Iter: 751700 training loss: 1.94505
Global Iter: 751700 training acc: 0.25
Global Iter: 751800 training loss: 1.92571
Global Iter: 751800 training acc: 0.25
Global Iter: 751900 training loss: 2.08404
Global Iter: 751900 training acc: 0.21875
Global Iter: 752000 training loss: 2.00401
Global Iter: 752000 training acc: 0.15625
Global Iter: 752100 training loss: 2.05763
Global Iter: 752100 training acc: 0.21875
Global Iter: 752200 training loss: 1.93924
Global Iter: 752200 training acc: 0.3125
Global Iter: 752300 training loss: 1.98598
Global Iter: 752300 training acc: 0.21875
Global Iter: 752400 training loss: 1.95958
Global Iter: 752400 training acc: 0.1875
Global Iter: 752500 training loss: 1.91364
Global Iter: 752500 training acc: 0.28125
Global Iter: 752600 training loss: 1.89592
Global Iter: 752600 training acc: 0.1875
Global Iter: 752700 training loss: 1.94826
Global Iter: 752700 training acc: 0.1875
Global Iter: 752800 training loss: 1.94308
Global Iter: 752800 training acc: 0.15625
Global Iter: 752900 training loss: 2.01847
Global Iter: 752900 training acc: 0.15625
Global Iter: 753000 training loss: 2.0201
Global Iter: 753000 training acc: 0.21875
Global Iter: 753100 training loss: 1.99718
Global Iter: 753100 training acc: 0.125
Global Iter: 753200 training loss: 1.90376
Global Iter: 753200 training acc: 0.25
Global Iter: 753300 training loss: 2.03291
Global Iter: 753300 training acc: 0.125
Global Iter: 753400 training loss: 1.92476
Global Iter: 753400 training acc: 0.21875
Global Iter: 753500 training loss: 1.91587
Global Iter: 753500 training acc: 0.15625
Global Iter: 753600 training loss: 2.0271
Global Iter: 753600 training acc: 0.21875
Global Iter: 753700 training loss: 2.07208
Global Iter: 753700 training acc: 0.125
Global Iter: 753800 training loss: 1.89489
Global Iter: 753800 training acc: 0.28125
Global Iter: 753900 training loss: 2.0012
Global Iter: 753900 training acc: 0.15625
Global Iter: 754000 training loss: 1.94844
Global Iter: 754000 training acc: 0.21875
Global Iter: 754100 training loss: 2.02745
Global Iter: 754100 training acc: 0.15625
Global Iter: 754200 training loss: 1.97574
Global Iter: 754200 training acc: 0.125
Global Iter: 754300 training loss: 1.97596
Global Iter: 754300 training acc: 0.21875
Global Iter: 754400 training loss: 1.99704
Global Iter: 754400 training acc: 0.25
Global Iter: 754500 training loss: 2.09263
Global Iter: 754500 training acc: 0.125
Global Iter: 754600 training loss: 1.85511
Global Iter: 754600 training acc: 0.3125
Global Iter: 754700 training loss: 2.01271
Global Iter: 754700 training acc: 0.125
Global Iter: 754800 training loss: 1.90164
Global Iter: 754800 training acc: 0.34375
Global Iter: 754900 training loss: 1.92885
Global Iter: 754900 training acc: 0.15625
Global Iter: 755000 training loss: 1.99014
Global Iter: 755000 training acc: 0.1875
Global Iter: 755100 training loss: 1.91183
Global Iter: 755100 training acc: 0.25
Globa2017-06-22 02:43:01.298489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-758870
l Iter: 755200 training loss: 1.96271
Global Iter: 755200 training acc: 0.09375
Global Iter: 755300 training loss: 1.93605
Global Iter: 755300 training acc: 0.21875
Global Iter: 755400 training loss: 1.97963
Global Iter: 755400 training acc: 0.0625
Global Iter: 755500 training loss: 2.09852
Global Iter: 755500 training acc: 0.09375
Global Iter: 755600 training loss: 2.0914
Global Iter: 755600 training acc: 0.1875
Global Iter: 755700 training loss: 1.85353
Global Iter: 755700 training acc: 0.34375
Global Iter: 755800 training loss: 1.9578
Global Iter: 755800 training acc: 0.15625
Global Iter: 755900 training loss: 1.89125
Global Iter: 755900 training acc: 0.34375
Global Iter: 756000 training loss: 2.10829
Global Iter: 756000 training acc: 0.0625
Global Iter: 756100 training loss: 1.98624
Global Iter: 756100 training acc: 0.09375
Global Iter: 756200 training loss: 1.94851
Global Iter: 756200 training acc: 0.25
Global Iter: 756300 training loss: 1.99841
Global Iter: 756300 training acc: 0.09375
Global Iter: 756400 training loss: 2.00192
Global Iter: 756400 training acc: 0.21875
Global Iter: 756500 training loss: 1.97818
Global Iter: 756500 training acc: 0.1875
Global Iter: 756600 training loss: 1.98308
Global Iter: 756600 training acc: 0.1875
Global Iter: 756700 training loss: 1.96201
Global Iter: 756700 training acc: 0.3125
Global Iter: 756800 training loss: 1.93217
Global Iter: 756800 training acc: 0.1875
Global Iter: 756900 training loss: 2.01831
Global Iter: 756900 training acc: 0.125
Global Iter: 757000 training loss: 2.02351
Global Iter: 757000 training acc: 0.25
Global Iter: 757100 training loss: 1.98507
Global Iter: 757100 training acc: 0.1875
Global Iter: 757200 training loss: 2.04028
Global Iter: 757200 training acc: 0.21875
Global Iter: 757300 training loss: 2.04232
Global Iter: 757300 training acc: 0.125
Global Iter: 757400 training loss: 1.99061
Global Iter: 757400 training acc: 0.1875
Global Iter: 757500 training loss: 1.95087
Global Iter: 757500 training acc: 0.15625
Global Iter: 757600 training loss: 2.02038
Global Iter: 757600 training acc: 0.125
Global Iter: 757700 training loss: 1.90154
Global Iter: 757700 training acc: 0.1875
Global Iter: 757800 training loss: 1.97907
Global Iter: 757800 training acc: 0.125
Global Iter: 757900 training loss: 1.94349
Global Iter: 757900 training acc: 0.15625
Global Iter: 758000 training loss: 1.92422
Global Iter: 758000 training acc: 0.25
Global Iter: 758100 training loss: 2.07355
Global Iter: 758100 training acc: 0.25
Global Iter: 758200 training loss: 1.97379
Global Iter: 758200 training acc: 0.21875
Global Iter: 758300 training loss: 1.94783
Global Iter: 758300 training acc: 0.1875
Global Iter: 758400 training loss: 1.82334
Global Iter: 758400 training acc: 0.34375
Global Iter: 758500 training loss: 1.99182
Global Iter: 758500 training acc: 0.21875
Global Iter: 758600 training loss: 2.12156
Global Iter: 758600 training acc: 0.125
Global Iter: 758700 training loss: 1.88338
Global Iter: 758700 training acc: 0.09375
Global Iter: 758800 training loss: 1.9375
Global Iter: 758800 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-758870
Number of Patches: 185981
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-758870
Global Iter: 758900 training loss: 2.0212
Global Iter: 758900 training acc: 0.15625
Global Iter: 759000 training loss: 1.95385
Global Iter: 759000 training acc: 0.15625
Global Iter: 759100 training loss: 1.90138
Global Iter: 759100 training acc: 0.15625
Global Iter: 759200 training loss: 1.90167
Global Iter: 759200 training acc: 0.21875
Global Iter: 759300 training loss: 1.99312
Global Iter: 759300 training acc: 0.21875
Global Iter: 759400 training loss: 2.01812
Global Iter: 759400 training acc: 0.21875
Global Iter: 759500 training loss: 2.04879
Global Iter: 759500 training acc: 0.09375
Global Iter: 759600 training loss: 2.01377
Global Iter: 759600 training acc: 0.125
Global Iter: 759700 training loss: 1.99878
Global Iter: 759700 training acc: 0.28125
Global Iter: 759800 training loss: 1.96152
Global Iter: 759800 training acc: 0.15625
Global Iter: 759900 training loss: 2.14602
Global Iter: 759900 training acc: 0.125
Global Iter: 760000 training loss: 1.98587
Global Iter: 760000 training acc: 0.34375
Global Iter: 760100 training loss: 2.05209
Global Iter: 760100 training acc: 0.09375
Global Iter: 760200 training loss: 2.04949
Global Iter: 760200 training acc: 0.1875
Global Iter: 760300 training loss: 2.07719
Global Iter: 760300 training acc: 0.125
Global Iter: 760400 training loss: 1.95032
Global Iter: 760400 training acc: 0.1875
Global Iter: 760500 training loss: 1.97066
Global Iter: 760500 training acc: 0.0625
Global Iter: 760600 training loss: 2.00569
Global Iter: 760600 training acc: 0.25
Global Iter: 760700 training loss: 1.95232
Global Iter: 760700 training acc: 0.125
Global Iter: 760800 training loss: 1.93808
Global Iter: 760800 training acc: 0.21875
Global Iter: 760900 training loss: 1.95426
Global Iter: 760900 training acc: 0.15625
Global Iter: 761000 training loss: 1.99212
Global Iter: 761000 training acc: 0.15625
Global Iter: 761100 training loss: 1.92908
Global Iter: 761100 training acc: 0.15625
Global Iter: 761200 training loss: 1.99941
Global Iter: 761200 training acc: 0.125
Global Iter: 761300 training loss: 2.05943
Global Iter: 761300 training acc: 0.25
Global Iter: 761400 training loss: 2.08453
Global Iter: 761400 training acc: 0.0625
Global Iter: 761500 training loss: 1.92452
Global Iter: 761500 training acc: 0.28125
Global Iter: 761600 training loss: 1.86228
Global Iter: 761600 training acc: 0.25
Global Iter: 761700 training loss: 2.07408
Global Iter: 761700 training acc: 0.28125
Global Iter: 761800 training loss: 1.959
Global Iter: 761800 training acc: 0.03125
Global Iter: 761900 training loss: 2.04434
Global Iter: 761900 training acc: 0.1875
Global Iter: 762000 training loss: 1.92417
Global Iter: 762000 training acc: 0.09375
Global Iter: 762100 training loss: 2.06921
Global Iter: 762100 training acc: 0.1875
Global Iter: 762200 training loss: 1.97577
Global Iter: 762200 training acc: 0.21875
Global Iter: 762300 training loss: 1.96091
Global Iter: 762300 training acc: 0.09375
Global Iter: 762400 training loss: 1.9206
Global Iter: 762400 training acc: 0.21875
Global Iter: 762500 training loss: 2.04452
Global Iter: 762500 training acc: 0.21875
Global Iter: 762600 training loss: 1.9457
Global Iter: 762600 training acc: 0.21875
Global Iter: 762700 training loss: 1.93039
Global Iter: 762700 training acc: 0.25
Global Iter: 762800 training loss: 1.98795
Global Iter: 762800 training acc: 0.09375
Global Iter: 762900 training loss: 1.91341
Global Iter: 762900 training acc: 0.125
Global Iter: 763000 training loss: 2.0214
Global Iter: 763000 training acc: 0.125
Global Iter: 763100 training loss: 1.8962
Global Iter: 763100 training acc: 0.25
Global Iter: 763200 training loss: 1.96455
Global Iter: 763200 training acc: 0.25
Global Iter: 763300 training loss: 2.00773
Global Iter: 763300 training acc: 0.09375
Global Iter: 763400 training loss: 1.95369
Global Iter: 763400 training acc: 0.25
Global Iter: 763500 training loss: 1.95499
Global Iter: 763500 training acc: 0.1875
Global Iter: 763600 training loss: 1.95489
Global Iter: 763600 training acc: 0.09375
Global Iter: 763700 training loss: 1.99455
Global Iter: 763700 training acc: 0.25
Global Iter: 763800 training loss: 1.97369
Global Iter: 763800 training acc: 0.21875
Global Iter: 763900 training loss: 2.05231
Global Iter: 763900 training acc: 0.15625
Global Iter: 764000 training loss: 1.99155
Global Iter: 764000 training acc: 0.15625
Global Iter: 764100 training loss: 2.0045
Global Iter: 764100 training acc: 0.21875
Global Iter: 764200 training loss: 1.97796
Global Iter: 764200 training acc: 0.1875
Global Iter: 764300 training loss: 2.01269
Global Iter: 764300 training acc: 0.125
Global Iter: 764400 training loss: 1.91733
Global Iter: 764400 training acc: 0.25
Global Iter: 764500 training loss: 2.06451
Global Iter: 764500 training acc: 0.09375
Global Iter: 764600 training loss: 1.99234
Global Iter: 764600 training acc: 0.0625
Global Iter: 764700 training loss: 1.98264
Global Iter: 764700 training acc: 0.09375
Global Iter: 764800 training loss: 1.98503
Global Iter: 764800 training acc: 0.0625
Global Iter: 764900 training loss: 1.98794
Global Iter: 764900 training acc: 0.125
Global Iter: 765000 training loss: 1.90551
Global Iter: 765000 training acc: 0.21875
Global Iter: 765100 training loss: 1.90911
Global Iter: 765100 training acc: 0.21875
Global Iter: 765200 training loss: 1.98269
Global Iter: 765200 training acc: 0.21875
Global Iter: 765300 training loss: 1.92514
Global Iter: 765300 training acc: 0.21875
Global Iter: 765400 training loss: 1.91399
Global Iter: 765400 training acc: 0.1875
Global Iter: 765500 training loss: 2.10588
Global Iter: 765500 training acc: 0.09375
Global Iter: 765600 training loss: 1.90244
Global Iter: 765600 training acc: 0.21875
Global Iter: 765700 training loss: 1.98721
Global Iter: 765700 training acc: 0.125
Global Iter: 765800 training loss: 2.01262
Global Iter: 765800 training acc: 0.15625
Global Iter: 765900 training loss: 2.04382
Global Iter: 765900 training acc: 0.1875
Global Iter: 766000 training loss: 2.20242
Global Iter: 766000 training acc: 0.21875
Global Iter: 766100 training loss: 2.01822
Global Iter: 766100 training acc: 0.09375
Global Iter: 766200 training loss: 2.07223
Global Iter: 766200 training acc: 0.15625
Global Iter: 766300 training loss: 2.15276
Global Iter: 766300 training acc: 0.21875
Global Iter: 766400 training loss: 2.06325
Global Iter: 766400 training acc: 0.1875
Global Iter: 766500 training loss: 1.97727
Global Iter: 766500 training acc: 0.28125
Global Iter: 766600 training loss: 2.04907
Global Iter: 766600 training acc: 0.0625
Global Iter: 766700 training loss: 1.93014
Global Iter: 766700 training acc: 0.15625
Global Iter: 766800 training loss: 1.93843
Global Iter: 766800 training acc: 0.1875
Global Iter: 766900 training loss: 2.04081
Global Iter: 766900 training acc: 0.03125
Global Iter: 767000 training loss: 1.94622
Global Iter: 767000 training acc: 0.15625
Global Iter: 767100 training loss: 1.99923
Global Iter: 767100 training acc: 0.15625
Global Iter: 767200 training loss: 2.12065
Global Iter: 767200 training acc: 0.125
Global Iter: 767300 training loss: 1.96756
Global Iter: 767300 training acc: 0.25
Global Iter: 767400 training loss: 2.05979
Global Iter: 767400 training acc: 0.1875
Global Iter: 767500 training loss: 2.01801
Global Iter: 767500 training acc: 0.1875
Global Iter: 767600 training loss: 1.94147
Global Iter: 767600 training acc: 0.34375
Global Iter: 767700 training loss: 1.94366
Global Iter: 767700 training acc: 0.125
Global Iter: 767800 training loss: 2.00587
Global Iter: 767800 training acc: 0.1875
Global Iter: 767900 training loss: 1.94911
Global Iter: 767900 training acc: 0.28125
Global Iter: 768000 training loss: 2.04023
Global Iter: 768000 training acc: 0.21875
Global Iter: 768100 training loss: 2.02641
Global Iter: 768100 training acc: 0.09375
Global Iter: 768200 training loss: 1.92382
Global Iter: 768200 training acc: 0.15625
Global Iter: 768300 training loss: 2.08342
Global Iter: 768300 training acc: 0.15625
Global Iter: 768400 training loss: 1.92429
Global Iter: 768400 training acc: 0.15625
Global Iter: 768500 training loss: 1.97805
Global Iter: 768500 training acc: 0.28125
Global Iter: 768600 training loss: 2.01802
Global Iter: 768600 training acc: 0.1875
Global Iter: 768700 training loss: 1.98377
Global Iter: 768700 training acc: 0.125
Global Iter: 768800 training loss: 2.08434
Global Iter: 768800 training acc: 0.15625
Global Iter: 768900 training loss: 2.05139
Global Iter: 768900 training acc: 0.125
Global Iter: 769000 training loss: 1.86404
Global Iter: 769000 training acc: 0.15625
Global Iter: 769100 training loss: 1.93743
Global Iter: 769100 training acc: 0.21875
Global Iter: 769200 training loss: 1.97895
Global Iter: 769200 training acc: 0.25
Global Iter: 769300 training loss: 1.82376
Global Iter: 769300 training acc: 0.3125
Global Iter: 769400 training loss: 1.94799
Global Iter: 769400 training acc: 0.1875
Global Iter: 769500 training loss: 2.124232017-06-22 03:03:13.825417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-770494

Global Iter: 769500 training acc: 0.1875
Global Iter: 769600 training loss: 1.86683
Global Iter: 769600 training acc: 0.3125
Global Iter: 769700 training loss: 2.03411
Global Iter: 769700 training acc: 0.15625
Global Iter: 769800 training loss: 2.00702
Global Iter: 769800 training acc: 0.21875
Global Iter: 769900 training loss: 1.9658
Global Iter: 769900 training acc: 0.125
Global Iter: 770000 training loss: 1.95284
Global Iter: 770000 training acc: 0.28125
Global Iter: 770100 training loss: 2.009
Global Iter: 770100 training acc: 0.3125
Global Iter: 770200 training loss: 1.93083
Global Iter: 770200 training acc: 0.21875
Global Iter: 770300 training loss: 2.02011
Global Iter: 770300 training acc: 0.15625
Global Iter: 770400 training loss: 1.99958
Global Iter: 770400 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-770494
Number of Patches: 184122
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-770494
Global Iter: 770500 training loss: 2.03321
Global Iter: 770500 training acc: 0.21875
Global Iter: 770600 training loss: 1.99621
Global Iter: 770600 training acc: 0.21875
Global Iter: 770700 training loss: 1.98554
Global Iter: 770700 training acc: 0.0625
Global Iter: 770800 training loss: 1.95255
Global Iter: 770800 training acc: 0.15625
Global Iter: 770900 training loss: 1.97222
Global Iter: 770900 training acc: 0.21875
Global Iter: 771000 training loss: 1.83696
Global Iter: 771000 training acc: 0.28125
Global Iter: 771100 training loss: 2.02531
Global Iter: 771100 training acc: 0.28125
Global Iter: 771200 training loss: 2.08142
Global Iter: 771200 training acc: 0.1875
Global Iter: 771300 training loss: 1.9588
Global Iter: 771300 training acc: 0.09375
Global Iter: 771400 training loss: 1.97908
Global Iter: 771400 training acc: 0.34375
Global Iter: 771500 training loss: 2.06047
Global Iter: 771500 training acc: 0.15625
Global Iter: 771600 training loss: 1.99861
Global Iter: 771600 training acc: 0.21875
Global Iter: 771700 training loss: 1.99182
Global Iter: 771700 training acc: 0.09375
Global Iter: 771800 training loss: 2.07193
Global Iter: 771800 training acc: 0.0625
Global Iter: 771900 training loss: 1.98221
Global Iter: 771900 training acc: 0.1875
Global Iter: 772000 training loss: 1.96553
Global Iter: 772000 training acc: 0.25
Global Iter: 772100 training loss: 1.9084
Global Iter: 772100 training acc: 0.21875
Global Iter: 772200 training loss: 1.96339
Global Iter: 772200 training acc: 0.1875
Global Iter: 772300 training loss: 2.02302
Global Iter: 772300 training acc: 0.125
Global Iter: 772400 training loss: 1.93281
Global Iter: 772400 training acc: 0.28125
Global Iter: 772500 training loss: 2.0571
Global Iter: 772500 training acc: 0.15625
Global Iter: 772600 training loss: 1.91059
Global Iter: 772600 training acc: 0.25
Global Iter: 772700 training loss: 2.2126
Global Iter: 772700 training acc: 0.0625
Global Iter: 772800 training loss: 1.97588
Global Iter: 772800 training acc: 0.1875
Global Iter: 772900 training loss: 1.9678
Global Iter: 772900 training acc: 0.15625
Global Iter: 773000 training loss: 1.90929
Global Iter: 773000 training acc: 0.1875
Global Iter: 773100 training loss: 1.93226
Global Iter: 773100 training acc: 0.1875
Global Iter: 773200 training loss: 1.98344
Global Iter: 773200 training acc: 0.21875
Global Iter: 773300 training loss: 2.0128
Global Iter: 773300 training acc: 0.1875
Global Iter: 773400 training loss: 1.90683
Global Iter: 773400 training acc: 0.21875
Global Iter: 773500 training loss: 1.92319
Global Iter: 773500 training acc: 0.15625
Global Iter: 773600 training loss: 1.94256
Global Iter: 773600 training acc: 0.15625
Global Iter: 773700 training loss: 1.92354
Global Iter: 773700 training acc: 0.25
Global Iter: 773800 training loss: 2.13047
Global Iter: 773800 training acc: 0.15625
Global Iter: 773900 training loss: 1.9566
Global Iter: 773900 training acc: 0.25
Global Iter: 774000 training loss: 2.02767
Global Iter: 774000 training acc: 0.28125
Global Iter: 774100 training loss: 1.99278
Global Iter: 774100 training acc: 0.25
Global Iter: 774200 training loss: 1.97396
Global Iter: 774200 training acc: 0.15625
Global Iter: 774300 training loss: 2.01084
Global Iter: 774300 training acc: 0.125
Global Iter: 774400 training loss: 2.10279
Global Iter: 774400 training acc: 0.125
Global Iter: 774500 training loss: 1.92553
Global Iter: 774500 training acc: 0.28125
Global Iter: 774600 training loss: 1.9287
Global Iter: 774600 training acc: 0.1875
Global Iter: 774700 training loss: 1.95578
Global Iter: 774700 training acc: 0.125
Global Iter: 774800 training loss: 1.97433
Global Iter: 774800 training acc: 0.0625
Global Iter: 774900 training loss: 2.0292
Global Iter: 774900 training acc: 0.1875
Global Iter: 775000 training loss: 1.91477
Global Iter: 775000 training acc: 0.28125
Global Iter: 775100 training loss: 1.94988
Global Iter: 775100 training acc: 0.125
Global Iter: 775200 training loss: 2.03484
Global Iter: 775200 training acc: 0.21875
Global Iter: 775300 training loss: 2.05373
Global Iter: 775300 training acc: 0.125
Global Iter: 775400 training loss: 2.11651
Global Iter: 775400 training acc: 0.09375
Global Iter: 775500 training loss: 1.91216
Global Iter: 775500 training acc: 0.21875
Global Iter: 775600 training loss: 1.96185
Global Iter: 775600 training acc: 0.25
Global Iter: 775700 training loss: 2.01712
Global Iter: 775700 training acc: 0.125
Global Iter: 775800 training loss: 2.03876
Global Iter: 775800 training acc: 0.15625
Global Iter: 775900 training loss: 1.99452
Global Iter: 775900 training acc: 0.03125
Global Iter: 776000 training loss: 2.08539
Global Iter: 776000 training acc: 0.125
Global Iter: 776100 training loss: 1.88319
Global Iter: 776100 training acc: 0.28125
Global Iter: 776200 training loss: 2.02772
Global Iter: 776200 training acc: 0.28125
Global Iter: 776300 training loss: 1.89398
Global Iter: 776300 training acc: 0.1875
Global Iter: 776400 training loss: 1.96873
Global Iter: 776400 training acc: 0.25
Global Iter: 776500 training loss: 2.01328
Global Iter: 776500 training acc: 0.15625
Global Iter: 776600 training loss: 1.89815
Global Iter: 776600 training acc: 0.15625
Global Iter: 776700 training loss: 1.95536
Global Iter: 776700 training acc: 0.1875
Global Iter: 776800 training loss: 1.99308
Global Iter: 776800 training acc: 0.125
Global Iter: 776900 training loss: 1.94273
Global Iter: 776900 training acc: 0.1875
Global Iter: 777000 training loss: 1.86829
Global Iter: 777000 training acc: 0.25
Global Iter: 777100 training loss: 1.99758
Global Iter: 777100 training acc: 0.0625
Global Iter: 777200 training loss: 2.00473
Global Iter: 777200 training acc: 0.09375
Global Iter: 777300 training loss: 2.10391
Global Iter: 777300 training acc: 0.21875
Global Iter: 777400 training loss: 2.01456
Global Iter: 777400 training acc: 0.1875
Global Iter: 777500 training loss: 2.12098
Global Iter: 777500 training acc: 0.125
Global Iter: 777600 training loss: 2.03322
Global Iter: 777600 training acc: 0.21875
Global Iter: 777700 training loss: 2.09993
Global Iter: 777700 training acc: 0.15625
Global Iter: 777800 training loss: 2.02155
Global Iter: 777800 training acc: 0.125
Global Iter: 777900 training loss: 1.96024
Global Iter: 777900 training acc: 0.15625
Global Iter: 778000 training loss: 2.02154
Global Iter: 778000 training acc: 0.3125
Global Iter: 778100 training loss: 2.05093
Global Iter: 778100 training acc: 0.09375
Global Iter: 778200 training loss: 1.91913
Global Iter: 778200 training acc: 0.1875
Global Iter: 778300 training loss: 1.91503
Global Iter: 778300 training acc: 0.21875
Global Iter: 778400 training loss: 2.06075
Global Iter: 778400 training acc: 0.1875
Global Iter: 778500 training loss: 2.07995
Global Iter: 778500 training acc: 0.09375
Global Iter: 778600 training loss: 2.09201
Global Iter: 778600 training acc: 0.1875
Global Iter: 778700 training loss: 1.9685
Global Iter: 778700 training acc: 0.0625
Global Iter: 778800 training loss: 2.01903
Global Iter: 778800 training acc: 0.21875
Global Iter: 778900 training loss: 2.07185
Global Iter: 778900 training acc: 0.21875
Global Iter: 779000 trai2017-06-22 03:23:05.049754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-782002
ning loss: 2.06008
Global Iter: 779000 training acc: 0.1875
Global Iter: 779100 training loss: 2.08179
Global Iter: 779100 training acc: 0.1875
Global Iter: 779200 training loss: 1.98937
Global Iter: 779200 training acc: 0.28125
Global Iter: 779300 training loss: 2.00059
Global Iter: 779300 training acc: 0.21875
Global Iter: 779400 training loss: 1.91511
Global Iter: 779400 training acc: 0.15625
Global Iter: 779500 training loss: 1.94818
Global Iter: 779500 training acc: 0.15625
Global Iter: 779600 training loss: 2.03467
Global Iter: 779600 training acc: 0.1875
Global Iter: 779700 training loss: 1.94399
Global Iter: 779700 training acc: 0.125
Global Iter: 779800 training loss: 1.93762
Global Iter: 779800 training acc: 0.1875
Global Iter: 779900 training loss: 1.93048
Global Iter: 779900 training acc: 0.15625
Global Iter: 780000 training loss: 2.03069
Global Iter: 780000 training acc: 0.15625
Global Iter: 780100 training loss: 2.04102
Global Iter: 780100 training acc: 0.21875
Global Iter: 780200 training loss: 1.94914
Global Iter: 780200 training acc: 0.15625
Global Iter: 780300 training loss: 1.98281
Global Iter: 780300 training acc: 0.21875
Global Iter: 780400 training loss: 1.91949
Global Iter: 780400 training acc: 0.1875
Global Iter: 780500 training loss: 1.92929
Global Iter: 780500 training acc: 0.125
Global Iter: 780600 training loss: 2.01729
Global Iter: 780600 training acc: 0.1875
Global Iter: 780700 training loss: 1.9292
Global Iter: 780700 training acc: 0.28125
Global Iter: 780800 training loss: 1.94255
Global Iter: 780800 training acc: 0.21875
Global Iter: 780900 training loss: 2.01114
Global Iter: 780900 training acc: 0.125
Global Iter: 781000 training loss: 2.01188
Global Iter: 781000 training acc: 0.125
Global Iter: 781100 training loss: 2.00728
Global Iter: 781100 training acc: 0.21875
Global Iter: 781200 training loss: 1.97919
Global Iter: 781200 training acc: 0.1875
Global Iter: 781300 training loss: 1.92455
Global Iter: 781300 training acc: 0.28125
Global Iter: 781400 training loss: 1.96143
Global Iter: 781400 training acc: 0.25
Global Iter: 781500 training loss: 1.95028
Global Iter: 781500 training acc: 0.25
Global Iter: 781600 training loss: 1.92968
Global Iter: 781600 training acc: 0.09375
Global Iter: 781700 training loss: 2.00496
Global Iter: 781700 training acc: 0.21875
Global Iter: 781800 training loss: 1.97868
Global Iter: 781800 training acc: 0.09375
Global Iter: 781900 training loss: 1.89333
Global Iter: 781900 training acc: 0.34375
Global Iter: 782000 training loss: 1.90761
Global Iter: 782000 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-782002
Number of Patches: 182281
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-782002
Global Iter: 782100 training loss: 1.92598
Global Iter: 782100 training acc: 0.21875
Global Iter: 782200 training loss: 2.05002
Global Iter: 782200 training acc: 0.0625
Global Iter: 782300 training loss: 1.93373
Global Iter: 782300 training acc: 0.1875
Global Iter: 782400 training loss: 1.98718
Global Iter: 782400 training acc: 0.3125
Global Iter: 782500 training loss: 1.9826
Global Iter: 782500 training acc: 0.0625
Global Iter: 782600 training loss: 1.97724
Global Iter: 782600 training acc: 0.125
Global Iter: 782700 training loss: 2.01613
Global Iter: 782700 training acc: 0.09375
Global Iter: 782800 training loss: 2.05474
Global Iter: 782800 training acc: 0.25
Global Iter: 782900 training loss: 1.85131
Global Iter: 782900 training acc: 0.1875
Global Iter: 783000 training loss: 1.94097
Global Iter: 783000 training acc: 0.21875
Global Iter: 783100 training loss: 1.92787
Global Iter: 783100 training acc: 0.15625
Global Iter: 783200 training loss: 2.10893
Global Iter: 783200 training acc: 0.1875
Global Iter: 783300 training loss: 1.96863
Global Iter: 783300 training acc: 0.21875
Global Iter: 783400 training loss: 1.97462
Global Iter: 783400 training acc: 0.25
Global Iter: 783500 training loss: 2.02758
Global Iter: 783500 training acc: 0.09375
Global Iter: 783600 training loss: 2.05903
Global Iter: 783600 training acc: 0.1875
Global Iter: 783700 training loss: 1.97381
Global Iter: 783700 training acc: 0.1875
Global Iter: 783800 training loss: 2.02228
Global Iter: 783800 training acc: 0.15625
Global Iter: 783900 training loss: 1.93646
Global Iter: 783900 training acc: 0.21875
Global Iter: 784000 training loss: 2.01097
Global Iter: 784000 training acc: 0.15625
Global Iter: 784100 training loss: 1.96751
Global Iter: 784100 training acc: 0.09375
Global Iter: 784200 training loss: 1.96261
Global Iter: 784200 training acc: 0.125
Global Iter: 784300 training loss: 2.01239
Global Iter: 784300 training acc: 0.09375
Global Iter: 784400 training loss: 1.99851
Global Iter: 784400 training acc: 0.15625
Global Iter: 784500 training loss: 1.8684
Global Iter: 784500 training acc: 0.40625
Global Iter: 784600 training loss: 1.96113
Global Iter: 784600 training acc: 0.25
Global Iter: 784700 training loss: 1.95182
Global Iter: 784700 training acc: 0.21875
Global Iter: 784800 training loss: 1.95059
Global Iter: 784800 training acc: 0.21875
Global Iter: 784900 training loss: 2.02607
Global Iter: 784900 training acc: 0.28125
Global Iter: 785000 training loss: 2.04616
Global Iter: 785000 training acc: 0.15625
Global Iter: 785100 training loss: 2.05203
Global Iter: 785100 training acc: 0.21875
Global Iter: 785200 training loss: 2.00881
Global Iter: 785200 training acc: 0.09375
Global Iter: 785300 training loss: 2.01288
Global Iter: 785300 training acc: 0.125
Global Iter: 785400 training loss: 2.05479
Global Iter: 785400 training acc: 0.25
Global Iter: 785500 training loss: 2.06211
Global Iter: 785500 training acc: 0.15625
Global Iter: 785600 training loss: 1.91109
Global Iter: 785600 training acc: 0.25
Global Iter: 785700 training loss: 2.04278
Global Iter: 785700 training acc: 0.125
Global Iter: 785800 training loss: 2.04288
Global Iter: 785800 training acc: 0.125
Global Iter: 785900 training loss: 2.02333
Global Iter: 785900 training acc: 0.15625
Global Iter: 786000 training loss: 2.05382
Global Iter: 786000 training acc: 0.15625
Global Iter: 786100 training loss: 1.96425
Global Iter: 786100 training acc: 0.09375
Global Iter: 786200 training loss: 2.10836
Global Iter: 786200 training acc: 0.25
Global Iter: 786300 training loss: 1.93996
Global Iter: 786300 training acc: 0.28125
Global Iter: 786400 training loss: 1.95222
Global Iter: 786400 training acc: 0.3125
Global Iter: 786500 training loss: 1.98557
Global Iter: 786500 training acc: 0.1875
Global Iter: 786600 training loss: 2.01256
Global Iter: 786600 training acc: 0.125
Global Iter: 786700 training loss: 2.08561
Global Iter: 786700 training acc: 0.1875
Global Iter: 786800 training loss: 2.06147
Global Iter: 786800 training acc: 0.1875
Global Iter: 786900 training loss: 2.0334
Global Iter: 786900 training acc: 0.125
Global Iter: 787000 training loss: 1.91065
Global Iter: 787000 training acc: 0.09375
Global Iter: 787100 training loss: 1.90645
Global Iter: 787100 training acc: 0.34375
Global Iter: 787200 training loss: 1.91507
Global Iter: 787200 training acc: 0.1875
Global Iter: 787300 training loss: 1.96332
Global Iter: 787300 training acc: 0.0625
Global Iter: 787400 training loss: 2.01179
Global Iter: 787400 training acc: 0.28125
Global Iter: 787500 training loss: 1.99481
Global Iter: 787500 training acc: 0.15625
Global Iter: 787600 training loss: 2.02236
Global Iter: 787600 training acc: 0.09375
Global Iter: 787700 training loss: 1.87944
Global Iter: 787700 training acc: 0.21875
Global Iter: 787800 training loss: 1.99734
Global Iter: 787800 training acc: 0.09375
Global Iter: 787900 training loss: 1.98607
Global Iter: 787900 training acc: 0.25
Global Iter: 788000 training loss: 1.98652
Global Iter: 788000 training acc: 0.1875
Global Iter: 788100 training loss: 1.98301
Global Iter: 788100 training acc: 0.1875
Global Iter: 788200 training loss: 1.96655
Global Iter: 788200 training acc: 0.09375
Global Iter: 788300 training loss: 1.94562
Global Iter: 788300 training acc: 0.21875
Global Iter: 788400 training loss: 1.95199
Global Iter: 788400 training acc: 0.25
Global Iter: 788500 training loss: 1.92739
Global Iter: 788500 training acc: 0.28125
Global Iter: 788600 training loss: 1.8939
Global Iter: 788600 training acc: 0.28125
Global Iter: 788700 training loss: 2.15127
Global Iter: 788700 training acc: 0.125
Global Iter: 788800 training loss: 1.9992
Global Iter: 788800 training acc: 0.15625
Global Iter: 788900 training loss: 1.83666
Global Iter: 788900 training acc: 0.34375
Global Iter: 789000 training loss: 1.99016
Global Iter: 789000 training acc: 0.21875
Global Iter: 789100 training loss: 1.87097
Global Iter: 789100 training acc: 0.15625
Global Iter: 789200 training loss: 1.93401
Global Iter: 789200 training acc: 0.09375
Global Iter: 789300 training loss: 1.98874
Global Iter: 789300 training acc: 0.25
Global Iter: 789400 training loss: 2.03096
Global Iter: 789400 training acc: 0.09375
Global Iter: 789500 training loss: 1.97684
Global Iter: 789500 training acc: 0.1875
Global Iter: 789600 training loss: 1.97589
Global Iter: 789600 training acc: 0.1875
Global Iter: 789700 training loss: 1.97091
Global Iter: 789700 training acc: 0.25
Global Iter: 789800 training loss: 1.93803
Global Iter: 789800 training acc: 0.0625
Global Iter: 789900 training loss: 1.98243
Global Iter: 789900 training acc: 0.1875
Global Iter: 790000 training loss: 1.94817
Global Iter: 790000 training acc: 0.15625
Global Iter: 790100 training loss: 2.01641
Global Iter: 790100 training acc: 0.21875
Global Iter: 790200 training loss: 1.8763
Global Iter: 790200 training acc: 0.15625
Global Iter: 790300 training loss: 2.02883
Global Iter: 790300 training acc: 0.21875
Global Iter: 790400 training loss: 2.04071
Global Iter: 790400 training acc: 0.1875
Global Iter: 790500 training loss: 1.94881
Global Iter: 790500 training acc: 0.21875
Global Iter: 790600 training loss: 1.99282
Global Iter: 790600 training acc: 0.25
Global Iter: 790700 training loss: 2.0279
Global Iter: 790700 training acc: 0.21875
Global Iter: 790800 training loss: 1.97937
Global Iter: 790800 training acc: 0.15625
Global Iter: 790900 training loss: 2.00334
Global Iter: 790900 training acc: 0.21875
Global Iter: 791000 training loss: 1.99053
Global Iter: 791000 training acc: 0.28125
Global Iter: 791100 training loss: 2.09867
Global Iter: 791100 training acc: 0.1875
Global Iter: 791200 training loss: 1.93985
Global Iter: 791200 training acc: 0.1875
Global Iter: 791300 training loss: 2.12
Global Iter: 791300 training acc: 0.15625
Global Iter: 791400 training loss: 1.9289
Global Iter: 791400 training acc: 0.15625
Global Iter: 791500 training loss: 1.88404
Global Iter: 791500 training acc: 0.25
Global Iter: 791600 training loss: 1.97458
Global Iter: 791600 training acc: 0.1875
Global Iter: 791700 training loss: 2.00423
Global Iter: 791700 training acc: 0.25
Global Iter: 791800 training loss: 1.89131
Global Iter: 791800 training acc: 0.3125
Global Iter: 791900 training loss: 2.03632
Global Iter: 791900 training acc: 0.21875
Global Iter: 792000 training loss: 1.90137
Global Iter: 792000 training acc: 0.21875
Global Iter: 792100 training loss: 1.95245
Global Iter: 792100 training acc: 0.1875
Global Iter: 792200 training loss: 1.91494
Global Iter: 792200 training acc: 0.3125
Global Iter: 792300 training loss: 2.00664
Global Iter: 792300 training acc: 0.25
Global Iter: 792400 training loss: 1.88534
Global Iter: 792400 training acc: 0.3125
Global Iter: 792500 training loss: 1.92165
Global Iter: 792500 training acc: 0.1875
Global Iter: 792600 training loss: 1.94141
Global Iter: 792600 training acc: 0.34375
Global Iter: 792700 training loss: 1.99076
Global Iter: 792700 training acc: 0.25
Global Iter: 792800 training loss: 1.96805
Global Iter: 792800 training acc: 0.15625
Global Iter: 792900 training loss: 1.99886
Global Iter: 792900 training acc: 0.125
Global Iter: 793000 training loss: 1.92952
Global Iter: 793000 training acc: 0.25
Global Iter: 793100 training loss: 2.05836
Global Iter: 793100 training acc: 0.25
Global Iter: 793200 training loss: 2.0741
Global Iter: 793200 training acc: 0.15625
Global Iter: 793300 training loss: 1.85901
Global Iter: 793300 tra2017-06-22 03:42:32.404410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-793395
ining acc: 0.3125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-793395
Number of Patches: 180459
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-793395
Global Iter: 793400 training loss: 2.05047
Global Iter: 793400 training acc: 0.28125
Global Iter: 793500 training loss: 1.91694
Global Iter: 793500 training acc: 0.25
Global Iter: 793600 training loss: 2.05586
Global Iter: 793600 training acc: 0.21875
Global Iter: 793700 training loss: 1.95584
Global Iter: 793700 training acc: 0.125
Global Iter: 793800 training loss: 2.05168
Global Iter: 793800 training acc: 0.1875
Global Iter: 793900 training loss: 1.96997
Global Iter: 793900 training acc: 0.125
Global Iter: 794000 training loss: 1.98011
Global Iter: 794000 training acc: 0.1875
Global Iter: 794100 training loss: 2.02319
Global Iter: 794100 training acc: 0.21875
Global Iter: 794200 training loss: 2.03345
Global Iter: 794200 training acc: 0.28125
Global Iter: 794300 training loss: 1.96906
Global Iter: 794300 training acc: 0.125
Global Iter: 794400 training loss: 2.01855
Global Iter: 794400 training acc: 0.15625
Global Iter: 794500 training loss: 1.98612
Global Iter: 794500 training acc: 0.1875
Global Iter: 794600 training loss: 2.07663
Global Iter: 794600 training acc: 0.09375
Global Iter: 794700 training loss: 2.11973
Global Iter: 794700 training acc: 0.125
Global Iter: 794800 training loss: 2.00064
Global Iter: 794800 training acc: 0.1875
Global Iter: 794900 training loss: 1.98174
Global Iter: 794900 training acc: 0.1875
Global Iter: 795000 training loss: 1.91722
Global Iter: 795000 training acc: 0.1875
Global Iter: 795100 training loss: 1.95604
Global Iter: 795100 training acc: 0.21875
Global Iter: 795200 training loss: 1.99119
Global Iter: 795200 training acc: 0.25
Global Iter: 795300 training loss: 1.90376
Global Iter: 795300 training acc: 0.25
Global Iter: 795400 training loss: 2.08786
Global Iter: 795400 training acc: 0.09375
Global Iter: 795500 training loss: 1.96163
Global Iter: 795500 training acc: 0.15625
Global Iter: 795600 training loss: 1.98922
Global Iter: 795600 training acc: 0.09375
Global Iter: 795700 training loss: 2.03664
Global Iter: 795700 training acc: 0.0
Global Iter: 795800 training loss: 1.95495
Global Iter: 795800 training acc: 0.21875
Global Iter: 795900 training loss: 2.07787
Global Iter: 795900 training acc: 0.0625
Global Iter: 796000 training loss: 1.98344
Global Iter: 796000 training acc: 0.3125
Global Iter: 796100 training loss: 1.98826
Global Iter: 796100 training acc: 0.125
Global Iter: 796200 training loss: 1.8968
Global Iter: 796200 training acc: 0.25
Global Iter: 796300 training loss: 1.97883
Global Iter: 796300 training acc: 0.15625
Global Iter: 796400 training loss: 1.98032
Global Iter: 796400 training acc: 0.15625
Global Iter: 796500 training loss: 2.15047
Global Iter: 796500 training acc: 0.15625
Global Iter: 796600 training loss: 1.96793
Global Iter: 796600 training acc: 0.09375
Global Iter: 796700 training loss: 2.16929
Global Iter: 796700 training acc: 0.21875
Global Iter: 796800 training loss: 2.01998
Global Iter: 796800 training acc: 0.125
Global Iter: 796900 training loss: 2.01016
Global Iter: 796900 training acc: 0.1875
Global Iter: 797000 training loss: 1.93035
Global Iter: 797000 training acc: 0.21875
Global Iter: 797100 training loss: 2.00623
Global Iter: 797100 training acc: 0.21875
Global Iter: 797200 training loss: 2.11534
Global Iter: 797200 training acc: 0.125
Global Iter: 797300 training loss: 2.03545
Global Iter: 797300 training acc: 0.21875
Global Iter: 797400 training loss: 1.98449
Global Iter: 797400 training acc: 0.28125
Global Iter: 797500 training loss: 2.04427
Global Iter: 797500 training acc: 0.1875
Global Iter: 797600 training loss: 1.91648
Global Iter: 797600 training acc: 0.1875
Global Iter: 797700 training loss: 2.04583
Global Iter: 797700 training acc: 0.21875
Global Iter: 797800 training loss: 1.96118
Global Iter: 797800 training acc: 0.15625
Global Iter: 797900 training loss: 1.92957
Global Iter: 797900 training acc: 0.28125
Global Iter: 798000 training loss: 2.171
Global Iter: 798000 training acc: 0.1875
Global Iter: 798100 training loss: 1.9206
Global Iter: 798100 training acc: 0.3125
Global Iter: 798200 training loss: 2.00349
Global Iter: 798200 training acc: 0.1875
Global Iter: 798300 training loss: 1.9642
Global Iter: 798300 training acc: 0.1875
Global Iter: 798400 training loss: 1.97729
Global Iter: 798400 training acc: 0.25
Global Iter: 798500 training loss: 1.94961
Global Iter: 798500 training acc: 0.15625
Global Iter: 798600 training loss: 1.93191
Global Iter: 798600 training acc: 0.15625
Global Iter: 798700 training loss: 1.98872
Global Iter: 798700 training acc: 0.15625
Global Iter: 798800 training loss: 1.98991
Global Iter: 798800 training acc: 0.15625
Global Iter: 798900 training loss: 1.94421
Global Iter: 798900 training acc: 0.28125
Global Iter: 799000 training loss: 2.05513
Global Iter: 799000 training acc: 0.09375
Global Iter: 799100 training loss: 1.95965
Global Iter: 799100 training acc: 0.21875
Global Iter: 799200 training loss: 1.86624
Global Iter: 799200 training acc: 0.125
Global Iter: 799300 training loss: 1.91463
Global Iter: 799300 training acc: 0.09375
Global Iter: 799400 training loss: 1.89772
Global Iter: 799400 training acc: 0.1875
Global Iter: 799500 training loss: 2.06967
Global Iter: 799500 training acc: 0.09375
Global Iter: 799600 training loss: 1.87829
Global Iter: 799600 training acc: 0.3125
Global Iter: 799700 training loss: 1.99906
Global Iter: 799700 training acc: 0.09375
Global Iter: 799800 training loss: 1.94857
Global Iter: 799800 training acc: 0.15625
Global Iter: 799900 training loss: 1.89754
Global Iter: 799900 training acc: 0.28125
Global Iter: 800000 training loss: 2.00734
Global Iter: 800000 training acc: 0.09375
Global Iter: 800100 training loss: 2.03114
Global Iter: 800100 training acc: 0.09375
Global Iter: 800200 training loss: 1.95159
Global Iter: 800200 training acc: 0.1875
Global Iter: 800300 training loss: 2.01547
Global Iter: 800300 training acc: 0.1875
Global Iter: 800400 training loss: 1.95503
Global Iter: 800400 training acc: 0.15625
Global Iter: 800500 training loss: 2.05399
Global Iter: 800500 training acc: 0.09375
Global Iter: 800600 training loss: 2.03166
Global Iter: 800600 training acc: 0.125
Global Iter: 800700 training loss: 2.08595
Global Iter: 800700 training acc: 0.1875
Global Iter: 800800 training loss: 2.0459
Global Iter: 800800 training acc: 0.1875
Global Iter: 800900 training loss: 1.89487
Global Iter: 800900 training acc: 0.34375
Global Iter: 801000 training loss: 1.88743
Global Iter: 801000 training acc: 0.1875
Global Iter: 801100 training loss: 2.00064
Global Iter: 801100 training acc: 0.125
Global Iter: 801200 training loss: 1.926
Global Iter: 801200 training acc: 0.25
Global Iter: 801300 training loss: 2.02398
Global Iter: 801300 training acc: 0.0625
Global Iter: 801400 training loss: 1.93157
Global Iter: 801400 training acc: 0.25
Global Iter: 801500 training loss: 2.0424
Global Iter: 801500 training acc: 0.1875
Global Iter: 801600 training loss: 1.93955
Global Iter: 801600 training acc: 0.15625
Global Iter: 801700 training loss: 1.95528
Global Iter: 801700 training acc: 0.1875
Global Iter: 801800 training loss: 1.99917
Global Iter: 801800 training acc: 0.1875
Global Iter: 801900 training loss: 2.04714
Global Iter: 801900 training acc: 0.09375
Global Iter: 802000 training loss: 2.05329
Global Iter: 802000 training acc: 0.125
Global Iter: 802100 training loss: 2.03936
Global Iter: 802100 training acc: 0.15625
Global Iter: 802200 training loss: 1.99254
Global Iter: 802200 training acc: 0.1875
Global Iter: 802300 training loss: 1.93605
Global Iter: 802300 training acc: 0.125
Global Iter: 802400 training loss: 2.06948
Global Iter: 802400 training acc: 0.09375
Global Iter: 802500 training loss: 1.93583
Global Iter: 802500 training acc: 0.21875
Global Iter: 802600 training loss: 1.97996
Global Iter: 802600 training acc: 0.1875
Global Iter: 802700 training loss: 2.09495
Global Iter: 802700 training acc: 0.09375
Global Iter: 802800 training loss: 1.97548
Glob2017-06-22 04:01:51.610152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-804674
al Iter: 802800 training acc: 0.34375
Global Iter: 802900 training loss: 1.90837
Global Iter: 802900 training acc: 0.25
Global Iter: 803000 training loss: 1.93315
Global Iter: 803000 training acc: 0.28125
Global Iter: 803100 training loss: 2.00935
Global Iter: 803100 training acc: 0.09375
Global Iter: 803200 training loss: 1.96774
Global Iter: 803200 training acc: 0.1875
Global Iter: 803300 training loss: 1.96909
Global Iter: 803300 training acc: 0.25
Global Iter: 803400 training loss: 2.07301
Global Iter: 803400 training acc: 0.09375
Global Iter: 803500 training loss: 1.99467
Global Iter: 803500 training acc: 0.28125
Global Iter: 803600 training loss: 2.07608
Global Iter: 803600 training acc: 0.15625
Global Iter: 803700 training loss: 2.03122
Global Iter: 803700 training acc: 0.28125
Global Iter: 803800 training loss: 1.97896
Global Iter: 803800 training acc: 0.28125
Global Iter: 803900 training loss: 2.02549
Global Iter: 803900 training acc: 0.28125
Global Iter: 804000 training loss: 2.0944
Global Iter: 804000 training acc: 0.1875
Global Iter: 804100 training loss: 2.06445
Global Iter: 804100 training acc: 0.0625
Global Iter: 804200 training loss: 1.92766
Global Iter: 804200 training acc: 0.3125
Global Iter: 804300 training loss: 2.01456
Global Iter: 804300 training acc: 0.125
Global Iter: 804400 training loss: 2.14056
Global Iter: 804400 training acc: 0.1875
Global Iter: 804500 training loss: 2.05722
Global Iter: 804500 training acc: 0.21875
Global Iter: 804600 training loss: 1.97116
Global Iter: 804600 training acc: 0.3125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-804674
Number of Patches: 178655
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-804674
Global Iter: 804700 training loss: 1.93704
Global Iter: 804700 training acc: 0.28125
Global Iter: 804800 training loss: 1.99054
Global Iter: 804800 training acc: 0.15625
Global Iter: 804900 training loss: 2.05233
Global Iter: 804900 training acc: 0.21875
Global Iter: 805000 training loss: 2.05078
Global Iter: 805000 training acc: 0.15625
Global Iter: 805100 training loss: 2.03593
Global Iter: 805100 training acc: 0.125
Global Iter: 805200 training loss: 1.98185
Global Iter: 805200 training acc: 0.25
Global Iter: 805300 training loss: 1.93373
Global Iter: 805300 training acc: 0.21875
Global Iter: 805400 training loss: 1.92901
Global Iter: 805400 training acc: 0.28125
Global Iter: 805500 training loss: 1.98246
Global Iter: 805500 training acc: 0.0625
Global Iter: 805600 training loss: 1.91343
Global Iter: 805600 training acc: 0.34375
Global Iter: 805700 training loss: 2.0439
Global Iter: 805700 training acc: 0.15625
Global Iter: 805800 training loss: 1.94804
Global Iter: 805800 training acc: 0.09375
Global Iter: 805900 training loss: 1.97234
Global Iter: 805900 training acc: 0.15625
Global Iter: 806000 training loss: 1.99932
Global Iter: 806000 training acc: 0.0625
Global Iter: 806100 training loss: 1.87118
Global Iter: 806100 training acc: 0.21875
Global Iter: 806200 training loss: 1.95562
Global Iter: 806200 training acc: 0.125
Global Iter: 806300 training loss: 2.0302
Global Iter: 806300 training acc: 0.15625
Global Iter: 806400 training loss: 1.89521
Global Iter: 806400 training acc: 0.21875
Global Iter: 806500 training loss: 1.88873
Global Iter: 806500 training acc: 0.15625
Global Iter: 806600 training loss: 2.03274
Global Iter: 806600 training acc: 0.21875
Global Iter: 806700 training loss: 1.96582
Global Iter: 806700 training acc: 0.1875
Global Iter: 806800 training loss: 1.98172
Global Iter: 806800 training acc: 0.1875
Global Iter: 806900 training loss: 1.99334
Global Iter: 806900 training acc: 0.28125
Global Iter: 807000 training loss: 1.97817
Global Iter: 807000 training acc: 0.21875
Global Iter: 807100 training loss: 2.11894
Global Iter: 807100 training acc: 0.15625
Global Iter: 807200 training loss: 1.93874
Global Iter: 807200 training acc: 0.3125
Global Iter: 807300 training loss: 2.00456
Global Iter: 807300 training acc: 0.3125
Global Iter: 807400 training loss: 2.00092
Global Iter: 807400 training acc: 0.21875
Global Iter: 807500 training loss: 2.0715
Global Iter: 807500 training acc: 0.28125
Global Iter: 807600 training loss: 2.02543
Global Iter: 807600 training acc: 0.09375
Global Iter: 807700 training loss: 1.98025
Global Iter: 807700 training acc: 0.125
Global Iter: 807800 training loss: 1.9589
Global Iter: 807800 training acc: 0.09375
Global Iter: 807900 training loss: 1.9502
Global Iter: 807900 training acc: 0.125
Global Iter: 808000 training loss: 2.01137
Global Iter: 808000 training acc: 0.25
Global Iter: 808100 training loss: 1.93479
Global Iter: 808100 training acc: 0.21875
Global Iter: 808200 training loss: 1.90504
Global Iter: 808200 training acc: 0.25
Global Iter: 808300 training loss: 2.08101
Global Iter: 808300 training acc: 0.09375
Global Iter: 808400 training loss: 2.0535
Global Iter: 808400 training acc: 0.125
Global Iter: 808500 training loss: 1.99196
Global Iter: 808500 training acc: 0.15625
Global Iter: 808600 training loss: 2.05603
Global Iter: 808600 training acc: 0.1875
Global Iter: 808700 training loss: 2.03137
Global Iter: 808700 training acc: 0.25
Global Iter: 808800 training loss: 2.05
Global Iter: 808800 training acc: 0.28125
Global Iter: 808900 training loss: 2.00519
Global Iter: 808900 training acc: 0.1875
Global Iter: 809000 training loss: 1.99412
Global Iter: 809000 training acc: 0.21875
Global Iter: 809100 training loss: 1.98019
Global Iter: 809100 training acc: 0.15625
Global Iter: 809200 training loss: 2.01207
Global Iter: 809200 training acc: 0.21875
Global Iter: 809300 training loss: 2.08455
Global Iter: 809300 training acc: 0.1875
Global Iter: 809400 training loss: 1.94077
Global Iter: 809400 training acc: 0.21875
Global Iter: 809500 training loss: 2.02025
Global Iter: 809500 training acc: 0.21875
Global Iter: 809600 training loss: 1.99076
Global Iter: 809600 training acc: 0.15625
Global Iter: 809700 training loss: 1.96101
Global Iter: 809700 training acc: 0.125
Global Iter: 809800 training loss: 2.01405
Global Iter: 809800 training acc: 0.125
Global Iter: 809900 training loss: 1.93069
Global Iter: 809900 training acc: 0.25
Global Iter: 810000 training loss: 2.08364
Global Iter: 810000 training acc: 0.15625
Global Iter: 810100 training loss: 2.02137
Global Iter: 810100 training acc: 0.1875
Global Iter: 810200 training loss: 2.00723
Global Iter: 810200 training acc: 0.1875
Global Iter: 810300 training loss: 1.95989
Global Iter: 810300 training acc: 0.1875
Global Iter: 810400 training loss: 1.95723
Global Iter: 810400 training acc: 0.25
Global Iter: 810500 training loss: 1.97033
Global Iter: 810500 training acc: 0.1875
Global Iter: 810600 training loss: 1.95759
Global Iter: 810600 training acc: 0.15625
Global Iter: 810700 training loss: 1.98627
Global Iter: 810700 training acc: 0.1875
Global Iter: 810800 training loss: 1.97971
Global Iter: 810800 training acc: 0.125
Global Iter: 810900 training loss: 2.02157
Global Iter: 810900 training acc: 0.21875
Global Iter: 811000 training loss: 2.03583
Global Iter: 811000 training acc: 0.09375
Global Iter: 811100 training loss: 2.09729
Global Iter: 811100 training acc: 0.09375
Global Iter: 811200 training loss: 2.04201
Global Iter: 811200 training acc: 0.09375
Global Iter: 811300 training loss: 2.01392
Global Iter: 811300 training acc: 0.125
Global Iter: 811400 training loss: 2.01996
Global Iter: 811400 training acc: 0.3125
Global Iter: 811500 training loss: 1.88784
Global Iter: 811500 training acc: 0.21875
Global Iter: 811600 training loss: 2.0576
Global Iter: 811600 training acc: 0.21875
Global Iter: 811700 training loss: 1.95763
Global Iter: 811700 training acc: 0.28125
Global Iter: 811800 training loss: 2.0
Global Iter: 811800 training acc: 0.25
Global Iter: 811900 training loss: 2.02879
Global Iter: 811900 training acc: 0.15625
Global Iter: 812000 training loss: 2.06106
Global Iter: 812000 training acc: 0.125
Global Iter: 812100 training loss: 2.11804
Global Iter: 812100 training acc: 0.09375
Global Iter: 812200 training loss: 1.9619
Global Iter: 812200 training acc: 0.15625
Global Iter: 812300 trainin2017-06-22 04:20:59.691335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-815840
g loss: 2.0411
Global Iter: 812300 training acc: 0.21875
Global Iter: 812400 training loss: 2.01947
Global Iter: 812400 training acc: 0.15625
Global Iter: 812500 training loss: 2.10831
Global Iter: 812500 training acc: 0.25
Global Iter: 812600 training loss: 2.07641
Global Iter: 812600 training acc: 0.125
Global Iter: 812700 training loss: 2.00405
Global Iter: 812700 training acc: 0.3125
Global Iter: 812800 training loss: 2.08352
Global Iter: 812800 training acc: 0.09375
Global Iter: 812900 training loss: 1.94835
Global Iter: 812900 training acc: 0.28125
Global Iter: 813000 training loss: 1.97248
Global Iter: 813000 training acc: 0.25
Global Iter: 813100 training loss: 2.01843
Global Iter: 813100 training acc: 0.25
Global Iter: 813200 training loss: 1.93753
Global Iter: 813200 training acc: 0.25
Global Iter: 813300 training loss: 1.95207
Global Iter: 813300 training acc: 0.21875
Global Iter: 813400 training loss: 2.02805
Global Iter: 813400 training acc: 0.1875
Global Iter: 813500 training loss: 1.9044
Global Iter: 813500 training acc: 0.375
Global Iter: 813600 training loss: 1.97975
Global Iter: 813600 training acc: 0.15625
Global Iter: 813700 training loss: 1.9632
Global Iter: 813700 training acc: 0.28125
Global Iter: 813800 training loss: 1.94539
Global Iter: 813800 training acc: 0.09375
Global Iter: 813900 training loss: 2.04127
Global Iter: 813900 training acc: 0.1875
Global Iter: 814000 training loss: 2.06848
Global Iter: 814000 training acc: 0.15625
Global Iter: 814100 training loss: 1.93378
Global Iter: 814100 training acc: 0.15625
Global Iter: 814200 training loss: 1.97381
Global Iter: 814200 training acc: 0.09375
Global Iter: 814300 training loss: 1.98313
Global Iter: 814300 training acc: 0.15625
Global Iter: 814400 training loss: 2.01588
Global Iter: 814400 training acc: 0.21875
Global Iter: 814500 training loss: 2.07486
Global Iter: 814500 training acc: 0.09375
Global Iter: 814600 training loss: 1.89124
Global Iter: 814600 training acc: 0.28125
Global Iter: 814700 training loss: 1.97688
Global Iter: 814700 training acc: 0.21875
Global Iter: 814800 training loss: 1.8567
Global Iter: 814800 training acc: 0.28125
Global Iter: 814900 training loss: 2.12655
Global Iter: 814900 training acc: 0.0625
Global Iter: 815000 training loss: 1.97551
Global Iter: 815000 training acc: 0.09375
Global Iter: 815100 training loss: 1.92074
Global Iter: 815100 training acc: 0.125
Global Iter: 815200 training loss: 1.96804
Global Iter: 815200 training acc: 0.21875
Global Iter: 815300 training loss: 1.91445
Global Iter: 815300 training acc: 0.125
Global Iter: 815400 training loss: 1.90507
Global Iter: 815400 training acc: 0.15625
Global Iter: 815500 training loss: 1.99918
Global Iter: 815500 training acc: 0.3125
Global Iter: 815600 training loss: 1.91804
Global Iter: 815600 training acc: 0.15625
Global Iter: 815700 training loss: 2.014
Global Iter: 815700 training acc: 0.15625
Global Iter: 815800 training loss: 1.96117
Global Iter: 815800 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-815840
Number of Patches: 176869
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-815840
Global Iter: 815900 training loss: 1.98827
Global Iter: 815900 training acc: 0.15625
Global Iter: 816000 training loss: 1.99239
Global Iter: 816000 training acc: 0.15625
Global Iter: 816100 training loss: 1.9859
Global Iter: 816100 training acc: 0.125
Global Iter: 816200 training loss: 1.93264
Global Iter: 816200 training acc: 0.34375
Global Iter: 816300 training loss: 1.93724
Global Iter: 816300 training acc: 0.21875
Global Iter: 816400 training loss: 2.04651
Global Iter: 816400 training acc: 0.09375
Global Iter: 816500 training loss: 1.98668
Global Iter: 816500 training acc: 0.25
Global Iter: 816600 training loss: 1.92068
Global Iter: 816600 training acc: 0.28125
Global Iter: 816700 training loss: 2.00851
Global Iter: 816700 training acc: 0.125
Global Iter: 816800 training loss: 1.88338
Global Iter: 816800 training acc: 0.3125
Global Iter: 816900 training loss: 2.04597
Global Iter: 816900 training acc: 0.21875
Global Iter: 817000 training loss: 2.00912
Global Iter: 817000 training acc: 0.3125
Global Iter: 817100 training loss: 1.97232
Global Iter: 817100 training acc: 0.15625
Global Iter: 817200 training loss: 1.98473
Global Iter: 817200 training acc: 0.1875
Global Iter: 817300 training loss: 1.94615
Global Iter: 817300 training acc: 0.125
Global Iter: 817400 training loss: 1.91001
Global Iter: 817400 training acc: 0.3125
Global Iter: 817500 training loss: 2.01257
Global Iter: 817500 training acc: 0.09375
Global Iter: 817600 training loss: 2.08068
Global Iter: 817600 training acc: 0.09375
Global Iter: 817700 training loss: 1.97531
Global Iter: 817700 training acc: 0.125
Global Iter: 817800 training loss: 1.97986
Global Iter: 817800 training acc: 0.1875
Global Iter: 817900 training loss: 1.91465
Global Iter: 817900 training acc: 0.1875
Global Iter: 818000 training loss: 2.037
Global Iter: 818000 training acc: 0.25
Global Iter: 818100 training loss: 2.26455
Global Iter: 818100 training acc: 0.03125
Global Iter: 818200 training loss: 2.13867
Global Iter: 818200 training acc: 0.1875
Global Iter: 818300 training loss: 1.99981
Global Iter: 818300 training acc: 0.21875
Global Iter: 818400 training loss: 1.90302
Global Iter: 818400 training acc: 0.375
Global Iter: 818500 training loss: 1.94147
Global Iter: 818500 training acc: 0.1875
Global Iter: 818600 training loss: 1.95072
Global Iter: 818600 training acc: 0.09375
Global Iter: 818700 training loss: 1.90022
Global Iter: 818700 training acc: 0.21875
Global Iter: 818800 training loss: 2.07797
Global Iter: 818800 training acc: 0.21875
Global Iter: 818900 training loss: 1.91309
Global Iter: 818900 training acc: 0.3125
Global Iter: 819000 training loss: 1.97725
Global Iter: 819000 training acc: 0.09375
Global Iter: 819100 training loss: 2.03089
Global Iter: 819100 training acc: 0.21875
Global Iter: 819200 training loss: 1.99745
Global Iter: 819200 training acc: 0.15625
Global Iter: 819300 training loss: 1.96644
Global Iter: 819300 training acc: 0.1875
Global Iter: 819400 training loss: 1.94197
Global Iter: 819400 training acc: 0.1875
Global Iter: 819500 training loss: 2.04183
Global Iter: 819500 training acc: 0.0625
Global Iter: 819600 training loss: 2.07063
Global Iter: 819600 training acc: 0.21875
Global Iter: 819700 training loss: 2.03973
Global Iter: 819700 training acc: 0.125
Global Iter: 819800 training loss: 2.03386
Global Iter: 819800 training acc: 0.3125
Global Iter: 819900 training loss: 2.08033
Global Iter: 819900 training acc: 0.125
Global Iter: 820000 training loss: 2.09715
Global Iter: 820000 training acc: 0.125
Global Iter: 820100 training loss: 1.94186
Global Iter: 820100 training acc: 0.1875
Global Iter: 820200 training loss: 2.0617
Global Iter: 820200 training acc: 0.15625
Global Iter: 820300 training loss: 2.02627
Global Iter: 820300 training acc: 0.0625
Global Iter: 820400 training loss: 2.10525
Global Iter: 820400 training acc: 0.15625
Global Iter: 820500 training loss: 1.98457
Global Iter: 820500 training acc: 0.25
Global Iter: 820600 training loss: 2.03364
Global Iter: 820600 training acc: 0.09375
Global Iter: 820700 training loss: 2.04943
Global Iter: 820700 training acc: 0.03125
Global Iter: 820800 training loss: 1.94136
Global Iter: 820800 training acc: 0.25
Global Iter: 820900 training loss: 2.00366
Global Iter: 820900 training acc: 0.1875
Global Iter: 821000 training loss: 1.96268
Global Iter: 821000 training acc: 0.25
Global Iter: 821100 training loss: 1.94188
Global Iter: 821100 training acc: 0.15625
Global Iter: 821200 training loss: 2.00803
Global Iter: 821200 training acc: 0.15625
Global Iter: 821300 training loss: 1.93788
Global Iter: 821300 training acc: 0.28125
Global Iter: 821400 training loss: 2.03054
Global Iter: 821400 training acc: 0.125
Global Iter: 821500 training loss: 1.98937
Global Iter: 821500 training acc: 0.125
Global Iter: 821600 training loss: 1.97342
Global Iter: 821600 training acc: 0.1875
Global Iter: 821700 training loss: 1.88286
Global Iter: 821700 training acc: 0.28125
Global Iter: 821800 training loss: 1.99775
Global Iter: 821800 training acc: 0.1875
Global Iter: 821900 training loss: 2.08414
Global Iter: 821900 training acc: 0.15625
Global Iter: 822000 training loss: 1.99131
Global Iter: 822000 training acc: 0.25
Global Iter: 822100 training loss: 2.02947
Global Iter: 822100 training acc: 0.28125
Global Iter: 822200 training loss: 1.87242
Global Iter: 822200 training acc: 0.21875
Global Iter: 822300 training loss: 1.93438
Global Iter: 822300 training acc: 0.34375
Global Iter: 822400 training loss: 1.96258
Global Iter: 822400 training acc: 0.21875
Global Iter: 822500 training loss: 2.02699
Global Iter: 822500 training acc: 0.1875
Global Iter: 822600 training loss: 2.03311
Global Iter: 822600 training acc: 0.25
Global Iter: 822700 training loss: 2.00495
Global Iter: 822700 training acc: 0.21875
Global Iter: 822800 training loss: 1.93748
Global Iter: 822800 training acc: 0.15625
Global Iter: 822900 training loss: 1.95002
Global Iter: 822900 training acc: 0.1875
Global Iter: 823000 training loss: 1.99402
Global Iter: 823000 training acc: 0.15625
Global Iter: 823100 training loss: 2.06387
Global Iter: 823100 training acc: 0.1875
Global Iter: 823200 training loss: 2.04611
Global Iter: 823200 training acc: 0.15625
Global Iter: 823300 training loss: 1.96604
Global Iter: 823300 training acc: 0.09375
Global Iter: 823400 training loss: 2.05594
Global Iter: 823400 training acc: 0.1875
Global Iter: 823500 training loss: 1.86381
Global Iter: 823500 training acc: 0.25
Global Iter: 823600 training loss: 1.97195
Global Iter: 823600 training acc: 0.28125
Global Iter: 823700 training loss: 2.12267
Global Iter: 823700 training acc: 0.125
Global Iter: 823800 training loss: 2.1056
Global Iter: 823800 training acc: 0.125
Global Iter: 823900 training loss: 1.8811
Global Iter: 823900 training acc: 0.21875
Global Iter: 824000 training loss: 1.95425
Global Iter: 824000 training acc: 0.1875
Global Iter: 824100 training loss: 2.00626
Global Iter: 824100 training acc: 0.25
Global Iter: 824200 training loss: 1.89336
Global Iter: 824200 training acc: 0.34375
Global Iter: 824300 training loss: 2.05689
Global Iter: 824300 training acc: 0.09375
Global Iter: 824400 training loss: 1.9705
Global Iter: 824400 training acc: 0.21875
Global Iter: 824500 training loss: 1.90099
Global Iter: 824500 training acc: 0.1875
Global Iter: 824600 training loss: 1.98184
Global Iter: 824600 training acc: 0.1875
Global Iter: 824700 training loss: 1.93781
Global Iter: 824700 training acc: 0.21875
Global Iter: 824800 training loss: 2.04075
Global Iter: 824800 training acc: 0.0625
Global Iter: 824900 training loss: 1.93569
Global Iter: 824900 training acc: 0.21875
Global Iter: 825000 training loss: 1.99684
Global Iter: 825000 training acc: 0.25
Global Iter: 825100 training loss: 1.97024
Global Iter: 825100 training acc: 0.125
Global Iter: 825200 training loss: 1.94691
Global Iter: 825200 training acc: 0.25
Global Iter: 825300 training loss: 2.02434
Global Iter: 825300 training acc: 0.09375
Global Iter: 825400 training loss: 2.0303
Global Iter: 825400 training acc: 0.1875
Global Iter: 825500 training loss: 2.02312
Global Iter: 825500 training acc: 0.25
Global Iter: 825600 training loss: 1.94841
Global Iter: 825600 training acc: 0.09375
Global Iter: 825700 training loss: 1.90776
Global Iter: 825700 training acc: 0.28125
Global Iter: 825800 training loss: 1.8766
Global Iter: 825800 training acc: 0.28125
Global Iter: 825900 training loss: 1.89683
Global Iter: 825900 training acc: 0.28125
Global Iter: 826000 training loss: 2.07055
Global Iter: 826000 training acc: 0.15625
Global Iter: 826100 training loss: 1.93021
Global Iter: 826100 training acc: 0.125
Global Iter: 826200 training loss: 1.97297
Global Iter: 826200 training acc: 0.21875
Global Iter: 826300 training loss: 2.03778
Global Iter: 826300 training acc: 0.1875
Global Iter: 826400 training loss: 2.0465
Global Iter: 826400 training acc: 0.09375
Global Iter: 826500 training loss: 1.99975
Global Iter: 826500 training acc: 0.15625
Global Iter: 826600 training loss: 1.94914
Global Iter: 826600 training2017-06-22 04:40:11.603992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-826895
 acc: 0.25
Global Iter: 826700 training loss: 1.95395
Global Iter: 826700 training acc: 0.21875
Global Iter: 826800 training loss: 2.00884
Global Iter: 826800 training acc: 0.3125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-826895
Number of Patches: 175101
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-826895
Global Iter: 826900 training loss: 1.89611
Global Iter: 826900 training acc: 0.3125
Global Iter: 827000 training loss: 1.97477
Global Iter: 827000 training acc: 0.15625
Global Iter: 827100 training loss: 2.10387
Global Iter: 827100 training acc: 0.25
Global Iter: 827200 training loss: 1.94592
Global Iter: 827200 training acc: 0.09375
Global Iter: 827300 training loss: 1.9309
Global Iter: 827300 training acc: 0.125
Global Iter: 827400 training loss: 1.95006
Global Iter: 827400 training acc: 0.21875
Global Iter: 827500 training loss: 1.91935
Global Iter: 827500 training acc: 0.1875
Global Iter: 827600 training loss: 1.93221
Global Iter: 827600 training acc: 0.25
Global Iter: 827700 training loss: 1.89956
Global Iter: 827700 training acc: 0.21875
Global Iter: 827800 training loss: 2.06024
Global Iter: 827800 training acc: 0.09375
Global Iter: 827900 training loss: 1.93753
Global Iter: 827900 training acc: 0.21875
Global Iter: 828000 training loss: 2.01124
Global Iter: 828000 training acc: 0.125
Global Iter: 828100 training loss: 2.08248
Global Iter: 828100 training acc: 0.15625
Global Iter: 828200 training loss: 1.97834
Global Iter: 828200 training acc: 0.15625
Global Iter: 828300 training loss: 1.97801
Global Iter: 828300 training acc: 0.1875
Global Iter: 828400 training loss: 2.03041
Global Iter: 828400 training acc: 0.15625
Global Iter: 828500 training loss: 2.07155
Global Iter: 828500 training acc: 0.09375
Global Iter: 828600 training loss: 1.91595
Global Iter: 828600 training acc: 0.21875
Global Iter: 828700 training loss: 1.91323
Global Iter: 828700 training acc: 0.25
Global Iter: 828800 training loss: 2.02582
Global Iter: 828800 training acc: 0.15625
Global Iter: 828900 training loss: 1.95576
Global Iter: 828900 training acc: 0.15625
Global Iter: 829000 training loss: 2.12084
Global Iter: 829000 training acc: 0.15625
Global Iter: 829100 training loss: 1.95626
Global Iter: 829100 training acc: 0.125
Global Iter: 829200 training loss: 2.00297
Global Iter: 829200 training acc: 0.15625
Global Iter: 829300 training loss: 2.24397
Global Iter: 829300 training acc: 0.09375
Global Iter: 829400 training loss: 1.90069
Global Iter: 829400 training acc: 0.25
Global Iter: 829500 training loss: 1.95093
Global Iter: 829500 training acc: 0.125
Global Iter: 829600 training loss: 1.93986
Global Iter: 829600 training acc: 0.3125
Global Iter: 829700 training loss: 1.94871
Global Iter: 829700 training acc: 0.25
Global Iter: 829800 training loss: 1.89129
Global Iter: 829800 training acc: 0.28125
Global Iter: 829900 training loss: 1.95057
Global Iter: 829900 training acc: 0.15625
Global Iter: 830000 training loss: 2.10155
Global Iter: 830000 training acc: 0.09375
Global Iter: 830100 training loss: 1.94663
Global Iter: 830100 training acc: 0.375
Global Iter: 830200 training loss: 1.83354
Global Iter: 830200 training acc: 0.25
Global Iter: 830300 training loss: 1.9687
Global Iter: 830300 training acc: 0.21875
Global Iter: 830400 training loss: 2.1189
Global Iter: 830400 training acc: 0.125
Global Iter: 830500 training loss: 1.89681
Global Iter: 830500 training acc: 0.28125
Global Iter: 830600 training loss: 1.97609
Global Iter: 830600 training acc: 0.15625
Global Iter: 830700 training loss: 1.90396
Global Iter: 830700 training acc: 0.21875
Global Iter: 830800 training loss: 2.00555
Global Iter: 830800 training acc: 0.25
Global Iter: 830900 training loss: 1.94602
Global Iter: 830900 training acc: 0.15625
Global Iter: 831000 training loss: 1.9215
Global Iter: 831000 training acc: 0.34375
Global Iter: 831100 training loss: 2.09921
Global Iter: 831100 training acc: 0.21875
Global Iter: 831200 training loss: 2.01793
Global Iter: 831200 training acc: 0.21875
Global Iter: 831300 training loss: 2.02467
Global Iter: 831300 training acc: 0.1875
Global Iter: 831400 training loss: 1.92618
Global Iter: 831400 training acc: 0.125
Global Iter: 831500 training loss: 1.95891
Global Iter: 831500 training acc: 0.1875
Global Iter: 831600 training loss: 1.93896
Global Iter: 831600 training acc: 0.21875
Global Iter: 831700 training loss: 1.92377
Global Iter: 831700 training acc: 0.3125
Global Iter: 831800 training loss: 1.94843
Global Iter: 831800 training acc: 0.3125
Global Iter: 831900 training loss: 2.00934
Global Iter: 831900 training acc: 0.15625
Global Iter: 832000 training loss: 1.9192
Global Iter: 832000 training acc: 0.21875
Global Iter: 832100 training loss: 1.88054
Global Iter: 832100 training acc: 0.1875
Global Iter: 832200 training loss: 2.02235
Global Iter: 832200 training acc: 0.1875
Global Iter: 832300 training loss: 1.92935
Global Iter: 832300 training acc: 0.21875
Global Iter: 832400 training loss: 2.00586
Global Iter: 832400 training acc: 0.09375
Global Iter: 832500 training loss: 2.04283
Global Iter: 832500 training acc: 0.15625
Global Iter: 832600 training loss: 2.06839
Global Iter: 832600 training acc: 0.15625
Global Iter: 832700 training loss: 2.06409
Global Iter: 832700 training acc: 0.1875
Global Iter: 832800 training loss: 1.91305
Global Iter: 832800 training acc: 0.15625
Global Iter: 832900 training loss: 1.98067
Global Iter: 832900 training acc: 0.1875
Global Iter: 833000 training loss: 1.92253
Global Iter: 833000 training acc: 0.3125
Global Iter: 833100 training loss: 2.074
Global Iter: 833100 training acc: 0.15625
Global Iter: 833200 training loss: 1.93749
Global Iter: 833200 training acc: 0.125
Global Iter: 833300 training loss: 1.90144
Global Iter: 833300 training acc: 0.28125
Global Iter: 833400 training loss: 1.96527
Global Iter: 833400 training acc: 0.09375
Global Iter: 833500 training loss: 1.97146
Global Iter: 833500 training acc: 0.09375
Global Iter: 833600 training loss: 2.02514
Global Iter: 833600 training acc: 0.21875
Global Iter: 833700 training loss: 2.02975
Global Iter: 833700 training acc: 0.03125
Global Iter: 833800 training loss: 2.05093
Global Iter: 833800 training acc: 0.09375
Global Iter: 833900 training loss: 1.97408
Global Iter: 833900 training acc: 0.15625
Global Iter: 834000 training loss: 2.07628
Global Iter: 834000 training acc: 0.1875
Global Iter: 834100 training loss: 1.94917
Global Iter: 834100 training acc: 0.15625
Global Iter: 834200 training loss: 1.93553
Global Iter: 834200 training acc: 0.1875
Global Iter: 834300 training loss: 1.94466
Global Iter: 834300 training acc: 0.25
Global Iter: 834400 training loss: 2.09868
Global Iter: 834400 training acc: 0.1875
Global Iter: 834500 training loss: 1.95125
Global Iter: 834500 training acc: 0.09375
Global Iter: 834600 training loss: 1.90282
Global Iter: 834600 training acc: 0.21875
Global Iter: 834700 training loss: 1.94389
Global Iter: 834700 training acc: 0.28125
Global Iter: 834800 training loss: 2.06347
Global Iter: 834800 training acc: 0.21875
Global Iter: 834900 training loss: 1.96386
Global Iter: 834900 training acc: 0.1875
Global Iter: 835000 training loss: 1.92993
Global Iter: 835000 training acc: 0.34375
Global Iter: 835100 training loss: 2.00347
Global Iter: 835100 training acc: 0.1875
Global Iter: 835200 training loss: 2.06394
Global Iter: 835200 training acc: 0.15625
Global Iter: 835300 training loss: 2.01269
Global Iter: 835300 training acc: 0.1875
Global Iter: 835400 training loss: 2.00827
Global Iter: 835400 training acc: 0.1875
Global Iter: 835500 training loss: 1.97031
Global Iter: 835500 training acc: 0.125
Global Iter: 835600 training loss: 1.98518
Global Iter: 835600 training acc: 0.125
Global Iter: 835700 training loss: 1.93847
Global Iter: 835700 training acc: 0.15625
Global Iter: 835800 training loss: 1.97732
Global Iter: 835800 training acc: 0.25
Global Iter: 835900 training loss: 2.02472
Global Iter: 835900 training acc: 0.0625
Global Iter: 836000 training loss: 2.07179
Global Iter: 836000 training acc: 0.21875
Global Iter: 836100 training loss: 1.87881
G2017-06-22 04:59:01.646206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-837839
lobal Iter: 836100 training acc: 0.15625
Global Iter: 836200 training loss: 2.03026
Global Iter: 836200 training acc: 0.1875
Global Iter: 836300 training loss: 1.95213
Global Iter: 836300 training acc: 0.25
Global Iter: 836400 training loss: 1.85767
Global Iter: 836400 training acc: 0.4375
Global Iter: 836500 training loss: 2.05933
Global Iter: 836500 training acc: 0.125
Global Iter: 836600 training loss: 1.99809
Global Iter: 836600 training acc: 0.21875
Global Iter: 836700 training loss: 2.04492
Global Iter: 836700 training acc: 0.1875
Global Iter: 836800 training loss: 2.06759
Global Iter: 836800 training acc: 0.1875
Global Iter: 836900 training loss: 2.00897
Global Iter: 836900 training acc: 0.15625
Global Iter: 837000 training loss: 1.99573
Global Iter: 837000 training acc: 0.25
Global Iter: 837100 training loss: 2.06051
Global Iter: 837100 training acc: 0.15625
Global Iter: 837200 training loss: 2.00764
Global Iter: 837200 training acc: 0.125
Global Iter: 837300 training loss: 1.91199
Global Iter: 837300 training acc: 0.25
Global Iter: 837400 training loss: 1.9245
Global Iter: 837400 training acc: 0.1875
Global Iter: 837500 training loss: 1.99213
Global Iter: 837500 training acc: 0.1875
Global Iter: 837600 training loss: 1.97876
Global Iter: 837600 training acc: 0.1875
Global Iter: 837700 training loss: 2.0328
Global Iter: 837700 training acc: 0.34375
Global Iter: 837800 training loss: 1.8977
Global Iter: 837800 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-837839
Number of Patches: 173350
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-837839
Global Iter: 837900 training loss: 2.10058
Global Iter: 837900 training acc: 0.1875
Global Iter: 838000 training loss: 1.92209
Global Iter: 838000 training acc: 0.1875
Global Iter: 838100 training loss: 1.89646
Global Iter: 838100 training acc: 0.1875
Global Iter: 838200 training loss: 2.04423
Global Iter: 838200 training acc: 0.3125
Global Iter: 838300 training loss: 1.9696
Global Iter: 838300 training acc: 0.21875
Global Iter: 838400 training loss: 1.85816
Global Iter: 838400 training acc: 0.34375
Global Iter: 838500 training loss: 1.89647
Global Iter: 838500 training acc: 0.125
Global Iter: 838600 training loss: 1.96318
Global Iter: 838600 training acc: 0.21875
Global Iter: 838700 training loss: 1.96316
Global Iter: 838700 training acc: 0.125
Global Iter: 838800 training loss: 1.99572
Global Iter: 838800 training acc: 0.1875
Global Iter: 838900 training loss: 1.86864
Global Iter: 838900 training acc: 0.21875
Global Iter: 839000 training loss: 1.93248
Global Iter: 839000 training acc: 0.25
Global Iter: 839100 training loss: 1.97273
Global Iter: 839100 training acc: 0.15625
Global Iter: 839200 training loss: 1.96259
Global Iter: 839200 training acc: 0.21875
Global Iter: 839300 training loss: 2.01597
Global Iter: 839300 training acc: 0.09375
Global Iter: 839400 training loss: 2.00698
Global Iter: 839400 training acc: 0.3125
Global Iter: 839500 training loss: 1.97696
Global Iter: 839500 training acc: 0.15625
Global Iter: 839600 training loss: 2.10689
Global Iter: 839600 training acc: 0.15625
Global Iter: 839700 training loss: 2.00281
Global Iter: 839700 training acc: 0.21875
Global Iter: 839800 training loss: 2.00804
Global Iter: 839800 training acc: 0.21875
Global Iter: 839900 training loss: 1.9852
Global Iter: 839900 training acc: 0.21875
Global Iter: 840000 training loss: 2.02751
Global Iter: 840000 training acc: 0.15625
Global Iter: 840100 training loss: 1.99736
Global Iter: 840100 training acc: 0.125
Global Iter: 840200 training loss: 2.07756
Global Iter: 840200 training acc: 0.15625
Global Iter: 840300 training loss: 1.98568
Global Iter: 840300 training acc: 0.1875
Global Iter: 840400 training loss: 1.95059
Global Iter: 840400 training acc: 0.21875
Global Iter: 840500 training loss: 2.00872
Global Iter: 840500 training acc: 0.21875
Global Iter: 840600 training loss: 2.01506
Global Iter: 840600 training acc: 0.125
Global Iter: 840700 training loss: 2.03815
Global Iter: 840700 training acc: 0.09375
Global Iter: 840800 training loss: 1.89652
Global Iter: 840800 training acc: 0.1875
Global Iter: 840900 training loss: 1.92913
Global Iter: 840900 training acc: 0.1875
Global Iter: 841000 training loss: 2.01145
Global Iter: 841000 training acc: 0.125
Global Iter: 841100 training loss: 1.94766
Global Iter: 841100 training acc: 0.125
Global Iter: 841200 training loss: 1.92329
Global Iter: 841200 training acc: 0.1875
Global Iter: 841300 training loss: 1.96679
Global Iter: 841300 training acc: 0.125
Global Iter: 841400 training loss: 2.05523
Global Iter: 841400 training acc: 0.09375
Global Iter: 841500 training loss: 1.98116
Global Iter: 841500 training acc: 0.09375
Global Iter: 841600 training loss: 1.92942
Global Iter: 841600 training acc: 0.28125
Global Iter: 841700 training loss: 2.12154
Global Iter: 841700 training acc: 0.125
Global Iter: 841800 training loss: 2.06708
Global Iter: 841800 training acc: 0.3125
Global Iter: 841900 training loss: 2.05088
Global Iter: 841900 training acc: 0.21875
Global Iter: 842000 training loss: 1.94872
Global Iter: 842000 training acc: 0.09375
Global Iter: 842100 training loss: 2.07725
Global Iter: 842100 training acc: 0.09375
Global Iter: 842200 training loss: 2.11007
Global Iter: 842200 training acc: 0.21875
Global Iter: 842300 training loss: 2.04608
Global Iter: 842300 training acc: 0.21875
Global Iter: 842400 training loss: 2.04325
Global Iter: 842400 training acc: 0.15625
Global Iter: 842500 training loss: 2.0097
Global Iter: 842500 training acc: 0.09375
Global Iter: 842600 training loss: 2.0616
Global Iter: 842600 training acc: 0.21875
Global Iter: 842700 training loss: 1.95967
Global Iter: 842700 training acc: 0.15625
Global Iter: 842800 training loss: 2.0025
Global Iter: 842800 training acc: 0.25
Global Iter: 842900 training loss: 2.03423
Global Iter: 842900 training acc: 0.1875
Global Iter: 843000 training loss: 1.89728
Global Iter: 843000 training acc: 0.21875
Global Iter: 843100 training loss: 1.9022
Global Iter: 843100 training acc: 0.1875
Global Iter: 843200 training loss: 2.04574
Global Iter: 843200 training acc: 0.09375
Global Iter: 843300 training loss: 2.17602
Global Iter: 843300 training acc: 0.125
Global Iter: 843400 training loss: 1.84789
Global Iter: 843400 training acc: 0.3125
Global Iter: 843500 training loss: 1.96468
Global Iter: 843500 training acc: 0.21875
Global Iter: 843600 training loss: 1.92446
Global Iter: 843600 training acc: 0.28125
Global Iter: 843700 training loss: 1.89409
Global Iter: 843700 training acc: 0.1875
Global Iter: 843800 training loss: 1.97555
Global Iter: 843800 training acc: 0.25
Global Iter: 843900 training loss: 2.09191
Global Iter: 843900 training acc: 0.09375
Global Iter: 844000 training loss: 1.97295
Global Iter: 844000 training acc: 0.125
Global Iter: 844100 training loss: 1.92816
Global Iter: 844100 training acc: 0.3125
Global Iter: 844200 training loss: 1.96551
Global Iter: 844200 training acc: 0.21875
Global Iter: 844300 training loss: 2.02795
Global Iter: 844300 training acc: 0.15625
Global Iter: 844400 training loss: 1.99435
Global Iter: 844400 training acc: 0.25
Global Iter: 844500 training loss: 1.97756
Global Iter: 844500 training acc: 0.28125
Global Iter: 844600 training loss: 1.97981
Global Iter: 844600 training acc: 0.25
Global Iter: 844700 training loss: 2.06335
Global Iter: 844700 training acc: 0.09375
Global Iter: 844800 training loss: 1.88944
Global Iter: 844800 training acc: 0.25
Global Iter: 844900 training loss: 2.03215
Global Iter: 844900 training acc: 0.1875
Global Iter: 845000 training loss: 1.97281
Global Iter: 845000 training acc: 0.15625
Global Iter: 845100 training loss: 1.9184
Global Iter: 845100 training acc: 0.15625
Global Iter: 845200 training loss: 2.00019
Global Iter: 845200 training acc: 0.09375
Global Iter: 845300 training loss: 2.11202
Global Iter: 845300 training acc: 0.09375
Global Iter: 845400 training loss: 2.0299
Global Iter: 845400 training acc: 0.21875
Global Iter: 845500 training loss: 1.94421
Global Iter: 845500 training acc: 0.0625
Global Iter: 845600 traini2017-06-22 05:17:39.299502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-848674
ng loss: 1.99129
Global Iter: 845600 training acc: 0.21875
Global Iter: 845700 training loss: 1.85408
Global Iter: 845700 training acc: 0.28125
Global Iter: 845800 training loss: 2.07684
Global Iter: 845800 training acc: 0.21875
Global Iter: 845900 training loss: 1.96753
Global Iter: 845900 training acc: 0.21875
Global Iter: 846000 training loss: 1.9481
Global Iter: 846000 training acc: 0.15625
Global Iter: 846100 training loss: 2.00608
Global Iter: 846100 training acc: 0.1875
Global Iter: 846200 training loss: 2.03765
Global Iter: 846200 training acc: 0.15625
Global Iter: 846300 training loss: 1.87815
Global Iter: 846300 training acc: 0.3125
Global Iter: 846400 training loss: 1.94455
Global Iter: 846400 training acc: 0.25
Global Iter: 846500 training loss: 2.19394
Global Iter: 846500 training acc: 0.0625
Global Iter: 846600 training loss: 1.98554
Global Iter: 846600 training acc: 0.1875
Global Iter: 846700 training loss: 1.96841
Global Iter: 846700 training acc: 0.125
Global Iter: 846800 training loss: 2.02152
Global Iter: 846800 training acc: 0.21875
Global Iter: 846900 training loss: 2.02087
Global Iter: 846900 training acc: 0.15625
Global Iter: 847000 training loss: 1.97705
Global Iter: 847000 training acc: 0.21875
Global Iter: 847100 training loss: 2.13458
Global Iter: 847100 training acc: 0.0625
Global Iter: 847200 training loss: 1.90184
Global Iter: 847200 training acc: 0.3125
Global Iter: 847300 training loss: 1.97877
Global Iter: 847300 training acc: 0.15625
Global Iter: 847400 training loss: 2.07166
Global Iter: 847400 training acc: 0.15625
Global Iter: 847500 training loss: 1.99373
Global Iter: 847500 training acc: 0.0625
Global Iter: 847600 training loss: 1.99295
Global Iter: 847600 training acc: 0.21875
Global Iter: 847700 training loss: 1.92373
Global Iter: 847700 training acc: 0.09375
Global Iter: 847800 training loss: 1.92667
Global Iter: 847800 training acc: 0.15625
Global Iter: 847900 training loss: 1.97186
Global Iter: 847900 training acc: 0.1875
Global Iter: 848000 training loss: 1.97902
Global Iter: 848000 training acc: 0.1875
Global Iter: 848100 training loss: 1.89299
Global Iter: 848100 training acc: 0.28125
Global Iter: 848200 training loss: 1.94082
Global Iter: 848200 training acc: 0.1875
Global Iter: 848300 training loss: 2.13682
Global Iter: 848300 training acc: 0.1875
Global Iter: 848400 training loss: 2.06131
Global Iter: 848400 training acc: 0.125
Global Iter: 848500 training loss: 1.93383
Global Iter: 848500 training acc: 0.21875
Global Iter: 848600 training loss: 1.93583
Global Iter: 848600 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-848674
Number of Patches: 171617
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-848674
Global Iter: 848700 training loss: 2.03284
Global Iter: 848700 training acc: 0.1875
Global Iter: 848800 training loss: 1.98849
Global Iter: 848800 training acc: 0.25
Global Iter: 848900 training loss: 1.91486
Global Iter: 848900 training acc: 0.1875
Global Iter: 849000 training loss: 2.16881
Global Iter: 849000 training acc: 0.1875
Global Iter: 849100 training loss: 2.02247
Global Iter: 849100 training acc: 0.15625
Global Iter: 849200 training loss: 1.9652
Global Iter: 849200 training acc: 0.1875
Global Iter: 849300 training loss: 1.95512
Global Iter: 849300 training acc: 0.28125
Global Iter: 849400 training loss: 1.98749
Global Iter: 849400 training acc: 0.15625
Global Iter: 849500 training loss: 1.98923
Global Iter: 849500 training acc: 0.1875
Global Iter: 849600 training loss: 1.91397
Global Iter: 849600 training acc: 0.25
Global Iter: 849700 training loss: 1.95739
Global Iter: 849700 training acc: 0.25
Global Iter: 849800 training loss: 1.99098
Global Iter: 849800 training acc: 0.25
Global Iter: 849900 training loss: 2.03869
Global Iter: 849900 training acc: 0.1875
Global Iter: 850000 training loss: 2.0706
Global Iter: 850000 training acc: 0.15625
Global Iter: 850100 training loss: 1.92604
Global Iter: 850100 training acc: 0.1875
Global Iter: 850200 training loss: 1.97226
Global Iter: 850200 training acc: 0.1875
Global Iter: 850300 training loss: 2.03316
Global Iter: 850300 training acc: 0.0625
Global Iter: 850400 training loss: 1.93816
Global Iter: 850400 training acc: 0.21875
Global Iter: 850500 training loss: 1.92356
Global Iter: 850500 training acc: 0.3125
Global Iter: 850600 training loss: 2.10619
Global Iter: 850600 training acc: 0.1875
Global Iter: 850700 training loss: 2.01709
Global Iter: 850700 training acc: 0.1875
Global Iter: 850800 training loss: 1.93082
Global Iter: 850800 training acc: 0.15625
Global Iter: 850900 training loss: 1.9427
Global Iter: 850900 training acc: 0.15625
Global Iter: 851000 training loss: 1.95078
Global Iter: 851000 training acc: 0.15625
Global Iter: 851100 training loss: 1.97319
Global Iter: 851100 training acc: 0.21875
Global Iter: 851200 training loss: 2.12387
Global Iter: 851200 training acc: 0.09375
Global Iter: 851300 training loss: 2.07441
Global Iter: 851300 training acc: 0.25
Global Iter: 851400 training loss: 2.01929
Global Iter: 851400 training acc: 0.125
Global Iter: 851500 training loss: 1.94559
Global Iter: 851500 training acc: 0.15625
Global Iter: 851600 training loss: 1.95547
Global Iter: 851600 training acc: 0.15625
Global Iter: 851700 training loss: 2.06528
Global Iter: 851700 training acc: 0.09375
Global Iter: 851800 training loss: 1.96803
Global Iter: 851800 training acc: 0.25
Global Iter: 851900 training loss: 2.00184
Global Iter: 851900 training acc: 0.125
Global Iter: 852000 training loss: 2.11873
Global Iter: 852000 training acc: 0.1875
Global Iter: 852100 training loss: 1.97081
Global Iter: 852100 training acc: 0.3125
Global Iter: 852200 training loss: 1.94805
Global Iter: 852200 training acc: 0.15625
Global Iter: 852300 training loss: 1.97145
Global Iter: 852300 training acc: 0.25
Global Iter: 852400 training loss: 1.99183
Global Iter: 852400 training acc: 0.1875
Global Iter: 852500 training loss: 1.91512
Global Iter: 852500 training acc: 0.21875
Global Iter: 852600 training loss: 1.975
Global Iter: 852600 training acc: 0.25
Global Iter: 852700 training loss: 2.15401
Global Iter: 852700 training acc: 0.1875
Global Iter: 852800 training loss: 1.93789
Global Iter: 852800 training acc: 0.1875
Global Iter: 852900 training loss: 2.04834
Global Iter: 852900 training acc: 0.125
Global Iter: 853000 training loss: 1.98749
Global Iter: 853000 training acc: 0.21875
Global Iter: 853100 training loss: 1.96453
Global Iter: 853100 training acc: 0.1875
Global Iter: 853200 training loss: 1.94582
Global Iter: 853200 training acc: 0.1875
Global Iter: 853300 training loss: 2.02108
Global Iter: 853300 training acc: 0.1875
Global Iter: 853400 training loss: 2.01074
Global Iter: 853400 training acc: 0.1875
Global Iter: 853500 training loss: 2.04103
Global Iter: 853500 training acc: 0.1875
Global Iter: 853600 training loss: 1.87894
Global Iter: 853600 training acc: 0.25
Global Iter: 853700 training loss: 1.95517
Global Iter: 853700 training acc: 0.1875
Global Iter: 853800 training loss: 2.00041
Global Iter: 853800 training acc: 0.0625
Global Iter: 853900 training loss: 2.16529
Global Iter: 853900 training acc: 0.125
Global Iter: 854000 training loss: 1.93666
Global Iter: 854000 training acc: 0.25
Global Iter: 854100 training loss: 2.0425
Global Iter: 854100 training acc: 0.125
Global Iter: 854200 training loss: 1.89018
Global Iter: 854200 training acc: 0.25
Global Iter: 854300 training loss: 2.00964
Global Iter: 854300 training acc: 0.09375
Global Iter: 854400 training loss: 1.97823
Global Iter: 854400 training acc: 0.09375
Global Iter: 854500 training loss: 1.93249
Global Iter: 854500 training acc: 0.25
Global Iter: 854600 training loss: 1.9678
Global Iter: 854600 training acc: 0.25
Global Iter: 854700 training loss: 2.0018
Global Iter: 854700 training acc: 0.21875
Global Iter: 854800 training loss: 1.991
Global Iter: 854800 training acc: 0.21875
Global Iter: 854900 training loss: 1.95054
Global Iter: 854900 training acc: 0.25
Global Iter: 855000 training loss: 2.06562
Global Iter: 855000 training acc: 0.125
Global Iter: 855100 training2017-06-22 05:36:07.126433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-859401
 loss: 1.96155
Global Iter: 855100 training acc: 0.25
Global Iter: 855200 training loss: 1.98362
Global Iter: 855200 training acc: 0.21875
Global Iter: 855300 training loss: 2.05321
Global Iter: 855300 training acc: 0.25
Global Iter: 855400 training loss: 1.99808
Global Iter: 855400 training acc: 0.15625
Global Iter: 855500 training loss: 2.00592
Global Iter: 855500 training acc: 0.1875
Global Iter: 855600 training loss: 2.02238
Global Iter: 855600 training acc: 0.09375
Global Iter: 855700 training loss: 1.99955
Global Iter: 855700 training acc: 0.28125
Global Iter: 855800 training loss: 2.01554
Global Iter: 855800 training acc: 0.0625
Global Iter: 855900 training loss: 2.03645
Global Iter: 855900 training acc: 0.25
Global Iter: 856000 training loss: 1.93352
Global Iter: 856000 training acc: 0.34375
Global Iter: 856100 training loss: 1.87574
Global Iter: 856100 training acc: 0.34375
Global Iter: 856200 training loss: 1.92907
Global Iter: 856200 training acc: 0.25
Global Iter: 856300 training loss: 1.88624
Global Iter: 856300 training acc: 0.28125
Global Iter: 856400 training loss: 1.91862
Global Iter: 856400 training acc: 0.34375
Global Iter: 856500 training loss: 1.9786
Global Iter: 856500 training acc: 0.28125
Global Iter: 856600 training loss: 1.96586
Global Iter: 856600 training acc: 0.25
Global Iter: 856700 training loss: 2.04577
Global Iter: 856700 training acc: 0.15625
Global Iter: 856800 training loss: 2.0425
Global Iter: 856800 training acc: 0.1875
Global Iter: 856900 training loss: 2.06492
Global Iter: 856900 training acc: 0.09375
Global Iter: 857000 training loss: 1.99359
Global Iter: 857000 training acc: 0.375
Global Iter: 857100 training loss: 1.9511
Global Iter: 857100 training acc: 0.15625
Global Iter: 857200 training loss: 2.06514
Global Iter: 857200 training acc: 0.09375
Global Iter: 857300 training loss: 1.92241
Global Iter: 857300 training acc: 0.21875
Global Iter: 857400 training loss: 2.02101
Global Iter: 857400 training acc: 0.1875
Global Iter: 857500 training loss: 2.04762
Global Iter: 857500 training acc: 0.09375
Global Iter: 857600 training loss: 2.10555
Global Iter: 857600 training acc: 0.15625
Global Iter: 857700 training loss: 1.92368
Global Iter: 857700 training acc: 0.1875
Global Iter: 857800 training loss: 2.05959
Global Iter: 857800 training acc: 0.1875
Global Iter: 857900 training loss: 1.92345
Global Iter: 857900 training acc: 0.15625
Global Iter: 858000 training loss: 2.02226
Global Iter: 858000 training acc: 0.125
Global Iter: 858100 training loss: 2.01856
Global Iter: 858100 training acc: 0.15625
Global Iter: 858200 training loss: 2.04971
Global Iter: 858200 training acc: 0.125
Global Iter: 858300 training loss: 1.92232
Global Iter: 858300 training acc: 0.125
Global Iter: 858400 training loss: 1.90905
Global Iter: 858400 training acc: 0.34375
Global Iter: 858500 training loss: 1.92735
Global Iter: 858500 training acc: 0.28125
Global Iter: 858600 training loss: 1.93694
Global Iter: 858600 training acc: 0.1875
Global Iter: 858700 training loss: 2.01261
Global Iter: 858700 training acc: 0.1875
Global Iter: 858800 training loss: 1.95587
Global Iter: 858800 training acc: 0.21875
Global Iter: 858900 training loss: 2.00113
Global Iter: 858900 training acc: 0.21875
Global Iter: 859000 training loss: 1.97461
Global Iter: 859000 training acc: 0.1875
Global Iter: 859100 training loss: 2.08543
Global Iter: 859100 training acc: 0.125
Global Iter: 859200 training loss: 1.98174
Global Iter: 859200 training acc: 0.125
Global Iter: 859300 training loss: 1.95526
Global Iter: 859300 training acc: 0.125
Global Iter: 859400 training loss: 1.98273
Global Iter: 859400 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-859401
Number of Patches: 169901
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-859401
Global Iter: 859500 training loss: 1.95876
Global Iter: 859500 training acc: 0.09375
Global Iter: 859600 training loss: 1.91338
Global Iter: 859600 training acc: 0.21875
Global Iter: 859700 training loss: 2.04384
Global Iter: 859700 training acc: 0.125
Global Iter: 859800 training loss: 1.969
Global Iter: 859800 training acc: 0.125
Global Iter: 859900 training loss: 1.93442
Global Iter: 859900 training acc: 0.21875
Global Iter: 860000 training loss: 1.97279
Global Iter: 860000 training acc: 0.1875
Global Iter: 860100 training loss: 1.94529
Global Iter: 860100 training acc: 0.09375
Global Iter: 860200 training loss: 1.99732
Global Iter: 860200 training acc: 0.1875
Global Iter: 860300 training loss: 2.02532
Global Iter: 860300 training acc: 0.1875
Global Iter: 860400 training loss: 1.99705
Global Iter: 860400 training acc: 0.21875
Global Iter: 860500 training loss: 2.04179
Global Iter: 860500 training acc: 0.0625
Global Iter: 860600 training loss: 1.91548
Global Iter: 860600 training acc: 0.25
Global Iter: 860700 training loss: 2.15686
Global Iter: 860700 training acc: 0.1875
Global Iter: 860800 training loss: 2.06173
Global Iter: 860800 training acc: 0.1875
Global Iter: 860900 training loss: 2.01608
Global Iter: 860900 training acc: 0.09375
Global Iter: 861000 training loss: 2.08911
Global Iter: 861000 training acc: 0.15625
Global Iter: 861100 training loss: 2.02916
Global Iter: 861100 training acc: 0.3125
Global Iter: 861200 training loss: 1.99881
Global Iter: 861200 training acc: 0.1875
Global Iter: 861300 training loss: 2.03731
Global Iter: 861300 training acc: 0.09375
Global Iter: 861400 training loss: 2.00288
Global Iter: 861400 training acc: 0.3125
Global Iter: 861500 training loss: 2.00053
Global Iter: 861500 training acc: 0.21875
Global Iter: 861600 training loss: 2.04298
Global Iter: 861600 training acc: 0.3125
Global Iter: 861700 training loss: 1.86881
Global Iter: 861700 training acc: 0.28125
Global Iter: 861800 training loss: 2.17355
Global Iter: 861800 training acc: 0.21875
Global Iter: 861900 training loss: 1.98518
Global Iter: 861900 training acc: 0.15625
Global Iter: 862000 training loss: 1.99833
Global Iter: 862000 training acc: 0.34375
Global Iter: 862100 training loss: 2.02499
Global Iter: 862100 training acc: 0.15625
Global Iter: 862200 training loss: 1.96225
Global Iter: 862200 training acc: 0.1875
Global Iter: 862300 training loss: 1.89758
Global Iter: 862300 training acc: 0.21875
Global Iter: 862400 training loss: 1.943
Global Iter: 862400 training acc: 0.1875
Global Iter: 862500 training loss: 1.93874
Global Iter: 862500 training acc: 0.1875
Global Iter: 862600 training loss: 1.95935
Global Iter: 862600 training acc: 0.3125
Global Iter: 862700 training loss: 1.88294
Global Iter: 862700 training acc: 0.125
Global Iter: 862800 training loss: 2.06008
Global Iter: 862800 training acc: 0.21875
Global Iter: 862900 training loss: 2.02612
Global Iter: 862900 training acc: 0.15625
Global Iter: 863000 training loss: 1.92599
Global Iter: 863000 training acc: 0.34375
Global Iter: 863100 training loss: 1.95211
Global Iter: 863100 training acc: 0.1875
Global Iter: 863200 training loss: 1.99341
Global Iter: 863200 training acc: 0.1875
Global Iter: 863300 training loss: 1.98188
Global Iter: 863300 training acc: 0.28125
Global Iter: 863400 training loss: 1.98363
Global Iter: 863400 training acc: 0.28125
Global Iter: 863500 training loss: 2.03878
Global Iter: 863500 training acc: 0.125
Global Iter: 863600 training loss: 1.92368
Global Iter: 863600 training acc: 0.125
Global Iter: 863700 training loss: 1.97349
Global Iter: 863700 training acc: 0.25
Global Iter: 863800 training loss: 2.05267
Global Iter: 863800 training acc: 0.15625
Global Iter: 863900 training loss: 1.9908
Global Iter: 863900 training acc: 0.1875
Global Iter: 864000 training loss: 1.90794
Global Iter: 864000 training acc: 0.09375
Global Iter: 864100 training loss: 2.07302
Global Iter: 864100 training acc: 0.125
Global Iter: 864200 training loss: 1.95448
Global Iter: 864200 training acc: 0.15625
Global Iter: 864300 training loss: 2.02453
Global Iter: 864300 training acc: 0.0
Global Iter: 864400 training loss: 1.90645
Global Iter: 864400 training acc: 0.21875
Global Iter: 864500 training loss: 1.93681
Global Iter: 864500 training acc: 0.21875
Global Iter: 864600 training loss: 2.01433
Global Iter: 864600 training acc: 0.125
Global Iter: 864700 training loss: 2.03159
Global Iter: 864700 training acc: 0.1875
Global Iter: 864800 training loss: 1.95947
Global Iter: 864800 training acc: 0.15625
Global Iter: 864900 training loss: 1.9249
Global Iter: 864900 training acc: 0.21875
Global Iter: 865000 training loss: 1.9208
Global Iter: 865000 training acc: 0.25
Global Iter: 865100 training loss: 2.00076
Global Iter: 865100 training acc: 0.1875
Global Iter: 865200 training loss: 1.91866
Global Iter: 865200 training acc: 0.1875
Global Iter: 865300 training loss: 1.9636
Global Iter: 865300 training acc: 0.09375
Global Iter: 865400 training loss: 2.0134
Global Iter: 865400 training acc: 0.1875
Global Iter: 865500 training loss: 1.92602
Global Iter: 865500 training acc: 0.15625
Global Iter: 865600 training loss: 1.93213
Global Iter: 865600 training acc: 0.25
Global Iter: 865700 training loss: 1.9962
Global Iter: 865700 training acc: 0.28125
Global Iter: 865800 training loss: 1.9582
Global Iter: 865800 training acc: 0.15625
Global Iter: 865900 training loss: 2.04183
Global Iter: 865900 training acc: 0.15625
Global Iter: 866000 training loss: 1.91625
Global Iter: 866000 training acc: 0.3125
Global Iter: 866100 training loss: 1.93948
Global Iter: 866100 training acc: 0.21875
Global Iter: 866200 training loss: 1.97311
Global Iter: 866200 training acc: 0.15625
Global Iter: 866300 training loss: 1.98716
Global Iter: 866300 training acc: 0.21875
Global Iter: 866400 training loss: 1.99034
Global Iter: 866400 training acc: 0.03125
Global Iter: 866500 training loss: 2.01476
Global Iter: 866500 training acc: 0.1875
Global Iter: 866600 training loss: 1.91216
Global Iter: 866600 training acc: 0.21875
Global Iter: 866700 training loss: 2.02949
Global Iter: 866700 training acc: 0.21875
Global Iter: 866800 training loss: 2.00442
Global Iter: 866800 training acc: 0.25
Global Iter: 866900 training loss: 2.02875
Global Iter: 866900 training acc: 0.28125
Global Iter: 867000 training loss: 2.03294
Global Iter: 867000 training acc: 0.125
Global Iter: 867100 training loss: 1.94037
Global Iter: 867100 training acc: 0.21875
Global Iter: 867200 training loss: 2.09995
Global Iter: 867200 training acc: 0.0625
Global Iter: 867300 training loss: 2.00434
Global Iter: 867300 training acc: 0.25
Global Iter: 867400 training loss: 1.93036
Global Iter: 867400 training acc: 0.3125
Global Iter: 867500 training loss: 1.98474
Global Iter: 867500 training acc: 0.125
Global Iter: 867600 training loss: 1.98839
Global Iter: 867600 training acc: 0.1875
Global Iter: 867700 training loss: 1.92556
Global Iter: 867700 training acc: 0.15625
Global Iter: 867800 training loss: 1.91309
Global Iter: 867800 training acc: 0.1875
Global Iter: 867900 training loss: 1.91588
Global Iter: 867900 training acc: 0.25
Global Iter: 868000 training loss: 2.09038
Global Iter: 868000 training acc: 0.15625
Global Iter: 868100 training loss: 1.96679
Global Iter: 868100 training acc: 0.21875
Global Iter: 868200 training loss: 2.03337
Global Iter: 868200 training acc: 0.09375
Global Iter: 868300 training loss: 1.92859
Global Iter: 868300 training acc: 0.21875
Global Iter: 868400 training loss: 2.07048
Global Iter: 868400 training acc: 0.28125
Global Iter: 868500 training loss: 1.93437
Global Iter: 868500 training acc: 0.1875
Global Iter: 868600 training loss: 2.01528
Global Iter: 868600 training acc: 0.1875
Global Iter: 868700 training loss: 2.00205
Global Iter: 868700 training acc: 0.09375
Global Iter: 868800 training loss: 2.01847
Global Iter: 868800 training acc: 0.21875
Global Iter: 868900 training loss: 1.94117
Global Iter: 868900 training acc: 0.1875
Global Iter: 869000 training loss: 1.9425
Global Iter: 869000 training acc: 0.3125
Global Iter: 869100 training loss: 1.93198
Global Iter: 869100 training acc: 0.1875
Global Iter: 869200 training loss: 1.99532
Global Iter: 869200 training acc: 0.125
Global Iter: 869300 training loss: 2.01173
Global Iter: 869300 training acc: 0.1875
Global Iter: 869400 training loss: 2.18241
Global Iter: 869400 training 2017-06-22 05:54:28.894227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-870020
acc: 0.0625
Global Iter: 869500 training loss: 2.001
Global Iter: 869500 training acc: 0.25
Global Iter: 869600 training loss: 1.96737
Global Iter: 869600 training acc: 0.25
Global Iter: 869700 training loss: 2.03222
Global Iter: 869700 training acc: 0.125
Global Iter: 869800 training loss: 2.06425
Global Iter: 869800 training acc: 0.125
Global Iter: 869900 training loss: 1.9625
Global Iter: 869900 training acc: 0.0625
Global Iter: 870000 training loss: 2.06725
Global Iter: 870000 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-870020
Number of Patches: 168202
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-870020
Global Iter: 870100 training loss: 2.03582
Global Iter: 870100 training acc: 0.09375
Global Iter: 870200 training loss: 1.97201
Global Iter: 870200 training acc: 0.15625
Global Iter: 870300 training loss: 2.14666
Global Iter: 870300 training acc: 0.15625
Global Iter: 870400 training loss: 2.07287
Global Iter: 870400 training acc: 0.15625
Global Iter: 870500 training loss: 1.91459
Global Iter: 870500 training acc: 0.21875
Global Iter: 870600 training loss: 1.90193
Global Iter: 870600 training acc: 0.25
Global Iter: 870700 training loss: 1.94239
Global Iter: 870700 training acc: 0.1875
Global Iter: 870800 training loss: 1.95262
Global Iter: 870800 training acc: 0.21875
Global Iter: 870900 training loss: 1.9252
Global Iter: 870900 training acc: 0.25
Global Iter: 871000 training loss: 1.92786
Global Iter: 871000 training acc: 0.34375
Global Iter: 871100 training loss: 1.97377
Global Iter: 871100 training acc: 0.15625
Global Iter: 871200 training loss: 1.99892
Global Iter: 871200 training acc: 0.09375
Global Iter: 871300 training loss: 1.93542
Global Iter: 871300 training acc: 0.25
Global Iter: 871400 training loss: 1.96252
Global Iter: 871400 training acc: 0.15625
Global Iter: 871500 training loss: 2.05565
Global Iter: 871500 training acc: 0.15625
Global Iter: 871600 training loss: 1.98866
Global Iter: 871600 training acc: 0.21875
Global Iter: 871700 training loss: 1.91409
Global Iter: 871700 training acc: 0.21875
Global Iter: 871800 training loss: 1.96432
Global Iter: 871800 training acc: 0.15625
Global Iter: 871900 training loss: 1.8495
Global Iter: 871900 training acc: 0.25
Global Iter: 872000 training loss: 1.9013
Global Iter: 872000 training acc: 0.125
Global Iter: 872100 training loss: 1.97932
Global Iter: 872100 training acc: 0.1875
Global Iter: 872200 training loss: 2.0923
Global Iter: 872200 training acc: 0.09375
Global Iter: 872300 training loss: 1.90932
Global Iter: 872300 training acc: 0.21875
Global Iter: 872400 training loss: 2.06019
Global Iter: 872400 training acc: 0.15625
Global Iter: 872500 training loss: 2.02169
Global Iter: 872500 training acc: 0.25
Global Iter: 872600 training loss: 1.9599
Global Iter: 872600 training acc: 0.21875
Global Iter: 872700 training loss: 2.1192
Global Iter: 872700 training acc: 0.15625
Global Iter: 872800 training loss: 1.95143
Global Iter: 872800 training acc: 0.1875
Global Iter: 872900 training loss: 2.02937
Global Iter: 872900 training acc: 0.125
Global Iter: 873000 training loss: 2.04549
Global Iter: 873000 training acc: 0.1875
Global Iter: 873100 training loss: 1.92044
Global Iter: 873100 training acc: 0.25
Global Iter: 873200 training loss: 1.91619
Global Iter: 873200 training acc: 0.28125
Global Iter: 873300 training loss: 2.06809
Global Iter: 873300 training acc: 0.125
Global Iter: 873400 training loss: 1.97147
Global Iter: 873400 training acc: 0.1875
Global Iter: 873500 training loss: 1.89047
Global Iter: 873500 training acc: 0.25
Global Iter: 873600 training loss: 1.95842
Global Iter: 873600 training acc: 0.3125
Global Iter: 873700 training loss: 1.93401
Global Iter: 873700 training acc: 0.25
Global Iter: 873800 training loss: 1.9022
Global Iter: 873800 training acc: 0.1875
Global Iter: 873900 training loss: 1.94956
Global Iter: 873900 training acc: 0.21875
Global Iter: 874000 training loss: 1.99396
Global Iter: 874000 training acc: 0.09375
Global Iter: 874100 training loss: 2.03543
Global Iter: 874100 training acc: 0.125
Global Iter: 874200 training loss: 2.02656
Global Iter: 874200 training acc: 0.125
Global Iter: 874300 training loss: 1.94657
Global Iter: 874300 training acc: 0.125
Global Iter: 874400 training loss: 2.16189
Global Iter: 874400 training acc: 0.125
Global Iter: 874500 training loss: 1.91173
Global Iter: 874500 training acc: 0.125
Global Iter: 874600 training loss: 1.98412
Global Iter: 874600 training acc: 0.1875
Global Iter: 874700 training loss: 1.9637
Global Iter: 874700 training acc: 0.125
Global Iter: 874800 training loss: 2.17856
Global Iter: 874800 training acc: 0.0625
Global Iter: 874900 training loss: 1.98039
Global Iter: 874900 training acc: 0.125
Global Iter: 875000 training loss: 1.94226
Global Iter: 875000 training acc: 0.1875
Global Iter: 875100 training loss: 2.04657
Global Iter: 875100 training acc: 0.1875
Global Iter: 875200 training loss: 1.95594
Global Iter: 875200 training acc: 0.21875
Global Iter: 875300 training loss: 1.99598
Global Iter: 875300 training acc: 0.15625
Global Iter: 875400 training loss: 2.0563
Global Iter: 875400 training acc: 0.28125
Global Iter: 875500 training loss: 1.97899
Global Iter: 875500 training acc: 0.09375
Global Iter: 875600 training loss: 2.00117
Global Iter: 875600 training acc: 0.15625
Global Iter: 875700 training loss: 1.94912
Global Iter: 875700 training acc: 0.125
Global Iter: 875800 training loss: 2.02385
Global Iter: 875800 training acc: 0.15625
Global Iter: 875900 training loss: 1.91833
Global Iter: 875900 training acc: 0.21875
Global Iter: 876000 training loss: 1.92712
Global Iter: 876000 training acc: 0.1875
Global Iter: 876100 training loss: 2.02881
Global Iter: 876100 training acc: 0.1875
Global Iter: 876200 training loss: 2.11793
Global Iter: 876200 training acc: 0.15625
Global Iter: 876300 training loss: 1.901
Global Iter: 876300 training acc: 0.1875
Global Iter: 876400 training loss: 1.96934
Global Iter: 876400 training acc: 0.1875
Global Iter: 876500 training loss: 2.00522
Global Iter: 876500 training acc: 0.09375
Global Iter: 876600 training loss: 2.00982
Global Iter: 876600 training acc: 0.0625
Global Iter: 876700 training loss: 1.9911
Global Iter: 876700 training acc: 0.1875
Global Iter: 876800 training loss: 1.84429
Global Iter: 876800 training acc: 0.28125
Global Iter: 876900 training loss: 2.06949
Global Iter: 876900 training acc: 0.125
Global Iter: 877000 training loss: 1.94888
Global Iter: 877000 training acc: 0.15625
Global Iter: 877100 training loss: 2.02164
Global Iter: 877100 training acc: 0.125
Global Iter: 877200 training loss: 2.11946
Global Iter: 877200 training acc: 0.15625
Global Iter: 877300 training loss: 2.0011
Global Iter: 877300 training acc: 0.1875
Global Iter: 877400 training loss: 1.95103
Global Iter: 877400 training acc: 0.21875
Global Iter: 877500 training loss: 2.01357
Global Iter: 877500 training acc: 0.125
Global Iter: 877600 training loss: 1.90368
Global Iter: 877600 training acc: 0.3125
Global Iter: 877700 training loss: 2.03741
Global Iter: 877700 training acc: 0.125
Global Iter: 877800 training loss: 2.05029
Global Iter: 877800 training acc: 0.15625
Global Iter: 877900 training loss: 2.04836
Global Iter: 877900 training acc: 0.15625
Global Iter: 878000 training loss: 1.93178
Global Iter: 878000 training acc: 0.25
Global Iter: 878100 training loss: 1.91008
Global Iter: 878100 training acc: 0.375
Global Iter: 878200 training loss: 1.92505
Global Iter: 878200 training acc: 0.28125
Global Iter: 878300 training loss: 1.95216
Global Iter: 878300 training acc: 0.34375
Global Iter: 878400 training loss: 1.99826
Global Iter: 878400 training acc: 0.21875
Global Iter: 878500 training loss: 1.96514
Global Iter: 878500 training acc: 0.125
Global Iter: 878600 training loss: 1.97319
Global Iter: 878600 training acc: 0.21875
Global Iter: 878700 training loss: 1.94903
Global Iter: 878700 training acc: 0.1875
Global Iter: 878800 training loss: 1.88381
Global Iter: 878800 training acc: 0.375
Global Iter: 878900 training loss: 1.95754
Global Iter: 878900 training acc: 02017-06-22 06:12:30.315359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-880533
.15625
Global Iter: 879000 training loss: 1.99645
Global Iter: 879000 training acc: 0.1875
Global Iter: 879100 training loss: 2.12746
Global Iter: 879100 training acc: 0.125
Global Iter: 879200 training loss: 2.0308
Global Iter: 879200 training acc: 0.125
Global Iter: 879300 training loss: 1.99757
Global Iter: 879300 training acc: 0.3125
Global Iter: 879400 training loss: 1.99775
Global Iter: 879400 training acc: 0.1875
Global Iter: 879500 training loss: 2.04957
Global Iter: 879500 training acc: 0.15625
Global Iter: 879600 training loss: 2.03879
Global Iter: 879600 training acc: 0.15625
Global Iter: 879700 training loss: 1.96186
Global Iter: 879700 training acc: 0.25
Global Iter: 879800 training loss: 2.04477
Global Iter: 879800 training acc: 0.25
Global Iter: 879900 training loss: 2.0564
Global Iter: 879900 training acc: 0.1875
Global Iter: 880000 training loss: 2.00838
Global Iter: 880000 training acc: 0.1875
Global Iter: 880100 training loss: 2.00289
Global Iter: 880100 training acc: 0.125
Global Iter: 880200 training loss: 1.9899
Global Iter: 880200 training acc: 0.1875
Global Iter: 880300 training loss: 1.99735
Global Iter: 880300 training acc: 0.15625
Global Iter: 880400 training loss: 2.08459
Global Iter: 880400 training acc: 0.125
Global Iter: 880500 training loss: 1.95968
Global Iter: 880500 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-880533
Number of Patches: 166520
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-880533
Global Iter: 880600 training loss: 2.06251
Global Iter: 880600 training acc: 0.09375
Global Iter: 880700 training loss: 1.934
Global Iter: 880700 training acc: 0.25
Global Iter: 880800 training loss: 2.04634
Global Iter: 880800 training acc: 0.28125
Global Iter: 880900 training loss: 2.1202
Global Iter: 880900 training acc: 0.15625
Global Iter: 881000 training loss: 1.95375
Global Iter: 881000 training acc: 0.21875
Global Iter: 881100 training loss: 2.05515
Global Iter: 881100 training acc: 0.15625
Global Iter: 881200 training loss: 1.92394
Global Iter: 881200 training acc: 0.1875
Global Iter: 881300 training loss: 1.98782
Global Iter: 881300 training acc: 0.125
Global Iter: 881400 training loss: 2.09266
Global Iter: 881400 training acc: 0.1875
Global Iter: 881500 training loss: 1.98479
Global Iter: 881500 training acc: 0.25
Global Iter: 881600 training loss: 1.97724
Global Iter: 881600 training acc: 0.28125
Global Iter: 881700 training loss: 2.0193
Global Iter: 881700 training acc: 0.09375
Global Iter: 881800 training loss: 2.01883
Global Iter: 881800 training acc: 0.15625
Global Iter: 881900 training loss: 2.06602
Global Iter: 881900 training acc: 0.15625
Global Iter: 882000 training loss: 1.93354
Global Iter: 882000 training acc: 0.1875
Global Iter: 882100 training loss: 2.05769
Global Iter: 882100 training acc: 0.15625
Global Iter: 882200 training loss: 2.04254
Global Iter: 882200 training acc: 0.1875
Global Iter: 882300 training loss: 1.99101
Global Iter: 882300 training acc: 0.15625
Global Iter: 882400 training loss: 1.94706
Global Iter: 882400 training acc: 0.28125
Global Iter: 882500 training loss: 2.14759
Global Iter: 882500 training acc: 0.15625
Global Iter: 882600 training loss: 1.86765
Global Iter: 882600 training acc: 0.21875
Global Iter: 882700 training loss: 1.99048
Global Iter: 882700 training acc: 0.28125
Global Iter: 882800 training loss: 1.9681
Global Iter: 882800 training acc: 0.15625
Global Iter: 882900 training loss: 1.87429
Global Iter: 882900 training acc: 0.375
Global Iter: 883000 training loss: 1.97859
Global Iter: 883000 training acc: 0.125
Global Iter: 883100 training loss: 1.95509
Global Iter: 883100 training acc: 0.21875
Global Iter: 883200 training loss: 2.0163
Global Iter: 883200 training acc: 0.15625
Global Iter: 883300 training loss: 1.96078
Global Iter: 883300 training acc: 0.09375
Global Iter: 883400 training loss: 2.07448
Global Iter: 883400 training acc: 0.09375
Global Iter: 883500 training loss: 1.95806
Global Iter: 883500 training acc: 0.25
Global Iter: 883600 training loss: 1.90788
Global Iter: 883600 training acc: 0.21875
Global Iter: 883700 training loss: 2.00755
Global Iter: 883700 training acc: 0.21875
Global Iter: 883800 training loss: 1.9852
Global Iter: 883800 training acc: 0.0625
Global Iter: 883900 training loss: 2.0147
Global Iter: 883900 training acc: 0.21875
Global Iter: 884000 training loss: 2.00906
Global Iter: 884000 training acc: 0.25
Global Iter: 884100 training loss: 1.93309
Global Iter: 884100 training acc: 0.15625
Global Iter: 884200 training loss: 1.98476
Global Iter: 884200 training acc: 0.1875
Global Iter: 884300 training loss: 2.01884
Global Iter: 884300 training acc: 0.125
Global Iter: 884400 training loss: 1.96793
Global Iter: 884400 training acc: 0.21875
Global Iter: 884500 training loss: 2.13702
Global Iter: 884500 training acc: 0.125
Global Iter: 884600 training loss: 2.0677
Global Iter: 884600 training acc: 0.125
Global Iter: 884700 training loss: 2.09455
Global Iter: 884700 training acc: 0.09375
Global Iter: 884800 training loss: 2.05438
Global Iter: 884800 training acc: 0.09375
Global Iter: 884900 training loss: 2.03203
Global Iter: 884900 training acc: 0.25
Global Iter: 885000 training loss: 1.92972
Global Iter: 885000 training acc: 0.25
Global Iter: 885100 training loss: 1.87027
Global Iter: 885100 training acc: 0.3125
Global Iter: 885200 training loss: 2.06308
Global Iter: 885200 training acc: 0.34375
Global Iter: 885300 training loss: 2.00464
Global Iter: 885300 training acc: 0.125
Global Iter: 885400 training loss: 1.95859
Global Iter: 885400 training acc: 0.25
Global Iter: 885500 training loss: 1.94212
Global Iter: 885500 training acc: 0.125
Global Iter: 885600 training loss: 2.03738
Global Iter: 885600 training acc: 0.09375
Global Iter: 885700 training loss: 1.95889
Global Iter: 885700 training acc: 0.1875
Global Iter: 885800 training loss: 1.94505
Global Iter: 885800 training acc: 0.125
Global Iter: 885900 training loss: 2.05423
Global Iter: 885900 training acc: 0.1875
Global Iter: 886000 training loss: 2.04695
Global Iter: 886000 training acc: 0.09375
Global Iter: 886100 training loss: 1.9823
Global Iter: 886100 training acc: 0.25
Global Iter: 886200 training loss: 1.91428
Global Iter: 886200 training acc: 0.125
Global Iter: 886300 training loss: 1.9975
Global Iter: 886300 training acc: 0.21875
Global Iter: 886400 training loss: 2.01023
Global Iter: 886400 training acc: 0.09375
Global Iter: 886500 training loss: 1.99171
Global Iter: 886500 training acc: 0.15625
Global Iter: 886600 training loss: 1.96499
Global Iter: 886600 training acc: 0.21875
Global Iter: 886700 training loss: 2.05199
Global Iter: 886700 training acc: 0.125
Global Iter: 886800 training loss: 2.09214
Global Iter: 886800 training acc: 0.0625
Global Iter: 886900 training loss: 2.08048
Global Iter: 886900 training acc: 0.09375
Global Iter: 887000 training loss: 1.94187
Global Iter: 887000 training acc: 0.1875
Global Iter: 887100 training loss: 2.00907
Global Iter: 887100 training acc: 0.125
Global Iter: 887200 training loss: 1.9913
Global Iter: 887200 training acc: 0.21875
Global Iter: 887300 training loss: 1.92646
Global Iter: 887300 training acc: 0.21875
Global Iter: 887400 training loss: 1.98875
Global Iter: 887400 training acc: 0.25
Global Iter: 887500 training loss: 2.03739
Global Iter: 887500 training acc: 0.125
Global Iter: 887600 training loss: 1.9383
Global Iter: 887600 training acc: 0.15625
Global Iter: 887700 training loss: 2.01511
Global Iter: 887700 training acc: 0.0625
Global Iter: 887800 training loss: 2.09911
Global Iter: 887800 training acc: 0.25
Global Iter: 887900 training loss: 2.08382
Global Iter: 887900 training acc: 0.09375
Global Iter: 888000 training loss: 2.04149
Global Iter: 888000 training acc: 0.1875
Global Iter: 888100 training loss: 1.96238
Global Iter: 888100 training acc: 0.15625
Global Iter: 888200 training loss: 2.02243
Global Iter: 888200 training acc: 0.15625
Global Iter: 888300 training loss: 2.03402
Global Iter: 888300 training acc: 0.15625
Global Iter: 888400 training loss: 2.06119
Global Iter: 888400 training acc: 0.2017-06-22 06:30:17.019508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-890941
125
Global Iter: 888500 training loss: 1.9817
Global Iter: 888500 training acc: 0.09375
Global Iter: 888600 training loss: 1.9791
Global Iter: 888600 training acc: 0.1875
Global Iter: 888700 training loss: 1.92373
Global Iter: 888700 training acc: 0.25
Global Iter: 888800 training loss: 2.01774
Global Iter: 888800 training acc: 0.15625
Global Iter: 888900 training loss: 2.012
Global Iter: 888900 training acc: 0.21875
Global Iter: 889000 training loss: 2.02142
Global Iter: 889000 training acc: 0.09375
Global Iter: 889100 training loss: 1.85818
Global Iter: 889100 training acc: 0.28125
Global Iter: 889200 training loss: 1.9919
Global Iter: 889200 training acc: 0.28125
Global Iter: 889300 training loss: 1.93008
Global Iter: 889300 training acc: 0.21875
Global Iter: 889400 training loss: 1.95937
Global Iter: 889400 training acc: 0.28125
Global Iter: 889500 training loss: 2.0408
Global Iter: 889500 training acc: 0.1875
Global Iter: 889600 training loss: 1.99268
Global Iter: 889600 training acc: 0.09375
Global Iter: 889700 training loss: 1.94176
Global Iter: 889700 training acc: 0.21875
Global Iter: 889800 training loss: 2.00957
Global Iter: 889800 training acc: 0.1875
Global Iter: 889900 training loss: 2.04658
Global Iter: 889900 training acc: 0.09375
Global Iter: 890000 training loss: 1.96055
Global Iter: 890000 training acc: 0.21875
Global Iter: 890100 training loss: 2.03696
Global Iter: 890100 training acc: 0.1875
Global Iter: 890200 training loss: 2.02528
Global Iter: 890200 training acc: 0.15625
Global Iter: 890300 training loss: 1.9571
Global Iter: 890300 training acc: 0.1875
Global Iter: 890400 training loss: 1.95887
Global Iter: 890400 training acc: 0.15625
Global Iter: 890500 training loss: 1.95795
Global Iter: 890500 training acc: 0.1875
Global Iter: 890600 training loss: 1.97938
Global Iter: 890600 training acc: 0.25
Global Iter: 890700 training loss: 2.04087
Global Iter: 890700 training acc: 0.21875
Global Iter: 890800 training loss: 1.96894
Global Iter: 890800 training acc: 0.1875
Global Iter: 890900 training loss: 2.04767
Global Iter: 890900 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-890941
Number of Patches: 164855
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-890941
Global Iter: 891000 training loss: 1.95878
Global Iter: 891000 training acc: 0.25
Global Iter: 891100 training loss: 1.99996
Global Iter: 891100 training acc: 0.125
Global Iter: 891200 training loss: 1.90819
Global Iter: 891200 training acc: 0.25
Global Iter: 891300 training loss: 2.00844
Global Iter: 891300 training acc: 0.15625
Global Iter: 891400 training loss: 1.88735
Global Iter: 891400 training acc: 0.125
Global Iter: 891500 training loss: 2.10405
Global Iter: 891500 training acc: 0.1875
Global Iter: 891600 training loss: 1.99661
Global Iter: 891600 training acc: 0.1875
Global Iter: 891700 training loss: 2.0012
Global Iter: 891700 training acc: 0.15625
Global Iter: 891800 training loss: 2.0891
Global Iter: 891800 training acc: 0.1875
Global Iter: 891900 training loss: 1.9929
Global Iter: 891900 training acc: 0.21875
Global Iter: 892000 training loss: 2.01734
Global Iter: 892000 training acc: 0.21875
Global Iter: 892100 training loss: 1.969
Global Iter: 892100 training acc: 0.1875
Global Iter: 892200 training loss: 1.96312
Global Iter: 892200 training acc: 0.21875
Global Iter: 892300 training loss: 1.98969
Global Iter: 892300 training acc: 0.1875
Global Iter: 892400 training loss: 2.01956
Global Iter: 892400 training acc: 0.25
Global Iter: 892500 training loss: 1.95057
Global Iter: 892500 training acc: 0.15625
Global Iter: 892600 training loss: 2.03056
Global Iter: 892600 training acc: 0.25
Global Iter: 892700 training loss: 2.11088
Global Iter: 892700 training acc: 0.1875
Global Iter: 892800 training loss: 2.01267
Global Iter: 892800 training acc: 0.21875
Global Iter: 892900 training loss: 2.03376
Global Iter: 892900 training acc: 0.25
Global Iter: 893000 training loss: 2.0463
Global Iter: 893000 training acc: 0.125
Global Iter: 893100 training loss: 1.97095
Global Iter: 893100 training acc: 0.125
Global Iter: 893200 training loss: 2.00163
Global Iter: 893200 training acc: 0.21875
Global Iter: 893300 training loss: 1.9972
Global Iter: 893300 training acc: 0.125
Global Iter: 893400 training loss: 2.02071
Global Iter: 893400 training acc: 0.15625
Global Iter: 893500 training loss: 2.05728
Global Iter: 893500 training acc: 0.09375
Global Iter: 893600 training loss: 1.88556
Global Iter: 893600 training acc: 0.25
Global Iter: 893700 training loss: 1.99752
Global Iter: 893700 training acc: 0.125
Global Iter: 893800 training loss: 1.98314
Global Iter: 893800 training acc: 0.21875
Global Iter: 893900 training loss: 2.00465
Global Iter: 893900 training acc: 0.1875
Global Iter: 894000 training loss: 1.97818
Global Iter: 894000 training acc: 0.15625
Global Iter: 894100 training loss: 2.151
Global Iter: 894100 training acc: 0.125
Global Iter: 894200 training loss: 1.99599
Global Iter: 894200 training acc: 0.15625
Global Iter: 894300 training loss: 1.9086
Global Iter: 894300 training acc: 0.09375
Global Iter: 894400 training loss: 1.9198
Global Iter: 894400 training acc: 0.1875
Global Iter: 894500 training loss: 1.94056
Global Iter: 894500 training acc: 0.15625
Global Iter: 894600 training loss: 2.10281
Global Iter: 894600 training acc: 0.09375
Global Iter: 894700 training loss: 1.94557
Global Iter: 894700 training acc: 0.1875
Global Iter: 894800 training loss: 2.04556
Global Iter: 894800 training acc: 0.125
Global Iter: 894900 training loss: 2.00823
Global Iter: 894900 training acc: 0.15625
Global Iter: 895000 training loss: 1.9258
Global Iter: 895000 training acc: 0.15625
Global Iter: 895100 training loss: 1.92746
Global Iter: 895100 training acc: 0.21875
Global Iter: 895200 training loss: 1.96318
Global Iter: 895200 training acc: 0.21875
Global Iter: 895300 training loss: 2.03937
Global Iter: 895300 training acc: 0.1875
Global Iter: 895400 training loss: 2.00026
Global Iter: 895400 training acc: 0.09375
Global Iter: 895500 training loss: 1.88387
Global Iter: 895500 training acc: 0.125
Global Iter: 895600 training loss: 1.93635
Global Iter: 895600 training acc: 0.15625
Global Iter: 895700 training loss: 1.93281
Global Iter: 895700 training acc: 0.15625
Global Iter: 895800 training loss: 1.93709
Global Iter: 895800 training acc: 0.15625
Global Iter: 895900 training loss: 1.9177
Global Iter: 895900 training acc: 0.125
Global Iter: 896000 training loss: 1.90574
Global Iter: 896000 training acc: 0.4375
Global Iter: 896100 training loss: 1.98717
Global Iter: 896100 training acc: 0.21875
Global Iter: 896200 training loss: 1.9107
Global Iter: 896200 training acc: 0.21875
Global Iter: 896300 training loss: 1.92032
Global Iter: 896300 training acc: 0.15625
Global Iter: 896400 training loss: 1.93132
Global Iter: 896400 training acc: 0.25
Global Iter: 896500 training loss: 1.94376
Global Iter: 896500 training acc: 0.1875
Global Iter: 896600 training loss: 1.94876
Global Iter: 896600 training acc: 0.1875
Global Iter: 896700 training loss: 1.97837
Global Iter: 896700 training acc: 0.15625
Global Iter: 896800 training loss: 1.94438
Global Iter: 896800 training acc: 0.25
Global Iter: 896900 training loss: 1.95781
Global Iter: 896900 training acc: 0.15625
Global Iter: 897000 training loss: 1.9729
Global Iter: 897000 training acc: 0.15625
Global Iter: 897100 training loss: 2.1149
Global Iter: 897100 training acc: 0.15625
Global Iter: 897200 training loss: 2.16028
Global Iter: 897200 training acc: 0.125
Global Iter: 897300 training loss: 1.96789
Global Iter: 897300 training acc: 0.21875
Global Iter: 897400 training loss: 2.03542
Global Iter: 897400 training acc: 0.15625
Global Iter: 897500 training loss: 1.98826
Global Iter: 897500 training acc: 0.3125
Global Iter: 897600 training loss: 2.06902
Global Iter: 897600 training acc: 0.09375
Global Iter: 897700 training loss: 2.00427
Global Iter: 897700 training acc: 0.28125
Global Iter: 897800 training loss: 2.02736
Global Iter: 897800 training acc: 0.15625
Global Iter: 897900 training loss: 2.10005
Global Iter: 897900 training 2017-06-22 06:47:58.656182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-901245
acc: 0.1875
Global Iter: 898000 training loss: 1.91913
Global Iter: 898000 training acc: 0.3125
Global Iter: 898100 training loss: 1.95298
Global Iter: 898100 training acc: 0.1875
Global Iter: 898200 training loss: 1.97607
Global Iter: 898200 training acc: 0.15625
Global Iter: 898300 training loss: 1.90623
Global Iter: 898300 training acc: 0.28125
Global Iter: 898400 training loss: 1.93096
Global Iter: 898400 training acc: 0.125
Global Iter: 898500 training loss: 2.05555
Global Iter: 898500 training acc: 0.125
Global Iter: 898600 training loss: 2.08089
Global Iter: 898600 training acc: 0.125
Global Iter: 898700 training loss: 1.94824
Global Iter: 898700 training acc: 0.15625
Global Iter: 898800 training loss: 2.06444
Global Iter: 898800 training acc: 0.1875
Global Iter: 898900 training loss: 2.00047
Global Iter: 898900 training acc: 0.125
Global Iter: 899000 training loss: 1.96157
Global Iter: 899000 training acc: 0.3125
Global Iter: 899100 training loss: 2.0139
Global Iter: 899100 training acc: 0.15625
Global Iter: 899200 training loss: 2.01327
Global Iter: 899200 training acc: 0.1875
Global Iter: 899300 training loss: 2.02378
Global Iter: 899300 training acc: 0.1875
Global Iter: 899400 training loss: 1.91233
Global Iter: 899400 training acc: 0.1875
Global Iter: 899500 training loss: 2.06874
Global Iter: 899500 training acc: 0.3125
Global Iter: 899600 training loss: 1.99872
Global Iter: 899600 training acc: 0.125
Global Iter: 899700 training loss: 2.06916
Global Iter: 899700 training acc: 0.15625
Global Iter: 899800 training loss: 1.98753
Global Iter: 899800 training acc: 0.25
Global Iter: 899900 training loss: 2.0432
Global Iter: 899900 training acc: 0.0625
Global Iter: 900000 training loss: 1.9009
Global Iter: 900000 training acc: 0.0625
Global Iter: 900100 training loss: 1.90718
Global Iter: 900100 training acc: 0.125
Global Iter: 900200 training loss: 1.93751
Global Iter: 900200 training acc: 0.1875
Global Iter: 900300 training loss: 1.96106
Global Iter: 900300 training acc: 0.125
Global Iter: 900400 training loss: 1.90689
Global Iter: 900400 training acc: 0.1875
Global Iter: 900500 training loss: 1.96168
Global Iter: 900500 training acc: 0.3125
Global Iter: 900600 training loss: 2.01736
Global Iter: 900600 training acc: 0.15625
Global Iter: 900700 training loss: 1.9112
Global Iter: 900700 training acc: 0.28125
Global Iter: 900800 training loss: 1.9542
Global Iter: 900800 training acc: 0.1875
Global Iter: 900900 training loss: 1.87934
Global Iter: 900900 training acc: 0.3125
Global Iter: 901000 training loss: 1.96297
Global Iter: 901000 training acc: 0.125
Global Iter: 901100 training loss: 1.84138
Global Iter: 901100 training acc: 0.3125
Global Iter: 901200 training loss: 1.9556
Global Iter: 901200 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-901245
Number of Patches: 163211
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-901245
Global Iter: 901300 training loss: 1.92793
Global Iter: 901300 training acc: 0.25
Global Iter: 901400 training loss: 2.02098
Global Iter: 901400 training acc: 0.15625
Global Iter: 901500 training loss: 1.91704
Global Iter: 901500 training acc: 0.15625
Global Iter: 901600 training loss: 2.12155
Global Iter: 901600 training acc: 0.15625
Global Iter: 901700 training loss: 2.05397
Global Iter: 901700 training acc: 0.1875
Global Iter: 901800 training loss: 1.87334
Global Iter: 901800 training acc: 0.21875
Global Iter: 901900 training loss: 2.03616
Global Iter: 901900 training acc: 0.125
Global Iter: 902000 training loss: 1.90453
Global Iter: 902000 training acc: 0.125
Global Iter: 902100 training loss: 1.94114
Global Iter: 902100 training acc: 0.25
Global Iter: 902200 training loss: 1.91094
Global Iter: 902200 training acc: 0.3125
Global Iter: 902300 training loss: 2.01121
Global Iter: 902300 training acc: 0.1875
Global Iter: 902400 training loss: 1.92468
Global Iter: 902400 training acc: 0.21875
Global Iter: 902500 training loss: 1.96195
Global Iter: 902500 training acc: 0.21875
Global Iter: 902600 training loss: 1.88316
Global Iter: 902600 training acc: 0.25
Global Iter: 902700 training loss: 1.9953
Global Iter: 902700 training acc: 0.125
Global Iter: 902800 training loss: 1.88314
Global Iter: 902800 training acc: 0.28125
Global Iter: 902900 training loss: 2.02996
Global Iter: 902900 training acc: 0.1875
Global Iter: 903000 training loss: 2.05485
Global Iter: 903000 training acc: 0.125
Global Iter: 903100 training loss: 2.02502
Global Iter: 903100 training acc: 0.15625
Global Iter: 903200 training loss: 1.90426
Global Iter: 903200 training acc: 0.15625
Global Iter: 903300 training loss: 1.96009
Global Iter: 903300 training acc: 0.1875
Global Iter: 903400 training loss: 2.13644
Global Iter: 903400 training acc: 0.125
Global Iter: 903500 training loss: 1.99703
Global Iter: 903500 training acc: 0.1875
Global Iter: 903600 training loss: 2.04699
Global Iter: 903600 training acc: 0.03125
Global Iter: 903700 training loss: 1.90616
Global Iter: 903700 training acc: 0.3125
Global Iter: 903800 training loss: 2.00116
Global Iter: 903800 training acc: 0.15625
Global Iter: 903900 training loss: 2.05881
Global Iter: 903900 training acc: 0.125
Global Iter: 904000 training loss: 2.02491
Global Iter: 904000 training acc: 0.25
Global Iter: 904100 training loss: 1.9812
Global Iter: 904100 training acc: 0.125
Global Iter: 904200 training loss: 1.94264
Global Iter: 904200 training acc: 0.21875
Global Iter: 904300 training loss: 1.96958
Global Iter: 904300 training acc: 0.125
Global Iter: 904400 training loss: 1.96616
Global Iter: 904400 training acc: 0.25
Global Iter: 904500 training loss: 1.98133
Global Iter: 904500 training acc: 0.1875
Global Iter: 904600 training loss: 2.03017
Global Iter: 904600 training acc: 0.0625
Global Iter: 904700 training loss: 2.03716
Global Iter: 904700 training acc: 0.125
Global Iter: 904800 training loss: 2.02614
Global Iter: 904800 training acc: 0.28125
Global Iter: 904900 training loss: 1.92283
Global Iter: 904900 training acc: 0.15625
Global Iter: 905000 training loss: 1.96678
Global Iter: 905000 training acc: 0.21875
Global Iter: 905100 training loss: 1.99578
Global Iter: 905100 training acc: 0.09375
Global Iter: 905200 training loss: 1.89554
Global Iter: 905200 training acc: 0.25
Global Iter: 905300 training loss: 2.07473
Global Iter: 905300 training acc: 0.1875
Global Iter: 905400 training loss: 2.0288
Global Iter: 905400 training acc: 0.25
Global Iter: 905500 training loss: 1.86355
Global Iter: 905500 training acc: 0.25
Global Iter: 905600 training loss: 2.05774
Global Iter: 905600 training acc: 0.125
Global Iter: 905700 training loss: 1.95284
Global Iter: 905700 training acc: 0.25
Global Iter: 905800 training loss: 2.04124
Global Iter: 905800 training acc: 0.15625
Global Iter: 905900 training loss: 1.91324
Global Iter: 905900 training acc: 0.15625
Global Iter: 906000 training loss: 2.03141
Global Iter: 906000 training acc: 0.28125
Global Iter: 906100 training loss: 1.92577
Global Iter: 906100 training acc: 0.25
Global Iter: 906200 training loss: 1.97044
Global Iter: 906200 training acc: 0.21875
Global Iter: 906300 training loss: 2.05244
Global Iter: 906300 training acc: 0.1875
Global Iter: 906400 training loss: 1.90229
Global Iter: 906400 training acc: 0.1875
Global Iter: 906500 training loss: 1.96738
Global Iter: 906500 training acc: 0.15625
Global Iter: 906600 training loss: 1.97887
Global Iter: 906600 training acc: 0.125
Global Iter: 906700 training loss: 1.98354
Global Iter: 906700 training acc: 0.21875
Global Iter: 906800 training loss: 1.94349
Global Iter: 906800 training acc: 0.15625
Global Iter: 906900 training loss: 2.0312
Global Iter: 906900 training acc: 0.21875
Global Iter: 907000 training loss: 1.98863
Global Iter: 907000 training acc: 0.28125
Global Iter: 907100 training loss: 1.92847
Global Iter: 907100 training acc: 0.1875
Global Iter: 907200 training loss: 2.04653
Global Iter: 907200 training acc: 0.25
Global Iter: 907300 training loss: 1.96114
Global Iter: 907300 training acc: 0.1875
Global Iter: 907400 training loss: 2.03022
Global Iter: 907400 training acc: 0.282017-06-22 07:05:43.723776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-911446
125
Global Iter: 907500 training loss: 2.0762
Global Iter: 907500 training acc: 0.125
Global Iter: 907600 training loss: 2.00105
Global Iter: 907600 training acc: 0.125
Global Iter: 907700 training loss: 1.94774
Global Iter: 907700 training acc: 0.3125
Global Iter: 907800 training loss: 2.08078
Global Iter: 907800 training acc: 0.0625
Global Iter: 907900 training loss: 2.01291
Global Iter: 907900 training acc: 0.125
Global Iter: 908000 training loss: 2.00593
Global Iter: 908000 training acc: 0.15625
Global Iter: 908100 training loss: 2.00316
Global Iter: 908100 training acc: 0.125
Global Iter: 908200 training loss: 2.04211
Global Iter: 908200 training acc: 0.15625
Global Iter: 908300 training loss: 2.11907
Global Iter: 908300 training acc: 0.0625
Global Iter: 908400 training loss: 2.00112
Global Iter: 908400 training acc: 0.125
Global Iter: 908500 training loss: 1.94056
Global Iter: 908500 training acc: 0.3125
Global Iter: 908600 training loss: 1.97072
Global Iter: 908600 training acc: 0.21875
Global Iter: 908700 training loss: 1.98668
Global Iter: 908700 training acc: 0.1875
Global Iter: 908800 training loss: 2.03309
Global Iter: 908800 training acc: 0.21875
Global Iter: 908900 training loss: 2.03464
Global Iter: 908900 training acc: 0.125
Global Iter: 909000 training loss: 1.95264
Global Iter: 909000 training acc: 0.0625
Global Iter: 909100 training loss: 1.93663
Global Iter: 909100 training acc: 0.1875
Global Iter: 909200 training loss: 1.97972
Global Iter: 909200 training acc: 0.15625
Global Iter: 909300 training loss: 1.94885
Global Iter: 909300 training acc: 0.28125
Global Iter: 909400 training loss: 2.04381
Global Iter: 909400 training acc: 0.1875
Global Iter: 909500 training loss: 2.02256
Global Iter: 909500 training acc: 0.25
Global Iter: 909600 training loss: 2.11086
Global Iter: 909600 training acc: 0.125
Global Iter: 909700 training loss: 1.95023
Global Iter: 909700 training acc: 0.15625
Global Iter: 909800 training loss: 1.99023
Global Iter: 909800 training acc: 0.25
Global Iter: 909900 training loss: 1.98902
Global Iter: 909900 training acc: 0.21875
Global Iter: 910000 training loss: 1.94956
Global Iter: 910000 training acc: 0.28125
Global Iter: 910100 training loss: 1.89653
Global Iter: 910100 training acc: 0.25
Global Iter: 910200 training loss: 2.03621
Global Iter: 910200 training acc: 0.125
Global Iter: 910300 training loss: 2.01962
Global Iter: 910300 training acc: 0.25
Global Iter: 910400 training loss: 1.94652
Global Iter: 910400 training acc: 0.21875
Global Iter: 910500 training loss: 2.04521
Global Iter: 910500 training acc: 0.125
Global Iter: 910600 training loss: 1.972
Global Iter: 910600 training acc: 0.15625
Global Iter: 910700 training loss: 1.95699
Global Iter: 910700 training acc: 0.09375
Global Iter: 910800 training loss: 1.98013
Global Iter: 910800 training acc: 0.0625
Global Iter: 910900 training loss: 2.01753
Global Iter: 910900 training acc: 0.09375
Global Iter: 911000 training loss: 2.05391
Global Iter: 911000 training acc: 0.1875
Global Iter: 911100 training loss: 2.01116
Global Iter: 911100 training acc: 0.09375
Global Iter: 911200 training loss: 2.15597
Global Iter: 911200 training acc: 0.1875
Global Iter: 911300 training loss: 2.02764
Global Iter: 911300 training acc: 0.125
Global Iter: 911400 training loss: 1.92721
Global Iter: 911400 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-911446
Number of Patches: 161579
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-911446
Global Iter: 911500 training loss: 2.07191
Global Iter: 911500 training acc: 0.15625
Global Iter: 911600 training loss: 1.90585
Global Iter: 911600 training acc: 0.1875
Global Iter: 911700 training loss: 1.98678
Global Iter: 911700 training acc: 0.1875
Global Iter: 911800 training loss: 1.98109
Global Iter: 911800 training acc: 0.09375
Global Iter: 911900 training loss: 1.87479
Global Iter: 911900 training acc: 0.28125
Global Iter: 912000 training loss: 2.02857
Global Iter: 912000 training acc: 0.03125
Global Iter: 912100 training loss: 1.93207
Global Iter: 912100 training acc: 0.15625
Global Iter: 912200 training loss: 2.0452
Global Iter: 912200 training acc: 0.21875
Global Iter: 912300 training loss: 1.96043
Global Iter: 912300 training acc: 0.1875
Global Iter: 912400 training loss: 2.06794
Global Iter: 912400 training acc: 0.09375
Global Iter: 912500 training loss: 1.93561
Global Iter: 912500 training acc: 0.15625
Global Iter: 912600 training loss: 2.03843
Global Iter: 912600 training acc: 0.25
Global Iter: 912700 training loss: 1.95062
Global Iter: 912700 training acc: 0.1875
Global Iter: 912800 training loss: 2.06848
Global Iter: 912800 training acc: 0.03125
Global Iter: 912900 training loss: 1.9033
Global Iter: 912900 training acc: 0.1875
Global Iter: 913000 training loss: 2.03218
Global Iter: 913000 training acc: 0.25
Global Iter: 913100 training loss: 1.93213
Global Iter: 913100 training acc: 0.25
Global Iter: 913200 training loss: 2.0485
Global Iter: 913200 training acc: 0.09375
Global Iter: 913300 training loss: 1.90768
Global Iter: 913300 training acc: 0.3125
Global Iter: 913400 training loss: 1.97608
Global Iter: 913400 training acc: 0.21875
Global Iter: 913500 training loss: 1.95875
Global Iter: 913500 training acc: 0.25
Global Iter: 913600 training loss: 1.98526
Global Iter: 913600 training acc: 0.21875
Global Iter: 913700 training loss: 1.97549
Global Iter: 913700 training acc: 0.28125
Global Iter: 913800 training loss: 1.97947
Global Iter: 913800 training acc: 0.1875
Global Iter: 913900 training loss: 2.08944
Global Iter: 913900 training acc: 0.21875
Global Iter: 914000 training loss: 1.92147
Global Iter: 914000 training acc: 0.1875
Global Iter: 914100 training loss: 1.97157
Global Iter: 914100 training acc: 0.21875
Global Iter: 914200 training loss: 1.99659
Global Iter: 914200 training acc: 0.15625
Global Iter: 914300 training loss: 2.10216
Global Iter: 914300 training acc: 0.125
Global Iter: 914400 training loss: 1.91556
Global Iter: 914400 training acc: 0.34375
Global Iter: 914500 training loss: 2.03232
Global Iter: 914500 training acc: 0.1875
Global Iter: 914600 training loss: 2.01579
Global Iter: 914600 training acc: 0.0625
Global Iter: 914700 training loss: 2.07624
Global Iter: 914700 training acc: 0.21875
Global Iter: 914800 training loss: 1.95243
Global Iter: 914800 training acc: 0.28125
Global Iter: 914900 training loss: 2.06576
Global Iter: 914900 training acc: 0.1875
Global Iter: 915000 training loss: 2.0446
Global Iter: 915000 training acc: 0.1875
Global Iter: 915100 training loss: 1.98464
Global Iter: 915100 training acc: 0.1875
Global Iter: 915200 training loss: 1.95056
Global Iter: 915200 training acc: 0.15625
Global Iter: 915300 training loss: 1.93398
Global Iter: 915300 training acc: 0.21875
Global Iter: 915400 training loss: 2.01487
Global Iter: 915400 training acc: 0.21875
Global Iter: 915500 training loss: 1.88988
Global Iter: 915500 training acc: 0.25
Global Iter: 915600 training loss: 2.14744
Global Iter: 915600 training acc: 0.1875
Global Iter: 915700 training loss: 1.97748
Global Iter: 915700 training acc: 0.125
Global Iter: 915800 training loss: 1.99308
Global Iter: 915800 training acc: 0.3125
Global Iter: 915900 training loss: 1.95319
Global Iter: 915900 training acc: 0.21875
Global Iter: 916000 training loss: 2.04076
Global Iter: 916000 training acc: 0.15625
Global Iter: 916100 training loss: 1.97416
Global Iter: 916100 training acc: 0.28125
Global Iter: 916200 training loss: 1.89335
Global Iter: 916200 training acc: 0.1875
Global Iter: 916300 training loss: 2.01012
Global Iter: 916300 training acc: 0.1875
Global Iter: 916400 training loss: 2.07749
Global Iter: 916400 training acc: 0.1875
Global Iter: 916500 training loss: 2.08489
Global Iter: 916500 training acc: 0.1875
Global Iter: 916600 training loss: 1.88803
Global Iter: 916600 training acc: 0.1875
Global Iter: 916700 training loss: 2.10229
Global Iter: 916700 training acc: 0.125
Global Iter: 916800 training loss: 1.89315
Global Iter: 916800 training acc: 0.1875
Global Iter: 916900 training loss: 2.04301
Global Iter: 916900 trainin2017-06-22 07:23:04.724233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
g acc: 0.21875
Global Iter: 917000 training loss: 1.99438
Global Iter: 917000 training acc: 0.125
Global Iter: 917100 training loss: 1.96321
Global Iter: 917100 training acc: 0.15625
Global Iter: 917200 training loss: 1.97688
Global Iter: 917200 training acc: 0.21875
Global Iter: 917300 training loss: 1.9907
Global Iter: 917300 training acc: 0.21875
Global Iter: 917400 training loss: 2.02947
Global Iter: 917400 training acc: 0.15625
Global Iter: 917500 training loss: 2.01616
Global Iter: 917500 training acc: 0.21875
Global Iter: 917600 training loss: 1.97001
Global Iter: 917600 training acc: 0.1875
Global Iter: 917700 training loss: 1.89149
Global Iter: 917700 training acc: 0.28125
Global Iter: 917800 training loss: 1.95475
Global Iter: 917800 training acc: 0.125
Global Iter: 917900 training loss: 1.9054
Global Iter: 917900 training acc: 0.1875
Global Iter: 918000 training loss: 2.02243
Global Iter: 918000 training acc: 0.1875
Global Iter: 918100 training loss: 1.89881
Global Iter: 918100 training acc: 0.34375
Global Iter: 918200 training loss: 2.04768
Global Iter: 918200 training acc: 0.125
Global Iter: 918300 training loss: 1.99651
Global Iter: 918300 training acc: 0.21875
Global Iter: 918400 training loss: 2.11475
Global Iter: 918400 training acc: 0.125
Global Iter: 918500 training loss: 1.98735
Global Iter: 918500 training acc: 0.21875
Global Iter: 918600 training loss: 1.94738
Global Iter: 918600 training acc: 0.21875
Global Iter: 918700 training loss: 2.01066
Global Iter: 918700 training acc: 0.125
Global Iter: 918800 training loss: 2.02927
Global Iter: 918800 training acc: 0.1875
Global Iter: 918900 training loss: 1.90877
Global Iter: 918900 training acc: 0.28125
Global Iter: 919000 training loss: 1.97417
Global Iter: 919000 training acc: 0.15625
Global Iter: 919100 training loss: 2.02487
Global Iter: 919100 training acc: 0.1875
Global Iter: 919200 training loss: 1.92502
Global Iter: 919200 training acc: 0.25
Global Iter: 919300 training loss: 1.99612
Global Iter: 919300 training acc: 0.125
Global Iter: 919400 training loss: 1.97737
Global Iter: 919400 training acc: 0.1875
Global Iter: 919500 training loss: 2.04349
Global Iter: 919500 training acc: 0.25
Global Iter: 919600 training loss: 2.00945
Global Iter: 919600 training acc: 0.125
Global Iter: 919700 training loss: 2.1084
Global Iter: 919700 training acc: 0.1875
Global Iter: 919800 training loss: 1.98647
Global Iter: 919800 training acc: 0.21875
Global Iter: 919900 training loss: 2.12152
Global Iter: 919900 training acc: 0.21875
Global Iter: 920000 training loss: 1.99321
Global Iter: 920000 training acc: 0.125
Global Iter: 920100 training loss: 2.05333
Global Iter: 920100 training acc: 0.3125
Global Iter: 920200 training loss: 1.98719
Global Iter: 920200 training acc: 0.15625
Global Iter: 920300 training loss: 1.97332
Global Iter: 920300 training acc: 0.21875
Global Iter: 920400 training loss: 1.98502
Global Iter: 920400 training acc: 0.25
Global Iter: 920500 training loss: 1.96496
Global Iter: 920500 training acc: 0.21875
Global Iter: 920600 training loss: 1.87988
Global Iter: 920600 training acc: 0.1875
Global Iter: 920700 training loss: 2.01929
Global Iter: 920700 training acc: 0.125
Global Iter: 920800 training loss: 2.0932
Global Iter: 920800 training acc: 0.1875
Global Iter: 920900 training loss: 1.97759
Global Iter: 920900 training acc: 0.0625
Global Iter: 921000 training loss: 2.01816
Global Iter: 921000 training acc: 0.15625
Global Iter: 921100 training loss: 2.02124
Global Iter: 921100 training acc: 0.1875
Global Iter: 921200 training loss: 1.99454
Global Iter: 921200 training acc: 0.0625
Global Iter: 921300 training loss: 1.97339
Global Iter: 921300 training acc: 0.125
Global Iter: 921400 training loss: 2.05567
Global Iter: 921400 training acc: 0.21875
Global Iter: 921500 training loss: 2.03365
Global Iter: 921500 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-921545
Number of Patches: 159969
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-9215INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-921545
45
Global Iter: 921600 training loss: 1.91237
Global Iter: 921600 training acc: 0.21875
Global Iter: 921700 training loss: 1.95857
Global Iter: 921700 training acc: 0.25
Global Iter: 921800 training loss: 2.05671
Global Iter: 921800 training acc: 0.1875
Global Iter: 921900 training loss: 2.07478
Global Iter: 921900 training acc: 0.15625
Global Iter: 922000 training loss: 1.96294
Global Iter: 922000 training acc: 0.125
Global Iter: 922100 training loss: 1.94754
Global Iter: 922100 training acc: 0.125
Global Iter: 922200 training loss: 2.08917
Global Iter: 922200 training acc: 0.15625
Global Iter: 922300 training loss: 2.04222
Global Iter: 922300 training acc: 0.1875
Global Iter: 922400 training loss: 1.92955
Global Iter: 922400 training acc: 0.25
Global Iter: 922500 training loss: 1.93924
Global Iter: 922500 training acc: 0.1875
Global Iter: 922600 training loss: 2.01242
Global Iter: 922600 training acc: 0.125
Global Iter: 922700 training loss: 1.97923
Global Iter: 922700 training acc: 0.25
Global Iter: 922800 training loss: 1.99599
Global Iter: 922800 training acc: 0.09375
Global Iter: 922900 training loss: 2.01864
Global Iter: 922900 training acc: 0.21875
Global Iter: 923000 training loss: 1.92317
Global Iter: 923000 training acc: 0.21875
Global Iter: 923100 training loss: 2.02255
Global Iter: 923100 training acc: 0.0625
Global Iter: 923200 training loss: 2.01741
Global Iter: 923200 training acc: 0.09375
Global Iter: 923300 training loss: 2.00482
Global Iter: 923300 training acc: 0.15625
Global Iter: 923400 training loss: 1.94357
Global Iter: 923400 training acc: 0.125
Global Iter: 923500 training loss: 2.07331
Global Iter: 923500 training acc: 0.09375
Global Iter: 923600 training loss: 2.0268
Global Iter: 923600 training acc: 0.1875
Global Iter: 923700 training loss: 1.9463
Global Iter: 923700 training acc: 0.25
Global Iter: 923800 training loss: 2.18969
Global Iter: 923800 training acc: 0.09375
Global Iter: 923900 training loss: 2.0302
Global Iter: 923900 training acc: 0.3125
Global Iter: 924000 training loss: 1.95091
Global Iter: 924000 training acc: 0.15625
Global Iter: 924100 training loss: 2.00494
Global Iter: 924100 training acc: 0.1875
Global Iter: 924200 training loss: 1.93315
Global Iter: 924200 training acc: 0.15625
Global Iter: 924300 training loss: 1.90609
Global Iter: 924300 training acc: 0.21875
Global Iter: 924400 training loss: 2.07014
Global Iter: 924400 training acc: 0.1875
Global Iter: 924500 training loss: 2.04267
Global Iter: 924500 training acc: 0.0625
Global Iter: 924600 training loss: 2.03073
Global Iter: 924600 training acc: 0.15625
Global Iter: 924700 training loss: 1.99875
Global Iter: 924700 training acc: 0.21875
Global Iter: 924800 training loss: 2.02251
Global Iter: 924800 training acc: 0.1875
Global Iter: 924900 training loss: 1.98801
Global Iter: 924900 training acc: 0.28125
Global Iter: 925000 training loss: 2.00719
Global Iter: 925000 training acc: 0.15625
Global Iter: 925100 training loss: 1.98092
Global Iter: 925100 training acc: 0.34375
Global Iter: 925200 training loss: 1.98241
Global Iter: 925200 training acc: 0.03125
Global Iter: 925300 training loss: 1.99698
Global Iter: 925300 training acc: 0.125
Global Iter: 925400 training loss: 2.00075
Global Iter: 925400 training acc: 0.25
Global Iter: 925500 training loss: 2.19647
Global Iter: 925500 training acc: 0.0625
Global Iter: 925600 training loss: 2.03722
Global Iter: 925600 training acc: 0.125
Global Iter: 925700 training loss: 2.05674
Global Iter: 925700 training acc: 0.09375
Global Iter: 925800 training loss: 2.07386
Global Iter: 925800 training acc: 0.15625
Global Iter: 925900 training loss: 1.93795
Global Iter: 925900 training acc: 0.15625
Global Iter: 926000 training loss: 2.01535
Global Iter: 926000 training acc: 0.21875
Global Iter: 926100 training loss: 2.07761
Global Iter: 926100 training acc: 0.1875
Global Iter: 926200 training loss: 1.98535
Global Iter: 926200 training acc: 0.15625
Global Iter: 926300 training loss: 1.9629
Global Iter: 926300 training acc: 0.21875
Global Iter: 926400 training loss: 2.05331
Global Iter: 926400 training acc: 0.1875
Global Iter: 926500 training loss: 1.99007
Global Iter: 926500 training acc: 0.0625
Global Iter: 926600 training loss: 1.95073
Global Iter: 926600 training acc: 0.09375
Global Iter: 926700 training loss: 1.96588
Global Iter: 926700 training acc: 0.25
Global Iter: 926800 training loss: 2.06806
Global Iter: 926800 training acc: 0.15625
Global Iter: 926900 training loss: 1.95106
Global Iter: 926900 training acc: 0.15625
Global Iter: 927000 training loss: 1.87737
Global Iter: 927000 training acc: 0.1875
Global Iter: 927100 training loss: 2.02223
Global Iter: 927100 training acc: 0.21875
Global Iter: 927200 training loss: 1.92736
Global Iter: 927200 training acc: 0.1875
Global Iter: 927300 training loss: 1.97191
Global Iter: 927300 training acc: 0.25
Global Iter: 927400 training loss: 1.87923
Global Iter: 927400 training acc: 0.28125
Global Iter: 927500 training loss: 1.99962
Global Iter: 927500 training acc: 0.09375
Global Iter: 927600 training loss: 2.01401
Global Iter: 927600 training acc: 0.21875
Global Iter: 927700 training loss: 1.96111
Global Iter: 927700 training acc: 0.25
Global Iter: 927800 training loss: 1.91165
Global Iter: 927800 training acc: 0.21875
Global Iter: 927900 training loss: 1.94704
Global Iter: 927900 training acc: 0.15625
Global Iter: 928000 training loss: 1.97357
Global Iter: 928000 training acc: 0.15625
Global Iter: 928100 training loss: 2.15002
Global Iter: 928100 training acc: 0.15625
Global Iter: 928200 training loss: 1.99657
Global Iter: 928200 training acc: 0.1875
Global Iter: 928300 training loss: 2.05225
Global Iter: 928300 training acc: 0.28125
Global Iter: 928400 training loss: 1.89232
Global Iter: 928400 training acc: 0.1875
Global Iter: 928500 training loss: 1.97367
Global Iter: 928500 training acc: 0.25
Global Iter: 928600 training loss: 1.97311
Global Iter: 928600 training acc: 0.125
Global Iter: 928700 training loss: 1.95568
Global Iter: 928700 training acc: 0.21875
Global Iter: 928800 training loss: 1.96764
Global Iter: 928800 training acc: 0.34375
Global Iter: 928900 training loss: 2.10124
Global Iter: 928900 training acc: 0.125
Global Iter: 929000 training loss: 1.92697
Global Iter: 929000 training acc: 0.28125
Global Iter: 929100 training loss: 2.05254
Global Iter: 929100 training acc: 0.09375
Global Iter: 929200 training loss: 1.96511
Global Iter: 929200 training acc: 0.28125
Global Iter: 929300 training loss: 2.07091
Global Iter: 929300 training acc: 0.09375
Global Iter: 929400 training loss: 1.99979
Global Iter: 929400 training acc: 0.21875
Global Iter: 929500 training loss: 2.00382
Global Iter: 929500 training acc: 0.15625
Global Iter: 929600 training loss: 2.05435
Global Iter: 929600 training acc: 0.1875
Global Iter: 929700 training loss: 2.02298
Global Iter: 929700 training acc: 0.15625
Global Iter: 929800 training loss: 2.01939
Global Iter: 929800 training acc: 0.25
Global Iter: 929900 training loss: 2.08466
Global Iter: 929900 training acc: 0.03125
Global Iter: 930000 training loss: 2.04647
Global Iter: 930000 training acc: 0.1875
Global Iter: 930100 training loss: 1.98854
Global Iter: 930100 training acc: 0.1875
Global Iter: 930200 training loss: 2.01824
Global Iter: 930200 training acc: 0.15625
Global Iter: 930300 training loss: 2.00644
Global Iter: 930300 training acc: 0.21875
Global Iter: 930400 training loss: 1.99283
Global Iter: 930400 training acc: 0.09375
Global Iter: 930500 training loss: 2.01378
Global Iter: 930500 training acc: 0.1875
Global Iter: 930600 training loss: 1.99107
Global Iter: 930600 training acc: 0.09375
Global Iter: 930700 training loss: 1.95019
Global Iter: 930700 training acc: 0.34375
Global Iter: 930800 training loss: 2.06485
Global Iter: 930800 training acc: 0.125
Global Iter: 930900 training loss: 2.03339
Global Iter: 930900 training acc: 0.0625
Global Iter: 931000 training loss: 2.0267
Global Iter: 931000 training acc: 0.125
Global Iter: 931100 training loss: 2.07373
Global Iter: 931100 training acc: 0.0625
Global Iter: 931200 training loss: 2.01303
Global Iter: 931200 training acc: 0.0625
Global Iter: 931300 trai2017-06-22 07:40:36.299386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-931544
ning loss: 1.97239
Global Iter: 931300 training acc: 0.125
Global Iter: 931400 training loss: 2.0959
Global Iter: 931400 training acc: 0.15625
Global Iter: 931500 training loss: 2.0242
Global Iter: 931500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-931544
Number of Patches: 158370
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-931544
Global Iter: 931600 training loss: 1.92716
Global Iter: 931600 training acc: 0.21875
Global Iter: 931700 training loss: 1.96637
Global Iter: 931700 training acc: 0.15625
Global Iter: 931800 training loss: 2.01068
Global Iter: 931800 training acc: 0.125
Global Iter: 931900 training loss: 2.00565
Global Iter: 931900 training acc: 0.21875
Global Iter: 932000 training loss: 2.00616
Global Iter: 932000 training acc: 0.15625
Global Iter: 932100 training loss: 1.98309
Global Iter: 932100 training acc: 0.15625
Global Iter: 932200 training loss: 2.04136
Global Iter: 932200 training acc: 0.28125
Global Iter: 932300 training loss: 1.98652
Global Iter: 932300 training acc: 0.15625
Global Iter: 932400 training loss: 1.90725
Global Iter: 932400 training acc: 0.3125
Global Iter: 932500 training loss: 2.03731
Global Iter: 932500 training acc: 0.125
Global Iter: 932600 training loss: 1.91005
Global Iter: 932600 training acc: 0.1875
Global Iter: 932700 training loss: 1.95282
Global Iter: 932700 training acc: 0.15625
Global Iter: 932800 training loss: 1.91985
Global Iter: 932800 training acc: 0.3125
Global Iter: 932900 training loss: 1.90314
Global Iter: 932900 training acc: 0.34375
Global Iter: 933000 training loss: 1.97041
Global Iter: 933000 training acc: 0.1875
Global Iter: 933100 training loss: 1.95008
Global Iter: 933100 training acc: 0.34375
Global Iter: 933200 training loss: 1.93951
Global Iter: 933200 training acc: 0.125
Global Iter: 933300 training loss: 2.06235
Global Iter: 933300 training acc: 0.21875
Global Iter: 933400 training loss: 1.97348
Global Iter: 933400 training acc: 0.125
Global Iter: 933500 training loss: 2.06756
Global Iter: 933500 training acc: 0.125
Global Iter: 933600 training loss: 2.02919
Global Iter: 933600 training acc: 0.21875
Global Iter: 933700 training loss: 1.9739
Global Iter: 933700 training acc: 0.15625
Global Iter: 933800 training loss: 1.96447
Global Iter: 933800 training acc: 0.15625
Global Iter: 933900 training loss: 2.01218
Global Iter: 933900 training acc: 0.125
Global Iter: 934000 training loss: 1.94237
Global Iter: 934000 training acc: 0.09375
Global Iter: 934100 training loss: 2.0167
Global Iter: 934100 training acc: 0.09375
Global Iter: 934200 training loss: 1.96982
Global Iter: 934200 training acc: 0.1875
Global Iter: 934300 training loss: 1.93963
Global Iter: 934300 training acc: 0.125
Global Iter: 934400 training loss: 2.00502
Global Iter: 934400 training acc: 0.125
Global Iter: 934500 training loss: 2.0213
Global Iter: 934500 training acc: 0.25
Global Iter: 934600 training loss: 2.04326
Global Iter: 934600 training acc: 0.15625
Global Iter: 934700 training loss: 1.90038
Global Iter: 934700 training acc: 0.09375
Global Iter: 934800 training loss: 1.8938
Global Iter: 934800 training acc: 0.21875
Global Iter: 934900 training loss: 2.03004
Global Iter: 934900 training acc: 0.09375
Global Iter: 935000 training loss: 2.0334
Global Iter: 935000 training acc: 0.1875
Global Iter: 935100 training loss: 1.93633
Global Iter: 935100 training acc: 0.28125
Global Iter: 935200 training loss: 1.93577
Global Iter: 935200 training acc: 0.21875
Global Iter: 935300 training loss: 2.02917
Global Iter: 935300 training acc: 0.25
Global Iter: 935400 training loss: 1.98459
Global Iter: 935400 training acc: 0.125
Global Iter: 935500 training loss: 1.93483
Global Iter: 935500 training acc: 0.3125
Global Iter: 935600 training loss: 1.98941
Global Iter: 935600 training acc: 0.1875
Global Iter: 935700 training loss: 2.00144
Global Iter: 935700 training acc: 0.21875
Global Iter: 935800 training loss: 2.00523
Global Iter: 935800 training acc: 0.28125
Global Iter: 935900 training loss: 1.99168
Global Iter: 935900 training acc: 0.25
Global Iter: 936000 training loss: 1.91061
Global Iter: 936000 training acc: 0.28125
Global Iter: 936100 training loss: 2.03451
Global Iter: 936100 training acc: 0.28125
Global Iter: 936200 training loss: 1.92797
Global Iter: 936200 training acc: 0.28125
Global Iter: 936300 training loss: 1.98346
Global Iter: 936300 training acc: 0.09375
Global Iter: 936400 training loss: 1.90586
Global Iter: 936400 training acc: 0.21875
Global Iter: 936500 training loss: 1.92811
Global Iter: 936500 training acc: 0.1875
Global Iter: 936600 training loss: 1.96507
Global Iter: 936600 training acc: 0.15625
Global Iter: 936700 training loss: 2.0035
Global Iter: 936700 training acc: 0.09375
Global Iter: 936800 training loss: 2.01699
Global Iter: 936800 training acc: 0.21875
Global Iter: 936900 training loss: 2.00677
Global Iter: 936900 training acc: 0.15625
Global Iter: 937000 training loss: 1.96004
Global Iter: 937000 training acc: 0.21875
Global Iter: 937100 training loss: 2.02675
Global Iter: 937100 training acc: 0.3125
Global Iter: 937200 training loss: 1.97452
Global Iter: 937200 training acc: 0.1875
Global Iter: 937300 training loss: 1.92657
Global Iter: 937300 training acc: 0.28125
Global Iter: 937400 training loss: 2.03673
Global Iter: 937400 training acc: 0.125
Global Iter: 937500 training loss: 1.93744
Global Iter: 937500 training acc: 0.1875
Global Iter: 937600 training loss: 1.96002
Global Iter: 937600 training acc: 0.125
Global Iter: 937700 training loss: 1.9235
Global Iter: 937700 training acc: 0.3125
Global Iter: 937800 training loss: 1.90928
Global Iter: 937800 training acc: 0.3125
Global Iter: 937900 training loss: 1.97205
Global Iter: 937900 training acc: 0.15625
Global Iter: 938000 training loss: 1.9464
Global Iter: 938000 training acc: 0.3125
Global Iter: 938100 training loss: 1.94727
Global Iter: 938100 training acc: 0.15625
Global Iter: 938200 training loss: 2.08346
Global Iter: 938200 training acc: 0.1875
Global Iter: 938300 training loss: 2.0252
Global Iter: 938300 training acc: 0.125
Global Iter: 938400 training loss: 2.07634
Global Iter: 938400 training acc: 0.125
Global Iter: 938500 training loss: 2.04471
Global Iter: 938500 training acc: 0.21875
Global Iter: 938600 training loss: 1.95394
Global Iter: 938600 training acc: 0.1875
Global Iter: 938700 training loss: 1.96602
Global Iter: 938700 training acc: 0.125
Global Iter: 938800 training loss: 2.01473
Global Iter: 938800 training acc: 0.09375
Global Iter: 938900 training loss: 1.95161
Global Iter: 938900 training acc: 0.09375
Global Iter: 939000 training loss: 2.02324
Global Iter: 939000 training acc: 0.09375
Global Iter: 939100 training loss: 1.97617
Global Iter: 939100 training acc: 0.15625
Global Iter: 939200 training loss: 1.92365
Global Iter: 939200 training acc: 0.15625
Global Iter: 939300 training loss: 2.06653
Global Iter: 939300 training acc: 0.125
Global Iter: 939400 training loss: 1.95756
Global Iter: 939400 training acc: 0.3125
Global Iter: 939500 training loss: 2.03886
Global Iter: 939500 training acc: 0.15625
Global Iter: 939600 training loss: 1.90283
Global Iter: 939600 training acc: 0.0625
Global Iter: 939700 training loss: 1.89375
Global Iter: 939700 training acc: 0.21875
Global Iter: 939800 training loss: 2.04375
Global Iter: 939800 training acc: 0.09375
Global Iter: 939900 training loss: 1.97745
Global Iter: 939900 training acc: 0.1875
Global Iter: 940000 training loss: 1.94196
Global Iter: 940000 training acc: 0.28125
Global Iter: 940100 training loss: 1.93575
Global Iter: 940100 training acc: 0.21875
Global Iter: 940200 training loss: 2.04433
Global Iter: 940200 training acc: 0.25
Global Iter: 940300 training loss: 1.98129
Global Iter: 940300 training acc: 0.15625
Global Iter: 940400 training loss: 1.93331
Global Iter: 940400 training acc: 0.3125
Global Iter: 940500 training loss: 1.98421
Global Iter: 940500 training acc: 0.1875
Global Iter: 940600 training loss: 1.99114
Global Iter: 940600 training acc: 0.21875
Global Iter: 940700 training loss: 2.02783
Global Iter: 940700 training acc: 0.22017-06-22 07:57:54.817328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-941443
5
Global Iter: 940800 training loss: 2.00388
Global Iter: 940800 training acc: 0.25
Global Iter: 940900 training loss: 1.98613
Global Iter: 940900 training acc: 0.25
Global Iter: 941000 training loss: 2.00666
Global Iter: 941000 training acc: 0.3125
Global Iter: 941100 training loss: 1.93347
Global Iter: 941100 training acc: 0.25
Global Iter: 941200 training loss: 1.9849
Global Iter: 941200 training acc: 0.09375
Global Iter: 941300 training loss: 1.92013
Global Iter: 941300 training acc: 0.21875
Global Iter: 941400 training loss: 1.9281
Global Iter: 941400 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-941443
Number of Patches: 156801
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-941443
Global Iter: 941500 training loss: 2.07751
Global Iter: 941500 training acc: 0.125
Global Iter: 941600 training loss: 1.9736
Global Iter: 941600 training acc: 0.25
Global Iter: 941700 training loss: 1.94595
Global Iter: 941700 training acc: 0.15625
Global Iter: 941800 training loss: 1.95609
Global Iter: 941800 training acc: 0.21875
Global Iter: 941900 training loss: 1.92768
Global Iter: 941900 training acc: 0.34375
Global Iter: 942000 training loss: 1.9106
Global Iter: 942000 training acc: 0.1875
Global Iter: 942100 training loss: 2.02378
Global Iter: 942100 training acc: 0.1875
Global Iter: 942200 training loss: 2.12704
Global Iter: 942200 training acc: 0.1875
Global Iter: 942300 training loss: 2.03216
Global Iter: 942300 training acc: 0.09375
Global Iter: 942400 training loss: 1.96329
Global Iter: 942400 training acc: 0.21875
Global Iter: 942500 training loss: 1.96453
Global Iter: 942500 training acc: 0.34375
Global Iter: 942600 training loss: 1.9638
Global Iter: 942600 training acc: 0.25
Global Iter: 942700 training loss: 1.99179
Global Iter: 942700 training acc: 0.1875
Global Iter: 942800 training loss: 1.9476
Global Iter: 942800 training acc: 0.15625
Global Iter: 942900 training loss: 1.98747
Global Iter: 942900 training acc: 0.15625
Global Iter: 943000 training loss: 1.95155
Global Iter: 943000 training acc: 0.25
Global Iter: 943100 training loss: 1.92953
Global Iter: 943100 training acc: 0.3125
Global Iter: 943200 training loss: 1.93278
Global Iter: 943200 training acc: 0.25
Global Iter: 943300 training loss: 1.88845
Global Iter: 943300 training acc: 0.46875
Global Iter: 943400 training loss: 1.97748
Global Iter: 943400 training acc: 0.1875
Global Iter: 943500 training loss: 1.99712
Global Iter: 943500 training acc: 0.15625
Global Iter: 943600 training loss: 1.9439
Global Iter: 943600 training acc: 0.21875
Global Iter: 943700 training loss: 2.00848
Global Iter: 943700 training acc: 0.15625
Global Iter: 943800 training loss: 1.91186
Global Iter: 943800 training acc: 0.125
Global Iter: 943900 training loss: 1.93877
Global Iter: 943900 training acc: 0.1875
Global Iter: 944000 training loss: 1.9845
Global Iter: 944000 training acc: 0.15625
Global Iter: 944100 training loss: 2.05471
Global Iter: 944100 training acc: 0.21875
Global Iter: 944200 training loss: 1.92973
Global Iter: 944200 training acc: 0.28125
Global Iter: 944300 training loss: 2.02149
Global Iter: 944300 training acc: 0.09375
Global Iter: 944400 training loss: 2.02516
Global Iter: 944400 training acc: 0.09375
Global Iter: 944500 training loss: 2.10234
Global Iter: 944500 training acc: 0.09375
Global Iter: 944600 training loss: 1.95151
Global Iter: 944600 training acc: 0.1875
Global Iter: 944700 training loss: 1.96615
Global Iter: 944700 training acc: 0.15625
Global Iter: 944800 training loss: 1.87697
Global Iter: 944800 training acc: 0.25
Global Iter: 944900 training loss: 1.89948
Global Iter: 944900 training acc: 0.21875
Global Iter: 945000 training loss: 1.93684
Global Iter: 945000 training acc: 0.21875
Global Iter: 945100 training loss: 2.04476
Global Iter: 945100 training acc: 0.09375
Global Iter: 945200 training loss: 1.97353
Global Iter: 945200 training acc: 0.21875
Global Iter: 945300 training loss: 1.97338
Global Iter: 945300 training acc: 0.21875
Global Iter: 945400 training loss: 2.04789
Global Iter: 945400 training acc: 0.15625
Global Iter: 945500 training loss: 1.99788
Global Iter: 945500 training acc: 0.21875
Global Iter: 945600 training loss: 2.03311
Global Iter: 945600 training acc: 0.25
Global Iter: 945700 training loss: 2.05463
Global Iter: 945700 training acc: 0.1875
Global Iter: 945800 training loss: 2.01265
Global Iter: 945800 training acc: 0.1875
Global Iter: 945900 training loss: 1.92077
Global Iter: 945900 training acc: 0.3125
Global Iter: 946000 training loss: 1.98426
Global Iter: 946000 training acc: 0.1875
Global Iter: 946100 training loss: 1.93932
Global Iter: 946100 training acc: 0.1875
Global Iter: 946200 training loss: 2.00843
Global Iter: 946200 training acc: 0.1875
Global Iter: 946300 training loss: 2.03369
Global Iter: 946300 training acc: 0.1875
Global Iter: 946400 training loss: 1.93705
Global Iter: 946400 training acc: 0.15625
Global Iter: 946500 training loss: 2.03385
Global Iter: 946500 training acc: 0.0625
Global Iter: 946600 training loss: 1.94678
Global Iter: 946600 training acc: 0.28125
Global Iter: 946700 training loss: 2.14244
Global Iter: 946700 training acc: 0.09375
Global Iter: 946800 training loss: 2.08318
Global Iter: 946800 training acc: 0.125
Global Iter: 946900 training loss: 1.96028
Global Iter: 946900 training acc: 0.21875
Global Iter: 947000 training loss: 2.04325
Global Iter: 947000 training acc: 0.15625
Global Iter: 947100 training loss: 1.97419
Global Iter: 947100 training acc: 0.21875
Global Iter: 947200 training loss: 1.97606
Global Iter: 947200 training acc: 0.3125
Global Iter: 947300 training loss: 1.98822
Global Iter: 947300 training acc: 0.15625
Global Iter: 947400 training loss: 1.93863
Global Iter: 947400 training acc: 0.1875
Global Iter: 947500 training loss: 1.93554
Global Iter: 947500 training acc: 0.15625
Global Iter: 947600 training loss: 1.97295
Global Iter: 947600 training acc: 0.25
Global Iter: 947700 training loss: 2.10635
Global Iter: 947700 training acc: 0.125
Global Iter: 947800 training loss: 1.99323
Global Iter: 947800 training acc: 0.15625
Global Iter: 947900 training loss: 1.95938
Global Iter: 947900 training acc: 0.1875
Global Iter: 948000 training loss: 1.95157
Global Iter: 948000 training acc: 0.21875
Global Iter: 948100 training loss: 1.99525
Global Iter: 948100 training acc: 0.1875
Global Iter: 948200 training loss: 1.96667
Global Iter: 948200 training acc: 0.15625
Global Iter: 948300 training loss: 1.94912
Global Iter: 948300 training acc: 0.15625
Global Iter: 948400 training loss: 2.27061
Global Iter: 948400 training acc: 0.15625
Global Iter: 948500 training loss: 1.98361
Global Iter: 948500 training acc: 0.125
Global Iter: 948600 training loss: 2.01103
Global Iter: 948600 training acc: 0.1875
Global Iter: 948700 training loss: 2.04323
Global Iter: 948700 training acc: 0.125
Global Iter: 948800 training loss: 1.97438
Global Iter: 948800 training acc: 0.1875
Global Iter: 948900 training loss: 1.92636
Global Iter: 948900 training acc: 0.25
Global Iter: 949000 training loss: 1.94713
Global Iter: 949000 training acc: 0.1875
Global Iter: 949100 training loss: 2.01543
Global Iter: 949100 training acc: 0.1875
Global Iter: 949200 training loss: 1.9659
Global Iter: 949200 training acc: 0.1875
Global Iter: 949300 training loss: 1.94323
Global Iter: 949300 training acc: 0.3125
Global Iter: 949400 training loss: 1.96533
Global Iter: 949400 training acc: 0.1875
Global Iter: 949500 training loss: 2.03098
Global Iter: 949500 training acc: 0.0625
Global Iter: 949600 training loss: 2.02368
Global Iter: 949600 training acc: 0.15625
Global Iter: 949700 training loss: 1.98505
Global Iter: 949700 training acc: 0.09375
Global Iter: 949800 training loss: 1.97506
Global Iter: 949800 training acc: 0.09375
Global Iter: 949900 training loss: 1.91721
Global Iter: 949900 training acc: 0.25
Global Iter: 950000 training loss: 1.95818
Global Iter: 950000 training acc: 0.25
Global Iter: 950100 training loss: 1.94186
Global Iter: 950100 training acc: 0.21875
Global Iter: 950200 training loss: 1.94356
Global Iter: 950200 trai2017-06-22 08:14:56.212985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-951244
ning acc: 0.21875
Global Iter: 950300 training loss: 1.9043
Global Iter: 950300 training acc: 0.3125
Global Iter: 950400 training loss: 1.97985
Global Iter: 950400 training acc: 0.1875
Global Iter: 950500 training loss: 1.97156
Global Iter: 950500 training acc: 0.125
Global Iter: 950600 training loss: 1.979
Global Iter: 950600 training acc: 0.15625
Global Iter: 950700 training loss: 1.96439
Global Iter: 950700 training acc: 0.28125
Global Iter: 950800 training loss: 1.96878
Global Iter: 950800 training acc: 0.0625
Global Iter: 950900 training loss: 2.01135
Global Iter: 950900 training acc: 0.125
Global Iter: 951000 training loss: 2.12739
Global Iter: 951000 training acc: 0.15625
Global Iter: 951100 training loss: 2.04886
Global Iter: 951100 training acc: 0.15625
Global Iter: 951200 training loss: 1.97199
Global Iter: 951200 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-951244
Number of Patches: 155233
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-951244
Global Iter: 951300 training loss: 1.98012
Global Iter: 951300 training acc: 0.125
Global Iter: 951400 training loss: 2.20373
Global Iter: 951400 training acc: 0.09375
Global Iter: 951500 training loss: 1.94426
Global Iter: 951500 training acc: 0.28125
Global Iter: 951600 training loss: 2.03885
Global Iter: 951600 training acc: 0.15625
Global Iter: 951700 training loss: 2.14822
Global Iter: 951700 training acc: 0.15625
Global Iter: 951800 training loss: 1.9377
Global Iter: 951800 training acc: 0.1875
Global Iter: 951900 training loss: 1.98546
Global Iter: 951900 training acc: 0.28125
Global Iter: 952000 training loss: 1.99725
Global Iter: 952000 training acc: 0.0625
Global Iter: 952100 training loss: 1.97201
Global Iter: 952100 training acc: 0.15625
Global Iter: 952200 training loss: 2.08509
Global Iter: 952200 training acc: 0.15625
Global Iter: 952300 training loss: 1.97805
Global Iter: 952300 training acc: 0.125
Global Iter: 952400 training loss: 2.05345
Global Iter: 952400 training acc: 0.1875
Global Iter: 952500 training loss: 1.98586
Global Iter: 952500 training acc: 0.1875
Global Iter: 952600 training loss: 2.02694
Global Iter: 952600 training acc: 0.21875
Global Iter: 952700 training loss: 2.09201
Global Iter: 952700 training acc: 0.0625
Global Iter: 952800 training loss: 1.91982
Global Iter: 952800 training acc: 0.25
Global Iter: 952900 training loss: 2.14221
Global Iter: 952900 training acc: 0.03125
Global Iter: 953000 training loss: 1.97122
Global Iter: 953000 training acc: 0.3125
Global Iter: 953100 training loss: 1.95562
Global Iter: 953100 training acc: 0.0625
Global Iter: 953200 training loss: 2.05285
Global Iter: 953200 training acc: 0.1875
Global Iter: 953300 training loss: 1.98999
Global Iter: 953300 training acc: 0.1875
Global Iter: 953400 training loss: 2.00265
Global Iter: 953400 training acc: 0.0625
Global Iter: 953500 training loss: 1.97569
Global Iter: 953500 training acc: 0.25
Global Iter: 953600 training loss: 1.99779
Global Iter: 953600 training acc: 0.09375
Global Iter: 953700 training loss: 2.03199
Global Iter: 953700 training acc: 0.125
Global Iter: 953800 training loss: 2.08206
Global Iter: 953800 training acc: 0.15625
Global Iter: 953900 training loss: 2.02538
Global Iter: 953900 training acc: 0.21875
Global Iter: 954000 training loss: 1.93462
Global Iter: 954000 training acc: 0.3125
Global Iter: 954100 training loss: 1.9602
Global Iter: 954100 training acc: 0.0625
Global Iter: 954200 training loss: 2.02529
Global Iter: 954200 training acc: 0.15625
Global Iter: 954300 training loss: 1.94029
Global Iter: 954300 training acc: 0.21875
Global Iter: 954400 training loss: 1.97328
Global Iter: 954400 training acc: 0.1875
Global Iter: 954500 training loss: 1.89052
Global Iter: 954500 training acc: 0.21875
Global Iter: 954600 training loss: 1.98413
Global Iter: 954600 training acc: 0.28125
Global Iter: 954700 training loss: 2.01501
Global Iter: 954700 training acc: 0.1875
Global Iter: 954800 training loss: 2.00685
Global Iter: 954800 training acc: 0.15625
Global Iter: 954900 training loss: 1.95717
Global Iter: 954900 training acc: 0.21875
Global Iter: 955000 training loss: 1.9156
Global Iter: 955000 training acc: 0.1875
Global Iter: 955100 training loss: 1.97682
Global Iter: 955100 training acc: 0.25
Global Iter: 955200 training loss: 1.89104
Global Iter: 955200 training acc: 0.21875
Global Iter: 955300 training loss: 1.934
Global Iter: 955300 training acc: 0.34375
Global Iter: 955400 training loss: 1.91381
Global Iter: 955400 training acc: 0.1875
Global Iter: 955500 training loss: 2.02022
Global Iter: 955500 training acc: 0.09375
Global Iter: 955600 training loss: 2.0376
Global Iter: 955600 training acc: 0.125
Global Iter: 955700 training loss: 2.03795
Global Iter: 955700 training acc: 0.0625
Global Iter: 955800 training loss: 1.99688
Global Iter: 955800 training acc: 0.15625
Global Iter: 955900 training loss: 2.12187
Global Iter: 955900 training acc: 0.09375
Global Iter: 956000 training loss: 2.06285
Global Iter: 956000 training acc: 0.15625
Global Iter: 956100 training loss: 1.99442
Global Iter: 956100 training acc: 0.21875
Global Iter: 956200 training loss: 1.98226
Global Iter: 956200 training acc: 0.125
Global Iter: 956300 training loss: 1.88876
Global Iter: 956300 training acc: 0.375
Global Iter: 956400 training loss: 1.93093
Global Iter: 956400 training acc: 0.15625
Global Iter: 956500 training loss: 2.00943
Global Iter: 956500 training acc: 0.25
Global Iter: 956600 training loss: 1.84294
Global Iter: 956600 training acc: 0.46875
Global Iter: 956700 training loss: 2.06881
Global Iter: 956700 training acc: 0.125
Global Iter: 956800 training loss: 1.91032
Global Iter: 956800 training acc: 0.1875
Global Iter: 956900 training loss: 2.01832
Global Iter: 956900 training acc: 0.15625
Global Iter: 957000 training loss: 2.00315
Global Iter: 957000 training acc: 0.25
Global Iter: 957100 training loss: 2.07282
Global Iter: 957100 training acc: 0.09375
Global Iter: 957200 training loss: 1.93965
Global Iter: 957200 training acc: 0.1875
Global Iter: 957300 training loss: 1.99325
Global Iter: 957300 training acc: 0.15625
Global Iter: 957400 training loss: 2.05854
Global Iter: 957400 training acc: 0.125
Global Iter: 957500 training loss: 2.1951
Global Iter: 957500 training acc: 0.15625
Global Iter: 957600 training loss: 1.96538
Global Iter: 957600 training acc: 0.1875
Global Iter: 957700 training loss: 2.00856
Global Iter: 957700 training acc: 0.3125
Global Iter: 957800 training loss: 2.07793
Global Iter: 957800 training acc: 0.21875
Global Iter: 957900 training loss: 2.00568
Global Iter: 957900 training acc: 0.15625
Global Iter: 958000 training loss: 1.98114
Global Iter: 958000 training acc: 0.15625
Global Iter: 958100 training loss: 1.98863
Global Iter: 958100 training acc: 0.09375
Global Iter: 958200 training loss: 2.07153
Global Iter: 958200 training acc: 0.125
Global Iter: 958300 training loss: 2.13043
Global Iter: 958300 training acc: 0.03125
Global Iter: 958400 training loss: 2.05761
Global Iter: 958400 training acc: 0.21875
Global Iter: 958500 training loss: 1.98313
Global Iter: 958500 training acc: 0.21875
Global Iter: 958600 training loss: 2.02144
Global Iter: 958600 training acc: 0.15625
Global Iter: 958700 training loss: 2.05284
Global Iter: 958700 training acc: 0.15625
Global Iter: 958800 training loss: 2.20192
Global Iter: 958800 training acc: 0.125
Global Iter: 958900 training loss: 1.95736
Global Iter: 958900 training acc: 0.09375
Global Iter: 959000 training loss: 1.89716
Global Iter: 959000 training acc: 0.28125
Global Iter: 959100 training loss: 2.03773
Global Iter: 959100 training acc: 0.15625
Global Iter: 959200 training loss: 2.01142
Global Iter: 959200 training acc: 0.125
Global Iter: 959300 training loss: 1.99948
Global Iter: 959300 training acc: 0.15625
Global Iter: 959400 training loss: 1.99391
Global Iter: 959400 training acc: 0.15625
Global Iter: 959500 training loss: 1.97008
Global Iter: 959500 training acc: 0.15625
Global Iter: 959600 training loss: 2.04873
Global Iter: 959600 training acc: 0.1875
Global Iter: 959700 training loss: 1.2017-06-22 08:31:45.676193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-960947
94125
Global Iter: 959700 training acc: 0.1875
Global Iter: 959800 training loss: 1.83996
Global Iter: 959800 training acc: 0.34375
Global Iter: 959900 training loss: 1.93483
Global Iter: 959900 training acc: 0.25
Global Iter: 960000 training loss: 2.04544
Global Iter: 960000 training acc: 0.1875
Global Iter: 960100 training loss: 2.03668
Global Iter: 960100 training acc: 0.0625
Global Iter: 960200 training loss: 2.03477
Global Iter: 960200 training acc: 0.09375
Global Iter: 960300 training loss: 2.03007
Global Iter: 960300 training acc: 0.25
Global Iter: 960400 training loss: 1.95509
Global Iter: 960400 training acc: 0.28125
Global Iter: 960500 training loss: 1.95572
Global Iter: 960500 training acc: 0.21875
Global Iter: 960600 training loss: 1.9705
Global Iter: 960600 training acc: 0.25
Global Iter: 960700 training loss: 2.01174
Global Iter: 960700 training acc: 0.1875
Global Iter: 960800 training loss: 1.89527
Global Iter: 960800 training acc: 0.28125
Global Iter: 960900 training loss: 2.04384
Global Iter: 960900 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-960947
Number of Patches: 153681
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-960947
Global Iter: 961000 training loss: 1.98485
Global Iter: 961000 training acc: 0.1875
Global Iter: 961100 training loss: 2.06393
Global Iter: 961100 training acc: 0.1875
Global Iter: 961200 training loss: 2.08904
Global Iter: 961200 training acc: 0.0625
Global Iter: 961300 training loss: 2.05567
Global Iter: 961300 training acc: 0.15625
Global Iter: 961400 training loss: 2.01279
Global Iter: 961400 training acc: 0.125
Global Iter: 961500 training loss: 1.97474
Global Iter: 961500 training acc: 0.09375
Global Iter: 961600 training loss: 1.90821
Global Iter: 961600 training acc: 0.1875
Global Iter: 961700 training loss: 1.91172
Global Iter: 961700 training acc: 0.21875
Global Iter: 961800 training loss: 2.05564
Global Iter: 961800 training acc: 0.15625
Global Iter: 961900 training loss: 2.07392
Global Iter: 961900 training acc: 0.125
Global Iter: 962000 training loss: 1.97282
Global Iter: 962000 training acc: 0.15625
Global Iter: 962100 training loss: 1.99023
Global Iter: 962100 training acc: 0.15625
Global Iter: 962200 training loss: 2.04316
Global Iter: 962200 training acc: 0.09375
Global Iter: 962300 training loss: 2.02053
Global Iter: 962300 training acc: 0.21875
Global Iter: 962400 training loss: 1.93379
Global Iter: 962400 training acc: 0.28125
Global Iter: 962500 training loss: 1.93333
Global Iter: 962500 training acc: 0.21875
Global Iter: 962600 training loss: 2.04901
Global Iter: 962600 training acc: 0.125
Global Iter: 962700 training loss: 1.91057
Global Iter: 962700 training acc: 0.09375
Global Iter: 962800 training loss: 1.95219
Global Iter: 962800 training acc: 0.25
Global Iter: 962900 training loss: 1.96355
Global Iter: 962900 training acc: 0.25
Global Iter: 963000 training loss: 1.92807
Global Iter: 963000 training acc: 0.28125
Global Iter: 963100 training loss: 1.99487
Global Iter: 963100 training acc: 0.1875
Global Iter: 963200 training loss: 1.90803
Global Iter: 963200 training acc: 0.21875
Global Iter: 963300 training loss: 1.8725
Global Iter: 963300 training acc: 0.34375
Global Iter: 963400 training loss: 2.14626
Global Iter: 963400 training acc: 0.21875
Global Iter: 963500 training loss: 1.96444
Global Iter: 963500 training acc: 0.21875
Global Iter: 963600 training loss: 1.94034
Global Iter: 963600 training acc: 0.21875
Global Iter: 963700 training loss: 2.05538
Global Iter: 963700 training acc: 0.1875
Global Iter: 963800 training loss: 1.87725
Global Iter: 963800 training acc: 0.125
Global Iter: 963900 training loss: 1.98098
Global Iter: 963900 training acc: 0.09375
Global Iter: 964000 training loss: 2.01582
Global Iter: 964000 training acc: 0.15625
Global Iter: 964100 training loss: 2.03502
Global Iter: 964100 training acc: 0.1875
Global Iter: 964200 training loss: 1.95989
Global Iter: 964200 training acc: 0.34375
Global Iter: 964300 training loss: 1.95101
Global Iter: 964300 training acc: 0.1875
Global Iter: 964400 training loss: 2.02378
Global Iter: 964400 training acc: 0.125
Global Iter: 964500 training loss: 2.00395
Global Iter: 964500 training acc: 0.09375
Global Iter: 964600 training loss: 2.05676
Global Iter: 964600 training acc: 0.125
Global Iter: 964700 training loss: 1.90789
Global Iter: 964700 training acc: 0.1875
Global Iter: 964800 training loss: 1.99749
Global Iter: 964800 training acc: 0.125
Global Iter: 964900 training loss: 1.97724
Global Iter: 964900 training acc: 0.21875
Global Iter: 965000 training loss: 2.11345
Global Iter: 965000 training acc: 0.21875
Global Iter: 965100 training loss: 2.08785
Global Iter: 965100 training acc: 0.1875
Global Iter: 965200 training loss: 2.13345
Global Iter: 965200 training acc: 0.34375
Global Iter: 965300 training loss: 2.05147
Global Iter: 965300 training acc: 0.125
Global Iter: 965400 training loss: 2.06752
Global Iter: 965400 training acc: 0.0625
Global Iter: 965500 training loss: 2.04493
Global Iter: 965500 training acc: 0.03125
Global Iter: 965600 training loss: 1.99992
Global Iter: 965600 training acc: 0.21875
Global Iter: 965700 training loss: 2.07637
Global Iter: 965700 training acc: 0.1875
Global Iter: 965800 training loss: 1.95962
Global Iter: 965800 training acc: 0.21875
Global Iter: 965900 training loss: 2.03532
Global Iter: 965900 training acc: 0.125
Global Iter: 966000 training loss: 1.96794
Global Iter: 966000 training acc: 0.1875
Global Iter: 966100 training loss: 2.02156
Global Iter: 966100 training acc: 0.21875
Global Iter: 966200 training loss: 1.93152
Global Iter: 966200 training acc: 0.1875
Global Iter: 966300 training loss: 2.10567
Global Iter: 966300 training acc: 0.125
Global Iter: 966400 training loss: 2.00892
Global Iter: 966400 training acc: 0.28125
Global Iter: 966500 training loss: 2.07408
Global Iter: 966500 training acc: 0.21875
Global Iter: 966600 training loss: 1.93193
Global Iter: 966600 training acc: 0.125
Global Iter: 966700 training loss: 1.97502
Global Iter: 966700 training acc: 0.21875
Global Iter: 966800 training loss: 2.0568
Global Iter: 966800 training acc: 0.15625
Global Iter: 966900 training loss: 2.06461
Global Iter: 966900 training acc: 0.125
Global Iter: 967000 training loss: 2.0302
Global Iter: 967000 training acc: 0.21875
Global Iter: 967100 training loss: 2.06982
Global Iter: 967100 training acc: 0.15625
Global Iter: 967200 training loss: 2.04008
Global Iter: 967200 training acc: 0.21875
Global Iter: 967300 training loss: 1.93474
Global Iter: 967300 training acc: 0.21875
Global Iter: 967400 training loss: 2.02178
Global Iter: 967400 training acc: 0.25
Global Iter: 967500 training loss: 2.0105
Global Iter: 967500 training acc: 0.21875
Global Iter: 967600 training loss: 1.99168
Global Iter: 967600 training acc: 0.25
Global Iter: 967700 training loss: 1.97518
Global Iter: 967700 training acc: 0.21875
Global Iter: 967800 training loss: 2.04827
Global Iter: 967800 training acc: 0.15625
Global Iter: 967900 training loss: 2.02728
Global Iter: 967900 training acc: 0.15625
Global Iter: 968000 training loss: 2.01167
Global Iter: 968000 training acc: 0.21875
Global Iter: 968100 training loss: 2.05827
Global Iter: 968100 training acc: 0.0625
Global Iter: 968200 training loss: 2.02542
Global Iter: 968200 training acc: 0.1875
Global Iter: 968300 training loss: 2.00698
Global Iter: 968300 training acc: 0.1875
Global Iter: 968400 training loss: 1.91017
Global Iter: 968400 training acc: 0.3125
Global Iter: 968500 training loss: 1.9684
Global Iter: 968500 training acc: 0.125
Global Iter: 968600 training loss: 2.00645
Global Iter: 968600 training acc: 0.15625
Global Iter: 968700 training loss: 1.96651
Global Iter: 968700 training acc: 0.1875
Global Iter: 968800 training loss: 1.94564
Global Iter: 968800 training acc: 0.21875
Global Iter: 968900 training loss: 1.98101
Global Iter: 968900 training acc: 0.1875
Global Iter: 969000 training loss: 2.02926
Global Iter: 969000 training acc: 0.03125
Global Iter: 969100 training loss: 1.90296
Global Iter: 969100 training acc: 0.34375
Global Iter2017-06-22 08:48:15.413006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-970553
: 969200 training loss: 2.04866
Global Iter: 969200 training acc: 0.25
Global Iter: 969300 training loss: 1.93434
Global Iter: 969300 training acc: 0.21875
Global Iter: 969400 training loss: 2.05928
Global Iter: 969400 training acc: 0.125
Global Iter: 969500 training loss: 1.88974
Global Iter: 969500 training acc: 0.34375
Global Iter: 969600 training loss: 1.93165
Global Iter: 969600 training acc: 0.1875
Global Iter: 969700 training loss: 2.0819
Global Iter: 969700 training acc: 0.125
Global Iter: 969800 training loss: 1.9313
Global Iter: 969800 training acc: 0.1875
Global Iter: 969900 training loss: 1.92442
Global Iter: 969900 training acc: 0.3125
Global Iter: 970000 training loss: 1.96241
Global Iter: 970000 training acc: 0.15625
Global Iter: 970100 training loss: 2.08763
Global Iter: 970100 training acc: 0.21875
Global Iter: 970200 training loss: 2.01919
Global Iter: 970200 training acc: 0.25
Global Iter: 970300 training loss: 1.94574
Global Iter: 970300 training acc: 0.3125
Global Iter: 970400 training loss: 1.96948
Global Iter: 970400 training acc: 0.125
Global Iter: 970500 training loss: 1.88611
Global Iter: 970500 training acc: 0.375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-970553
Number of Patches: 152145
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-970553
Global Iter: 970600 training loss: 1.99297
Global Iter: 970600 training acc: 0.1875
Global Iter: 970700 training loss: 1.99579
Global Iter: 970700 training acc: 0.28125
Global Iter: 970800 training loss: 2.09435
Global Iter: 970800 training acc: 0.1875
Global Iter: 970900 training loss: 1.9835
Global Iter: 970900 training acc: 0.21875
Global Iter: 971000 training loss: 2.11171
Global Iter: 971000 training acc: 0.15625
Global Iter: 971100 training loss: 1.94122
Global Iter: 971100 training acc: 0.15625
Global Iter: 971200 training loss: 2.09787
Global Iter: 971200 training acc: 0.09375
Global Iter: 971300 training loss: 1.91059
Global Iter: 971300 training acc: 0.1875
Global Iter: 971400 training loss: 1.87437
Global Iter: 971400 training acc: 0.25
Global Iter: 971500 training loss: 1.98381
Global Iter: 971500 training acc: 0.1875
Global Iter: 971600 training loss: 1.99332
Global Iter: 971600 training acc: 0.28125
Global Iter: 971700 training loss: 1.99864
Global Iter: 971700 training acc: 0.1875
Global Iter: 971800 training loss: 1.94933
Global Iter: 971800 training acc: 0.1875
Global Iter: 971900 training loss: 2.09106
Global Iter: 971900 training acc: 0.21875
Global Iter: 972000 training loss: 1.97696
Global Iter: 972000 training acc: 0.25
Global Iter: 972100 training loss: 1.99239
Global Iter: 972100 training acc: 0.21875
Global Iter: 972200 training loss: 1.9607
Global Iter: 972200 training acc: 0.15625
Global Iter: 972300 training loss: 2.02005
Global Iter: 972300 training acc: 0.28125
Global Iter: 972400 training loss: 2.0801
Global Iter: 972400 training acc: 0.1875
Global Iter: 972500 training loss: 1.96991
Global Iter: 972500 training acc: 0.125
Global Iter: 972600 training loss: 2.03174
Global Iter: 972600 training acc: 0.15625
Global Iter: 972700 training loss: 2.11528
Global Iter: 972700 training acc: 0.21875
Global Iter: 972800 training loss: 2.11908
Global Iter: 972800 training acc: 0.0625
Global Iter: 972900 training loss: 1.97538
Global Iter: 972900 training acc: 0.21875
Global Iter: 973000 training loss: 2.00525
Global Iter: 973000 training acc: 0.15625
Global Iter: 973100 training loss: 1.90341
Global Iter: 973100 training acc: 0.25
Global Iter: 973200 training loss: 1.95379
Global Iter: 973200 training acc: 0.1875
Global Iter: 973300 training loss: 1.9799
Global Iter: 973300 training acc: 0.28125
Global Iter: 973400 training loss: 1.95379
Global Iter: 973400 training acc: 0.15625
Global Iter: 973500 training loss: 1.9937
Global Iter: 973500 training acc: 0.0625
Global Iter: 973600 training loss: 2.07347
Global Iter: 973600 training acc: 0.1875
Global Iter: 973700 training loss: 2.06778
Global Iter: 973700 training acc: 0.09375
Global Iter: 973800 training loss: 2.02753
Global Iter: 973800 training acc: 0.09375
Global Iter: 973900 training loss: 2.00152
Global Iter: 973900 training acc: 0.09375
Global Iter: 974000 training loss: 1.96154
Global Iter: 974000 training acc: 0.15625
Global Iter: 974100 training loss: 1.98962
Global Iter: 974100 training acc: 0.15625
Global Iter: 974200 training loss: 2.01367
Global Iter: 974200 training acc: 0.09375
Global Iter: 974300 training loss: 1.95222
Global Iter: 974300 training acc: 0.25
Global Iter: 974400 training loss: 2.05077
Global Iter: 974400 training acc: 0.21875
Global Iter: 974500 training loss: 2.05719
Global Iter: 974500 training acc: 0.1875
Global Iter: 974600 training loss: 1.96661
Global Iter: 974600 training acc: 0.15625
Global Iter: 974700 training loss: 1.92414
Global Iter: 974700 training acc: 0.1875
Global Iter: 974800 training loss: 1.94281
Global Iter: 974800 training acc: 0.1875
Global Iter: 974900 training loss: 2.08104
Global Iter: 974900 training acc: 0.09375
Global Iter: 975000 training loss: 2.03493
Global Iter: 975000 training acc: 0.1875
Global Iter: 975100 training loss: 1.99708
Global Iter: 975100 training acc: 0.125
Global Iter: 975200 training loss: 1.99652
Global Iter: 975200 training acc: 0.21875
Global Iter: 975300 training loss: 1.96556
Global Iter: 975300 training acc: 0.09375
Global Iter: 975400 training loss: 2.0609
Global Iter: 975400 training acc: 0.1875
Global Iter: 975500 training loss: 1.93596
Global Iter: 975500 training acc: 0.28125
Global Iter: 975600 training loss: 1.9904
Global Iter: 975600 training acc: 0.15625
Global Iter: 975700 training loss: 2.02342
Global Iter: 975700 training acc: 0.09375
Global Iter: 975800 training loss: 2.1274
Global Iter: 975800 training acc: 0.09375
Global Iter: 975900 training loss: 1.92318
Global Iter: 975900 training acc: 0.25
Global Iter: 976000 training loss: 1.96186
Global Iter: 976000 training acc: 0.3125
Global Iter: 976100 training loss: 1.94268
Global Iter: 976100 training acc: 0.375
Global Iter: 976200 training loss: 1.96131
Global Iter: 976200 training acc: 0.1875
Global Iter: 976300 training loss: 1.95131
Global Iter: 976300 training acc: 0.15625
Global Iter: 976400 training loss: 2.02817
Global Iter: 976400 training acc: 0.09375
Global Iter: 976500 training loss: 2.0163
Global Iter: 976500 training acc: 0.1875
Global Iter: 976600 training loss: 2.0266
Global Iter: 976600 training acc: 0.15625
Global Iter: 976700 training loss: 2.02559
Global Iter: 976700 training acc: 0.25
Global Iter: 976800 training loss: 2.07546
Global Iter: 976800 training acc: 0.1875
Global Iter: 976900 training loss: 1.95766
Global Iter: 976900 training acc: 0.28125
Global Iter: 977000 training loss: 1.92756
Global Iter: 977000 training acc: 0.28125
Global Iter: 977100 training loss: 1.96334
Global Iter: 977100 training acc: 0.125
Global Iter: 977200 training loss: 1.95973
Global Iter: 977200 training acc: 0.15625
Global Iter: 977300 training loss: 2.00129
Global Iter: 977300 training acc: 0.1875
Global Iter: 977400 training loss: 2.09779
Global Iter: 977400 training acc: 0.09375
Global Iter: 977500 training loss: 1.88353
Global Iter: 977500 training acc: 0.3125
Global Iter: 977600 training loss: 1.95291
Global Iter: 977600 training acc: 0.1875
Global Iter: 977700 training loss: 1.97215
Global Iter: 977700 training acc: 0.125
Global Iter: 977800 training loss: 1.91495
Global Iter: 977800 training acc: 0.1875
Global Iter: 977900 training loss: 1.97272
Global Iter: 977900 training acc: 0.375
Global Iter: 978000 training loss: 1.96901
Global Iter: 978000 training acc: 0.125
Global Iter: 978100 training loss: 2.01987
Global Iter: 978100 training acc: 0.15625
Global Iter: 978200 training loss: 1.98728
Global Iter: 978200 training acc: 0.0625
Global Iter: 978300 training loss: 2.08886
Global Iter: 978300 training acc: 0.125
Global Iter: 978400 training loss: 1.95739
Global Iter: 978400 training acc: 0.15625
Global Iter: 978500 training loss: 1.99415
Global Iter: 978500 training acc: 0.125
Global Iter: 978600 training loss: 1.99087
Global Iter: 978600 training acc: 0.152017-06-22 09:04:31.717718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-980063
625
Global Iter: 978700 training loss: 2.04092
Global Iter: 978700 training acc: 0.0625
Global Iter: 978800 training loss: 1.92817
Global Iter: 978800 training acc: 0.1875
Global Iter: 978900 training loss: 2.05182
Global Iter: 978900 training acc: 0.125
Global Iter: 979000 training loss: 1.98082
Global Iter: 979000 training acc: 0.28125
Global Iter: 979100 training loss: 2.09047
Global Iter: 979100 training acc: 0.125
Global Iter: 979200 training loss: 1.90302
Global Iter: 979200 training acc: 0.21875
Global Iter: 979300 training loss: 1.97763
Global Iter: 979300 training acc: 0.3125
Global Iter: 979400 training loss: 1.95554
Global Iter: 979400 training acc: 0.34375
Global Iter: 979500 training loss: 2.04048
Global Iter: 979500 training acc: 0.15625
Global Iter: 979600 training loss: 1.94239
Global Iter: 979600 training acc: 0.15625
Global Iter: 979700 training loss: 1.99984
Global Iter: 979700 training acc: 0.1875
Global Iter: 979800 training loss: 1.9913
Global Iter: 979800 training acc: 0.125
Global Iter: 979900 training loss: 1.89412
Global Iter: 979900 training acc: 0.3125
Global Iter: 980000 training loss: 2.02556
Global Iter: 980000 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-980063
Number of Patches: 150624
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-980063
Global Iter: 980100 training loss: 2.14623
Global Iter: 980100 training acc: 0.15625
Global Iter: 980200 training loss: 2.09858
Global Iter: 980200 training acc: 0.1875
Global Iter: 980300 training loss: 2.00325
Global Iter: 980300 training acc: 0.09375
Global Iter: 980400 training loss: 1.95947
Global Iter: 980400 training acc: 0.21875
Global Iter: 980500 training loss: 1.94209
Global Iter: 980500 training acc: 0.15625
Global Iter: 980600 training loss: 1.94241
Global Iter: 980600 training acc: 0.21875
Global Iter: 980700 training loss: 2.11751
Global Iter: 980700 training acc: 0.125
Global Iter: 980800 training loss: 1.99311
Global Iter: 980800 training acc: 0.15625
Global Iter: 980900 training loss: 1.91651
Global Iter: 980900 training acc: 0.1875
Global Iter: 981000 training loss: 2.10496
Global Iter: 981000 training acc: 0.125
Global Iter: 981100 training loss: 1.95441
Global Iter: 981100 training acc: 0.28125
Global Iter: 981200 training loss: 2.14952
Global Iter: 981200 training acc: 0.15625
Global Iter: 981300 training loss: 1.90466
Global Iter: 981300 training acc: 0.28125
Global Iter: 981400 training loss: 1.95066
Global Iter: 981400 training acc: 0.21875
Global Iter: 981500 training loss: 1.93723
Global Iter: 981500 training acc: 0.125
Global Iter: 981600 training loss: 1.98963
Global Iter: 981600 training acc: 0.15625
Global Iter: 981700 training loss: 1.93203
Global Iter: 981700 training acc: 0.25
Global Iter: 981800 training loss: 1.88699
Global Iter: 981800 training acc: 0.28125
Global Iter: 981900 training loss: 1.87371
Global Iter: 981900 training acc: 0.28125
Global Iter: 982000 training loss: 2.04697
Global Iter: 982000 training acc: 0.21875
Global Iter: 982100 training loss: 2.16825
Global Iter: 982100 training acc: 0.125
Global Iter: 982200 training loss: 2.08015
Global Iter: 982200 training acc: 0.21875
Global Iter: 982300 training loss: 1.98966
Global Iter: 982300 training acc: 0.21875
Global Iter: 982400 training loss: 1.99504
Global Iter: 982400 training acc: 0.21875
Global Iter: 982500 training loss: 1.97513
Global Iter: 982500 training acc: 0.15625
Global Iter: 982600 training loss: 2.13259
Global Iter: 982600 training acc: 0.09375
Global Iter: 982700 training loss: 1.91936
Global Iter: 982700 training acc: 0.03125
Global Iter: 982800 training loss: 1.99482
Global Iter: 982800 training acc: 0.09375
Global Iter: 982900 training loss: 1.9731
Global Iter: 982900 training acc: 0.15625
Global Iter: 983000 training loss: 2.08757
Global Iter: 983000 training acc: 0.09375
Global Iter: 983100 training loss: 1.9548
Global Iter: 983100 training acc: 0.15625
Global Iter: 983200 training loss: 1.98176
Global Iter: 983200 training acc: 0.1875
Global Iter: 983300 training loss: 1.95907
Global Iter: 983300 training acc: 0.28125
Global Iter: 983400 training loss: 2.0018
Global Iter: 983400 training acc: 0.09375
Global Iter: 983500 training loss: 2.07875
Global Iter: 983500 training acc: 0.15625
Global Iter: 983600 training loss: 2.01588
Global Iter: 983600 training acc: 0.1875
Global Iter: 983700 training loss: 2.02245
Global Iter: 983700 training acc: 0.28125
Global Iter: 983800 training loss: 1.95247
Global Iter: 983800 training acc: 0.15625
Global Iter: 983900 training loss: 2.03042
Global Iter: 983900 training acc: 0.21875
Global Iter: 984000 training loss: 1.96557
Global Iter: 984000 training acc: 0.25
Global Iter: 984100 training loss: 2.07109
Global Iter: 984100 training acc: 0.125
Global Iter: 984200 training loss: 1.96034
Global Iter: 984200 training acc: 0.25
Global Iter: 984300 training loss: 2.07731
Global Iter: 984300 training acc: 0.28125
Global Iter: 984400 training loss: 1.92273
Global Iter: 984400 training acc: 0.21875
Global Iter: 984500 training loss: 2.03982
Global Iter: 984500 training acc: 0.1875
Global Iter: 984600 training loss: 2.00972
Global Iter: 984600 training acc: 0.15625
Global Iter: 984700 training loss: 1.97933
Global Iter: 984700 training acc: 0.21875
Global Iter: 984800 training loss: 1.93167
Global Iter: 984800 training acc: 0.09375
Global Iter: 984900 training loss: 1.88818
Global Iter: 984900 training acc: 0.1875
Global Iter: 985000 training loss: 2.09021
Global Iter: 985000 training acc: 0.09375
Global Iter: 985100 training loss: 1.93235
Global Iter: 985100 training acc: 0.21875
Global Iter: 985200 training loss: 1.96538
Global Iter: 985200 training acc: 0.15625
Global Iter: 985300 training loss: 1.96474
Global Iter: 985300 training acc: 0.1875
Global Iter: 985400 training loss: 2.03381
Global Iter: 985400 training acc: 0.09375
Global Iter: 985500 training loss: 1.97878
Global Iter: 985500 training acc: 0.125
Global Iter: 985600 training loss: 2.02072
Global Iter: 985600 training acc: 0.125
Global Iter: 985700 training loss: 1.99199
Global Iter: 985700 training acc: 0.15625
Global Iter: 985800 training loss: 1.95697
Global Iter: 985800 training acc: 0.125
Global Iter: 985900 training loss: 1.89962
Global Iter: 985900 training acc: 0.34375
Global Iter: 986000 training loss: 1.94061
Global Iter: 986000 training acc: 0.21875
Global Iter: 986100 training loss: 1.86191
Global Iter: 986100 training acc: 0.3125
Global Iter: 986200 training loss: 1.99043
Global Iter: 986200 training acc: 0.21875
Global Iter: 986300 training loss: 1.9852
Global Iter: 986300 training acc: 0.1875
Global Iter: 986400 training loss: 1.93547
Global Iter: 986400 training acc: 0.40625
Global Iter: 986500 training loss: 1.98731
Global Iter: 986500 training acc: 0.15625
Global Iter: 986600 training loss: 2.0445
Global Iter: 986600 training acc: 0.09375
Global Iter: 986700 training loss: 2.03213
Global Iter: 986700 training acc: 0.09375
Global Iter: 986800 training loss: 1.98675
Global Iter: 986800 training acc: 0.125
Global Iter: 986900 training loss: 1.93161
Global Iter: 986900 training acc: 0.25
Global Iter: 987000 training loss: 1.96305
Global Iter: 987000 training acc: 0.15625
Global Iter: 987100 training loss: 1.98309
Global Iter: 987100 training acc: 0.1875
Global Iter: 987200 training loss: 2.02845
Global Iter: 987200 training acc: 0.1875
Global Iter: 987300 training loss: 1.97346
Global Iter: 987300 training acc: 0.15625
Global Iter: 987400 training loss: 1.97241
Global Iter: 987400 training acc: 0.15625
Global Iter: 987500 training loss: 2.03923
Global Iter: 987500 training acc: 0.125
Global Iter: 987600 training loss: 1.91709
Global Iter: 987600 training acc: 0.15625
Global Iter: 987700 training loss: 1.98267
Global Iter: 987700 training acc: 0.25
Global Iter: 987800 training loss: 1.99539
Global Iter: 987800 training acc: 0.1875
Global Iter: 987900 training loss: 2.03976
Global Iter: 987900 training acc: 0.28125
Global Iter: 988000 training loss: 1.9882
Global Iter: 988000 training acc: 0.21875
Global Iter: 988100 training loss: 1.929112017-06-22 09:20:39.874172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-989477

Global Iter: 988100 training acc: 0.21875
Global Iter: 988200 training loss: 1.92262
Global Iter: 988200 training acc: 0.3125
Global Iter: 988300 training loss: 2.04766
Global Iter: 988300 training acc: 0.21875
Global Iter: 988400 training loss: 1.99498
Global Iter: 988400 training acc: 0.09375
Global Iter: 988500 training loss: 2.08481
Global Iter: 988500 training acc: 0.3125
Global Iter: 988600 training loss: 1.91687
Global Iter: 988600 training acc: 0.28125
Global Iter: 988700 training loss: 1.94704
Global Iter: 988700 training acc: 0.1875
Global Iter: 988800 training loss: 1.97821
Global Iter: 988800 training acc: 0.21875
Global Iter: 988900 training loss: 2.04167
Global Iter: 988900 training acc: 0.15625
Global Iter: 989000 training loss: 1.99201
Global Iter: 989000 training acc: 0.125
Global Iter: 989100 training loss: 1.92749
Global Iter: 989100 training acc: 0.03125
Global Iter: 989200 training loss: 1.93644
Global Iter: 989200 training acc: 0.25
Global Iter: 989300 training loss: 2.04811
Global Iter: 989300 training acc: 0.1875
Global Iter: 989400 training loss: 1.95075
Global Iter: 989400 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-989477
Number of Patches: 149118
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-989477
Global Iter: 989500 training loss: 2.03297
Global Iter: 989500 training acc: 0.1875
Global Iter: 989600 training loss: 1.96839
Global Iter: 989600 training acc: 0.21875
Global Iter: 989700 training loss: 2.03425
Global Iter: 989700 training acc: 0.09375
Global Iter: 989800 training loss: 2.02233
Global Iter: 989800 training acc: 0.1875
Global Iter: 989900 training loss: 1.94958
Global Iter: 989900 training acc: 0.15625
Global Iter: 990000 training loss: 2.03368
Global Iter: 990000 training acc: 0.3125
Global Iter: 990100 training loss: 1.9597
Global Iter: 990100 training acc: 0.1875
Global Iter: 990200 training loss: 2.12232
Global Iter: 990200 training acc: 0.09375
Global Iter: 990300 training loss: 2.05403
Global Iter: 990300 training acc: 0.09375
Global Iter: 990400 training loss: 1.99807
Global Iter: 990400 training acc: 0.25
Global Iter: 990500 training loss: 2.04367
Global Iter: 990500 training acc: 0.28125
Global Iter: 990600 training loss: 2.11188
Global Iter: 990600 training acc: 0.25
Global Iter: 990700 training loss: 1.99546
Global Iter: 990700 training acc: 0.09375
Global Iter: 990800 training loss: 1.98257
Global Iter: 990800 training acc: 0.1875
Global Iter: 990900 training loss: 1.95615
Global Iter: 990900 training acc: 0.21875
Global Iter: 991000 training loss: 2.05363
Global Iter: 991000 training acc: 0.25
Global Iter: 991100 training loss: 2.02989
Global Iter: 991100 training acc: 0.1875
Global Iter: 991200 training loss: 2.02433
Global Iter: 991200 training acc: 0.25
Global Iter: 991300 training loss: 1.98142
Global Iter: 991300 training acc: 0.09375
Global Iter: 991400 training loss: 1.90919
Global Iter: 991400 training acc: 0.28125
Global Iter: 991500 training loss: 1.89325
Global Iter: 991500 training acc: 0.1875
Global Iter: 991600 training loss: 1.97961
Global Iter: 991600 training acc: 0.09375
Global Iter: 991700 training loss: 1.89205
Global Iter: 991700 training acc: 0.25
Global Iter: 991800 training loss: 2.05482
Global Iter: 991800 training acc: 0.125
Global Iter: 991900 training loss: 2.14751
Global Iter: 991900 training acc: 0.1875
Global Iter: 992000 training loss: 1.97167
Global Iter: 992000 training acc: 0.1875
Global Iter: 992100 training loss: 2.0811
Global Iter: 992100 training acc: 0.09375
Global Iter: 992200 training loss: 1.96028
Global Iter: 992200 training acc: 0.21875
Global Iter: 992300 training loss: 2.06243
Global Iter: 992300 training acc: 0.03125
Global Iter: 992400 training loss: 2.06685
Global Iter: 992400 training acc: 0.125
Global Iter: 992500 training loss: 2.07999
Global Iter: 992500 training acc: 0.25
Global Iter: 992600 training loss: 1.91869
Global Iter: 992600 training acc: 0.25
Global Iter: 992700 training loss: 2.01653
Global Iter: 992700 training acc: 0.25
Global Iter: 992800 training loss: 2.00067
Global Iter: 992800 training acc: 0.125
Global Iter: 992900 training loss: 2.0796
Global Iter: 992900 training acc: 0.03125
Global Iter: 993000 training loss: 2.00281
Global Iter: 993000 training acc: 0.125
Global Iter: 993100 training loss: 1.95845
Global Iter: 993100 training acc: 0.125
Global Iter: 993200 training loss: 2.03538
Global Iter: 993200 training acc: 0.15625
Global Iter: 993300 training loss: 2.01937
Global Iter: 993300 training acc: 0.09375
Global Iter: 993400 training loss: 1.92018
Global Iter: 993400 training acc: 0.21875
Global Iter: 993500 training loss: 2.0269
Global Iter: 993500 training acc: 0.1875
Global Iter: 993600 training loss: 1.98865
Global Iter: 993600 training acc: 0.1875
Global Iter: 993700 training loss: 2.09353
Global Iter: 993700 training acc: 0.0625
Global Iter: 993800 training loss: 1.87418
Global Iter: 993800 training acc: 0.28125
Global Iter: 993900 training loss: 1.98594
Global Iter: 993900 training acc: 0.1875
Global Iter: 994000 training loss: 1.9861
Global Iter: 994000 training acc: 0.1875
Global Iter: 994100 training loss: 1.98568
Global Iter: 994100 training acc: 0.21875
Global Iter: 994200 training loss: 1.92991
Global Iter: 994200 training acc: 0.15625
Global Iter: 994300 training loss: 1.96772
Global Iter: 994300 training acc: 0.125
Global Iter: 994400 training loss: 2.20774
Global Iter: 994400 training acc: 0.0625
Global Iter: 994500 training loss: 1.96877
Global Iter: 994500 training acc: 0.21875
Global Iter: 994600 training loss: 1.98629
Global Iter: 994600 training acc: 0.34375
Global Iter: 994700 training loss: 1.91179
Global Iter: 994700 training acc: 0.34375
Global Iter: 994800 training loss: 2.0283
Global Iter: 994800 training acc: 0.25
Global Iter: 994900 training loss: 1.99133
Global Iter: 994900 training acc: 0.28125
Global Iter: 995000 training loss: 2.01251
Global Iter: 995000 training acc: 0.1875
Global Iter: 995100 training loss: 2.01934
Global Iter: 995100 training acc: 0.09375
Global Iter: 995200 training loss: 1.98471
Global Iter: 995200 training acc: 0.21875
Global Iter: 995300 training loss: 2.03998
Global Iter: 995300 training acc: 0.125
Global Iter: 995400 training loss: 2.11089
Global Iter: 995400 training acc: 0.0
Global Iter: 995500 training loss: 1.99374
Global Iter: 995500 training acc: 0.1875
Global Iter: 995600 training loss: 2.00375
Global Iter: 995600 training acc: 0.3125
Global Iter: 995700 training loss: 2.10931
Global Iter: 995700 training acc: 0.1875
Global Iter: 995800 training loss: 1.91848
Global Iter: 995800 training acc: 0.28125
Global Iter: 995900 training loss: 1.98581
Global Iter: 995900 training acc: 0.125
Global Iter: 996000 training loss: 2.04425
Global Iter: 996000 training acc: 0.125
Global Iter: 996100 training loss: 1.98006
Global Iter: 996100 training acc: 0.25
Global Iter: 996200 training loss: 2.02492
Global Iter: 996200 training acc: 0.125
Global Iter: 996300 training loss: 1.96605
Global Iter: 996300 training acc: 0.09375
Global Iter: 996400 training loss: 1.98732
Global Iter: 996400 training acc: 0.15625
Global Iter: 996500 training loss: 1.96149
Global Iter: 996500 training acc: 0.1875
Global Iter: 996600 training loss: 1.9185
Global Iter: 996600 training acc: 0.09375
Global Iter: 996700 training loss: 2.02513
Global Iter: 996700 training acc: 0.1875
Global Iter: 996800 training loss: 1.92608
Global Iter: 996800 training acc: 0.125
Global Iter: 996900 training loss: 1.90665
Global Iter: 996900 training acc: 0.25
Global Iter: 997000 training loss: 1.98591
Global Iter: 997000 training acc: 0.21875
Global Iter: 997100 training loss: 1.96451
Global Iter: 997100 training acc: 0.15625
Global Iter: 997200 training loss: 1.92378
Global Iter: 997200 training acc: 0.125
Global Iter: 997300 training loss: 2.02361
Global Iter: 997300 training acc: 0.09375
Global Iter: 997400 training loss: 2.03348
Global Iter: 997400 training acc: 0.09375
Global Iter: 997500 training loss: 2.01401
Global Iter: 997500 training acc: 0.15625
Global Iter: 997600 training loss: 2017-06-22 09:36:48.553577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-998797
2.09616
Global Iter: 997600 training acc: 0.15625
Global Iter: 997700 training loss: 1.989
Global Iter: 997700 training acc: 0.1875
Global Iter: 997800 training loss: 1.97108
Global Iter: 997800 training acc: 0.15625
Global Iter: 997900 training loss: 2.04116
Global Iter: 997900 training acc: 0.28125
Global Iter: 998000 training loss: 2.06869
Global Iter: 998000 training acc: 0.21875
Global Iter: 998100 training loss: 1.98616
Global Iter: 998100 training acc: 0.15625
Global Iter: 998200 training loss: 2.01491
Global Iter: 998200 training acc: 0.125
Global Iter: 998300 training loss: 1.9339
Global Iter: 998300 training acc: 0.125
Global Iter: 998400 training loss: 2.07886
Global Iter: 998400 training acc: 0.15625
Global Iter: 998500 training loss: 2.00101
Global Iter: 998500 training acc: 0.15625
Global Iter: 998600 training loss: 2.10547
Global Iter: 998600 training acc: 0.25
Global Iter: 998700 training loss: 1.90715
Global Iter: 998700 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-998797
Number of Patches: 147627
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-998797
Global Iter: 998800 training loss: 1.99017
Global Iter: 998800 training acc: 0.1875
Global Iter: 998900 training loss: 2.02374
Global Iter: 998900 training acc: 0.1875
Global Iter: 999000 training loss: 2.11036
Global Iter: 999000 training acc: 0.125
Global Iter: 999100 training loss: 1.96648
Global Iter: 999100 training acc: 0.21875
Global Iter: 999200 training loss: 2.07637
Global Iter: 999200 training acc: 0.15625
Global Iter: 999300 training loss: 1.99643
Global Iter: 999300 training acc: 0.25
Global Iter: 999400 training loss: 1.99496
Global Iter: 999400 training acc: 0.25
Global Iter: 999500 training loss: 2.01663
Global Iter: 999500 training acc: 0.09375
Global Iter: 999600 training loss: 1.9714
Global Iter: 999600 training acc: 0.125
Global Iter: 999700 training loss: 1.99275
Global Iter: 999700 training acc: 0.15625
Global Iter: 999800 training loss: 1.95159
Global Iter: 999800 training acc: 0.125
Global Iter: 999900 training loss: 1.89582
Global Iter: 999900 training acc: 0.21875
Global Iter: 1000000 training loss: 1.86432
Global Iter: 1000000 training acc: 0.21875
Global Iter: 1000100 training loss: 1.8554
Global Iter: 1000100 training acc: 0.3125
Global Iter: 1000200 training loss: 1.97257
Global Iter: 1000200 training acc: 0.1875
Global Iter: 1000300 training loss: 1.96143
Global Iter: 1000300 training acc: 0.21875
Global Iter: 1000400 training loss: 2.02147
Global Iter: 1000400 training acc: 0.28125
Global Iter: 1000500 training loss: 2.05695
Global Iter: 1000500 training acc: 0.21875
Global Iter: 1000600 training loss: 1.99185
Global Iter: 1000600 training acc: 0.21875
Global Iter: 1000700 training loss: 2.03739
Global Iter: 1000700 training acc: 0.21875
Global Iter: 1000800 training loss: 1.95583
Global Iter: 1000800 training acc: 0.375
Global Iter: 1000900 training loss: 2.10286
Global Iter: 1000900 training acc: 0.1875
Global Iter: 1001000 training loss: 2.01276
Global Iter: 1001000 training acc: 0.25
Global Iter: 1001100 training loss: 1.93447
Global Iter: 1001100 training acc: 0.125
Global Iter: 1001200 training loss: 2.04551
Global Iter: 1001200 training acc: 0.15625
Global Iter: 1001300 training loss: 2.04957
Global Iter: 1001300 training acc: 0.1875
Global Iter: 1001400 training loss: 2.08574
Global Iter: 1001400 training acc: 0.21875
Global Iter: 1001500 training loss: 2.02443
Global Iter: 1001500 training acc: 0.125
Global Iter: 1001600 training loss: 2.0263
Global Iter: 1001600 training acc: 0.0625
Global Iter: 1001700 training loss: 1.86383
Global Iter: 1001700 training acc: 0.25
Global Iter: 1001800 training loss: 1.97135
Global Iter: 1001800 training acc: 0.1875
Global Iter: 1001900 training loss: 1.91074
Global Iter: 1001900 training acc: 0.21875
Global Iter: 1002000 training loss: 2.18492
Global Iter: 1002000 training acc: 0.125
Global Iter: 1002100 training loss: 1.99054
Global Iter: 1002100 training acc: 0.1875
Global Iter: 1002200 training loss: 1.97086
Global Iter: 1002200 training acc: 0.21875
Global Iter: 1002300 training loss: 2.03304
Global Iter: 1002300 training acc: 0.15625
Global Iter: 1002400 training loss: 1.91533
Global Iter: 1002400 training acc: 0.21875
Global Iter: 1002500 training loss: 1.91675
Global Iter: 1002500 training acc: 0.15625
Global Iter: 1002600 training loss: 2.03209
Global Iter: 1002600 training acc: 0.1875
Global Iter: 1002700 training loss: 1.96342
Global Iter: 1002700 training acc: 0.21875
Global Iter: 1002800 training loss: 1.91501
Global Iter: 1002800 training acc: 0.28125
Global Iter: 1002900 training loss: 1.9911
Global Iter: 1002900 training acc: 0.0625
Global Iter: 1003000 training loss: 1.85246
Global Iter: 1003000 training acc: 0.3125
Global Iter: 1003100 training loss: 2.01056
Global Iter: 1003100 training acc: 0.25
Global Iter: 1003200 training loss: 1.98136
Global Iter: 1003200 training acc: 0.25
Global Iter: 1003300 training loss: 2.00443
Global Iter: 1003300 training acc: 0.34375
Global Iter: 1003400 training loss: 2.02046
Global Iter: 1003400 training acc: 0.1875
Global Iter: 1003500 training loss: 2.01978
Global Iter: 1003500 training acc: 0.1875
Global Iter: 1003600 training loss: 1.96169
Global Iter: 1003600 training acc: 0.1875
Global Iter: 1003700 training loss: 2.05296
Global Iter: 1003700 training acc: 0.15625
Global Iter: 1003800 training loss: 2.08006
Global Iter: 1003800 training acc: 0.25
Global Iter: 1003900 training loss: 1.93332
Global Iter: 1003900 training acc: 0.1875
Global Iter: 1004000 training loss: 1.93548
Global Iter: 1004000 training acc: 0.1875
Global Iter: 1004100 training loss: 2.06235
Global Iter: 1004100 training acc: 0.15625
Global Iter: 1004200 training loss: 1.98696
Global Iter: 1004200 training acc: 0.09375
Global Iter: 1004300 training loss: 1.92388
Global Iter: 1004300 training acc: 0.1875
Global Iter: 1004400 training loss: 2.06048
Global Iter: 1004400 training acc: 0.1875
Global Iter: 1004500 training loss: 1.96596
Global Iter: 1004500 training acc: 0.15625
Global Iter: 1004600 training loss: 2.05606
Global Iter: 1004600 training acc: 0.15625
Global Iter: 1004700 training loss: 1.97702
Global Iter: 1004700 training acc: 0.125
Global Iter: 1004800 training loss: 2.08557
Global Iter: 1004800 training acc: 0.15625
Global Iter: 1004900 training loss: 2.01281
Global Iter: 1004900 training acc: 0.15625
Global Iter: 1005000 training loss: 1.92811
Global Iter: 1005000 training acc: 0.25
Global Iter: 1005100 training loss: 2.01475
Global Iter: 1005100 training acc: 0.09375
Global Iter: 1005200 training loss: 1.95104
Global Iter: 1005200 training acc: 0.15625
Global Iter: 1005300 training loss: 1.96402
Global Iter: 1005300 training acc: 0.21875
Global Iter: 1005400 training loss: 1.94187
Global Iter: 1005400 training acc: 0.21875
Global Iter: 1005500 training loss: 1.99364
Global Iter: 1005500 training acc: 0.1875
Global Iter: 1005600 training loss: 2.04994
Global Iter: 1005600 training acc: 0.125
Global Iter: 1005700 training loss: 2.03732
Global Iter: 1005700 training acc: 0.1875
Global Iter: 1005800 training loss: 2.01242
Global Iter: 1005800 training acc: 0.21875
Global Iter: 1005900 training loss: 2.03139
Global Iter: 1005900 training acc: 0.1875
Global Iter: 1006000 training loss: 1.95911
Global Iter: 1006000 training acc: 0.1875
Global Iter: 1006100 training loss: 2.02757
Global Iter: 1006100 training acc: 0.15625
Global Iter: 1006200 training loss: 1.94309
Global Iter: 1006200 training acc: 0.28125
Global Iter: 1006300 training loss: 1.87335
Global Iter: 1006300 training acc: 0.21875
Global Iter: 1006400 training loss: 2.04827
Global Iter: 1006400 training acc: 0.15625
Global Iter: 1006500 training loss: 2.00392
Global Iter: 1006500 training acc: 0.1875
Global Iter: 1006600 training loss: 2.11841
Global Iter: 1006600 training acc: 0.09375
Global Iter: 1006700 training loss: 1.91978
Global Iter: 1006700 training acc: 0.28125
Global Iter: 1006800 training loss: 1.93831
Global Iter: 1006800 training acc: 0.3125
Global Iter: 1006900 training loss: 2.027252017-06-22 09:52:36.260636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1008024

Global Iter: 1006900 training acc: 0.15625
Global Iter: 1007000 training loss: 1.98242
Global Iter: 1007000 training acc: 0.125
Global Iter: 1007100 training loss: 1.95821
Global Iter: 1007100 training acc: 0.09375
Global Iter: 1007200 training loss: 1.8664
Global Iter: 1007200 training acc: 0.25
Global Iter: 1007300 training loss: 2.11099
Global Iter: 1007300 training acc: 0.15625
Global Iter: 1007400 training loss: 1.97261
Global Iter: 1007400 training acc: 0.15625
Global Iter: 1007500 training loss: 1.93247
Global Iter: 1007500 training acc: 0.15625
Global Iter: 1007600 training loss: 2.10068
Global Iter: 1007600 training acc: 0.09375
Global Iter: 1007700 training loss: 2.12886
Global Iter: 1007700 training acc: 0.09375
Global Iter: 1007800 training loss: 2.08111
Global Iter: 1007800 training acc: 0.1875
Global Iter: 1007900 training loss: 1.94735
Global Iter: 1007900 training acc: 0.28125
Global Iter: 1008000 training loss: 1.9858
Global Iter: 1008000 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1008024
Number of Patches: 146151
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1008024
Global Iter: 1008100 training loss: 1.91794
Global Iter: 1008100 training acc: 0.1875
Global Iter: 1008200 training loss: 2.00231
Global Iter: 1008200 training acc: 0.25
Global Iter: 1008300 training loss: 1.92348
Global Iter: 1008300 training acc: 0.15625
Global Iter: 1008400 training loss: 1.94118
Global Iter: 1008400 training acc: 0.21875
Global Iter: 1008500 training loss: 2.11978
Global Iter: 1008500 training acc: 0.21875
Global Iter: 1008600 training loss: 2.03855
Global Iter: 1008600 training acc: 0.25
Global Iter: 1008700 training loss: 1.96106
Global Iter: 1008700 training acc: 0.34375
Global Iter: 1008800 training loss: 1.90768
Global Iter: 1008800 training acc: 0.375
Global Iter: 1008900 training loss: 1.9992
Global Iter: 1008900 training acc: 0.125
Global Iter: 1009000 training loss: 2.05104
Global Iter: 1009000 training acc: 0.1875
Global Iter: 1009100 training loss: 2.02787
Global Iter: 1009100 training acc: 0.21875
Global Iter: 1009200 training loss: 2.12515
Global Iter: 1009200 training acc: 0.1875
Global Iter: 1009300 training loss: 1.91112
Global Iter: 1009300 training acc: 0.25
Global Iter: 1009400 training loss: 2.01014
Global Iter: 1009400 training acc: 0.25
Global Iter: 1009500 training loss: 2.24738
Global Iter: 1009500 training acc: 0.09375
Global Iter: 1009600 training loss: 2.0264
Global Iter: 1009600 training acc: 0.21875
Global Iter: 1009700 training loss: 1.96485
Global Iter: 1009700 training acc: 0.28125
Global Iter: 1009800 training loss: 2.00699
Global Iter: 1009800 training acc: 0.125
Global Iter: 1009900 training loss: 2.06429
Global Iter: 1009900 training acc: 0.125
Global Iter: 1010000 training loss: 1.97402
Global Iter: 1010000 training acc: 0.25
Global Iter: 1010100 training loss: 1.96428
Global Iter: 1010100 training acc: 0.15625
Global Iter: 1010200 training loss: 1.97612
Global Iter: 1010200 training acc: 0.25
Global Iter: 1010300 training loss: 2.06644
Global Iter: 1010300 training acc: 0.125
Global Iter: 1010400 training loss: 2.1059
Global Iter: 1010400 training acc: 0.09375
Global Iter: 1010500 training loss: 1.97604
Global Iter: 1010500 training acc: 0.1875
Global Iter: 1010600 training loss: 1.98211
Global Iter: 1010600 training acc: 0.21875
Global Iter: 1010700 training loss: 1.89836
Global Iter: 1010700 training acc: 0.25
Global Iter: 1010800 training loss: 2.00702
Global Iter: 1010800 training acc: 0.15625
Global Iter: 1010900 training loss: 2.07023
Global Iter: 1010900 training acc: 0.15625
Global Iter: 1011000 training loss: 1.90732
Global Iter: 1011000 training acc: 0.28125
Global Iter: 1011100 training loss: 1.92977
Global Iter: 1011100 training acc: 0.1875
Global Iter: 1011200 training loss: 2.0056
Global Iter: 1011200 training acc: 0.09375
Global Iter: 1011300 training loss: 1.89718
Global Iter: 1011300 training acc: 0.15625
Global Iter: 1011400 training loss: 1.95918
Global Iter: 1011400 training acc: 0.1875
Global Iter: 1011500 training loss: 2.00676
Global Iter: 1011500 training acc: 0.09375
Global Iter: 1011600 training loss: 1.98472
Global Iter: 1011600 training acc: 0.21875
Global Iter: 1011700 training loss: 2.06289
Global Iter: 1011700 training acc: 0.15625
Global Iter: 1011800 training loss: 1.90507
Global Iter: 1011800 training acc: 0.28125
Global Iter: 1011900 training loss: 2.05054
Global Iter: 1011900 training acc: 0.21875
Global Iter: 1012000 training loss: 1.91507
Global Iter: 1012000 training acc: 0.125
Global Iter: 1012100 training loss: 1.96601
Global Iter: 1012100 training acc: 0.1875
Global Iter: 1012200 training loss: 2.0027
Global Iter: 1012200 training acc: 0.15625
Global Iter: 1012300 training loss: 2.11404
Global Iter: 1012300 training acc: 0.125
Global Iter: 1012400 training loss: 2.03825
Global Iter: 1012400 training acc: 0.09375
Global Iter: 1012500 training loss: 1.99135
Global Iter: 1012500 training acc: 0.125
Global Iter: 1012600 training loss: 1.9744
Global Iter: 1012600 training acc: 0.34375
Global Iter: 1012700 training loss: 2.01507
Global Iter: 1012700 training acc: 0.03125
Global Iter: 1012800 training loss: 2.01387
Global Iter: 1012800 training acc: 0.1875
Global Iter: 1012900 training loss: 2.06776
Global Iter: 1012900 training acc: 0.21875
Global Iter: 1013000 training loss: 2.01069
Global Iter: 1013000 training acc: 0.15625
Global Iter: 1013100 training loss: 2.0032
Global Iter: 1013100 training acc: 0.15625
Global Iter: 1013200 training loss: 2.1047
Global Iter: 1013200 training acc: 0.09375
Global Iter: 1013300 training loss: 2.04113
Global Iter: 1013300 training acc: 0.1875
Global Iter: 1013400 training loss: 1.93914
Global Iter: 1013400 training acc: 0.15625
Global Iter: 1013500 training loss: 2.03809
Global Iter: 1013500 training acc: 0.15625
Global Iter: 1013600 training loss: 1.91103
Global Iter: 1013600 training acc: 0.28125
Global Iter: 1013700 training loss: 2.04619
Global Iter: 1013700 training acc: 0.21875
Global Iter: 1013800 training loss: 1.95894
Global Iter: 1013800 training acc: 0.21875
Global Iter: 1013900 training loss: 2.0025
Global Iter: 1013900 training acc: 0.0625
Global Iter: 1014000 training loss: 1.97063
Global Iter: 1014000 training acc: 0.125
Global Iter: 1014100 training loss: 1.95858
Global Iter: 1014100 training acc: 0.21875
Global Iter: 1014200 training loss: 2.06004
Global Iter: 1014200 training acc: 0.15625
Global Iter: 1014300 training loss: 2.01792
Global Iter: 1014300 training acc: 0.15625
Global Iter: 1014400 training loss: 2.00718
Global Iter: 1014400 training acc: 0.125
Global Iter: 1014500 training loss: 1.98345
Global Iter: 1014500 training acc: 0.125
Global Iter: 1014600 training loss: 1.99455
Global Iter: 1014600 training acc: 0.21875
Global Iter: 1014700 training loss: 1.90581
Global Iter: 1014700 training acc: 0.3125
Global Iter: 1014800 training loss: 1.99166
Global Iter: 1014800 training acc: 0.21875
Global Iter: 1014900 training loss: 2.02611
Global Iter: 1014900 training acc: 0.21875
Global Iter: 1015000 training loss: 2.03081
Global Iter: 1015000 training acc: 0.125
Global Iter: 1015100 training loss: 1.98846
Global Iter: 1015100 training acc: 0.09375
Global Iter: 1015200 training loss: 2.04319
Global Iter: 1015200 training acc: 0.21875
Global Iter: 1015300 training loss: 2.12364
Global Iter: 1015300 training acc: 0.125
Global Iter: 1015400 training loss: 1.97763
Global Iter: 1015400 training acc: 0.3125
Global Iter: 1015500 training loss: 1.95436
Global Iter: 1015500 training acc: 0.09375
Global Iter: 1015600 training loss: 1.94067
Global Iter: 1015600 training acc: 0.25
Global Iter: 1015700 training loss: 2.07898
Global Iter: 1015700 training acc: 0.21875
Global Iter: 1015800 training loss: 2.09192
Global Iter: 1015800 training acc: 0.21875
Global Iter: 1015900 training loss: 2.05185
Global Iter: 1015900 training acc: 0.09375
Global Iter: 1016000 training loss: 1.92028
Global Iter: 1016000 training acc: 0.25
Global Iter: 1016100 training loss: 2.02205
Global Iter: 1016100 training acc: 0.21875
G2017-06-22 10:08:24.253218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1017159
lobal Iter: 1016200 training loss: 1.96882
Global Iter: 1016200 training acc: 0.21875
Global Iter: 1016300 training loss: 1.92285
Global Iter: 1016300 training acc: 0.15625
Global Iter: 1016400 training loss: 2.04914
Global Iter: 1016400 training acc: 0.21875
Global Iter: 1016500 training loss: 2.06035
Global Iter: 1016500 training acc: 0.28125
Global Iter: 1016600 training loss: 2.0873
Global Iter: 1016600 training acc: 0.15625
Global Iter: 1016700 training loss: 1.96091
Global Iter: 1016700 training acc: 0.125
Global Iter: 1016800 training loss: 2.12499
Global Iter: 1016800 training acc: 0.09375
Global Iter: 1016900 training loss: 2.02402
Global Iter: 1016900 training acc: 0.15625
Global Iter: 1017000 training loss: 2.08841
Global Iter: 1017000 training acc: 0.1875
Global Iter: 1017100 training loss: 1.95988
Global Iter: 1017100 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1017159
Number of Patches: 144690
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1017159
Global Iter: 1017200 training loss: 2.00914
Global Iter: 1017200 training acc: 0.3125
Global Iter: 1017300 training loss: 2.01456
Global Iter: 1017300 training acc: 0.125
Global Iter: 1017400 training loss: 1.9109
Global Iter: 1017400 training acc: 0.09375
Global Iter: 1017500 training loss: 2.04949
Global Iter: 1017500 training acc: 0.15625
Global Iter: 1017600 training loss: 2.06179
Global Iter: 1017600 training acc: 0.21875
Global Iter: 1017700 training loss: 1.97472
Global Iter: 1017700 training acc: 0.25
Global Iter: 1017800 training loss: 1.89252
Global Iter: 1017800 training acc: 0.15625
Global Iter: 1017900 training loss: 2.14118
Global Iter: 1017900 training acc: 0.125
Global Iter: 1018000 training loss: 2.15191
Global Iter: 1018000 training acc: 0.1875
Global Iter: 1018100 training loss: 1.96989
Global Iter: 1018100 training acc: 0.125
Global Iter: 1018200 training loss: 2.04287
Global Iter: 1018200 training acc: 0.0625
Global Iter: 1018300 training loss: 1.98665
Global Iter: 1018300 training acc: 0.15625
Global Iter: 1018400 training loss: 2.04938
Global Iter: 1018400 training acc: 0.15625
Global Iter: 1018500 training loss: 1.94423
Global Iter: 1018500 training acc: 0.25
Global Iter: 1018600 training loss: 1.99422
Global Iter: 1018600 training acc: 0.1875
Global Iter: 1018700 training loss: 1.95581
Global Iter: 1018700 training acc: 0.21875
Global Iter: 1018800 training loss: 2.03354
Global Iter: 1018800 training acc: 0.125
Global Iter: 1018900 training loss: 1.94277
Global Iter: 1018900 training acc: 0.1875
Global Iter: 1019000 training loss: 1.87942
Global Iter: 1019000 training acc: 0.21875
Global Iter: 1019100 training loss: 1.89563
Global Iter: 1019100 training acc: 0.34375
Global Iter: 1019200 training loss: 2.01405
Global Iter: 1019200 training acc: 0.1875
Global Iter: 1019300 training loss: 2.08398
Global Iter: 1019300 training acc: 0.1875
Global Iter: 1019400 training loss: 1.90491
Global Iter: 1019400 training acc: 0.28125
Global Iter: 1019500 training loss: 1.8886
Global Iter: 1019500 training acc: 0.25
Global Iter: 1019600 training loss: 2.02403
Global Iter: 1019600 training acc: 0.21875
Global Iter: 1019700 training loss: 1.89098
Global Iter: 1019700 training acc: 0.28125
Global Iter: 1019800 training loss: 1.88022
Global Iter: 1019800 training acc: 0.25
Global Iter: 1019900 training loss: 1.98013
Global Iter: 1019900 training acc: 0.1875
Global Iter: 1020000 training loss: 1.95025
Global Iter: 1020000 training acc: 0.125
Global Iter: 1020100 training loss: 1.97824
Global Iter: 1020100 training acc: 0.25
Global Iter: 1020200 training loss: 1.98054
Global Iter: 1020200 training acc: 0.09375
Global Iter: 1020300 training loss: 2.00235
Global Iter: 1020300 training acc: 0.125
Global Iter: 1020400 training loss: 2.09928
Global Iter: 1020400 training acc: 0.1875
Global Iter: 1020500 training loss: 2.02365
Global Iter: 1020500 training acc: 0.125
Global Iter: 1020600 training loss: 1.94594
Global Iter: 1020600 training acc: 0.25
Global Iter: 1020700 training loss: 2.0001
Global Iter: 1020700 training acc: 0.28125
Global Iter: 1020800 training loss: 1.93434
Global Iter: 1020800 training acc: 0.15625
Global Iter: 1020900 training loss: 1.96448
Global Iter: 1020900 training acc: 0.125
Global Iter: 1021000 training loss: 1.91313
Global Iter: 1021000 training acc: 0.15625
Global Iter: 1021100 training loss: 1.92239
Global Iter: 1021100 training acc: 0.28125
Global Iter: 1021200 training loss: 2.09796
Global Iter: 1021200 training acc: 0.0625
Global Iter: 1021300 training loss: 2.06364
Global Iter: 1021300 training acc: 0.0625
Global Iter: 1021400 training loss: 1.95944
Global Iter: 1021400 training acc: 0.1875
Global Iter: 1021500 training loss: 1.97597
Global Iter: 1021500 training acc: 0.3125
Global Iter: 1021600 training loss: 2.08011
Global Iter: 1021600 training acc: 0.125
Global Iter: 1021700 training loss: 1.97405
Global Iter: 1021700 training acc: 0.0625
Global Iter: 1021800 training loss: 1.99394
Global Iter: 1021800 training acc: 0.25
Global Iter: 1021900 training loss: 1.97182
Global Iter: 1021900 training acc: 0.15625
Global Iter: 1022000 training loss: 2.09762
Global Iter: 1022000 training acc: 0.125
Global Iter: 1022100 training loss: 1.88933
Global Iter: 1022100 training acc: 0.1875
Global Iter: 1022200 training loss: 2.03273
Global Iter: 1022200 training acc: 0.15625
Global Iter: 1022300 training loss: 1.95727
Global Iter: 1022300 training acc: 0.25
Global Iter: 1022400 training loss: 1.92161
Global Iter: 1022400 training acc: 0.28125
Global Iter: 1022500 training loss: 1.99494
Global Iter: 1022500 training acc: 0.1875
Global Iter: 1022600 training loss: 1.99005
Global Iter: 1022600 training acc: 0.21875
Global Iter: 1022700 training loss: 1.92002
Global Iter: 1022700 training acc: 0.3125
Global Iter: 1022800 training loss: 2.0251
Global Iter: 1022800 training acc: 0.15625
Global Iter: 1022900 training loss: 2.03861
Global Iter: 1022900 training acc: 0.1875
Global Iter: 1023000 training loss: 2.01351
Global Iter: 1023000 training acc: 0.15625
Global Iter: 1023100 training loss: 1.99817
Global Iter: 1023100 training acc: 0.09375
Global Iter: 1023200 training loss: 2.07707
Global Iter: 1023200 training acc: 0.15625
Global Iter: 1023300 training loss: 2.0142
Global Iter: 1023300 training acc: 0.125
Global Iter: 1023400 training loss: 2.00975
Global Iter: 1023400 training acc: 0.09375
Global Iter: 1023500 training loss: 1.99804
Global Iter: 1023500 training acc: 0.125
Global Iter: 1023600 training loss: 1.97609
Global Iter: 1023600 training acc: 0.1875
Global Iter: 1023700 training loss: 2.0302
Global Iter: 1023700 training acc: 0.25
Global Iter: 1023800 training loss: 1.92174
Global Iter: 1023800 training acc: 0.1875
Global Iter: 1023900 training loss: 1.95322
Global Iter: 1023900 training acc: 0.21875
Global Iter: 1024000 training loss: 1.95169
Global Iter: 1024000 training acc: 0.15625
Global Iter: 1024100 training loss: 1.97876
Global Iter: 1024100 training acc: 0.1875
Global Iter: 1024200 training loss: 1.99753
Global Iter: 1024200 training acc: 0.1875
Global Iter: 1024300 training loss: 1.9983
Global Iter: 1024300 training acc: 0.25
Global Iter: 1024400 training loss: 1.97608
Global Iter: 1024400 training acc: 0.21875
Global Iter: 1024500 training loss: 2.13136
Global Iter: 1024500 training acc: 0.1875
Global Iter: 1024600 training loss: 2.08996
Global Iter: 1024600 training acc: 0.25
Global Iter: 1024700 training loss: 1.97446
Global Iter: 1024700 training acc: 0.1875
Global Iter: 1024800 training loss: 2.00926
Global Iter: 1024800 training acc: 0.28125
Global Iter: 1024900 training loss: 1.97381
Global Iter: 1024900 training acc: 0.375
Global Iter: 1025000 training loss: 2.08943
Global Iter: 1025000 training acc: 0.15625
Global Iter: 1025100 training loss: 2.02357
Global Iter: 1025100 training acc: 0.125
Global Iter: 1025200 training loss: 2.0954
Global Iter: 1025200 training acc: 0.15625
Global Iter: 1025300 training loss: 2.02705
Global Iter: 1025300 training acc: 0.125
Global Iter: 1025400 training loss: 1.92801
Global Iter: 1025402017-06-22 10:23:54.157677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1026203
0 training acc: 0.21875
Global Iter: 1025500 training loss: 1.90524
Global Iter: 1025500 training acc: 0.1875
Global Iter: 1025600 training loss: 1.94465
Global Iter: 1025600 training acc: 0.15625
Global Iter: 1025700 training loss: 1.96472
Global Iter: 1025700 training acc: 0.15625
Global Iter: 1025800 training loss: 2.04797
Global Iter: 1025800 training acc: 0.09375
Global Iter: 1025900 training loss: 2.01547
Global Iter: 1025900 training acc: 0.125
Global Iter: 1026000 training loss: 2.01443
Global Iter: 1026000 training acc: 0.125
Global Iter: 1026100 training loss: 2.03361
Global Iter: 1026100 training acc: 0.28125
Global Iter: 1026200 training loss: 2.07111
Global Iter: 1026200 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1026203
Number of Patches: 143244
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1026203
Global Iter: 1026300 training loss: 1.94921
Global Iter: 1026300 training acc: 0.28125
Global Iter: 1026400 training loss: 1.95866
Global Iter: 1026400 training acc: 0.09375
Global Iter: 1026500 training loss: 2.01535
Global Iter: 1026500 training acc: 0.15625
Global Iter: 1026600 training loss: 1.98367
Global Iter: 1026600 training acc: 0.21875
Global Iter: 1026700 training loss: 2.02079
Global Iter: 1026700 training acc: 0.0625
Global Iter: 1026800 training loss: 1.931
Global Iter: 1026800 training acc: 0.25
Global Iter: 1026900 training loss: 1.97233
Global Iter: 1026900 training acc: 0.1875
Global Iter: 1027000 training loss: 2.09413
Global Iter: 1027000 training acc: 0.1875
Global Iter: 1027100 training loss: 1.98738
Global Iter: 1027100 training acc: 0.15625
Global Iter: 1027200 training loss: 1.95382
Global Iter: 1027200 training acc: 0.15625
Global Iter: 1027300 training loss: 1.95485
Global Iter: 1027300 training acc: 0.25
Global Iter: 1027400 training loss: 1.91262
Global Iter: 1027400 training acc: 0.1875
Global Iter: 1027500 training loss: 1.94362
Global Iter: 1027500 training acc: 0.375
Global Iter: 1027600 training loss: 1.95687
Global Iter: 1027600 training acc: 0.125
Global Iter: 1027700 training loss: 1.98258
Global Iter: 1027700 training acc: 0.09375
Global Iter: 1027800 training loss: 2.00265
Global Iter: 1027800 training acc: 0.125
Global Iter: 1027900 training loss: 2.0354
Global Iter: 1027900 training acc: 0.1875
Global Iter: 1028000 training loss: 1.97806
Global Iter: 1028000 training acc: 0.21875
Global Iter: 1028100 training loss: 1.96352
Global Iter: 1028100 training acc: 0.28125
Global Iter: 1028200 training loss: 2.04937
Global Iter: 1028200 training acc: 0.1875
Global Iter: 1028300 training loss: 1.88194
Global Iter: 1028300 training acc: 0.25
Global Iter: 1028400 training loss: 1.96981
Global Iter: 1028400 training acc: 0.15625
Global Iter: 1028500 training loss: 1.97148
Global Iter: 1028500 training acc: 0.25
Global Iter: 1028600 training loss: 1.92082
Global Iter: 1028600 training acc: 0.3125
Global Iter: 1028700 training loss: 1.92324
Global Iter: 1028700 training acc: 0.1875
Global Iter: 1028800 training loss: 1.95359
Global Iter: 1028800 training acc: 0.09375
Global Iter: 1028900 training loss: 1.99167
Global Iter: 1028900 training acc: 0.21875
Global Iter: 1029000 training loss: 1.99275
Global Iter: 1029000 training acc: 0.28125
Global Iter: 1029100 training loss: 2.03558
Global Iter: 1029100 training acc: 0.1875
Global Iter: 1029200 training loss: 1.98022
Global Iter: 1029200 training acc: 0.0625
Global Iter: 1029300 training loss: 2.0881
Global Iter: 1029300 training acc: 0.03125
Global Iter: 1029400 training loss: 1.96147
Global Iter: 1029400 training acc: 0.125
Global Iter: 1029500 training loss: 1.98266
Global Iter: 1029500 training acc: 0.125
Global Iter: 1029600 training loss: 2.0042
Global Iter: 1029600 training acc: 0.15625
Global Iter: 1029700 training loss: 2.01428
Global Iter: 1029700 training acc: 0.03125
Global Iter: 1029800 training loss: 1.95868
Global Iter: 1029800 training acc: 0.21875
Global Iter: 1029900 training loss: 1.93185
Global Iter: 1029900 training acc: 0.15625
Global Iter: 1030000 training loss: 1.9952
Global Iter: 1030000 training acc: 0.21875
Global Iter: 1030100 training loss: 1.98919
Global Iter: 1030100 training acc: 0.125
Global Iter: 1030200 training loss: 1.9056
Global Iter: 1030200 training acc: 0.21875
Global Iter: 1030300 training loss: 2.09024
Global Iter: 1030300 training acc: 0.1875
Global Iter: 1030400 training loss: 1.94544
Global Iter: 1030400 training acc: 0.15625
Global Iter: 1030500 training loss: 2.02625
Global Iter: 1030500 training acc: 0.25
Global Iter: 1030600 training loss: 1.94107
Global Iter: 1030600 training acc: 0.3125
Global Iter: 1030700 training loss: 1.83891
Global Iter: 1030700 training acc: 0.3125
Global Iter: 1030800 training loss: 1.94749
Global Iter: 1030800 training acc: 0.28125
Global Iter: 1030900 training loss: 1.9535
Global Iter: 1030900 training acc: 0.3125
Global Iter: 1031000 training loss: 1.96056
Global Iter: 1031000 training acc: 0.1875
Global Iter: 1031100 training loss: 2.06333
Global Iter: 1031100 training acc: 0.0625
Global Iter: 1031200 training loss: 1.97917
Global Iter: 1031200 training acc: 0.15625
Global Iter: 1031300 training loss: 2.1641
Global Iter: 1031300 training acc: 0.15625
Global Iter: 1031400 training loss: 2.10598
Global Iter: 1031400 training acc: 0.21875
Global Iter: 1031500 training loss: 2.05827
Global Iter: 1031500 training acc: 0.0625
Global Iter: 1031600 training loss: 2.0703
Global Iter: 1031600 training acc: 0.0625
Global Iter: 1031700 training loss: 1.94135
Global Iter: 1031700 training acc: 0.15625
Global Iter: 1031800 training loss: 1.94923
Global Iter: 1031800 training acc: 0.28125
Global Iter: 1031900 training loss: 2.03835
Global Iter: 1031900 training acc: 0.1875
Global Iter: 1032000 training loss: 1.88861
Global Iter: 1032000 training acc: 0.125
Global Iter: 1032100 training loss: 1.96853
Global Iter: 1032100 training acc: 0.21875
Global Iter: 1032200 training loss: 2.06153
Global Iter: 1032200 training acc: 0.15625
Global Iter: 1032300 training loss: 1.9207
Global Iter: 1032300 training acc: 0.28125
Global Iter: 1032400 training loss: 2.00353
Global Iter: 1032400 training acc: 0.15625
Global Iter: 1032500 training loss: 2.03684
Global Iter: 1032500 training acc: 0.1875
Global Iter: 1032600 training loss: 1.94076
Global Iter: 1032600 training acc: 0.21875
Global Iter: 1032700 training loss: 1.98128
Global Iter: 1032700 training acc: 0.15625
Global Iter: 1032800 training loss: 1.91194
Global Iter: 1032800 training acc: 0.21875
Global Iter: 1032900 training loss: 1.96829
Global Iter: 1032900 training acc: 0.21875
Global Iter: 1033000 training loss: 1.92797
Global Iter: 1033000 training acc: 0.25
Global Iter: 1033100 training loss: 1.91898
Global Iter: 1033100 training acc: 0.25
Global Iter: 1033200 training loss: 2.00223
Global Iter: 1033200 training acc: 0.15625
Global Iter: 1033300 training loss: 1.97805
Global Iter: 1033300 training acc: 0.21875
Global Iter: 1033400 training loss: 2.02582
Global Iter: 1033400 training acc: 0.21875
Global Iter: 1033500 training loss: 1.89853
Global Iter: 1033500 training acc: 0.28125
Global Iter: 1033600 training loss: 2.03695
Global Iter: 1033600 training acc: 0.09375
Global Iter: 1033700 training loss: 1.97265
Global Iter: 1033700 training acc: 0.0625
Global Iter: 1033800 training loss: 2.00687
Global Iter: 1033800 training acc: 0.125
Global Iter: 1033900 training loss: 1.95447
Global Iter: 1033900 training acc: 0.09375
Global Iter: 1034000 training loss: 1.95168
Global Iter: 1034000 training acc: 0.1875
Global Iter: 1034100 training loss: 2.02043
Global Iter: 1034100 training acc: 0.1875
Global Iter: 1034200 training loss: 2.07491
Global Iter: 1034200 training acc: 0.1875
Global Iter: 1034300 training loss: 1.94282
Global Iter: 1034300 training acc: 0.15625
Global Iter: 1034400 training loss: 1.94041
Global Iter: 1034400 training acc: 0.1875
Global Iter: 1034500 training loss: 1.91635
Global Iter: 1034500 training acc: 0.25
Global Iter: 1034600 training loss: 1.96162
Global Iter: 1034600 training acc: 0.28125
Global Iter: 10347002017-06-22 10:39:01.462190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1035156
 training loss: 2.02059
Global Iter: 1034700 training acc: 0.125
Global Iter: 1034800 training loss: 1.95052
Global Iter: 1034800 training acc: 0.15625
Global Iter: 1034900 training loss: 2.07282
Global Iter: 1034900 training acc: 0.21875
Global Iter: 1035000 training loss: 1.96024
Global Iter: 1035000 training acc: 0.1875
Global Iter: 1035100 training loss: 2.14828
Global Iter: 1035100 training acc: 0.28125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1035156
Number of Patches: 141812
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1035156
Global Iter: 1035200 training loss: 1.94558
Global Iter: 1035200 training acc: 0.15625
Global Iter: 1035300 training loss: 1.99467
Global Iter: 1035300 training acc: 0.1875
Global Iter: 1035400 training loss: 1.94059
Global Iter: 1035400 training acc: 0.15625
Global Iter: 1035500 training loss: 1.98253
Global Iter: 1035500 training acc: 0.25
Global Iter: 1035600 training loss: 2.09207
Global Iter: 1035600 training acc: 0.125
Global Iter: 1035700 training loss: 1.95567
Global Iter: 1035700 training acc: 0.1875
Global Iter: 1035800 training loss: 1.99055
Global Iter: 1035800 training acc: 0.1875
Global Iter: 1035900 training loss: 2.00601
Global Iter: 1035900 training acc: 0.125
Global Iter: 1036000 training loss: 1.96133
Global Iter: 1036000 training acc: 0.15625
Global Iter: 1036100 training loss: 1.99748
Global Iter: 1036100 training acc: 0.21875
Global Iter: 1036200 training loss: 1.93524
Global Iter: 1036200 training acc: 0.34375
Global Iter: 1036300 training loss: 2.01213
Global Iter: 1036300 training acc: 0.15625
Global Iter: 1036400 training loss: 1.97152
Global Iter: 1036400 training acc: 0.15625
Global Iter: 1036500 training loss: 2.07841
Global Iter: 1036500 training acc: 0.15625
Global Iter: 1036600 training loss: 2.12215
Global Iter: 1036600 training acc: 0.09375
Global Iter: 1036700 training loss: 2.01223
Global Iter: 1036700 training acc: 0.125
Global Iter: 1036800 training loss: 2.01928
Global Iter: 1036800 training acc: 0.25
Global Iter: 1036900 training loss: 2.04078
Global Iter: 1036900 training acc: 0.1875
Global Iter: 1037000 training loss: 2.1161
Global Iter: 1037000 training acc: 0.15625
Global Iter: 1037100 training loss: 2.0805
Global Iter: 1037100 training acc: 0.15625
Global Iter: 1037200 training loss: 2.01992
Global Iter: 1037200 training acc: 0.1875
Global Iter: 1037300 training loss: 1.98145
Global Iter: 1037300 training acc: 0.0625
Global Iter: 1037400 training loss: 1.9496
Global Iter: 1037400 training acc: 0.1875
Global Iter: 1037500 training loss: 2.10973
Global Iter: 1037500 training acc: 0.25
Global Iter: 1037600 training loss: 2.05532
Global Iter: 1037600 training acc: 0.125
Global Iter: 1037700 training loss: 2.01945
Global Iter: 1037700 training acc: 0.125
Global Iter: 1037800 training loss: 2.12342
Global Iter: 1037800 training acc: 0.28125
Global Iter: 1037900 training loss: 2.091
Global Iter: 1037900 training acc: 0.1875
Global Iter: 1038000 training loss: 1.99557
Global Iter: 1038000 training acc: 0.25
Global Iter: 1038100 training loss: 1.93545
Global Iter: 1038100 training acc: 0.1875
Global Iter: 1038200 training loss: 1.94349
Global Iter: 1038200 training acc: 0.21875
Global Iter: 1038300 training loss: 1.94566
Global Iter: 1038300 training acc: 0.1875
Global Iter: 1038400 training loss: 2.00881
Global Iter: 1038400 training acc: 0.1875
Global Iter: 1038500 training loss: 1.96624
Global Iter: 1038500 training acc: 0.1875
Global Iter: 1038600 training loss: 2.04433
Global Iter: 1038600 training acc: 0.1875
Global Iter: 1038700 training loss: 2.0975
Global Iter: 1038700 training acc: 0.03125
Global Iter: 1038800 training loss: 1.88872
Global Iter: 1038800 training acc: 0.21875
Global Iter: 1038900 training loss: 1.94368
Global Iter: 1038900 training acc: 0.25
Global Iter: 1039000 training loss: 2.17654
Global Iter: 1039000 training acc: 0.09375
Global Iter: 1039100 training loss: 1.9413
Global Iter: 1039100 training acc: 0.15625
Global Iter: 1039200 training loss: 2.03394
Global Iter: 1039200 training acc: 0.21875
Global Iter: 1039300 training loss: 1.96302
Global Iter: 1039300 training acc: 0.21875
Global Iter: 1039400 training loss: 1.93993
Global Iter: 1039400 training acc: 0.21875
Global Iter: 1039500 training loss: 1.96788
Global Iter: 1039500 training acc: 0.25
Global Iter: 1039600 training loss: 2.03749
Global Iter: 1039600 training acc: 0.21875
Global Iter: 1039700 training loss: 1.97893
Global Iter: 1039700 training acc: 0.1875
Global Iter: 1039800 training loss: 1.96355
Global Iter: 1039800 training acc: 0.21875
Global Iter: 1039900 training loss: 2.006
Global Iter: 1039900 training acc: 0.25
Global Iter: 1040000 training loss: 1.91555
Global Iter: 1040000 training acc: 0.375
Global Iter: 1040100 training loss: 2.10794
Global Iter: 1040100 training acc: 0.15625
Global Iter: 1040200 training loss: 1.99629
Global Iter: 1040200 training acc: 0.15625
Global Iter: 1040300 training loss: 1.98812
Global Iter: 1040300 training acc: 0.125
Global Iter: 1040400 training loss: 1.98815
Global Iter: 1040400 training acc: 0.15625
Global Iter: 1040500 training loss: 2.02557
Global Iter: 1040500 training acc: 0.09375
Global Iter: 1040600 training loss: 2.06857
Global Iter: 1040600 training acc: 0.0625
Global Iter: 1040700 training loss: 1.94379
Global Iter: 1040700 training acc: 0.1875
Global Iter: 1040800 training loss: 2.08763
Global Iter: 1040800 training acc: 0.21875
Global Iter: 1040900 training loss: 2.00432
Global Iter: 1040900 training acc: 0.21875
Global Iter: 1041000 training loss: 1.94497
Global Iter: 1041000 training acc: 0.25
Global Iter: 1041100 training loss: 2.0238
Global Iter: 1041100 training acc: 0.28125
Global Iter: 1041200 training loss: 1.97794
Global Iter: 1041200 training acc: 0.15625
Global Iter: 1041300 training loss: 1.95652
Global Iter: 1041300 training acc: 0.09375
Global Iter: 1041400 training loss: 2.06768
Global Iter: 1041400 training acc: 0.09375
Global Iter: 1041500 training loss: 1.99752
Global Iter: 1041500 training acc: 0.1875
Global Iter: 1041600 training loss: 1.97774
Global Iter: 1041600 training acc: 0.15625
Global Iter: 1041700 training loss: 1.98166
Global Iter: 1041700 training acc: 0.09375
Global Iter: 1041800 training loss: 2.00661
Global Iter: 1041800 training acc: 0.125
Global Iter: 1041900 training loss: 1.97403
Global Iter: 1041900 training acc: 0.1875
Global Iter: 1042000 training loss: 1.95293
Global Iter: 1042000 training acc: 0.25
Global Iter: 1042100 training loss: 2.02013
Global Iter: 1042100 training acc: 0.15625
Global Iter: 1042200 training loss: 1.96733
Global Iter: 1042200 training acc: 0.25
Global Iter: 1042300 training loss: 1.94703
Global Iter: 1042300 training acc: 0.125
Global Iter: 1042400 training loss: 1.91797
Global Iter: 1042400 training acc: 0.21875
Global Iter: 1042500 training loss: 2.12511
Global Iter: 1042500 training acc: 0.1875
Global Iter: 1042600 training loss: 1.95815
Global Iter: 1042600 training acc: 0.15625
Global Iter: 1042700 training loss: 2.00351
Global Iter: 1042700 training acc: 0.15625
Global Iter: 1042800 training loss: 2.14544
Global Iter: 1042800 training acc: 0.125
Global Iter: 1042900 training loss: 1.98775
Global Iter: 1042900 training acc: 0.1875
Global Iter: 1043000 training loss: 1.97999
Global Iter: 1043000 training acc: 0.15625
Global Iter: 1043100 training loss: 2.03151
Global Iter: 1043100 training acc: 0.15625
Global Iter: 1043200 training loss: 2.03209
Global Iter: 1043200 training acc: 0.1875
Global Iter: 1043300 training loss: 2.04851
Global Iter: 1043300 training acc: 0.21875
Global Iter: 1043400 training loss: 1.93986
Global Iter: 1043400 training acc: 0.25
Global Iter: 1043500 training loss: 2.04121
Global Iter: 1043500 training acc: 0.1875
Global Iter: 1043600 training loss: 1.93468
Global Iter: 1043600 training acc: 0.21875
Global Iter: 1043700 training loss: 1.91991
Global Iter: 1043700 training acc: 0.21875
Global Iter: 1043800 training loss: 1.97514
Global Iter: 1043800 training acc: 0.125
Global Iter: 1043900 training loss: 1.90585
Global Iter: 1043900 training2017-06-22 10:54:13.505684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1044020
 acc: 0.28125
Global Iter: 1044000 training loss: 1.98714
Global Iter: 1044000 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1044020
Number of Patches: 140394
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1044020
Global Iter: 1044100 training loss: 1.93027
Global Iter: 1044100 training acc: 0.25
Global Iter: 1044200 training loss: 1.98386
Global Iter: 1044200 training acc: 0.15625
Global Iter: 1044300 training loss: 1.93774
Global Iter: 1044300 training acc: 0.21875
Global Iter: 1044400 training loss: 2.01075
Global Iter: 1044400 training acc: 0.125
Global Iter: 1044500 training loss: 2.0586
Global Iter: 1044500 training acc: 0.125
Global Iter: 1044600 training loss: 1.96042
Global Iter: 1044600 training acc: 0.09375
Global Iter: 1044700 training loss: 2.02897
Global Iter: 1044700 training acc: 0.21875
Global Iter: 1044800 training loss: 2.00518
Global Iter: 1044800 training acc: 0.15625
Global Iter: 1044900 training loss: 2.04535
Global Iter: 1044900 training acc: 0.1875
Global Iter: 1045000 training loss: 1.89894
Global Iter: 1045000 training acc: 0.1875
Global Iter: 1045100 training loss: 2.08592
Global Iter: 1045100 training acc: 0.0625
Global Iter: 1045200 training loss: 1.90791
Global Iter: 1045200 training acc: 0.21875
Global Iter: 1045300 training loss: 1.95089
Global Iter: 1045300 training acc: 0.25
Global Iter: 1045400 training loss: 1.97461
Global Iter: 1045400 training acc: 0.21875
Global Iter: 1045500 training loss: 2.02793
Global Iter: 1045500 training acc: 0.125
Global Iter: 1045600 training loss: 2.00113
Global Iter: 1045600 training acc: 0.3125
Global Iter: 1045700 training loss: 1.96
Global Iter: 1045700 training acc: 0.15625
Global Iter: 1045800 training loss: 1.93497
Global Iter: 1045800 training acc: 0.1875
Global Iter: 1045900 training loss: 1.91865
Global Iter: 1045900 training acc: 0.15625
Global Iter: 1046000 training loss: 1.95594
Global Iter: 1046000 training acc: 0.125
Global Iter: 1046100 training loss: 1.93542
Global Iter: 1046100 training acc: 0.28125
Global Iter: 1046200 training loss: 2.03439
Global Iter: 1046200 training acc: 0.1875
Global Iter: 1046300 training loss: 1.92913
Global Iter: 1046300 training acc: 0.1875
Global Iter: 1046400 training loss: 2.06391
Global Iter: 1046400 training acc: 0.1875
Global Iter: 1046500 training loss: 1.94742
Global Iter: 1046500 training acc: 0.28125
Global Iter: 1046600 training loss: 2.02984
Global Iter: 1046600 training acc: 0.21875
Global Iter: 1046700 training loss: 1.9299
Global Iter: 1046700 training acc: 0.15625
Global Iter: 1046800 training loss: 1.96969
Global Iter: 1046800 training acc: 0.21875
Global Iter: 1046900 training loss: 2.00005
Global Iter: 1046900 training acc: 0.1875
Global Iter: 1047000 training loss: 1.94911
Global Iter: 1047000 training acc: 0.21875
Global Iter: 1047100 training loss: 2.13845
Global Iter: 1047100 training acc: 0.1875
Global Iter: 1047200 training loss: 2.04159
Global Iter: 1047200 training acc: 0.09375
Global Iter: 1047300 training loss: 2.03121
Global Iter: 1047300 training acc: 0.09375
Global Iter: 1047400 training loss: 1.91957
Global Iter: 1047400 training acc: 0.125
Global Iter: 1047500 training loss: 1.95438
Global Iter: 1047500 training acc: 0.15625
Global Iter: 1047600 training loss: 2.02128
Global Iter: 1047600 training acc: 0.1875
Global Iter: 1047700 training loss: 2.00269
Global Iter: 1047700 training acc: 0.125
Global Iter: 1047800 training loss: 1.9305
Global Iter: 1047800 training acc: 0.1875
Global Iter: 1047900 training loss: 2.0236
Global Iter: 1047900 training acc: 0.1875
Global Iter: 1048000 training loss: 2.02397
Global Iter: 1048000 training acc: 0.15625
Global Iter: 1048100 training loss: 1.89849
Global Iter: 1048100 training acc: 0.34375
Global Iter: 1048200 training loss: 1.94837
Global Iter: 1048200 training acc: 0.25
Global Iter: 1048300 training loss: 2.0259
Global Iter: 1048300 training acc: 0.0625
Global Iter: 1048400 training loss: 1.97784
Global Iter: 1048400 trainin2017-06-22 11:09:20.079182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1052795
g acc: 0.15625
Global Iter: 1048500 training loss: 1.91595
Global Iter: 1048500 training acc: 0.1875
Global Iter: 1048600 training loss: 1.95532
Global Iter: 1048600 training acc: 0.1875
Global Iter: 1048700 training loss: 1.98507
Global Iter: 1048700 training acc: 0.125
Global Iter: 1048800 training loss: 1.95041
Global Iter: 1048800 training acc: 0.21875
Global Iter: 1048900 training loss: 1.94446
Global Iter: 1048900 training acc: 0.25
Global Iter: 1049000 training loss: 2.05798
Global Iter: 1049000 training acc: 0.0625
Global Iter: 1049100 training loss: 2.03083
Global Iter: 1049100 training acc: 0.21875
Global Iter: 1049200 training loss: 2.10127
Global Iter: 1049200 training acc: 0.09375
Global Iter: 1049300 training loss: 1.87283
Global Iter: 1049300 training acc: 0.21875
Global Iter: 1049400 training loss: 1.97014
Global Iter: 1049400 training acc: 0.1875
Global Iter: 1049500 training loss: 1.99255
Global Iter: 1049500 training acc: 0.125
Global Iter: 1049600 training loss: 1.90335
Global Iter: 1049600 training acc: 0.28125
Global Iter: 1049700 training loss: 1.95804
Global Iter: 1049700 training acc: 0.1875
Global Iter: 1049800 training loss: 1.95641
Global Iter: 1049800 training acc: 0.09375
Global Iter: 1049900 training loss: 1.94801
Global Iter: 1049900 training acc: 0.125
Global Iter: 1050000 training loss: 2.01582
Global Iter: 1050000 training acc: 0.15625
Global Iter: 1050100 training loss: 1.94336
Global Iter: 1050100 training acc: 0.1875
Global Iter: 1050200 training loss: 1.87835
Global Iter: 1050200 training acc: 0.25
Global Iter: 1050300 training loss: 2.05922
Global Iter: 1050300 training acc: 0.03125
Global Iter: 1050400 training loss: 2.00271
Global Iter: 1050400 training acc: 0.21875
Global Iter: 1050500 training loss: 1.97408
Global Iter: 1050500 training acc: 0.1875
Global Iter: 1050600 training loss: 2.0697
Global Iter: 1050600 training acc: 0.21875
Global Iter: 1050700 training loss: 2.07491
Global Iter: 1050700 training acc: 0.1875
Global Iter: 1050800 training loss: 1.8942
Global Iter: 1050800 training acc: 0.25
Global Iter: 1050900 training loss: 2.0565
Global Iter: 1050900 training acc: 0.125
Global Iter: 1051000 training loss: 1.95322
Global Iter: 1051000 training acc: 0.125
Global Iter: 1051100 training loss: 1.93033
Global Iter: 1051100 training acc: 0.15625
Global Iter: 1051200 training loss: 1.86985
Global Iter: 1051200 training acc: 0.28125
Global Iter: 1051300 training loss: 1.9933
Global Iter: 1051300 training acc: 0.21875
Global Iter: 1051400 training loss: 1.93655
Global Iter: 1051400 training acc: 0.21875
Global Iter: 1051500 training loss: 2.00949
Global Iter: 1051500 training acc: 0.0625
Global Iter: 1051600 training loss: 1.98613
Global Iter: 1051600 training acc: 0.1875
Global Iter: 1051700 training loss: 2.06695
Global Iter: 1051700 training acc: 0.15625
Global Iter: 1051800 training loss: 2.04141
Global Iter: 1051800 training acc: 0.125
Global Iter: 1051900 training loss: 1.95008
Global Iter: 1051900 training acc: 0.25
Global Iter: 1052000 training loss: 1.96373
Global Iter: 1052000 training acc: 0.09375
Global Iter: 1052100 training loss: 2.11742
Global Iter: 1052100 training acc: 0.1875
Global Iter: 1052200 training loss: 2.03839
Global Iter: 1052200 training acc: 0.15625
Global Iter: 1052300 training loss: 2.05736
Global Iter: 1052300 training acc: 0.09375
Global Iter: 1052400 training loss: 1.94876
Global Iter: 1052400 training acc: 0.15625
Global Iter: 1052500 training loss: 2.00647
Global Iter: 1052500 training acc: 0.15625
Global Iter: 1052600 training loss: 2.01393
Global Iter: 1052600 training acc: 0.3125
Global Iter: 1052700 training loss: 1.97144
Global Iter: 1052700 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1052795
Number of Patches: 138991
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1052795
Global Iter: 1052800 training loss: 1.97304
Global Iter: 1052800 training acc: 0.15625
Global Iter: 1052900 training loss: 1.98878
Global Iter: 1052900 training acc: 0.21875
Global Iter: 1053000 training loss: 2.03416
Global Iter: 1053000 training acc: 0.0625
Global Iter: 1053100 training loss: 1.96033
Global Iter: 1053100 training acc: 0.15625
Global Iter: 1053200 training loss: 2.03596
Global Iter: 1053200 training acc: 0.1875
Global Iter: 1053300 training loss: 1.99686
Global Iter: 1053300 training acc: 0.21875
Global Iter: 1053400 training loss: 2.07981
Global Iter: 1053400 training acc: 0.09375
Global Iter: 1053500 training loss: 2.00738
Global Iter: 1053500 training acc: 0.15625
Global Iter: 1053600 training loss: 2.00861
Global Iter: 1053600 training acc: 0.21875
Global Iter: 1053700 training loss: 1.91202
Global Iter: 1053700 training acc: 0.28125
Global Iter: 1053800 training loss: 2.05447
Global Iter: 1053800 training acc: 0.25
Global Iter: 1053900 training loss: 1.98007
Global Iter: 1053900 training acc: 0.1875
Global Iter: 1054000 training loss: 2.06534
Global Iter: 1054000 training acc: 0.125
Global Iter: 1054100 training loss: 2.05753
Global Iter: 1054100 training acc: 0.1875
Global Iter: 1054200 training loss: 1.97685
Global Iter: 1054200 training acc: 0.125
Global Iter: 1054300 training loss: 1.98713
Global Iter: 1054300 training acc: 0.25
Global Iter: 1054400 training loss: 1.97997
Global Iter: 1054400 training acc: 0.15625
Global Iter: 1054500 training loss: 1.96666
Global Iter: 1054500 training acc: 0.3125
Global Iter: 1054600 training loss: 1.90415
Global Iter: 1054600 training acc: 0.21875
Global Iter: 1054700 training loss: 2.02221
Global Iter: 1054700 training acc: 0.1875
Global Iter: 1054800 training loss: 2.01902
Global Iter: 1054800 training acc: 0.1875
Global Iter: 1054900 training loss: 1.92313
Global Iter: 1054900 training acc: 0.28125
Global Iter: 1055000 training loss: 2.07267
Global Iter: 1055000 training acc: 0.125
Global Iter: 1055100 training loss: 2.12867
Global Iter: 1055100 training acc: 0.125
Global Iter: 1055200 training loss: 2.1366
Global Iter: 1055200 training acc: 0.21875
Global Iter: 1055300 training loss: 1.97787
Global Iter: 1055300 training acc: 0.15625
Global Iter: 1055400 training loss: 1.89912
Global Iter: 1055400 training acc: 0.28125
Global Iter: 1055500 training loss: 1.88992
Global Iter: 1055500 training acc: 0.3125
Global Iter: 1055600 training loss: 2.00419
Global Iter: 1055600 training acc: 0.15625
Global Iter: 1055700 training loss: 2.01367
Global Iter: 1055700 training acc: 0.125
Global Iter: 1055800 training loss: 1.94352
Global Iter: 1055800 training acc: 0.21875
Global Iter: 1055900 training loss: 1.99941
Global Iter: 1055900 training acc: 0.125
Global Iter: 1056000 training loss: 2.07947
Global Iter: 1056000 training acc: 0.15625
Global Iter: 1056100 training loss: 2.01908
Global Iter: 1056100 training acc: 0.25
Global Iter: 1056200 training loss: 2.04706
Global Iter: 1056200 training acc: 0.1875
Global Iter: 1056300 training loss: 1.93362
Global Iter: 1056300 training acc: 0.15625
Global Iter: 1056400 training loss: 1.97734
Global Iter: 1056400 training acc: 0.125
Global Iter: 1056500 training loss: 1.99693
Global Iter: 1056500 training acc: 0.21875
Global Iter: 1056600 training loss: 2.00621
Global Iter: 1056600 training acc: 0.15625
Global Iter: 1056700 training loss: 2.01545
Global Iter: 1056700 training acc: 0.125
Global Iter: 1056800 training loss: 1.97165
Global Iter: 1056800 training acc: 0.25
Global Iter: 1056900 training loss: 1.9596
Global Iter: 1056900 training acc: 0.25
Global Iter: 1057000 training loss: 1.97426
Global Iter: 1057000 training acc: 0.09375
Global Iter: 1057100 training loss: 1.94493
Global Iter: 1057100 training acc: 0.21875
Global Iter: 1057200 training loss: 2.04045
Global Iter: 1057200 training acc: 0.125
Global Iter: 1057300 training loss: 2.00553
Global Iter: 1057300 training acc: 0.15625
Global Iter: 1057400 training loss: 2.02243
Global Iter: 1057400 training acc: 0.21875
Global Iter: 1057500 training loss: 2.0268
Global Iter: 1057500 training acc: 0.21875
Global Iter: 1057600 training loss: 1.9374
Global Iter: 1057600 training acc: 0.21875
Global Iter: 1057700 training loss2017-06-22 11:24:09.671441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1061482
: 2.08598
Global Iter: 1057700 training acc: 0.0625
Global Iter: 1057800 training loss: 2.06241
Global Iter: 1057800 training acc: 0.15625
Global Iter: 1057900 training loss: 2.01939
Global Iter: 1057900 training acc: 0.21875
Global Iter: 1058000 training loss: 1.914
Global Iter: 1058000 training acc: 0.25
Global Iter: 1058100 training loss: 2.0718
Global Iter: 1058100 training acc: 0.125
Global Iter: 1058200 training loss: 1.93961
Global Iter: 1058200 training acc: 0.25
Global Iter: 1058300 training loss: 2.00846
Global Iter: 1058300 training acc: 0.15625
Global Iter: 1058400 training loss: 1.86172
Global Iter: 1058400 training acc: 0.21875
Global Iter: 1058500 training loss: 1.95292
Global Iter: 1058500 training acc: 0.09375
Global Iter: 1058600 training loss: 1.96615
Global Iter: 1058600 training acc: 0.21875
Global Iter: 1058700 training loss: 2.02541
Global Iter: 1058700 training acc: 0.125
Global Iter: 1058800 training loss: 1.98579
Global Iter: 1058800 training acc: 0.28125
Global Iter: 1058900 training loss: 1.90447
Global Iter: 1058900 training acc: 0.25
Global Iter: 1059000 training loss: 1.93695
Global Iter: 1059000 training acc: 0.25
Global Iter: 1059100 training loss: 1.95384
Global Iter: 1059100 training acc: 0.1875
Global Iter: 1059200 training loss: 1.99159
Global Iter: 1059200 training acc: 0.25
Global Iter: 1059300 training loss: 2.03604
Global Iter: 1059300 training acc: 0.15625
Global Iter: 1059400 training loss: 1.98726
Global Iter: 1059400 training acc: 0.25
Global Iter: 1059500 training loss: 1.95578
Global Iter: 1059500 training acc: 0.21875
Global Iter: 1059600 training loss: 1.97729
Global Iter: 1059600 training acc: 0.25
Global Iter: 1059700 training loss: 1.92653
Global Iter: 1059700 training acc: 0.25
Global Iter: 1059800 training loss: 1.8915
Global Iter: 1059800 training acc: 0.34375
Global Iter: 1059900 training loss: 2.05316
Global Iter: 1059900 training acc: 0.15625
Global Iter: 1060000 training loss: 1.98225
Global Iter: 1060000 training acc: 0.09375
Global Iter: 1060100 training loss: 1.92941
Global Iter: 1060100 training acc: 0.21875
Global Iter: 1060200 training loss: 1.98067
Global Iter: 1060200 training acc: 0.15625
Global Iter: 1060300 training loss: 2.00655
Global Iter: 1060300 training acc: 0.15625
Global Iter: 1060400 training loss: 2.04733
Global Iter: 1060400 training acc: 0.1875
Global Iter: 1060500 training loss: 2.08828
Global Iter: 1060500 training acc: 0.15625
Global Iter: 1060600 training loss: 1.96333
Global Iter: 1060600 training acc: 0.125
Global Iter: 1060700 training loss: 2.0431
Global Iter: 1060700 training acc: 0.03125
Global Iter: 1060800 training loss: 1.99267
Global Iter: 1060800 training acc: 0.1875
Global Iter: 1060900 training loss: 1.90411
Global Iter: 1060900 training acc: 0.28125
Global Iter: 1061000 training loss: 1.99407
Global Iter: 1061000 training acc: 0.28125
Global Iter: 1061100 training loss: 2.0299
Global Iter: 1061100 training acc: 0.15625
Global Iter: 1061200 training loss: 1.96496
Global Iter: 1061200 training acc: 0.1875
Global Iter: 1061300 training loss: 1.99148
Global Iter: 1061300 training acc: 0.125
Global Iter: 1061400 training loss: 1.98216
Global Iter: 1061400 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1061482
Number of Patches: 137609
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1061482
Global Iter: 1061500 training loss: 1.97985
Global Iter: 1061500 training acc: 0.1875
Global Iter: 1061600 training loss: 1.93283
Global Iter: 1061600 training acc: 0.21875
Global Iter: 1061700 training loss: 1.95603
Global Iter: 1061700 training acc: 0.0625
Global Iter: 1061800 training loss: 2.11084
Global Iter: 1061800 training acc: 0.15625
Global Iter: 1061900 training loss: 1.91253
Global Iter: 1061900 training acc: 0.28125
Global Iter: 1062000 training loss: 1.96875
Global Iter: 1062000 training acc: 0.15625
Global Iter: 1062100 training loss: 2.0921
Global Iter: 1062100 training acc: 0.25
Global Iter: 1062200 training loss: 1.94143
Global Iter: 1062200 training acc: 0.09375
Global Iter: 1062300 training loss: 1.98237
Global Iter: 1062300 training acc: 0.15625
Global Iter: 1062400 training loss: 2.04464
Global Iter: 1062400 training acc: 0.15625
Global Iter: 1062500 training loss: 1.97186
Global Iter: 1062500 training acc: 0.1875
Global Iter: 1062600 training loss: 1.94406
Global Iter: 1062600 training acc: 0.15625
Global Iter: 1062700 training loss: 2.0242
Global Iter: 1062700 training acc: 0.1875
Global Iter: 1062800 training loss: 2.02827
Global Iter: 1062800 training acc: 0.0625
Global Iter: 1062900 training loss: 1.91647
Global Iter: 1062900 training acc: 0.28125
Global Iter: 1063000 training loss: 1.91539
Global Iter: 1063000 training acc: 0.34375
Global Iter: 1063100 training loss: 1.95432
Global Iter: 1063100 training acc: 0.125
Global Iter: 1063200 training loss: 1.96971
Global Iter: 1063200 training acc: 0.21875
Global Iter: 1063300 training loss: 2.06
Global Iter: 1063300 training acc: 0.15625
Global Iter: 1063400 training loss: 1.88251
Global Iter: 1063400 training acc: 0.21875
Global Iter: 1063500 training loss: 1.96204
Global Iter: 1063500 training acc: 0.1875
Global Iter: 1063600 training loss: 2.00944
Global Iter: 1063600 training acc: 0.1875
Global Iter: 1063700 training loss: 1.97522
Global Iter: 1063700 training acc: 0.3125
Global Iter: 1063800 training loss: 1.94084
Global Iter: 1063800 training acc: 0.1875
Global Iter: 1063900 training loss: 2.15225
Global Iter: 1063900 training acc: 0.15625
Global Iter: 1064000 training loss: 1.94975
Global Iter: 1064000 training acc: 0.125
Global Iter: 1064100 training loss: 1.99761
Global Iter: 1064100 training acc: 0.125
Global Iter: 1064200 training loss: 1.94835
Global Iter: 1064200 training acc: 0.1875
Global Iter: 1064300 training loss: 1.95996
Global Iter: 1064300 training acc: 0.0625
Global Iter: 1064400 training loss: 2.0355
Global Iter: 1064400 training acc: 0.09375
Global Iter: 1064500 training loss: 2.03461
Global Iter: 1064500 training acc: 0.0625
Global Iter: 1064600 training loss: 1.94415
Global Iter: 1064600 training acc: 0.1875
Global Iter: 1064700 training loss: 2.03116
Global Iter: 1064700 training acc: 0.125
Global Iter: 1064800 training loss: 1.94673
Global Iter: 1064800 training acc: 0.15625
Global Iter: 1064900 training loss: 1.99204
Global Iter: 1064900 training acc: 0.15625
Global Iter: 1065000 training loss: 1.9431
Global Iter: 1065000 training acc: 0.34375
Global Iter: 1065100 training loss: 1.88526
Global Iter: 1065100 training acc: 0.15625
Global Iter: 1065200 training loss: 2.08989
Global Iter: 1065200 training acc: 0.15625
Global Iter: 1065300 training loss: 1.9917
Global Iter: 1065300 training acc: 0.25
Global Iter: 1065400 training loss: 1.99544
Global Iter: 1065400 training acc: 0.125
Global Iter: 1065500 training loss: 1.88531
Global Iter: 1065500 training acc: 0.25
Global Iter: 1065600 training loss: 1.94158
Global Iter: 1065600 training acc: 0.25
Global Iter: 1065700 training loss: 1.92425
Global Iter: 1065700 training acc: 0.21875
Global Iter: 1065800 training loss: 1.9619
Global Iter: 1065800 training acc: 0.09375
Global Iter: 1065900 training loss: 1.98213
Global Iter: 1065900 training acc: 0.125
Global Iter: 1066000 training loss: 2.01597
Global Iter: 1066000 training acc: 0.21875
Global Iter: 1066100 training loss: 1.95808
Global Iter: 1066100 training acc: 0.09375
Global Iter: 1066200 training loss: 2.1281
Global Iter: 1066200 training acc: 0.15625
Global Iter: 1066300 training loss: 2.04609
Global Iter: 1066300 training acc: 0.21875
Global Iter: 1066400 training loss: 1.93323
Global Iter: 1066400 training acc: 0.15625
Global Iter: 1066500 training loss: 1.98048
Global Iter: 1066500 training acc: 0.125
Global Iter: 1066600 training loss: 2.06242
Global Iter: 1066600 training acc: 0.1875
Global Iter: 1066700 training loss: 2.08364
Global Iter: 1066700 training acc: 0.09375
Global Iter: 1066800 training loss: 1.97817
Global Iter: 1066800 training acc: 0.15625
Global Iter: 1066900 training loss: 1.93792
Global Iter: 1066900 training acc: 0.09375
Global2017-06-22 11:38:48.204907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1070083
 Iter: 1067000 training loss: 2.03056
Global Iter: 1067000 training acc: 0.1875
Global Iter: 1067100 training loss: 2.06941
Global Iter: 1067100 training acc: 0.03125
Global Iter: 1067200 training loss: 1.94971
Global Iter: 1067200 training acc: 0.25
Global Iter: 1067300 training loss: 1.91272
Global Iter: 1067300 training acc: 0.15625
Global Iter: 1067400 training loss: 2.02229
Global Iter: 1067400 training acc: 0.09375
Global Iter: 1067500 training loss: 1.96642
Global Iter: 1067500 training acc: 0.25
Global Iter: 1067600 training loss: 1.93174
Global Iter: 1067600 training acc: 0.125
Global Iter: 1067700 training loss: 2.02743
Global Iter: 1067700 training acc: 0.1875
Global Iter: 1067800 training loss: 1.93942
Global Iter: 1067800 training acc: 0.25
Global Iter: 1067900 training loss: 2.03098
Global Iter: 1067900 training acc: 0.1875
Global Iter: 1068000 training loss: 1.96527
Global Iter: 1068000 training acc: 0.1875
Global Iter: 1068100 training loss: 1.96284
Global Iter: 1068100 training acc: 0.25
Global Iter: 1068200 training loss: 1.95969
Global Iter: 1068200 training acc: 0.28125
Global Iter: 1068300 training loss: 1.94624
Global Iter: 1068300 training acc: 0.125
Global Iter: 1068400 training loss: 1.98558
Global Iter: 1068400 training acc: 0.28125
Global Iter: 1068500 training loss: 1.94422
Global Iter: 1068500 training acc: 0.25
Global Iter: 1068600 training loss: 1.96955
Global Iter: 1068600 training acc: 0.15625
Global Iter: 1068700 training loss: 1.89847
Global Iter: 1068700 training acc: 0.1875
Global Iter: 1068800 training loss: 1.90407
Global Iter: 1068800 training acc: 0.1875
Global Iter: 1068900 training loss: 1.95059
Global Iter: 1068900 training acc: 0.125
Global Iter: 1069000 training loss: 1.98555
Global Iter: 1069000 training acc: 0.21875
Global Iter: 1069100 training loss: 1.9996
Global Iter: 1069100 training acc: 0.21875
Global Iter: 1069200 training loss: 1.92204
Global Iter: 1069200 training acc: 0.1875
Global Iter: 1069300 training loss: 1.90251
Global Iter: 1069300 training acc: 0.15625
Global Iter: 1069400 training loss: 1.95672
Global Iter: 1069400 training acc: 0.15625
Global Iter: 1069500 training loss: 1.9407
Global Iter: 1069500 training acc: 0.15625
Global Iter: 1069600 training loss: 1.9042
Global Iter: 1069600 training acc: 0.09375
Global Iter: 1069700 training loss: 2.00686
Global Iter: 1069700 training acc: 0.09375
Global Iter: 1069800 training loss: 2.03344
Global Iter: 1069800 training acc: 0.15625
Global Iter: 1069900 training loss: 1.96572
Global Iter: 1069900 training acc: 0.21875
Global Iter: 1070000 training loss: 2.01275
Global Iter: 1070000 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1070083
Number of Patches: 136233
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1070083
Global Iter: 1070100 training loss: 1.9202
Global Iter: 1070100 training acc: 0.125
Global Iter: 1070200 training loss: 1.95703
Global Iter: 1070200 training acc: 0.1875
Global Iter: 1070300 training loss: 1.92065
Global Iter: 1070300 training acc: 0.34375
Global Iter: 1070400 training loss: 2.08993
Global Iter: 1070400 training acc: 0.1875
Global Iter: 1070500 training loss: 2.07176
Global Iter: 1070500 training acc: 0.15625
Global Iter: 1070600 training loss: 2.18377
Global Iter: 1070600 training acc: 0.09375
Global Iter: 1070700 training loss: 2.03664
Global Iter: 1070700 training acc: 0.0625
Global Iter: 1070800 training loss: 2.01737
Global Iter: 1070800 training acc: 0.09375
Global Iter: 1070900 training loss: 1.99254
Global Iter: 1070900 training acc: 0.1875
Global Iter: 1071000 training loss: 1.9619
Global Iter: 1071000 training acc: 0.1875
Global Iter: 1071100 training loss: 1.90457
Global Iter: 1071100 training acc: 0.125
Global Iter: 1071200 training loss: 2.07294
Global Iter: 1071200 training acc: 0.125
Global Iter: 1071300 training loss: 2.04071
Global Iter: 1071300 training acc: 0.15625
Global Iter: 1071400 training loss: 1.96949
Global Iter: 1071400 training acc: 0.3125
Global Iter: 1071500 training loss: 1.95705
Global Iter: 1071500 training acc: 0.15625
Global Iter: 1071600 training loss: 1.95754
Global Iter: 1071600 training acc: 0.25
Global Iter: 1071700 training loss: 2.038
Global Iter: 1071700 training acc: 0.1875
Global Iter: 1071800 training loss: 1.96985
Global Iter: 1071800 training acc: 0.21875
Global Iter: 1071900 training loss: 1.94656
Global Iter: 1071900 training acc: 0.25
Global Iter: 1072000 training loss: 2.03394
Global Iter: 1072000 training acc: 0.1875
Global Iter: 1072100 training loss: 1.93224
Global Iter: 1072100 training acc: 0.1875
Global Iter: 1072200 training loss: 2.11337
Global Iter: 1072200 training acc: 0.25
Global Iter: 1072300 training loss: 2.04963
Global Iter: 1072300 training acc: 0.0625
Global Iter: 1072400 training loss: 1.9493
Global Iter: 1072400 training acc: 0.21875
Global Iter: 1072500 training loss: 2.02352
Global Iter: 1072500 training acc: 0.28125
Global Iter: 1072600 training loss: 1.92959
Global Iter: 1072600 training acc: 0.09375
Global Iter: 1072700 training loss: 1.98333
Global Iter: 1072700 training acc: 0.15625
Global Iter: 1072800 training loss: 1.91432
Global Iter: 1072800 training acc: 0.21875
Global Iter: 1072900 training loss: 1.95677
Global Iter: 1072900 training acc: 0.09375
Global Iter: 1073000 training loss: 1.97707
Global Iter: 1073000 training acc: 0.15625
Global Iter: 1073100 training loss: 1.96933
Global Iter: 1073100 training acc: 0.125
Global Iter: 1073200 training loss: 2.04918
Global Iter: 1073200 training acc: 0.03125
Global Iter: 1073300 training loss: 1.9887
Global Iter: 1073300 training acc: 0.125
Global Iter: 1073400 training loss: 1.98383
Global Iter: 1073400 training acc: 0.09375
Global Iter: 1073500 training loss: 1.89326
Global Iter: 1073500 training acc: 0.28125
Global Iter: 1073600 training loss: 1.97034
Global Iter: 1073600 training acc: 0.1875
Global Iter: 1073700 training loss: 1.9801
Global Iter: 1073700 training acc: 0.28125
Global Iter: 1073800 training loss: 2.07213
Global Iter: 1073800 training acc: 0.15625
Global Iter: 1073900 training loss: 1.96496
Global Iter: 1073900 training acc: 0.125
Global Iter: 1074000 training loss: 2.06587
Global Iter: 1074000 training acc: 0.09375
Global Iter: 1074100 training loss: 1.89295
Global Iter: 1074100 training acc: 0.09375
Global Iter: 1074200 training loss: 1.97709
Global Iter: 1074200 training acc: 0.1875
Global Iter: 1074300 training loss: 1.97507
Global Iter: 1074300 training acc: 0.1875
Global Iter: 1074400 training loss: 1.87213
Global Iter: 1074400 training acc: 0.28125
Global Iter: 1074500 training loss: 2.00252
Global Iter: 1074500 training acc: 0.15625
Global Iter: 1074600 training loss: 2.02173
Global Iter: 1074600 training acc: 0.28125
Global Iter: 1074700 training loss: 1.93531
Global Iter: 1074700 training acc: 0.15625
Global Iter: 1074800 training loss: 2.03685
Global Iter: 1074800 training acc: 0.1875
Global Iter: 1074900 training loss: 1.97827
Global Iter: 1074900 training acc: 0.28125
Global Iter: 1075000 training loss: 1.89492
Global Iter: 1075000 training acc: 0.28125
Global Iter: 1075100 training loss: 1.90236
Global Iter: 1075100 training acc: 0.28125
Global Iter: 1075200 training loss: 2.05531
Global Iter: 1075200 training acc: 0.03125
Global Iter: 1075300 training loss: 1.95506
Global Iter: 1075300 training acc: 0.0625
Global Iter: 1075400 training loss: 2.03263
Global Iter: 1075400 training acc: 0.15625
Global Iter: 1075500 training loss: 2.02252
Global Iter: 1075500 training acc: 0.21875
Global Iter: 1075600 training loss: 1.96326
Global Iter: 1075600 training acc: 0.15625
Global Iter: 1075700 training loss: 1.97144
Global Iter: 1075700 training acc: 0.1875
Global Iter: 1075800 training loss: 2.07332
Global Iter: 1075800 training acc: 0.125
Global Iter: 1075900 training loss: 2.04026
Global Iter: 1075900 training acc: 0.0625
Global Iter: 1076000 training loss: 2.11955
Global Iter: 1076000 training acc: 0.125
Global Iter: 1076100 training loss: 2.21696
Global Iter: 1076100 training acc: 0.09375
Global Iter: 1076200 training loss: 2.07566
Global 2017-06-22 11:53:10.333350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1078598
Iter: 1076200 training acc: 0.15625
Global Iter: 1076300 training loss: 1.97226
Global Iter: 1076300 training acc: 0.125
Global Iter: 1076400 training loss: 2.02764
Global Iter: 1076400 training acc: 0.125
Global Iter: 1076500 training loss: 2.00792
Global Iter: 1076500 training acc: 0.15625
Global Iter: 1076600 training loss: 2.07834
Global Iter: 1076600 training acc: 0.125
Global Iter: 1076700 training loss: 1.86854
Global Iter: 1076700 training acc: 0.15625
Global Iter: 1076800 training loss: 1.96035
Global Iter: 1076800 training acc: 0.125
Global Iter: 1076900 training loss: 1.9464
Global Iter: 1076900 training acc: 0.125
Global Iter: 1077000 training loss: 2.12363
Global Iter: 1077000 training acc: 0.125
Global Iter: 1077100 training loss: 1.98189
Global Iter: 1077100 training acc: 0.21875
Global Iter: 1077200 training loss: 2.10988
Global Iter: 1077200 training acc: 0.15625
Global Iter: 1077300 training loss: 1.90804
Global Iter: 1077300 training acc: 0.25
Global Iter: 1077400 training loss: 1.91661
Global Iter: 1077400 training acc: 0.21875
Global Iter: 1077500 training loss: 2.07202
Global Iter: 1077500 training acc: 0.1875
Global Iter: 1077600 training loss: 2.08102
Global Iter: 1077600 training acc: 0.15625
Global Iter: 1077700 training loss: 2.10014
Global Iter: 1077700 training acc: 0.125
Global Iter: 1077800 training loss: 1.92894
Global Iter: 1077800 training acc: 0.21875
Global Iter: 1077900 training loss: 2.13699
Global Iter: 1077900 training acc: 0.09375
Global Iter: 1078000 training loss: 1.9599
Global Iter: 1078000 training acc: 0.0625
Global Iter: 1078100 training loss: 2.07427
Global Iter: 1078100 training acc: 0.15625
Global Iter: 1078200 training loss: 2.0168
Global Iter: 1078200 training acc: 0.21875
Global Iter: 1078300 training loss: 2.00952
Global Iter: 1078300 training acc: 0.15625
Global Iter: 1078400 training loss: 1.96073
Global Iter: 1078400 training acc: 0.125
Global Iter: 1078500 training loss: 1.9648
Global Iter: 1078500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1078598
Number of Patches: 134871
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1078598
Global Iter: 1078600 training loss: 1.96374
Global Iter: 1078600 training acc: 0.34375
Global Iter: 1078700 training loss: 1.98038
Global Iter: 1078700 training acc: 0.15625
Global Iter: 1078800 training loss: 1.95744
Global Iter: 1078800 training acc: 0.25
Global Iter: 1078900 training loss: 1.89138
Global Iter: 1078900 training acc: 0.1875
Global Iter: 1079000 training loss: 1.87964
Global Iter: 1079000 training acc: 0.40625
Global Iter: 1079100 training loss: 2.01961
Global Iter: 1079100 training acc: 0.21875
Global Iter: 1079200 training loss: 1.95138
Global Iter: 1079200 training acc: 0.25
Global Iter: 1079300 training loss: 2.13454
Global Iter: 1079300 training acc: 0.15625
Global Iter: 1079400 training loss: 2.05604
Global Iter: 1079400 training acc: 0.1875
Global Iter: 1079500 training loss: 2.00177
Global Iter: 1079500 training acc: 0.15625
Global Iter: 1079600 training loss: 1.94493
Global Iter: 1079600 training acc: 0.21875
Global Iter: 1079700 training loss: 1.96333
Global Iter: 1079700 training acc: 0.25
Global Iter: 1079800 training loss: 1.93671
Global Iter: 1079800 training acc: 0.3125
Global Iter: 1079900 training loss: 1.99508
Global Iter: 1079900 training acc: 0.09375
Global Iter: 1080000 training loss: 2.0306
Global Iter: 1080000 training acc: 0.125
Global Iter: 1080100 training loss: 1.91967
Global Iter: 1080100 training acc: 0.3125
Global Iter: 1080200 training loss: 1.96567
Global Iter: 1080200 training acc: 0.28125
Global Iter: 1080300 training loss: 1.94806
Global Iter: 1080300 training acc: 0.1875
Global Iter: 1080400 training loss: 2.02161
Global Iter: 1080400 training acc: 0.09375
Global Iter: 1080500 training loss: 1.93096
Global Iter: 1080500 training acc: 0.25
Global Iter: 1080600 training loss: 2.03264
Global Iter: 1080600 training acc: 0.15625
Global Iter: 1080700 training loss: 2.00826
Global Iter: 1080700 training acc: 0.15625
Global Iter: 1080800 training loss: 1.99727
Global Iter: 1080800 training acc: 0.125
Global Iter: 1080900 training loss: 1.98573
Global Iter: 1080900 training acc: 0.3125
Global Iter: 1081000 training loss: 2.00555
Global Iter: 1081000 training acc: 0.15625
Global Iter: 1081100 training loss: 1.92232
Global Iter: 1081100 training acc: 0.25
Global Iter: 1081200 training loss: 1.98843
Global Iter: 1081200 training acc: 0.21875
Global Iter: 1081300 training loss: 2.01484
Global Iter: 1081300 training acc: 0.09375
Global Iter: 1081400 training loss: 2.02819
Global Iter: 1081400 training acc: 0.21875
Global Iter: 1081500 training loss: 1.98734
Global Iter: 1081500 training acc: 0.125
Global Iter: 1081600 training loss: 1.89979
Global Iter: 1081600 training acc: 0.15625
Global Iter: 1081700 training loss: 2.07516
Global Iter: 1081700 training acc: 0.125
Global Iter: 1081800 training loss: 1.93301
Global Iter: 1081800 training acc: 0.09375
Global Iter: 1081900 training loss: 2.01892
Global Iter: 1081900 training acc: 0.1875
Global Iter: 1082000 training loss: 1.98922
Global Iter: 1082000 training acc: 0.15625
Global Iter: 1082100 training loss: 2.01189
Global Iter: 1082100 training acc: 0.1875
Global Iter: 1082200 training loss: 2.04278
Global Iter: 1082200 training acc: 0.125
Global Iter: 1082300 training loss: 1.92124
Global Iter: 1082300 training acc: 0.15625
Global Iter: 1082400 training loss: 2.01129
Global Iter: 1082400 training acc: 0.1875
Global Iter: 1082500 training loss: 2.02636
Global Iter: 1082500 training acc: 0.1875
Global Iter: 1082600 training loss: 1.98093
Global Iter: 1082600 training acc: 0.25
Global Iter: 1082700 training loss: 1.95234
Global Iter: 1082700 training acc: 0.125
Global Iter: 1082800 training loss: 2.0619
Global Iter: 1082800 training acc: 0.21875
Global Iter: 1082900 training loss: 2.01228
Global Iter: 1082900 training acc: 0.28125
Global Iter: 1083000 training loss: 2.04045
Global Iter: 1083000 training acc: 0.15625
Global Iter: 1083100 training loss: 1.96715
Global Iter: 1083100 training acc: 0.25
Global Iter: 1083200 training loss: 2.1204
Global Iter: 1083200 training acc: 0.15625
Global Iter: 1083300 training loss: 2.0781
Global Iter: 1083300 training acc: 0.15625
Global Iter: 1083400 training loss: 2.00841
Global Iter: 1083400 training acc: 0.125
Global Iter: 1083500 training loss: 1.91606
Global Iter: 1083500 training acc: 0.21875
Global Iter: 1083600 training loss: 1.95773
Global Iter: 1083600 training acc: 0.25
Global Iter: 1083700 training loss: 2.0809
Global Iter: 1083700 training acc: 0.125
Global Iter: 1083800 training loss: 2.05918
Global Iter: 1083800 training acc: 0.15625
Global Iter: 1083900 training loss: 1.93375
Global Iter: 1083900 training acc: 0.21875
Global Iter: 1084000 training loss: 2.06154
Global Iter: 1084000 training acc: 0.21875
Global Iter: 1084100 training loss: 1.94388
Global Iter: 1084100 training acc: 0.28125
Global Iter: 1084200 training loss: 1.91118
Global Iter: 1084200 training acc: 0.1875
Global Iter: 1084300 training loss: 2.04214
Global Iter: 1084300 training acc: 0.15625
Global Iter: 1084400 training loss: 2.02932
Global Iter: 1084400 training acc: 0.21875
Global Iter: 1084500 training loss: 2.06472
Global Iter: 1084500 training acc: 0.21875
Global Iter: 1084600 training loss: 1.93554
Global Iter: 1084600 training acc: 0.125
Global Iter: 1084700 training loss: 2.03893
Global Iter: 1084700 training acc: 0.25
Global Iter: 1084800 training loss: 2.01433
Global Iter: 1084800 training acc: 0.09375
Global Iter: 1084900 training loss: 2.07684
Global Iter: 1084900 training acc: 0.125
Global Iter: 1085000 training loss: 2.0184
Global Iter: 1085000 training acc: 0.25
Global Iter: 1085100 training loss: 2.09199
Global Iter: 1085100 training acc: 0.15625
Global Iter: 1085200 training loss: 2.05582
Global Iter: 1085200 training acc: 0.1875
Global Iter: 1085300 training loss: 1.96514
Global Iter: 1085300 training acc: 0.21875
Global Iter: 1085400 training loss: 1.98895
Global Iter: 1085400 training acc: 0.21875
Global Iter: 10855002017-06-22 12:07:28.250359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1087028
 training loss: 1.95355
Global Iter: 1085500 training acc: 0.1875
Global Iter: 1085600 training loss: 1.98017
Global Iter: 1085600 training acc: 0.1875
Global Iter: 1085700 training loss: 1.93733
Global Iter: 1085700 training acc: 0.1875
Global Iter: 1085800 training loss: 2.0104
Global Iter: 1085800 training acc: 0.0625
Global Iter: 1085900 training loss: 1.9611
Global Iter: 1085900 training acc: 0.28125
Global Iter: 1086000 training loss: 2.01373
Global Iter: 1086000 training acc: 0.28125
Global Iter: 1086100 training loss: 1.91203
Global Iter: 1086100 training acc: 0.34375
Global Iter: 1086200 training loss: 2.04547
Global Iter: 1086200 training acc: 0.125
Global Iter: 1086300 training loss: 1.95506
Global Iter: 1086300 training acc: 0.21875
Global Iter: 1086400 training loss: 2.0781
Global Iter: 1086400 training acc: 0.1875
Global Iter: 1086500 training loss: 1.99431
Global Iter: 1086500 training acc: 0.1875
Global Iter: 1086600 training loss: 1.91338
Global Iter: 1086600 training acc: 0.28125
Global Iter: 1086700 training loss: 1.97572
Global Iter: 1086700 training acc: 0.3125
Global Iter: 1086800 training loss: 1.88575
Global Iter: 1086800 training acc: 0.15625
Global Iter: 1086900 training loss: 2.09142
Global Iter: 1086900 training acc: 0.03125
Global Iter: 1087000 training loss: 1.95744
Global Iter: 1087000 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1087028
Number of Patches: 133523
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1087028
Global Iter: 1087100 training loss: 2.0233
Global Iter: 1087100 training acc: 0.15625
Global Iter: 1087200 training loss: 2.10678
Global Iter: 1087200 training acc: 0.1875
Global Iter: 1087300 training loss: 1.97738
Global Iter: 1087300 training acc: 0.15625
Global Iter: 1087400 training loss: 2.03359
Global Iter: 1087400 training acc: 0.15625
Global Iter: 1087500 training loss: 2.027
Global Iter: 1087500 training acc: 0.1875
Global Iter: 1087600 training loss: 1.93816
Global Iter: 1087600 training acc: 0.1875
Global Iter: 1087700 training loss: 1.92687
Global Iter: 1087700 training acc: 0.1875
Global Iter: 1087800 training loss: 2.0082
Global Iter: 1087800 training acc: 0.21875
Global Iter: 1087900 training loss: 2.06233
Global Iter: 1087900 training acc: 0.125
Global Iter: 1088000 training loss: 1.98873
Global Iter: 1088000 training acc: 0.125
Global Iter: 1088100 training loss: 2.01817
Global Iter: 1088100 training acc: 0.1875
Global Iter: 1088200 training loss: 2.01921
Global Iter: 1088200 training acc: 0.15625
Global Iter: 1088300 training loss: 2.05828
Global Iter: 1088300 training acc: 0.15625
Global Iter: 1088400 training loss: 2.00145
Global Iter: 1088400 training acc: 0.09375
Global Iter: 1088500 training loss: 1.95644
Global Iter: 1088500 training acc: 0.09375
Global Iter: 1088600 training loss: 2.0056
Global Iter: 1088600 training acc: 0.1875
Global Iter: 1088700 training loss: 1.93726
Global Iter: 1088700 training acc: 0.125
Global Iter: 1088800 training loss: 1.93038
Global Iter: 1088800 training acc: 0.21875
Global Iter: 1088900 training loss: 2.05268
Global Iter: 1088900 training acc: 0.21875
Global Iter: 1089000 training loss: 1.94891
Global Iter: 1089000 training acc: 0.15625
Global Iter: 1089100 training loss: 2.0276
Global Iter: 1089100 training acc: 0.15625
Global Iter: 1089200 training loss: 1.96434
Global Iter: 1089200 training acc: 0.09375
Global Iter: 1089300 training loss: 1.932
Global Iter: 1089300 training acc: 0.1875
Global Iter: 1089400 training loss: 1.91457
Global Iter: 1089400 training acc: 0.28125
Global Iter: 1089500 training loss: 1.93176
Global Iter: 1089500 training acc: 0.3125
Global Iter: 1089600 training loss: 1.99032
Global Iter: 1089600 training acc: 0.1875
Global Iter: 1089700 training loss: 2.01229
Global Iter: 1089700 training acc: 0.09375
Global Iter: 1089800 training loss: 1.98405
Global Iter: 1089800 training acc: 0.15625
Global Iter: 1089900 training loss: 2.01345
Global Iter: 1089900 training acc: 0.21875
Global Iter: 1090000 training loss: 2.04663
Global Iter: 1090000 training acc: 0.21875
Global Iter: 1090100 training loss: 2.03496
Global Iter: 1090100 training acc: 0.1875
Global Iter: 1090200 training loss: 1.98102
Global Iter: 1090200 training acc: 0.25
Global Iter: 1090300 training loss: 1.97944
Global Iter: 1090300 training acc: 0.15625
Global Iter: 1090400 training loss: 1.93956
Global Iter: 1090400 training acc: 0.1875
Global Iter: 1090500 training loss: 1.96575
Global Iter: 1090500 training acc: 0.1875
Global Iter: 1090600 training loss: 2.08683
Global Iter: 1090600 training acc: 0.1875
Global Iter: 1090700 training loss: 2.15079
Global Iter: 1090700 training acc: 0.03125
Global Iter: 1090800 training loss: 2.0035
Global Iter: 1090800 training acc: 0.1875
Global Iter: 1090900 training loss: 1.8749
Global Iter: 1090900 training acc: 0.3125
Global Iter: 1091000 training loss: 2.03334
Global Iter: 1091000 training acc: 0.125
Global Iter: 1091100 training loss: 1.94147
Global Iter: 1091100 training acc: 0.1875
Global Iter: 1091200 training loss: 1.98468
Global Iter: 1091200 training acc: 0.1875
Global Iter: 1091300 training loss: 1.94908
Global Iter: 1091300 training acc: 0.1875
Global Iter: 1091400 training loss: 1.94505
Global Iter: 1091400 training acc: 0.25
Global Iter: 1091500 training loss: 1.95606
Global Iter: 1091500 training acc: 0.25
Global Iter: 1091600 training loss: 1.9695
Global Iter: 1091600 training acc: 0.15625
Global Iter: 1091700 training loss: 2.03913
Global Iter: 1091700 training acc: 0.09375
Global Iter: 1091800 training loss: 1.93831
Global Iter: 1091800 training acc: 0.15625
Global Iter: 1091900 training loss: 2.10506
Global Iter: 1091900 training acc: 0.15625
Global Iter: 1092000 training loss: 2.04727
Global Iter: 1092000 training acc: 0.09375
Global Iter: 1092100 training loss: 2.05226
Global Iter: 1092100 training acc: 0.125
Global Iter: 1092200 training loss: 2.0474
Global Iter: 1092200 training acc: 0.0625
Global Iter: 1092300 training loss: 1.91634
Global Iter: 1092300 training acc: 0.1875
Global Iter: 1092400 training loss: 1.94667
Global Iter: 1092400 training acc: 0.15625
Global Iter: 1092500 training loss: 2.02895
Global Iter: 1092500 training acc: 0.1875
Global Iter: 1092600 training loss: 1.95675
Global Iter: 1092600 training acc: 0.1875
Global Iter: 1092700 training loss: 2.08037
Global Iter: 1092700 training acc: 0.1875
Global Iter: 1092800 training loss: 2.02066
Global Iter: 1092800 training acc: 0.09375
Global Iter: 1092900 training loss: 2.05876
Global Iter: 1092900 training acc: 0.15625
Global Iter: 1093000 training loss: 1.96289
Global Iter: 1093000 training acc: 0.25
Global Iter: 1093100 training loss: 1.9579
Global Iter: 1093100 training acc: 0.1875
Global Iter: 1093200 training loss: 2.06156
Global Iter: 1093200 training acc: 0.09375
Global Iter: 1093300 training loss: 2.0896
Global Iter: 1093300 training acc: 0.09375
Global Iter: 1093400 training loss: 2.03266
Global Iter: 1093400 training acc: 0.1875
Global Iter: 1093500 training loss: 1.99929
Global Iter: 1093500 training acc: 0.1875
Global Iter: 1093600 training loss: 1.99872
Global Iter: 1093600 training acc: 0.125
Global Iter: 1093700 training loss: 1.97723
Global Iter: 1093700 training acc: 0.125
Global Iter: 1093800 training loss: 1.99173
Global Iter: 1093800 training acc: 0.15625
Global Iter: 1093900 training loss: 1.92263
Global Iter: 1093900 training acc: 0.1875
Global Iter: 1094000 training loss: 1.94627
Global Iter: 1094000 training acc: 0.15625
Global Iter: 1094100 training loss: 1.95437
Global Iter: 1094100 training acc: 0.15625
Global Iter: 1094200 training loss: 2.01438
Global Iter: 1094200 training acc: 0.15625
Global Iter: 1094300 training loss: 2.07044
Global Iter: 1094300 training acc: 0.21875
Global Iter: 1094400 training loss: 2.11652
Global Iter: 1094400 training acc: 0.125
Global Iter: 1094500 training loss: 1.94416
Global Iter: 1094500 training acc: 0.25
Global Iter: 1094600 training loss: 2.03332
Global Iter: 1094600 training acc: 0.125
Global Iter: 1094700 training loss: 1.91927
Global Iter: 1094700 trai2017-06-22 12:21:45.769720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1095374
ning acc: 0.28125
Global Iter: 1094800 training loss: 1.98615
Global Iter: 1094800 training acc: 0.21875
Global Iter: 1094900 training loss: 2.01159
Global Iter: 1094900 training acc: 0.15625
Global Iter: 1095000 training loss: 2.0341
Global Iter: 1095000 training acc: 0.125
Global Iter: 1095100 training loss: 1.95986
Global Iter: 1095100 training acc: 0.3125
Global Iter: 1095200 training loss: 1.97647
Global Iter: 1095200 training acc: 0.1875
Global Iter: 1095300 training loss: 2.00581
Global Iter: 1095300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1095374
Number of Patches: 132188
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1095374
Global Iter: 1095400 training loss: 1.96883
Global Iter: 1095400 training acc: 0.125
Global Iter: 1095500 training loss: 1.95224
Global Iter: 1095500 training acc: 0.09375
Global Iter: 1095600 training loss: 2.01538
Global Iter: 1095600 training acc: 0.125
Global Iter: 1095700 training loss: 1.96389
Global Iter: 1095700 training acc: 0.15625
Global Iter: 1095800 training loss: 2.04751
Global Iter: 1095800 training acc: 0.15625
Global Iter: 1095900 training loss: 1.99061
Global Iter: 1095900 training acc: 0.25
Global Iter: 1096000 training loss: 2.0469
Global Iter: 1096000 training acc: 0.15625
Global Iter: 1096100 training loss: 2.00609
Global Iter: 1096100 training acc: 0.21875
Global Iter: 1096200 training loss: 1.96912
Global Iter: 1096200 training acc: 0.25
Global Iter: 1096300 training loss: 2.07749
Global Iter: 1096300 training acc: 0.09375
Global Iter: 1096400 training loss: 1.91678
Global Iter: 1096400 training acc: 0.21875
Global Iter: 1096500 training loss: 1.92119
Global Iter: 1096500 training acc: 0.21875
Global Iter: 1096600 training loss: 1.96699
Global Iter: 1096600 training acc: 0.21875
Global Iter: 1096700 training loss: 2.00176
Global Iter: 1096700 training acc: 0.1875
Global Iter: 1096800 training loss: 1.96824
Global Iter: 1096800 training acc: 0.15625
Global Iter: 1096900 training loss: 2.12977
Global Iter: 1096900 training acc: 0.1875
Global Iter: 1097000 training loss: 2.09468
Global Iter: 1097000 training acc: 0.0625
Global Iter: 1097100 training loss: 2.03583
Global Iter: 1097100 training acc: 0.15625
Global Iter: 1097200 training loss: 2.07231
Global Iter: 1097200 training acc: 0.25
Global Iter: 1097300 training loss: 2.01082
Global Iter: 1097300 training acc: 0.15625
Global Iter: 1097400 training loss: 1.93478
Global Iter: 1097400 training acc: 0.03125
Global Iter: 1097500 training loss: 1.92426
Global Iter: 1097500 training acc: 0.3125
Global Iter: 1097600 training loss: 2.09204
Global Iter: 1097600 training acc: 0.0625
Global Iter: 1097700 training loss: 1.91699
Global Iter: 1097700 training acc: 0.09375
Global Iter: 1097800 training loss: 2.04589
Global Iter: 1097800 training acc: 0.15625
Global Iter: 1097900 training loss: 2.06531
Global Iter: 1097900 training acc: 0.15625
Global Iter: 1098000 training loss: 1.95618
Global Iter: 1098000 training acc: 0.28125
Global Iter: 1098100 training loss: 1.97281
Global Iter: 1098100 training acc: 0.125
Global Iter: 1098200 training loss: 1.98096
Global Iter: 1098200 training acc: 0.09375
Global Iter: 1098300 training loss: 2.04839
Global Iter: 1098300 training acc: 0.0
Global Iter: 1098400 training loss: 2.03423
Global Iter: 1098400 training acc: 0.1875
Global Iter: 1098500 training loss: 2.00657
Global Iter: 1098500 training acc: 0.1875
Global Iter: 1098600 training loss: 2.02489
Global Iter: 1098600 training acc: 0.09375
Global Iter: 1098700 training loss: 1.9363
Global Iter: 1098700 training acc: 0.28125
Global Iter: 1098800 training loss: 1.91426
Global Iter: 1098800 training acc: 0.3125
Global Iter: 1098900 training loss: 1.89998
Global Iter: 1098900 training acc: 0.21875
Global Iter: 1099000 training loss: 1.98323
Global Iter: 1099000 training acc: 0.28125
Global Iter: 1099100 training loss: 1.96362
Global Iter: 1099100 training acc: 0.15625
Global Iter: 1099200 training loss: 1.82839
Global Iter: 102017-06-22 12:35:47.685929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1103636
99200 training acc: 0.34375
Global Iter: 1099300 training loss: 1.97381
Global Iter: 1099300 training acc: 0.15625
Global Iter: 1099400 training loss: 2.00085
Global Iter: 1099400 training acc: 0.1875
Global Iter: 1099500 training loss: 1.94054
Global Iter: 1099500 training acc: 0.25
Global Iter: 1099600 training loss: 2.00629
Global Iter: 1099600 training acc: 0.09375
Global Iter: 1099700 training loss: 2.07215
Global Iter: 1099700 training acc: 0.21875
Global Iter: 1099800 training loss: 1.97087
Global Iter: 1099800 training acc: 0.15625
Global Iter: 1099900 training loss: 1.99163
Global Iter: 1099900 training acc: 0.1875
Global Iter: 1100000 training loss: 1.94061
Global Iter: 1100000 training acc: 0.15625
Global Iter: 1100100 training loss: 1.90044
Global Iter: 1100100 training acc: 0.15625
Global Iter: 1100200 training loss: 1.98858
Global Iter: 1100200 training acc: 0.21875
Global Iter: 1100300 training loss: 2.15109
Global Iter: 1100300 training acc: 0.1875
Global Iter: 1100400 training loss: 2.02373
Global Iter: 1100400 training acc: 0.15625
Global Iter: 1100500 training loss: 2.11338
Global Iter: 1100500 training acc: 0.15625
Global Iter: 1100600 training loss: 1.93246
Global Iter: 1100600 training acc: 0.25
Global Iter: 1100700 training loss: 1.95941
Global Iter: 1100700 training acc: 0.1875
Global Iter: 1100800 training loss: 2.11249
Global Iter: 1100800 training acc: 0.21875
Global Iter: 1100900 training loss: 1.95223
Global Iter: 1100900 training acc: 0.15625
Global Iter: 1101000 training loss: 2.03525
Global Iter: 1101000 training acc: 0.21875
Global Iter: 1101100 training loss: 2.00017
Global Iter: 1101100 training acc: 0.15625
Global Iter: 1101200 training loss: 1.92908
Global Iter: 1101200 training acc: 0.21875
Global Iter: 1101300 training loss: 1.86227
Global Iter: 1101300 training acc: 0.25
Global Iter: 1101400 training loss: 1.95565
Global Iter: 1101400 training acc: 0.3125
Global Iter: 1101500 training loss: 1.96253
Global Iter: 1101500 training acc: 0.1875
Global Iter: 1101600 training loss: 1.9867
Global Iter: 1101600 training acc: 0.21875
Global Iter: 1101700 training loss: 1.94353
Global Iter: 1101700 training acc: 0.09375
Global Iter: 1101800 training loss: 1.97605
Global Iter: 1101800 training acc: 0.15625
Global Iter: 1101900 training loss: 1.96168
Global Iter: 1101900 training acc: 0.21875
Global Iter: 1102000 training loss: 2.02542
Global Iter: 1102000 training acc: 0.1875
Global Iter: 1102100 training loss: 1.96461
Global Iter: 1102100 training acc: 0.1875
Global Iter: 1102200 training loss: 1.94039
Global Iter: 1102200 training acc: 0.09375
Global Iter: 1102300 training loss: 2.02824
Global Iter: 1102300 training acc: 0.125
Global Iter: 1102400 training loss: 2.00048
Global Iter: 1102400 training acc: 0.15625
Global Iter: 1102500 training loss: 1.93612
Global Iter: 1102500 training acc: 0.3125
Global Iter: 1102600 training loss: 1.99455
Global Iter: 1102600 training acc: 0.1875
Global Iter: 1102700 training loss: 1.92051
Global Iter: 1102700 training acc: 0.1875
Global Iter: 1102800 training loss: 1.88507
Global Iter: 1102800 training acc: 0.25
Global Iter: 1102900 training loss: 2.00558
Global Iter: 1102900 training acc: 0.125
Global Iter: 1103000 training loss: 1.96567
Global Iter: 1103000 training acc: 0.3125
Global Iter: 1103100 training loss: 1.97523
Global Iter: 1103100 training acc: 0.21875
Global Iter: 1103200 training loss: 1.92092
Global Iter: 1103200 training acc: 0.21875
Global Iter: 1103300 training loss: 1.97199
Global Iter: 1103300 training acc: 0.09375
Global Iter: 1103400 training loss: 1.89874
Global Iter: 1103400 training acc: 0.25
Global Iter: 1103500 training loss: 1.90254
Global Iter: 1103500 training acc: 0.25
Global Iter: 1103600 training loss: 1.97942
Global Iter: 1103600 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1103636
Number of Patches: 130867
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1103636
Global Iter: 1103700 training loss: 1.98502
Global Iter: 1103700 training acc: 0.15625
Global Iter: 1103800 training loss: 1.95985
Global Iter: 1103800 training acc: 0.15625
Global Iter: 1103900 training loss: 2.00346
Global Iter: 1103900 training acc: 0.1875
Global Iter: 1104000 training loss: 2.00793
Global Iter: 1104000 training acc: 0.15625
Global Iter: 1104100 training loss: 1.95813
Global Iter: 1104100 training acc: 0.21875
Global Iter: 1104200 training loss: 1.90198
Global Iter: 1104200 training acc: 0.1875
Global Iter: 1104300 training loss: 1.98039
Global Iter: 1104300 training acc: 0.40625
Global Iter: 1104400 training loss: 2.007
Global Iter: 1104400 training acc: 0.34375
Global Iter: 1104500 training loss: 2.03787
Global Iter: 1104500 training acc: 0.21875
Global Iter: 1104600 training loss: 2.03226
Global Iter: 1104600 training acc: 0.15625
Global Iter: 1104700 training loss: 1.94326
Global Iter: 1104700 training acc: 0.1875
Global Iter: 1104800 training loss: 2.02489
Global Iter: 1104800 training acc: 0.28125
Global Iter: 1104900 training loss: 1.93017
Global Iter: 1104900 training acc: 0.25
Global Iter: 1105000 training loss: 1.95153
Global Iter: 1105000 training acc: 0.1875
Global Iter: 1105100 training loss: 2.04186
Global Iter: 1105100 training acc: 0.125
Global Iter: 1105200 training loss: 1.95382
Global Iter: 1105200 training acc: 0.28125
Global Iter: 1105300 training loss: 2.11032
Global Iter: 1105300 training acc: 0.125
Global Iter: 1105400 training loss: 1.94342
Global Iter: 1105400 training acc: 0.1875
Global Iter: 1105500 training loss: 1.92789
Global Iter: 1105500 training acc: 0.1875
Global Iter: 1105600 training loss: 1.94389
Global Iter: 1105600 training acc: 0.125
Global Iter: 1105700 training loss: 2.04319
Global Iter: 1105700 training acc: 0.09375
Global Iter: 1105800 training loss: 1.97727
Global Iter: 1105800 training acc: 0.125
Global Iter: 1105900 training loss: 1.93837
Global Iter: 1105900 training acc: 0.28125
Global Iter: 1106000 training loss: 2.08822
Global Iter: 1106000 training acc: 0.03125
Global Iter: 1106100 training loss: 1.99935
Global Iter: 1106100 training acc: 0.125
Global Iter: 1106200 training loss: 2.01652
Global Iter: 1106200 training acc: 0.21875
Global Iter: 1106300 training loss: 1.94087
Global Iter: 1106300 training acc: 0.125
Global Iter: 1106400 training loss: 1.94801
Global Iter: 1106400 training acc: 0.21875
Global Iter: 1106500 training loss: 1.94679
Global Iter: 1106500 training acc: 0.15625
Global Iter: 1106600 training loss: 1.98096
Global Iter: 1106600 training acc: 0.34375
Global Iter: 1106700 training loss: 1.94915
Global Iter: 1106700 training acc: 0.21875
Global Iter: 1106800 training loss: 1.95358
Global Iter: 1106800 training acc: 0.1875
Global Iter: 1106900 training loss: 2.06002
Global Iter: 1106900 training acc: 0.125
Global Iter: 1107000 training loss: 2.08396
Global Iter: 1107000 training acc: 0.125
Global Iter: 1107100 training loss: 1.95279
Global Iter: 1107100 training acc: 0.25
Global Iter: 1107200 training loss: 2.05234
Global Iter: 1107200 training acc: 0.21875
Global Iter: 1107300 training loss: 1.93713
Global Iter: 1107300 training acc: 0.25
Global Iter: 1107400 training loss: 1.90124
Global Iter: 1107400 training acc: 0.15625
Global Iter: 1107500 training loss: 2.04902
Global Iter: 1107500 training acc: 0.125
Global Iter: 1107600 training loss: 2.00085
Global Iter: 1107600 training acc: 0.21875
Global Iter: 1107700 training loss: 2.05181
Global Iter: 1107700 training acc: 0.21875
Global Iter: 1107800 training loss: 1.95844
Global Iter: 1107800 training acc: 0.09375
Global Iter: 1107900 training loss: 1.99072
Global Iter: 1107900 training acc: 0.09375
Global Iter: 1108000 training loss: 2.0734
Global Iter: 1108000 training acc: 0.15625
Global Iter: 1108100 training loss: 1.98435
Global Iter: 1108100 training acc: 0.09375
Global Iter: 1108200 training loss: 2.02365
Global Iter: 1108200 training acc: 0.09375
Global Iter: 1108300 training loss: 1.95687
Global Iter: 1108300 training acc: 0.15625
Global Iter: 1108400 training loss: 2.00695
Global Iter: 1108400 training acc: 0.28125
Global Ite2017-06-22 12:49:34.404099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1111816
r: 1108500 training loss: 2.02341
Global Iter: 1108500 training acc: 0.15625
Global Iter: 1108600 training loss: 1.93706
Global Iter: 1108600 training acc: 0.1875
Global Iter: 1108700 training loss: 2.0427
Global Iter: 1108700 training acc: 0.1875
Global Iter: 1108800 training loss: 2.08725
Global Iter: 1108800 training acc: 0.1875
Global Iter: 1108900 training loss: 2.02996
Global Iter: 1108900 training acc: 0.09375
Global Iter: 1109000 training loss: 1.99968
Global Iter: 1109000 training acc: 0.28125
Global Iter: 1109100 training loss: 1.97383
Global Iter: 1109100 training acc: 0.1875
Global Iter: 1109200 training loss: 2.01003
Global Iter: 1109200 training acc: 0.28125
Global Iter: 1109300 training loss: 1.91651
Global Iter: 1109300 training acc: 0.25
Global Iter: 1109400 training loss: 2.01813
Global Iter: 1109400 training acc: 0.21875
Global Iter: 1109500 training loss: 1.97219
Global Iter: 1109500 training acc: 0.21875
Global Iter: 1109600 training loss: 1.9182
Global Iter: 1109600 training acc: 0.125
Global Iter: 1109700 training loss: 1.87078
Global Iter: 1109700 training acc: 0.21875
Global Iter: 1109800 training loss: 2.04079
Global Iter: 1109800 training acc: 0.28125
Global Iter: 1109900 training loss: 1.96243
Global Iter: 1109900 training acc: 0.1875
Global Iter: 1110000 training loss: 1.95413
Global Iter: 1110000 training acc: 0.15625
Global Iter: 1110100 training loss: 2.08783
Global Iter: 1110100 training acc: 0.15625
Global Iter: 1110200 training loss: 1.97997
Global Iter: 1110200 training acc: 0.1875
Global Iter: 1110300 training loss: 2.00584
Global Iter: 1110300 training acc: 0.1875
Global Iter: 1110400 training loss: 1.95088
Global Iter: 1110400 training acc: 0.21875
Global Iter: 1110500 training loss: 2.00305
Global Iter: 1110500 training acc: 0.21875
Global Iter: 1110600 training loss: 2.07962
Global Iter: 1110600 training acc: 0.0625
Global Iter: 1110700 training loss: 1.95518
Global Iter: 1110700 training acc: 0.1875
Global Iter: 1110800 training loss: 1.98975
Global Iter: 1110800 training acc: 0.21875
Global Iter: 1110900 training loss: 1.96464
Global Iter: 1110900 training acc: 0.1875
Global Iter: 1111000 training loss: 2.05398
Global Iter: 1111000 training acc: 0.125
Global Iter: 1111100 training loss: 2.00535
Global Iter: 1111100 training acc: 0.21875
Global Iter: 1111200 training loss: 2.00155
Global Iter: 1111200 training acc: 0.1875
Global Iter: 1111300 training loss: 2.02108
Global Iter: 1111300 training acc: 0.21875
Global Iter: 1111400 training loss: 1.95312
Global Iter: 1111400 training acc: 0.34375
Global Iter: 1111500 training loss: 2.20486
Global Iter: 1111500 training acc: 0.09375
Global Iter: 1111600 training loss: 1.91109
Global Iter: 1111600 training acc: 0.21875
Global Iter: 1111700 training loss: 1.99379
Global Iter: 1111700 training acc: 0.09375
Global Iter: 1111800 training loss: 1.98579
Global Iter: 1111800 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1111816
Number of Patches: 129559
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1111816
Global Iter: 1111900 training loss: 1.88703
Global Iter: 1111900 training acc: 0.4375
Global Iter: 1112000 training loss: 1.95111
Global Iter: 1112000 training acc: 0.125
Global Iter: 1112100 training loss: 1.98112
Global Iter: 1112100 training acc: 0.15625
Global Iter: 1112200 training loss: 1.97227
Global Iter: 1112200 training acc: 0.15625
Global Iter: 1112300 training loss: 1.92071
Global Iter: 1112300 training acc: 0.21875
Global Iter: 1112400 training loss: 1.96969
Global Iter: 1112400 training acc: 0.28125
Global Iter: 1112500 training loss: 2.02121
Global Iter: 1112500 training acc: 0.15625
Global Iter: 1112600 training loss: 1.96263
Global Iter: 1112600 training acc: 0.25
Global Iter: 1112700 training loss: 2.03544
Global Iter: 1112700 training acc: 0.28125
Global Iter: 1112800 training loss: 1.99449
Global Iter: 1112800 training acc: 0.09375
Global Iter: 1112900 training loss: 1.96326
Global Iter: 1112900 training acc: 0.25
Global Iter: 1113000 training loss: 2.13674
Global Iter: 1113000 training acc: 0.125
Global Iter: 1113100 training loss: 2.08081
Global Iter: 1113100 training acc: 0.25
Global Iter: 1113200 training loss: 1.99868
Global Iter: 1113200 training acc: 0.21875
Global Iter: 1113300 training loss: 1.97266
Global Iter: 1113300 training acc: 0.15625
Global Iter: 1113400 training loss: 1.99193
Global Iter: 1113400 training acc: 0.15625
Global Iter: 1113500 training loss: 2.03728
Global Iter: 1113500 training acc: 0.125
Global Iter: 1113600 training loss: 2.05454
Global Iter: 1113600 training acc: 0.0625
Global Iter: 1113700 training loss: 2.0054
Global Iter: 1113700 training acc: 0.21875
Global Iter: 1113800 training loss: 1.96025
Global Iter: 1113800 training acc: 0.3125
Global Iter: 1113900 training loss: 1.96986
Global Iter: 1113900 training acc: 0.28125
Global Iter: 1114000 training loss: 2.04819
Global Iter: 1114000 training acc: 0.21875
Global Iter: 1114100 training loss: 1.85899
Global Iter: 1114100 training acc: 0.3125
Global Iter: 1114200 training loss: 2.11027
Global Iter: 1114200 training acc: 0.125
Global Iter: 1114300 training loss: 2.14078
Global Iter: 1114300 training acc: 0.0625
Global Iter: 1114400 training loss: 2.02714
Global Iter: 1114400 training acc: 0.09375
Global Iter: 1114500 training loss: 1.98698
Global Iter: 1114500 training acc: 0.125
Global Iter: 1114600 training loss: 1.89955
Global Iter: 1114600 training acc: 0.3125
Global Iter: 1114700 training loss: 2.01996
Global Iter: 1114700 training acc: 0.15625
Global Iter: 1114800 training loss: 2.07722
Global Iter: 1114800 training acc: 0.125
Global Iter: 1114900 training loss: 2.13955
Global Iter: 1114900 training acc: 0.09375
Global Iter: 1115000 training loss: 1.98873
Global Iter: 1115000 training acc: 0.25
Global Iter: 1115100 training loss: 2.06615
Global Iter: 1115100 training acc: 0.125
Global Iter: 1115200 training loss: 1.90288
Global Iter: 1115200 training acc: 0.25
Global Iter: 1115300 training loss: 1.95845
Global Iter: 1115300 training acc: 0.21875
Global Iter: 1115400 training loss: 2.01498
Global Iter: 1115400 training acc: 0.1875
Global Iter: 1115500 training loss: 1.8931
Global Iter: 1115500 training acc: 0.21875
Global Iter: 1115600 training loss: 2.0442
Global Iter: 1115600 training acc: 0.21875
Global Iter: 1115700 training loss: 1.92542
Global Iter: 1115700 training acc: 0.1875
Global Iter: 1115800 training loss: 1.94409
Global Iter: 1115800 training acc: 0.25
Global Iter: 1115900 training loss: 1.95063
Global Iter: 1115900 training acc: 0.1875
Global Iter: 1116000 training loss: 1.95937
Global Iter: 1116000 training acc: 0.1875
Global Iter: 1116100 training loss: 2.03194
Global Iter: 1116100 training acc: 0.125
Global Iter: 1116200 training loss: 1.94298
Global Iter: 1116200 training acc: 0.21875
Global Iter: 1116300 training loss: 1.91907
Global Iter: 1116300 training acc: 0.15625
Global Iter: 1116400 training loss: 2.02104
Global Iter: 1116400 training acc: 0.125
Global Iter: 1116500 training loss: 2.10338
Global Iter: 1116500 training acc: 0.03125
Global Iter: 1116600 training loss: 1.94022
Global Iter: 1116600 training acc: 0.125
Global Iter: 1116700 training loss: 1.94892
Global Iter: 1116700 training acc: 0.1875
Global Iter: 1116800 training loss: 2.00352
Global Iter: 1116800 training acc: 0.09375
Global Iter: 1116900 training loss: 2.04204
Global Iter: 1116900 training acc: 0.125
Global Iter: 1117000 training loss: 2.00972
Global Iter: 1117000 training acc: 0.1875
Global Iter: 1117100 training loss: 1.95773
Global Iter: 1117100 training acc: 0.21875
Global Iter: 1117200 training loss: 1.97347
Global Iter: 1117200 training acc: 0.21875
Global Iter: 1117300 training loss: 1.98285
Global Iter: 1117300 training acc: 0.21875
Global Iter: 1117400 training loss: 1.96675
Global Iter: 1117400 training acc: 0.1875
Global Iter: 1117500 training loss: 1.94497
Global Iter: 1117500 training acc: 0.125
Global Iter: 1117600 training loss: 1.9546
Global Iter: 1117600 training acc: 0.25
Global Iter: 1117700 training loss: 2.07723
Global Iter2017-06-22 13:03:18.170976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1119914
: 1117700 training acc: 0.125
Global Iter: 1117800 training loss: 2.04855
Global Iter: 1117800 training acc: 0.21875
Global Iter: 1117900 training loss: 2.02877
Global Iter: 1117900 training acc: 0.125
Global Iter: 1118000 training loss: 1.97699
Global Iter: 1118000 training acc: 0.21875
Global Iter: 1118100 training loss: 2.03987
Global Iter: 1118100 training acc: 0.125
Global Iter: 1118200 training loss: 2.06664
Global Iter: 1118200 training acc: 0.1875
Global Iter: 1118300 training loss: 1.95891
Global Iter: 1118300 training acc: 0.125
Global Iter: 1118400 training loss: 1.96771
Global Iter: 1118400 training acc: 0.09375
Global Iter: 1118500 training loss: 2.02493
Global Iter: 1118500 training acc: 0.15625
Global Iter: 1118600 training loss: 1.96088
Global Iter: 1118600 training acc: 0.15625
Global Iter: 1118700 training loss: 2.062
Global Iter: 1118700 training acc: 0.28125
Global Iter: 1118800 training loss: 1.96337
Global Iter: 1118800 training acc: 0.15625
Global Iter: 1118900 training loss: 2.0584
Global Iter: 1118900 training acc: 0.25
Global Iter: 1119000 training loss: 2.06355
Global Iter: 1119000 training acc: 0.09375
Global Iter: 1119100 training loss: 1.98755
Global Iter: 1119100 training acc: 0.125
Global Iter: 1119200 training loss: 1.97682
Global Iter: 1119200 training acc: 0.15625
Global Iter: 1119300 training loss: 1.97873
Global Iter: 1119300 training acc: 0.15625
Global Iter: 1119400 training loss: 2.05343
Global Iter: 1119400 training acc: 0.15625
Global Iter: 1119500 training loss: 1.89924
Global Iter: 1119500 training acc: 0.0625
Global Iter: 1119600 training loss: 1.92726
Global Iter: 1119600 training acc: 0.21875
Global Iter: 1119700 training loss: 2.02801
Global Iter: 1119700 training acc: 0.1875
Global Iter: 1119800 training loss: 2.00265
Global Iter: 1119800 training acc: 0.125
Global Iter: 1119900 training loss: 1.9616
Global Iter: 1119900 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1119914
Number of Patches: 128264
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1119914
Global Iter: 1120000 training loss: 1.97462
Global Iter: 1120000 training acc: 0.28125
Global Iter: 1120100 training loss: 1.97941
Global Iter: 1120100 training acc: 0.1875
Global Iter: 1120200 training loss: 2.09165
Global Iter: 1120200 training acc: 0.0625
Global Iter: 1120300 training loss: 1.99477
Global Iter: 1120300 training acc: 0.28125
Global Iter: 1120400 training loss: 1.93888
Global Iter: 1120400 training acc: 0.28125
Global Iter: 1120500 training loss: 1.96445
Global Iter: 1120500 training acc: 0.25
Global Iter: 1120600 training loss: 1.96314
Global Iter: 1120600 training acc: 0.15625
Global Iter: 1120700 training loss: 1.99402
Global Iter: 1120700 training acc: 0.1875
Global Iter: 1120800 training loss: 1.96813
Global Iter: 1120800 training acc: 0.09375
Global Iter: 1120900 training loss: 1.94938
Global Iter: 1120900 training acc: 0.09375
Global Iter: 1121000 training loss: 2.00402
Global Iter: 1121000 training acc: 0.15625
Global Iter: 1121100 training loss: 1.90027
Global Iter: 1121100 training acc: 0.40625
Global Iter: 1121200 training loss: 1.97683
Global Iter: 1121200 training acc: 0.25
Global Iter: 1121300 training loss: 1.96801
Global Iter: 1121300 training acc: 0.125
Global Iter: 1121400 training loss: 2.01673
Global Iter: 1121400 training acc: 0.09375
Global Iter: 1121500 training loss: 2.00008
Global Iter: 1121500 training acc: 0.1875
Global Iter: 1121600 training loss: 1.89982
Global Iter: 1121600 training acc: 0.15625
Global Iter: 1121700 training loss: 2.01006
Global Iter: 1121700 training acc: 0.125
Global Iter: 1121800 training loss: 2.04726
Global Iter: 1121800 training acc: 0.09375
Global Iter: 1121900 training loss: 2.02113
Global Iter: 1121900 training acc: 0.1875
Global Iter: 1122000 training loss: 1.86553
Global Iter: 1122000 training acc: 0.21875
Global Iter: 1122100 training loss: 1.96387
Global Iter: 1122100 training acc: 0.28125
Global Iter: 1122200 training loss: 2.1666
Global Iter: 1122200 training acc: 0.09375
Global Iter: 1122300 training loss: 2.0294
Global Iter: 1122300 training acc: 0.09375
Global Iter: 1122400 training loss: 2.04102
Global Iter: 1122400 training acc: 0.1875
Global Iter: 1122500 training loss: 1.93845
Global Iter: 1122500 training acc: 0.28125
Global Iter: 1122600 training loss: 1.91649
Global Iter: 1122600 training acc: 0.125
Global Iter: 1122700 training loss: 1.98089
Global Iter: 1122700 training acc: 0.09375
Global Iter: 1122800 training loss: 2.145
Global Iter: 1122800 training acc: 0.21875
Global Iter: 1122900 training loss: 2.02439
Global Iter: 1122900 training acc: 0.09375
Global Iter: 1123000 training loss: 2.03877
Global Iter: 1123000 training acc: 0.125
Global Iter: 1123100 training loss: 2.00051
Global Iter: 1123100 training acc: 0.25
Global Iter: 1123200 training loss: 2.0314
Global Iter: 1123200 training acc: 0.03125
Global Iter: 1123300 training loss: 2.01834
Global Iter: 1123300 training acc: 0.0625
Global Iter: 1123400 training loss: 2.08069
Global Iter: 1123400 training acc: 0.1875
Global Iter: 1123500 training loss: 1.82757
Global Iter: 1123500 training acc: 0.4375
Global Iter: 1123600 training loss: 2.09557
Global Iter: 1123600 training acc: 0.15625
Global Iter: 1123700 training loss: 2.06167
Global Iter: 1123700 training acc: 0.0625
Global Iter: 1123800 training loss: 2.10224
Global Iter: 1123800 training acc: 0.1875
Global Iter: 1123900 training loss: 1.90412
Global Iter: 1123900 training acc: 0.28125
Global Iter: 1124000 training loss: 1.90307
Global Iter: 1124000 training acc: 0.15625
Global Iter: 1124100 training loss: 1.95151
Global Iter: 1124100 training acc: 0.1875
Global Iter: 1124200 training loss: 1.96846
Global Iter: 1124200 training acc: 0.25
Global Iter: 1124300 training loss: 2.00076
Global Iter: 1124300 training acc: 0.25
Global Iter: 1124400 training loss: 2.05393
Global Iter: 1124400 training acc: 0.0625
Global Iter: 1124500 training loss: 2.03255
Global Iter: 1124500 training acc: 0.0625
Global Iter: 1124600 training loss: 1.98346
Global Iter: 1124600 training acc: 0.1875
Global Iter: 1124700 training loss: 1.90113
Global Iter: 1124700 training acc: 0.3125
Global Iter: 1124800 training loss: 1.98893
Global Iter: 1124800 training acc: 0.1875
Global Iter: 1124900 training loss: 1.92134
Global Iter: 1124900 training acc: 0.375
Global Iter: 1125000 training loss: 1.96625
Global Iter: 1125000 training acc: 0.15625
Global Iter: 1125100 training loss: 1.98381
Global Iter: 1125100 training acc: 0.25
Global Iter: 1125200 training loss: 1.97444
Global Iter: 1125200 training acc: 0.21875
Global Iter: 1125300 training loss: 1.92081
Global Iter: 1125300 training acc: 0.1875
Global Iter: 1125400 training loss: 1.96358
Global Iter: 1125400 training acc: 0.21875
Global Iter: 1125500 training loss: 1.98631
Global Iter: 1125500 training acc: 0.15625
Global Iter: 1125600 training loss: 1.94319
Global Iter: 1125600 training acc: 0.28125
Global Iter: 1125700 training loss: 1.97629
Global Iter: 1125700 training acc: 0.09375
Global Iter: 1125800 training loss: 1.95777
Global Iter: 1125800 training acc: 0.21875
Global Iter: 1125900 training loss: 1.98916
Global Iter: 1125900 training acc: 0.25
Global Iter: 1126000 training loss: 1.96009
Global Iter: 1126000 training acc: 0.21875
Global Iter: 1126100 training loss: 1.95289
Global Iter: 1126100 training acc: 0.21875
Global Iter: 1126200 training loss: 1.95961
Global Iter: 1126200 training acc: 0.15625
Global Iter: 1126300 training loss: 1.90402
Global Iter: 1126300 training acc: 0.1875
Global Iter: 1126400 training loss: 1.96079
Global Iter: 1126400 training acc: 0.125
Global Iter: 1126500 training loss: 2.0751
Global Iter: 1126500 training acc: 0.1875
Global Iter: 1126600 training loss: 1.93543
Global Iter: 1126600 training acc: 0.0625
Global Iter: 1126700 training loss: 2.01944
Global Iter: 1126700 training acc: 0.09375
Global Iter: 1126800 training loss: 2.02957
Global Iter: 1126800 training acc: 0.21875
Global Iter: 1126900 training loss: 1.9427
Global Iter: 1126900 training acc: 0.1875
Global Iter: 112017-06-22 13:16:57.788915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1127931
27000 training loss: 2.0018
Global Iter: 1127000 training acc: 0.15625
Global Iter: 1127100 training loss: 2.1078
Global Iter: 1127100 training acc: 0.0625
Global Iter: 1127200 training loss: 1.88286
Global Iter: 1127200 training acc: 0.34375
Global Iter: 1127300 training loss: 2.00248
Global Iter: 1127300 training acc: 0.15625
Global Iter: 1127400 training loss: 1.99878
Global Iter: 1127400 training acc: 0.21875
Global Iter: 1127500 training loss: 1.93057
Global Iter: 1127500 training acc: 0.28125
Global Iter: 1127600 training loss: 1.93895
Global Iter: 1127600 training acc: 0.21875
Global Iter: 1127700 training loss: 2.01058
Global Iter: 1127700 training acc: 0.21875
Global Iter: 1127800 training loss: 1.93219
Global Iter: 1127800 training acc: 0.28125
Global Iter: 1127900 training loss: 1.93392
Global Iter: 1127900 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1127931
Number of Patches: 126982
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1127931
Global Iter: 1128000 training loss: 1.923
Global Iter: 1128000 training acc: 0.25
Global Iter: 1128100 training loss: 2.04234
Global Iter: 1128100 training acc: 0.0625
Global Iter: 1128200 training loss: 1.99083
Global Iter: 1128200 training acc: 0.3125
Global Iter: 1128300 training loss: 2.02973
Global Iter: 1128300 training acc: 0.125
Global Iter: 1128400 training loss: 2.05187
Global Iter: 1128400 training acc: 0.0625
Global Iter: 1128500 training loss: 1.98418
Global Iter: 1128500 training acc: 0.25
Global Iter: 1128600 training loss: 1.99869
Global Iter: 1128600 training acc: 0.1875
Global Iter: 1128700 training loss: 1.95857
Global Iter: 1128700 training acc: 0.09375
Global Iter: 1128800 training loss: 2.06629
Global Iter: 1128800 training acc: 0.1875
Global Iter: 1128900 training loss: 1.92778
Global Iter: 1128900 training acc: 0.25
Global Iter: 1129000 training loss: 2.07023
Global Iter: 1129000 training acc: 0.09375
Global Iter: 1129100 training loss: 1.97403
Global Iter: 1129100 training acc: 0.25
Global Iter: 1129200 training loss: 1.97139
Global Iter: 1129200 training acc: 0.21875
Global Iter: 1129300 training loss: 2.08008
Global Iter: 1129300 training acc: 0.0625
Global Iter: 1129400 training loss: 1.90982
Global Iter: 1129400 training acc: 0.21875
Global Iter: 1129500 training loss: 2.05355
Global Iter: 1129500 training acc: 0.15625
Global Iter: 1129600 training loss: 1.84893
Global Iter: 1129600 training acc: 0.40625
Global Iter: 1129700 training loss: 2.04825
Global Iter: 1129700 training acc: 0.21875
Global Iter: 1129800 training loss: 2.04582
Global Iter: 1129800 training acc: 0.28125
Global Iter: 1129900 training loss: 1.98159
Global Iter: 1129900 training acc: 0.1875
Global Iter: 1130000 training loss: 2.00131
Global Iter: 1130000 training acc: 0.21875
Global Iter: 1130100 training loss: 1.88609
Global Iter: 1130100 training acc: 0.25
Global Iter: 1130200 training loss: 2.01983
Global Iter: 1130200 training acc: 0.21875
Global Iter: 1130300 training loss: 2.03791
Global Iter: 1130300 training acc: 0.21875
Global Iter: 1130400 training loss: 2.01829
Global Iter: 1130400 training acc: 0.09375
Global Iter: 1130500 training loss: 2.01151
Global Iter: 1130500 training acc: 0.1875
Global Iter: 1130600 training loss: 1.99961
Global Iter: 1130600 training acc: 0.1875
Global Iter: 1130700 training loss: 1.90736
Global Iter: 1130700 training acc: 0.25
Global Iter: 1130800 training loss: 2.06928
Global Iter: 1130800 training acc: 0.21875
Global Iter: 1130900 training loss: 1.97546
Global Iter: 1130900 training acc: 0.125
Global Iter: 1131000 training loss: 2.04021
Global Iter: 1131000 training acc: 0.1875
Global Iter: 1131100 training loss: 1.98776
Global Iter: 1131100 training acc: 0.25
Global Iter: 1131200 training loss: 1.93769
Global Iter: 1131200 training acc: 0.1875
Global Iter: 1131300 training loss: 2.0205
Global Iter: 1131300 training acc: 0.125
Global Iter: 1131400 training loss: 2.14819
Global Iter: 1131400 training acc: 0.21875
Global Iter: 113152017-06-22 13:30:34.256437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1135868
00 training loss: 2.17376
Global Iter: 1131500 training acc: 0.0625
Global Iter: 1131600 training loss: 1.95879
Global Iter: 1131600 training acc: 0.09375
Global Iter: 1131700 training loss: 1.9689
Global Iter: 1131700 training acc: 0.25
Global Iter: 1131800 training loss: 2.00257
Global Iter: 1131800 training acc: 0.1875
Global Iter: 1131900 training loss: 2.10013
Global Iter: 1131900 training acc: 0.09375
Global Iter: 1132000 training loss: 1.92743
Global Iter: 1132000 training acc: 0.15625
Global Iter: 1132100 training loss: 1.9957
Global Iter: 1132100 training acc: 0.1875
Global Iter: 1132200 training loss: 1.89872
Global Iter: 1132200 training acc: 0.375
Global Iter: 1132300 training loss: 2.0272
Global Iter: 1132300 training acc: 0.15625
Global Iter: 1132400 training loss: 1.91943
Global Iter: 1132400 training acc: 0.15625
Global Iter: 1132500 training loss: 1.96424
Global Iter: 1132500 training acc: 0.125
Global Iter: 1132600 training loss: 1.96147
Global Iter: 1132600 training acc: 0.28125
Global Iter: 1132700 training loss: 2.03802
Global Iter: 1132700 training acc: 0.21875
Global Iter: 1132800 training loss: 1.96155
Global Iter: 1132800 training acc: 0.15625
Global Iter: 1132900 training loss: 1.98932
Global Iter: 1132900 training acc: 0.15625
Global Iter: 1133000 training loss: 2.08594
Global Iter: 1133000 training acc: 0.15625
Global Iter: 1133100 training loss: 1.96911
Global Iter: 1133100 training acc: 0.1875
Global Iter: 1133200 training loss: 2.03135
Global Iter: 1133200 training acc: 0.0625
Global Iter: 1133300 training loss: 2.1388
Global Iter: 1133300 training acc: 0.1875
Global Iter: 1133400 training loss: 1.99792
Global Iter: 1133400 training acc: 0.28125
Global Iter: 1133500 training loss: 2.02279
Global Iter: 1133500 training acc: 0.125
Global Iter: 1133600 training loss: 2.08964
Global Iter: 1133600 training acc: 0.1875
Global Iter: 1133700 training loss: 1.94094
Global Iter: 1133700 training acc: 0.21875
Global Iter: 1133800 training loss: 1.95307
Global Iter: 1133800 training acc: 0.21875
Global Iter: 1133900 training loss: 1.99441
Global Iter: 1133900 training acc: 0.15625
Global Iter: 1134000 training loss: 2.02084
Global Iter: 1134000 training acc: 0.15625
Global Iter: 1134100 training loss: 1.96238
Global Iter: 1134100 training acc: 0.15625
Global Iter: 1134200 training loss: 2.01362
Global Iter: 1134200 training acc: 0.1875
Global Iter: 1134300 training loss: 1.88745
Global Iter: 1134300 training acc: 0.25
Global Iter: 1134400 training loss: 2.03816
Global Iter: 1134400 training acc: 0.25
Global Iter: 1134500 training loss: 1.96636
Global Iter: 1134500 training acc: 0.125
Global Iter: 1134600 training loss: 1.91173
Global Iter: 1134600 training acc: 0.34375
Global Iter: 1134700 training loss: 1.97668
Global Iter: 1134700 training acc: 0.3125
Global Iter: 1134800 training loss: 1.9819
Global Iter: 1134800 training acc: 0.25
Global Iter: 1134900 training loss: 2.0824
Global Iter: 1134900 training acc: 0.0625
Global Iter: 1135000 training loss: 1.92959
Global Iter: 1135000 training acc: 0.28125
Global Iter: 1135100 training loss: 2.05078
Global Iter: 1135100 training acc: 0.28125
Global Iter: 1135200 training loss: 1.98687
Global Iter: 1135200 training acc: 0.09375
Global Iter: 1135300 training loss: 1.95173
Global Iter: 1135300 training acc: 0.125
Global Iter: 1135400 training loss: 1.92191
Global Iter: 1135400 training acc: 0.28125
Global Iter: 1135500 training loss: 2.00013
Global Iter: 1135500 training acc: 0.15625
Global Iter: 1135600 training loss: 1.972
Global Iter: 1135600 training acc: 0.125
Global Iter: 1135700 training loss: 2.06045
Global Iter: 1135700 training acc: 0.1875
Global Iter: 1135800 training loss: 1.94397
Global Iter: 1135800 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1135868
Number of Patches: 125713
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1135868
Global Iter: 1135900 training loss: 2.09464
Global Iter: 1135900 training acc: 0.09375
Global Iter: 1136000 training loss: 1.98345
Global Iter: 1136000 training acc: 0.125
Global Iter: 1136100 training loss: 1.98759
Global Iter: 1136100 training acc: 0.125
Global Iter: 1136200 training loss: 1.97516
Global Iter: 1136200 training acc: 0.1875
Global Iter: 1136300 training loss: 1.96398
Global Iter: 1136300 training acc: 0.21875
Global Iter: 1136400 training loss: 1.95056
Global Iter: 1136400 training acc: 0.09375
Global Iter: 1136500 training loss: 1.9015
Global Iter: 1136500 training acc: 0.28125
Global Iter: 1136600 training loss: 2.05577
Global Iter: 1136600 training acc: 0.03125
Global Iter: 1136700 training loss: 2.01117
Global Iter: 1136700 training acc: 0.09375
Global Iter: 1136800 training loss: 1.92837
Global Iter: 1136800 training acc: 0.15625
Global Iter: 1136900 training loss: 1.92436
Global Iter: 1136900 training acc: 0.15625
Global Iter: 1137000 training loss: 1.94064
Global Iter: 1137000 training acc: 0.25
Global Iter: 1137100 training loss: 1.99427
Global Iter: 1137100 training acc: 0.25
Global Iter: 1137200 training loss: 1.92654
Global Iter: 1137200 training acc: 0.1875
Global Iter: 1137300 training loss: 1.97721
Global Iter: 1137300 training acc: 0.15625
Global Iter: 1137400 training loss: 2.03163
Global Iter: 1137400 training acc: 0.21875
Global Iter: 1137500 training loss: 1.95018
Global Iter: 1137500 training acc: 0.09375
Global Iter: 1137600 training loss: 1.93098
Global Iter: 1137600 training acc: 0.21875
Global Iter: 1137700 training loss: 1.98466
Global Iter: 1137700 training acc: 0.09375
Global Iter: 1137800 training loss: 2.05498
Global Iter: 1137800 training acc: 0.15625
Global Iter: 1137900 training loss: 1.99617
Global Iter: 1137900 training acc: 0.1875
Global Iter: 1138000 training loss: 1.983
Global Iter: 1138000 training acc: 0.15625
Global Iter: 1138100 training loss: 1.97756
Global Iter: 1138100 training acc: 0.125
Global Iter: 1138200 training loss: 1.91686
Global Iter: 1138200 training acc: 0.25
Global Iter: 1138300 training loss: 2.0021
Global Iter: 1138300 training acc: 0.125
Global Iter: 1138400 training loss: 2.04629
Global Iter: 1138400 training acc: 0.28125
Global Iter: 1138500 training loss: 1.94173
Global Iter: 1138500 training acc: 0.21875
Global Iter: 1138600 training loss: 1.90683
Global Iter: 1138600 training acc: 0.25
Global Iter: 1138700 training loss: 1.96187
Global Iter: 1138700 training acc: 0.25
Global Iter: 1138800 training loss: 2.02172
Global Iter: 1138800 training acc: 0.0625
Global Iter: 1138900 training loss: 1.98365
Global Iter: 1138900 training acc: 0.15625
Global Iter: 1139000 training loss: 1.96088
Global Iter: 1139000 training acc: 0.1875
Global Iter: 1139100 training loss: 1.91457
Global Iter: 1139100 training acc: 0.1875
Global Iter: 1139200 training loss: 1.96559
Global Iter: 1139200 training acc: 0.25
Global Iter: 1139300 training loss: 1.87617
Global Iter: 1139300 training acc: 0.25
Global Iter: 1139400 training loss: 1.95487
Global Iter: 1139400 training acc: 0.09375
Global Iter: 1139500 training loss: 2.01136
Global Iter: 1139500 training acc: 0.15625
Global Iter: 1139600 training loss: 1.95454
Global Iter: 1139600 training acc: 0.15625
Global Iter: 1139700 training loss: 2.07748
Global Iter: 1139700 training acc: 0.125
Global Iter: 1139800 training loss: 1.86332
Global Iter: 1139800 training acc: 0.3125
Global Iter: 1139900 training loss: 1.91664
Global Iter: 1139900 training acc: 0.25
Global Iter: 1140000 training loss: 2.02499
Global Iter: 1140000 training acc: 0.28125
Global Iter: 1140100 training loss: 2.00538
Global Iter: 1140100 training acc: 0.25
Global Iter: 1140200 training loss: 2.00018
Global Iter: 1140200 training acc: 0.28125
Global Iter: 1140300 training loss: 2.02027
Global Iter: 1140300 training acc: 0.1875
Global Iter: 1140400 training loss: 2.05937
Global Iter: 1140400 training acc: 0.25
Global Iter: 1140500 training loss: 1.99972
Global Iter: 1140500 training acc: 0.125
Global Iter: 1140600 training loss: 1.96045
Global Iter: 1140600 training acc: 0.1875
Global Iter: 1140700 training loss: 2.08041
Global Iter: 1140700 training acc: 02017-06-22 13:44:03.588159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1143726
.25
Global Iter: 1140800 training loss: 2.14393
Global Iter: 1140800 training acc: 0.1875
Global Iter: 1140900 training loss: 1.97333
Global Iter: 1140900 training acc: 0.125
Global Iter: 1141000 training loss: 1.97668
Global Iter: 1141000 training acc: 0.21875
Global Iter: 1141100 training loss: 2.12001
Global Iter: 1141100 training acc: 0.125
Global Iter: 1141200 training loss: 1.97781
Global Iter: 1141200 training acc: 0.25
Global Iter: 1141300 training loss: 1.92318
Global Iter: 1141300 training acc: 0.15625
Global Iter: 1141400 training loss: 2.17458
Global Iter: 1141400 training acc: 0.09375
Global Iter: 1141500 training loss: 1.95669
Global Iter: 1141500 training acc: 0.21875
Global Iter: 1141600 training loss: 1.99387
Global Iter: 1141600 training acc: 0.21875
Global Iter: 1141700 training loss: 1.9684
Global Iter: 1141700 training acc: 0.1875
Global Iter: 1141800 training loss: 1.95077
Global Iter: 1141800 training acc: 0.21875
Global Iter: 1141900 training loss: 2.00946
Global Iter: 1141900 training acc: 0.15625
Global Iter: 1142000 training loss: 1.98221
Global Iter: 1142000 training acc: 0.25
Global Iter: 1142100 training loss: 2.05909
Global Iter: 1142100 training acc: 0.09375
Global Iter: 1142200 training loss: 1.98501
Global Iter: 1142200 training acc: 0.15625
Global Iter: 1142300 training loss: 2.03353
Global Iter: 1142300 training acc: 0.28125
Global Iter: 1142400 training loss: 2.07021
Global Iter: 1142400 training acc: 0.21875
Global Iter: 1142500 training loss: 2.04367
Global Iter: 1142500 training acc: 0.21875
Global Iter: 1142600 training loss: 2.02295
Global Iter: 1142600 training acc: 0.21875
Global Iter: 1142700 training loss: 2.14981
Global Iter: 1142700 training acc: 0.1875
Global Iter: 1142800 training loss: 1.93384
Global Iter: 1142800 training acc: 0.21875
Global Iter: 1142900 training loss: 1.99115
Global Iter: 1142900 training acc: 0.21875
Global Iter: 1143000 training loss: 1.99276
Global Iter: 1143000 training acc: 0.15625
Global Iter: 1143100 training loss: 1.97165
Global Iter: 1143100 training acc: 0.09375
Global Iter: 1143200 training loss: 1.9689
Global Iter: 1143200 training acc: 0.28125
Global Iter: 1143300 training loss: 1.97442
Global Iter: 1143300 training acc: 0.21875
Global Iter: 1143400 training loss: 1.95959
Global Iter: 1143400 training acc: 0.1875
Global Iter: 1143500 training loss: 2.01437
Global Iter: 1143500 training acc: 0.125
Global Iter: 1143600 training loss: 2.10987
Global Iter: 1143600 training acc: 0.15625
Global Iter: 1143700 training loss: 1.93013
Global Iter: 1143700 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1143726
Number of Patches: 124456
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1143726
Global Iter: 1143800 training loss: 1.94268
Global Iter: 1143800 training acc: 0.15625
Global Iter: 1143900 training loss: 1.97344
Global Iter: 1143900 training acc: 0.21875
Global Iter: 1144000 training loss: 2.0542
Global Iter: 1144000 training acc: 0.15625
Global Iter: 1144100 training loss: 1.94654
Global Iter: 1144100 training acc: 0.15625
Global Iter: 1144200 training loss: 1.91248
Global Iter: 1144200 training acc: 0.21875
Global Iter: 1144300 training loss: 1.94262
Global Iter: 1144300 training acc: 0.375
Global Iter: 1144400 training loss: 1.96669
Global Iter: 1144400 training acc: 0.125
Global Iter: 1144500 training loss: 1.98649
Global Iter: 1144500 training acc: 0.15625
Global Iter: 1144600 training loss: 1.91141
Global Iter: 1144600 training acc: 0.3125
Global Iter: 1144700 training loss: 2.08895
Global Iter: 1144700 training acc: 0.15625
Global Iter: 1144800 training loss: 2.02015
Global Iter: 1144800 training acc: 0.15625
Global Iter: 1144900 training loss: 1.92451
Global Iter: 1144900 training acc: 0.25
Global Iter: 1145000 training loss: 1.89004
Global Iter: 1145000 training acc: 0.15625
Global Iter: 1145100 training loss: 1.97981
Global Iter: 1145100 training acc: 0.125
Global Iter: 1145200 training loss: 2.04248
Global Iter: 1145200 training acc: 0.09375
Global Iter: 1145300 training loss: 1.92846
Global Iter: 1145300 training acc: 0.21875
Global Iter: 1145400 training loss: 1.94693
Global Iter: 1145400 training acc: 0.15625
Global Iter: 1145500 training loss: 1.87727
Global Iter: 1145500 training acc: 0.25
Global Iter: 1145600 training loss: 1.96137
Global Iter: 1145600 training acc: 0.15625
Global Iter: 1145700 training loss: 1.98421
Global Iter: 1145700 training acc: 0.15625
Global Iter: 1145800 training loss: 2.01386
Global Iter: 1145800 training acc: 0.09375
Global Iter: 1145900 training loss: 1.89503
Global Iter: 1145900 training acc: 0.21875
Global Iter: 1146000 training loss: 1.91767
Global Iter: 1146000 training acc: 0.21875
Global Iter: 1146100 training loss: 1.95174
Global Iter: 1146100 training acc: 0.1875
Global Iter: 1146200 training loss: 2.11229
Global Iter: 1146200 training acc: 0.21875
Global Iter: 1146300 training loss: 1.98233
Global Iter: 1146300 training acc: 0.21875
Global Iter: 1146400 training loss: 2.0076
Global Iter: 1146400 training acc: 0.3125
Global Iter: 1146500 training loss: 2.05951
Global Iter: 1146500 training acc: 0.28125
Global Iter: 1146600 training loss: 1.98536
Global Iter: 1146600 training acc: 0.15625
Global Iter: 1146700 training loss: 2.12706
Global Iter: 1146700 training acc: 0.1875
Global Iter: 1146800 training loss: 1.95567
Global Iter: 1146800 training acc: 0.21875
Global Iter: 1146900 training loss: 1.983
Global Iter: 1146900 training acc: 0.15625
Global Iter: 1147000 training loss: 1.87255
Global Iter: 1147000 training acc: 0.21875
Global Iter: 1147100 training loss: 2.00751
Global Iter: 1147100 training acc: 0.125
Global Iter: 1147200 training loss: 2.05516
Global Iter: 1147200 training acc: 0.25
Global Iter: 1147300 training loss: 2.01669
Global Iter: 1147300 training acc: 0.125
Global Iter: 1147400 training loss: 2.02262
Global Iter: 1147400 training acc: 0.15625
Global Iter: 1147500 training loss: 1.95417
Global Iter: 1147500 training acc: 0.25
Global Iter: 1147600 training loss: 2.10568
Global Iter: 1147600 training acc: 0.0625
Global Iter: 1147700 training loss: 2.04414
Global Iter: 1147700 training acc: 0.125
Global Iter: 1147800 training loss: 2.028
Global Iter: 1147800 training acc: 0.21875
Global Iter: 1147900 training loss: 1.94316
Global Iter: 1147900 training acc: 0.3125
Global Iter: 1148000 training loss: 2.06333
Global Iter: 1148000 training acc: 0.0625
Global Iter: 1148100 training loss: 2.00711
Global Iter: 1148100 training acc: 0.21875
Global Iter: 1148200 training loss: 2.03869
Global Iter: 1148200 training acc: 0.15625
Global Iter: 1148300 training loss: 2.09776
Global Iter: 1148300 training acc: 0.125
Global Iter: 1148400 training loss: 1.98832
Global Iter: 1148400 training acc: 0.15625
Global Iter: 1148500 training loss: 2.00694
Global Iter: 1148500 training acc: 0.21875
Global Iter: 1148600 training loss: 1.92869
Global Iter: 1148600 training acc: 0.1875
Global Iter: 1148700 training loss: 2.02653
Global Iter: 1148700 training acc: 0.21875
Global Iter: 1148800 training loss: 2.06685
Global Iter: 1148800 training acc: 0.09375
Global Iter: 1148900 training loss: 1.9715
Global Iter: 1148900 training acc: 0.21875
Global Iter: 1149000 training loss: 2.06375
Global Iter: 1149000 training acc: 0.09375
Global Iter: 1149100 training loss: 1.98164
Global Iter: 1149100 training acc: 0.15625
Global Iter: 1149200 training loss: 1.99698
Global Iter: 1149200 training acc: 0.21875
Global Iter: 1149300 training loss: 1.9572
Global Iter: 1149300 training acc: 0.28125
Global Iter: 1149400 training loss: 1.96442
Global Iter: 1149400 training acc: 0.15625
Global Iter: 1149500 training loss: 2.01
Global Iter: 1149500 training acc: 0.15625
Global Iter: 1149600 training loss: 1.96887
Global Iter: 1149600 training acc: 0.1875
Global Iter: 1149700 training loss: 1.92713
Global Iter: 1149700 training acc: 0.15625
Global Iter: 1149800 training loss: 1.94948
Global Iter: 1149800 training acc: 0.15625
Global Iter: 1149900 training loss: 2.05262
Global Iter: 1149900 training acc: 0.0625
Global Iter: 1150000 train2017-06-22 13:57:15.672575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1151505
ing loss: 1.91929
Global Iter: 1150000 training acc: 0.21875
Global Iter: 1150100 training loss: 2.00657
Global Iter: 1150100 training acc: 0.15625
Global Iter: 1150200 training loss: 2.05552
Global Iter: 1150200 training acc: 0.125
Global Iter: 1150300 training loss: 2.00953
Global Iter: 1150300 training acc: 0.03125
Global Iter: 1150400 training loss: 1.99171
Global Iter: 1150400 training acc: 0.09375
Global Iter: 1150500 training loss: 1.95681
Global Iter: 1150500 training acc: 0.0625
Global Iter: 1150600 training loss: 2.00702
Global Iter: 1150600 training acc: 0.21875
Global Iter: 1150700 training loss: 2.03491
Global Iter: 1150700 training acc: 0.15625
Global Iter: 1150800 training loss: 1.94632
Global Iter: 1150800 training acc: 0.3125
Global Iter: 1150900 training loss: 2.00615
Global Iter: 1150900 training acc: 0.21875
Global Iter: 1151000 training loss: 1.98888
Global Iter: 1151000 training acc: 0.1875
Global Iter: 1151100 training loss: 2.02387
Global Iter: 1151100 training acc: 0.09375
Global Iter: 1151200 training loss: 1.92192
Global Iter: 1151200 training acc: 0.21875
Global Iter: 1151300 training loss: 1.96359
Global Iter: 1151300 training acc: 0.21875
Global Iter: 1151400 training loss: 2.06825
Global Iter: 1151400 training acc: 0.21875
Global Iter: 1151500 training loss: 1.97022
Global Iter: 1151500 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1151505
Number of Patches: 123212
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1151505
Global Iter: 1151600 training loss: 2.00691
Global Iter: 1151600 training acc: 0.125
Global Iter: 1151700 training loss: 2.04927
Global Iter: 1151700 training acc: 0.25
Global Iter: 1151800 training loss: 1.98496
Global Iter: 1151800 training acc: 0.34375
Global Iter: 1151900 training loss: 1.9755
Global Iter: 1151900 training acc: 0.34375
Global Iter: 1152000 training loss: 2.00413
Global Iter: 1152000 training acc: 0.1875
Global Iter: 1152100 training loss: 1.99638
Global Iter: 1152100 training acc: 0.28125
Global Iter: 1152200 training loss: 2.0846
Global Iter: 1152200 training acc: 0.25
Global Iter: 1152300 training loss: 1.9642
Global Iter: 1152300 training acc: 0.15625
Global Iter: 1152400 training loss: 2.02046
Global Iter: 1152400 training acc: 0.15625
Global Iter: 1152500 training loss: 2.04551
Global Iter: 1152500 training acc: 0.09375
Global Iter: 1152600 training loss: 1.9498
Global Iter: 1152600 training acc: 0.09375
Global Iter: 1152700 training loss: 1.95322
Global Iter: 1152700 training acc: 0.15625
Global Iter: 1152800 training loss: 1.93097
Global Iter: 1152800 training acc: 0.21875
Global Iter: 1152900 training loss: 2.02891
Global Iter: 1152900 training acc: 0.09375
Global Iter: 1153000 training loss: 2.05941
Global Iter: 1153000 training acc: 0.09375
Global Iter: 1153100 training loss: 1.97172
Global Iter: 1153100 training acc: 0.15625
Global Iter: 1153200 training loss: 1.97068
Global Iter: 1153200 training acc: 0.09375
Global Iter: 1153300 training loss: 1.98863
Global Iter: 1153300 training acc: 0.15625
Global Iter: 1153400 training loss: 2.03655
Global Iter: 1153400 training acc: 0.15625
Global Iter: 1153500 training loss: 1.95932
Global Iter: 1153500 training acc: 0.1875
Global Iter: 1153600 training loss: 2.14762
Global Iter: 1153600 training acc: 0.09375
Global Iter: 1153700 training loss: 2.03178
Global Iter: 1153700 training acc: 0.03125
Global Iter: 1153800 training loss: 2.10035
Global Iter: 1153800 training acc: 0.15625
Global Iter: 1153900 training loss: 1.99932
Global Iter: 1153900 training acc: 0.15625
Global Iter: 1154000 training loss: 2.11539
Global Iter: 1154000 training acc: 0.15625
Global Iter: 1154100 training loss: 1.89915
Global Iter: 1154100 training acc: 0.34375
Global Iter: 1154200 training loss: 1.98166
Global Iter: 1154200 training acc: 0.1875
Global Iter: 1154300 training loss: 1.92494
Global Iter: 1154300 training acc: 0.15625
Global Iter: 1154400 training loss: 1.99855
Global Iter: 1154400 training acc: 0.25
Global Iter: 1154500 training loss: 2.05187
Global Iter: 1154500 training acc: 0.125
Global Iter: 1154600 training loss: 1.99774
Global Iter: 1154600 training acc: 0.21875
Global Iter: 1154700 training loss: 2.05295
Global Iter: 1154700 training acc: 0.1875
Global Iter: 1154800 training loss: 2.03795
Global Iter: 1154800 training acc: 0.125
Global Iter: 1154900 training loss: 2.026
Global Iter: 1154900 training acc: 0.28125
Global Iter: 1155000 training loss: 1.96979
Global Iter: 1155000 training acc: 0.1875
Global Iter: 1155100 training loss: 1.91425
Global Iter: 1155100 training acc: 0.21875
Global Iter: 1155200 training loss: 1.97669
Global Iter: 1155200 training acc: 0.1875
Global Iter: 1155300 training loss: 2.15369
Global Iter: 1155300 training acc: 0.1875
Global Iter: 1155400 training loss: 2.11031
Global Iter: 1155400 training acc: 0.09375
Global Iter: 1155500 training loss: 1.86324
Global Iter: 1155500 training acc: 0.15625
Global Iter: 1155600 training loss: 1.905
Global Iter: 1155600 training acc: 0.25
Global Iter: 1155700 training loss: 1.91177
Global Iter: 1155700 training acc: 0.25
Global Iter: 1155800 training loss: 2.01498
Global Iter: 1155800 training acc: 0.25
Global Iter: 1155900 training loss: 1.91285
Global Iter: 1155900 training acc: 0.1875
Global Iter: 1156000 training loss: 1.96227
Global Iter: 1156000 training acc: 0.1875
Global Iter: 1156100 training loss: 2.05138
Global Iter: 1156100 training acc: 0.0625
Global Iter: 1156200 training loss: 1.90975
Global Iter: 1156200 training acc: 0.21875
Global Iter: 1156300 training loss: 1.96351
Global Iter: 1156300 training acc: 0.21875
Global Iter: 1156400 training loss: 2.00722
Global Iter: 1156400 training acc: 0.25
Global Iter: 1156500 training loss: 2.02688
Global Iter: 1156500 training acc: 0.125
Global Iter: 1156600 training loss: 1.87122
Global Iter: 1156600 training acc: 0.21875
Global Iter: 1156700 training loss: 1.96458
Global Iter: 1156700 training acc: 0.125
Global Iter: 1156800 training loss: 1.97397
Global Iter: 1156800 training acc: 0.21875
Global Iter: 1156900 training loss: 2.08945
Global Iter: 1156900 training acc: 0.15625
Global Iter: 1157000 training loss: 1.92539
Global Iter: 1157000 training acc: 0.15625
Global Iter: 1157100 training loss: 2.02366
Global Iter: 1157100 training acc: 0.21875
Global Iter: 1157200 training loss: 1.96357
Global Iter: 1157200 training acc: 0.28125
Global Iter: 1157300 training loss: 2.0279
Global Iter: 1157300 training acc: 0.21875
Global Iter: 1157400 training loss: 2.0359
Global Iter: 1157400 training acc: 0.15625
Global Iter: 1157500 training loss: 1.86934
Global Iter: 1157500 training acc: 0.3125
Global Iter: 1157600 training loss: 1.92158
Global Iter: 1157600 training acc: 0.125
Global Iter: 1157700 training loss: 2.06155
Global Iter: 1157700 training acc: 0.21875
Global Iter: 1157800 training loss: 2.08803
Global Iter: 1157800 training acc: 0.03125
Global Iter: 1157900 training loss: 2.01423
Global Iter: 1157900 training acc: 0.0625
Global Iter: 1158000 training loss: 2.02871
Global Iter: 1158000 training acc: 0.25
Global Iter: 1158100 training loss: 1.9524
Global Iter: 1158100 training acc: 0.0625
Global Iter: 1158200 training loss: 1.93088
Global Iter: 1158200 training acc: 0.3125
Global Iter: 1158300 training loss: 2.03913
Global Iter: 1158300 training acc: 0.1875
Global Iter: 1158400 training loss: 2.02201
Global Iter: 1158400 training acc: 0.28125
Global Iter: 1158500 training loss: 1.96263
Global Iter: 1158500 training acc: 0.21875
Global Iter: 1158600 training loss: 2.02384
Global Iter: 1158600 training acc: 0.09375
Global Iter: 1158700 training loss: 1.94138
Global Iter: 1158700 training acc: 0.0625
Global Iter: 1158800 training loss: 1.99771
Global Iter: 1158800 training acc: 0.25
Global Iter: 1158900 training loss: 2.02378
Global Iter: 1158900 training acc: 0.125
Global Iter: 1159000 training loss: 2.07015
Global Iter: 1159000 training acc: 0.09375
Global Iter: 1159100 training loss: 1.97039
Global Iter: 1159100 training acc: 0.21875
Global Iter: 1159200 training loss: 1.96798
Global Iter: 115922017-06-22 14:10:28.261080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1159206
00 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1159206
Number of Patches: 121980
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1159206
Global Iter: 1159300 training loss: 2.00806
Global Iter: 1159300 training acc: 0.15625
Global Iter: 1159400 training loss: 1.98354
Global Iter: 1159400 training acc: 0.21875
Global Iter: 1159500 training loss: 2.05473
Global Iter: 1159500 training acc: 0.1875
Global Iter: 1159600 training loss: 2.06745
Global Iter: 1159600 training acc: 0.1875
Global Iter: 1159700 training loss: 1.95899
Global Iter: 1159700 training acc: 0.28125
Global Iter: 1159800 training loss: 2.0489
Global Iter: 1159800 training acc: 0.1875
Global Iter: 1159900 training loss: 1.97836
Global Iter: 1159900 training acc: 0.15625
Global Iter: 1160000 training loss: 1.99965
Global Iter: 1160000 training acc: 0.125
Global Iter: 1160100 training loss: 1.97672
Global Iter: 1160100 training acc: 0.15625
Global Iter: 1160200 training loss: 2.02469
Global Iter: 1160200 training acc: 0.15625
Global Iter: 1160300 training loss: 2.06298
Global Iter: 1160300 training acc: 0.15625
Global Iter: 1160400 training loss: 2.06311
Global Iter: 1160400 training acc: 0.21875
Global Iter: 1160500 training loss: 1.92185
Global Iter: 1160500 training acc: 0.1875
Global Iter: 1160600 training loss: 2.10055
Global Iter: 1160600 training acc: 0.15625
Global Iter: 1160700 training loss: 2.01225
Global Iter: 1160700 training acc: 0.28125
Global Iter: 1160800 training loss: 1.98742
Global Iter: 1160800 training acc: 0.125
Global Iter: 1160900 training loss: 1.95501
Global Iter: 1160900 training acc: 0.09375
Global Iter: 1161000 training loss: 2.03772
Global Iter: 1161000 training acc: 0.28125
Global Iter: 1161100 training loss: 1.9744
Global Iter: 1161100 training acc: 0.15625
Global Iter: 1161200 training loss: 1.90269
Global Iter: 1161200 training acc: 0.21875
Global Iter: 1161300 training loss: 2.15396
Global Iter: 1161300 training acc: 0.125
Global Iter: 1161400 training loss: 2.10029
Global Iter: 1161400 training acc: 0.09375
Global Iter: 1161500 training loss: 2.14253
Global Iter: 1161500 training acc: 0.1875
Global Iter: 1161600 training loss: 2.02724
Global Iter: 1161600 training acc: 0.21875
Global Iter: 1161700 training loss: 1.90183
Global Iter: 1161700 training acc: 0.28125
Global Iter: 1161800 training loss: 2.03321
Global Iter: 1161800 training acc: 0.1875
Global Iter: 1161900 training loss: 2.01293
Global Iter: 1161900 training acc: 0.15625
Global Iter: 1162000 training loss: 1.94025
Global Iter: 1162000 training acc: 0.125
Global Iter: 1162100 training loss: 1.98725
Global Iter: 1162100 training acc: 0.21875
Global Iter: 1162200 training loss: 1.98618
Global Iter: 1162200 training acc: 0.34375
Global Iter: 1162300 training loss: 2.11591
Global Iter: 1162300 training acc: 0.15625
Global Iter: 1162400 training loss: 1.97922
Global Iter: 1162400 training acc: 0.1875
Global Iter: 1162500 training loss: 1.95268
Global Iter: 1162500 training acc: 0.21875
Global Iter: 1162600 training loss: 1.92963
Global Iter: 1162600 training acc: 0.15625
Global Iter: 1162700 training loss: 1.93524
Global Iter: 1162700 training acc: 0.21875
Global Iter: 1162800 training loss: 1.9443
Global Iter: 1162800 training acc: 0.21875
Global Iter: 1162900 training loss: 1.95609
Global Iter: 1162900 training acc: 0.125
Global Iter: 1163000 training loss: 1.99439
Global Iter: 1163000 training acc: 0.25
Global Iter: 1163100 training loss: 1.94587
Global Iter: 1163100 training acc: 0.1875
Global Iter: 1163200 training loss: 2.04184
Global Iter: 1163200 training acc: 0.125
Global Iter: 1163300 training loss: 2.11374
Global Iter: 1163300 training acc: 0.1875
Global Iter: 1163400 training loss: 1.96114
Global Iter: 1163400 training acc: 0.3125
Global Iter: 1163500 training loss: 1.9706
Global Iter: 1163500 training acc: 0.1875
Global Iter: 1163600 training loss: 1.89825
Global Iter: 1163600 training acc: 0.15625
Global Iter: 1163700 training loss: 1.96072
Globa2017-06-22 14:23:26.265770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1166830
l Iter: 1163700 training acc: 0.34375
Global Iter: 1163800 training loss: 1.86487
Global Iter: 1163800 training acc: 0.34375
Global Iter: 1163900 training loss: 2.02454
Global Iter: 1163900 training acc: 0.0625
Global Iter: 1164000 training loss: 1.95487
Global Iter: 1164000 training acc: 0.21875
Global Iter: 1164100 training loss: 2.0232
Global Iter: 1164100 training acc: 0.1875
Global Iter: 1164200 training loss: 2.01982
Global Iter: 1164200 training acc: 0.28125
Global Iter: 1164300 training loss: 2.02054
Global Iter: 1164300 training acc: 0.1875
Global Iter: 1164400 training loss: 2.04595
Global Iter: 1164400 training acc: 0.15625
Global Iter: 1164500 training loss: 1.99416
Global Iter: 1164500 training acc: 0.125
Global Iter: 1164600 training loss: 1.90314
Global Iter: 1164600 training acc: 0.21875
Global Iter: 1164700 training loss: 2.00321
Global Iter: 1164700 training acc: 0.15625
Global Iter: 1164800 training loss: 1.90992
Global Iter: 1164800 training acc: 0.1875
Global Iter: 1164900 training loss: 1.94579
Global Iter: 1164900 training acc: 0.21875
Global Iter: 1165000 training loss: 2.00413
Global Iter: 1165000 training acc: 0.125
Global Iter: 1165100 training loss: 2.05084
Global Iter: 1165100 training acc: 0.09375
Global Iter: 1165200 training loss: 1.99222
Global Iter: 1165200 training acc: 0.125
Global Iter: 1165300 training loss: 1.96601
Global Iter: 1165300 training acc: 0.21875
Global Iter: 1165400 training loss: 1.94176
Global Iter: 1165400 training acc: 0.15625
Global Iter: 1165500 training loss: 1.99207
Global Iter: 1165500 training acc: 0.1875
Global Iter: 1165600 training loss: 1.88269
Global Iter: 1165600 training acc: 0.28125
Global Iter: 1165700 training loss: 1.93327
Global Iter: 1165700 training acc: 0.25
Global Iter: 1165800 training loss: 2.15055
Global Iter: 1165800 training acc: 0.09375
Global Iter: 1165900 training loss: 1.97879
Global Iter: 1165900 training acc: 0.09375
Global Iter: 1166000 training loss: 2.08039
Global Iter: 1166000 training acc: 0.15625
Global Iter: 1166100 training loss: 1.98017
Global Iter: 1166100 training acc: 0.28125
Global Iter: 1166200 training loss: 2.11649
Global Iter: 1166200 training acc: 0.15625
Global Iter: 1166300 training loss: 1.89757
Global Iter: 1166300 training acc: 0.21875
Global Iter: 1166400 training loss: 2.01367
Global Iter: 1166400 training acc: 0.09375
Global Iter: 1166500 training loss: 2.00593
Global Iter: 1166500 training acc: 0.125
Global Iter: 1166600 training loss: 2.08404
Global Iter: 1166600 training acc: 0.0625
Global Iter: 1166700 training loss: 2.04939
Global Iter: 1166700 training acc: 0.125
Global Iter: 1166800 training loss: 1.95355
Global Iter: 1166800 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1166830
Number of Patches: 120761
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1166830
Global Iter: 1166900 training loss: 1.98345
Global Iter: 1166900 training acc: 0.1875
Global Iter: 1167000 training loss: 1.94381
Global Iter: 1167000 training acc: 0.15625
Global Iter: 1167100 training loss: 1.92967
Global Iter: 1167100 training acc: 0.09375
Global Iter: 1167200 training loss: 1.91451
Global Iter: 1167200 training acc: 0.25
Global Iter: 1167300 training loss: 1.98918
Global Iter: 1167300 training acc: 0.1875
Global Iter: 1167400 training loss: 2.01595
Global Iter: 1167400 training acc: 0.0625
Global Iter: 1167500 training loss: 1.90248
Global Iter: 1167500 training acc: 0.1875
Global Iter: 1167600 training loss: 1.96469
Global Iter: 1167600 training acc: 0.28125
Global Iter: 1167700 training loss: 2.02295
Global Iter: 1167700 training acc: 0.15625
Global Iter: 1167800 training loss: 2.07931
Global Iter: 1167800 training acc: 0.28125
Global Iter: 1167900 training loss: 1.99726
Global Iter: 1167900 training acc: 0.125
Global Iter: 1168000 training loss: 2.0513
Global Iter: 1168000 training acc: 0.1875
Global Iter: 1168100 training loss: 1.98005
Global Iter: 1168100 training acc: 0.1875
Global Iter: 1168200 training loss: 1.98087
Global Iter: 1168200 training acc: 0.1875
Global Iter: 1168300 training loss: 2.01961
Global Iter: 1168300 training acc: 0.25
Global Iter: 1168400 training loss: 1.9572
Global Iter: 1168400 training acc: 0.15625
Global Iter: 1168500 training loss: 2.02132
Global Iter: 1168500 training acc: 0.15625
Global Iter: 1168600 training loss: 1.94528
Global Iter: 1168600 training acc: 0.1875
Global Iter: 1168700 training loss: 2.121
Global Iter: 1168700 training acc: 0.09375
Global Iter: 1168800 training loss: 2.05879
Global Iter: 1168800 training acc: 0.15625
Global Iter: 1168900 training loss: 2.02297
Global Iter: 1168900 training acc: 0.25
Global Iter: 1169000 training loss: 1.90173
Global Iter: 1169000 training acc: 0.125
Global Iter: 1169100 training loss: 1.98887
Global Iter: 1169100 training acc: 0.15625
Global Iter: 1169200 training loss: 1.92316
Global Iter: 1169200 training acc: 0.3125
Global Iter: 1169300 training loss: 1.95826
Global Iter: 1169300 training acc: 0.25
Global Iter: 1169400 training loss: 1.97768
Global Iter: 1169400 training acc: 0.21875
Global Iter: 1169500 training loss: 2.08714
Global Iter: 1169500 training acc: 0.1875
Global Iter: 1169600 training loss: 1.99849
Global Iter: 1169600 training acc: 0.21875
Global Iter: 1169700 training loss: 1.89305
Global Iter: 1169700 training acc: 0.21875
Global Iter: 1169800 training loss: 1.95657
Global Iter: 1169800 training acc: 0.375
Global Iter: 1169900 training loss: 1.98311
Global Iter: 1169900 training acc: 0.28125
Global Iter: 1170000 training loss: 1.98663
Global Iter: 1170000 training acc: 0.125
Global Iter: 1170100 training loss: 1.94985
Global Iter: 1170100 training acc: 0.1875
Global Iter: 1170200 training loss: 1.9499
Global Iter: 1170200 training acc: 0.125
Global Iter: 1170300 training loss: 2.06058
Global Iter: 1170300 training acc: 0.21875
Global Iter: 1170400 training loss: 1.96528
Global Iter: 1170400 training acc: 0.21875
Global Iter: 1170500 training loss: 1.91352
Global Iter: 1170500 training acc: 0.15625
Global Iter: 1170600 training loss: 2.08376
Global Iter: 1170600 training acc: 0.15625
Global Iter: 1170700 training loss: 2.03219
Global Iter: 1170700 training acc: 0.15625
Global Iter: 1170800 training loss: 1.92725
Global Iter: 1170800 training acc: 0.25
Global Iter: 1170900 training loss: 2.18537
Global Iter: 1170900 training acc: 0.1875
Global Iter: 1171000 training loss: 2.01902
Global Iter: 1171000 training acc: 0.15625
Global Iter: 1171100 training loss: 1.9369
Global Iter: 1171100 training acc: 0.125
Global Iter: 1171200 training loss: 1.95003
Global Iter: 1171200 training acc: 0.1875
Global Iter: 1171300 training loss: 2.01201
Global Iter: 1171300 training acc: 0.21875
Global Iter: 1171400 training loss: 2.01632
Global Iter: 1171400 training acc: 0.15625
Global Iter: 1171500 training loss: 2.05739
Global Iter: 1171500 training acc: 0.25
Global Iter: 1171600 training loss: 2.02137
Global Iter: 1171600 training acc: 0.28125
Global Iter: 1171700 training loss: 2.0008
Global Iter: 1171700 training acc: 0.125
Global Iter: 1171800 training loss: 2.00496
Global Iter: 1171800 training acc: 0.15625
Global Iter: 1171900 training loss: 2.09253
Global Iter: 1171900 training acc: 0.125
Global Iter: 1172000 training loss: 1.95072
Global Iter: 1172000 training acc: 0.0625
Global Iter: 1172100 training loss: 1.93087
Global Iter: 1172100 training acc: 0.15625
Global Iter: 1172200 training loss: 1.8963
Global Iter: 1172200 training acc: 0.25
Global Iter: 1172300 training loss: 2.05125
Global Iter: 1172300 training acc: 0.125
Global Iter: 1172400 training loss: 1.97382
Global Iter: 1172400 training acc: 0.25
Global Iter: 1172500 training loss: 1.98328
Global Iter: 1172500 training acc: 0.15625
Global Iter: 1172600 training loss: 1.96934
Global Iter: 1172600 training acc: 0.3125
Global Iter: 1172700 training loss: 2.11919
Global Iter: 1172700 training acc: 0.1875
Global Iter: 1172800 training loss: 1.90128
Global Iter: 1172800 training acc: 0.25
Global Iter: 1172900 training loss: 1.91552
Global Iter: 1172900 training acc: 0.21875
Global Iter: 12017-06-22 14:36:23.040374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1174378
173000 training loss: 1.96183
Global Iter: 1173000 training acc: 0.1875
Global Iter: 1173100 training loss: 1.98742
Global Iter: 1173100 training acc: 0.15625
Global Iter: 1173200 training loss: 1.98681
Global Iter: 1173200 training acc: 0.0625
Global Iter: 1173300 training loss: 2.10408
Global Iter: 1173300 training acc: 0.0625
Global Iter: 1173400 training loss: 2.05874
Global Iter: 1173400 training acc: 0.09375
Global Iter: 1173500 training loss: 1.96199
Global Iter: 1173500 training acc: 0.15625
Global Iter: 1173600 training loss: 2.07438
Global Iter: 1173600 training acc: 0.15625
Global Iter: 1173700 training loss: 1.91732
Global Iter: 1173700 training acc: 0.25
Global Iter: 1173800 training loss: 1.95332
Global Iter: 1173800 training acc: 0.40625
Global Iter: 1173900 training loss: 2.04218
Global Iter: 1173900 training acc: 0.125
Global Iter: 1174000 training loss: 1.95113
Global Iter: 1174000 training acc: 0.125
Global Iter: 1174100 training loss: 2.03749
Global Iter: 1174100 training acc: 0.21875
Global Iter: 1174200 training loss: 1.93583
Global Iter: 1174200 training acc: 0.1875
Global Iter: 1174300 training loss: 1.98034
Global Iter: 1174300 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1174378
Number of Patches: 119554
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1174378
Global Iter: 1174400 training loss: 2.04125
Global Iter: 1174400 training acc: 0.25
Global Iter: 1174500 training loss: 2.01586
Global Iter: 1174500 training acc: 0.28125
Global Iter: 1174600 training loss: 1.86388
Global Iter: 1174600 training acc: 0.25
Global Iter: 1174700 training loss: 2.07209
Global Iter: 1174700 training acc: 0.09375
Global Iter: 1174800 training loss: 1.93417
Global Iter: 1174800 training acc: 0.15625
Global Iter: 1174900 training loss: 1.92325
Global Iter: 1174900 training acc: 0.1875
Global Iter: 1175000 training loss: 2.05581
Global Iter: 1175000 training acc: 0.125
Global Iter: 1175100 training loss: 1.89933
Global Iter: 1175100 training acc: 0.3125
Global Iter: 1175200 training loss: 1.95666
Global Iter: 1175200 training acc: 0.1875
Global Iter: 1175300 training loss: 1.97095
Global Iter: 1175300 training acc: 0.1875
Global Iter: 1175400 training loss: 2.01053
Global Iter: 1175400 training acc: 0.125
Global Iter: 1175500 training loss: 1.98896
Global Iter: 1175500 training acc: 0.09375
Global Iter: 1175600 training loss: 2.08979
Global Iter: 1175600 training acc: 0.09375
Global Iter: 1175700 training loss: 2.0119
Global Iter: 1175700 training acc: 0.09375
Global Iter: 1175800 training loss: 2.02802
Global Iter: 1175800 training acc: 0.21875
Global Iter: 1175900 training loss: 1.97589
Global Iter: 1175900 training acc: 0.125
Global Iter: 1176000 training loss: 2.03563
Global Iter: 1176000 training acc: 0.15625
Global Iter: 1176100 training loss: 2.05197
Global Iter: 1176100 training acc: 0.09375
Global Iter: 1176200 training loss: 1.95684
Global Iter: 1176200 training acc: 0.125
Global Iter: 1176300 training loss: 1.90412
Global Iter: 1176300 training acc: 0.15625
Global Iter: 1176400 training loss: 1.90369
Global Iter: 1176400 training acc: 0.28125
Global Iter: 1176500 training loss: 1.96497
Global Iter: 1176500 training acc: 0.15625
Global Iter: 1176600 training loss: 1.99827
Global Iter: 1176600 training acc: 0.21875
Global Iter: 1176700 training loss: 2.03875
Global Iter: 1176700 training acc: 0.15625
Global Iter: 1176800 training loss: 1.94142
Global Iter: 1176800 training acc: 0.3125
Global Iter: 1176900 training loss: 2.03763
Global Iter: 1176900 training acc: 0.1875
Global Iter: 1177000 training loss: 2.10829
Global Iter: 1177000 training acc: 0.1875
Global Iter: 1177100 training loss: 2.01649
Global Iter: 1177100 training acc: 0.125
Global Iter: 1177200 training loss: 2.02629
Global Iter: 1177200 training acc: 0.0625
Global Iter: 1177300 training loss: 1.99099
Global Iter: 1177300 training acc: 0.21875
Global Iter: 1177400 training loss: 2.08662
Global Iter: 1177400 training acc: 0.0625
Global It2017-06-22 14:49:11.616966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1181851
er: 1177500 training loss: 1.89157
Global Iter: 1177500 training acc: 0.34375
Global Iter: 1177600 training loss: 2.03375
Global Iter: 1177600 training acc: 0.21875
Global Iter: 1177700 training loss: 2.04668
Global Iter: 1177700 training acc: 0.15625
Global Iter: 1177800 training loss: 2.03405
Global Iter: 1177800 training acc: 0.25
Global Iter: 1177900 training loss: 2.03526
Global Iter: 1177900 training acc: 0.0625
Global Iter: 1178000 training loss: 1.99613
Global Iter: 1178000 training acc: 0.09375
Global Iter: 1178100 training loss: 1.94722
Global Iter: 1178100 training acc: 0.125
Global Iter: 1178200 training loss: 2.03705
Global Iter: 1178200 training acc: 0.125
Global Iter: 1178300 training loss: 1.9832
Global Iter: 1178300 training acc: 0.15625
Global Iter: 1178400 training loss: 1.99448
Global Iter: 1178400 training acc: 0.15625
Global Iter: 1178500 training loss: 2.03226
Global Iter: 1178500 training acc: 0.21875
Global Iter: 1178600 training loss: 1.93377
Global Iter: 1178600 training acc: 0.21875
Global Iter: 1178700 training loss: 1.95538
Global Iter: 1178700 training acc: 0.125
Global Iter: 1178800 training loss: 2.03594
Global Iter: 1178800 training acc: 0.03125
Global Iter: 1178900 training loss: 1.93295
Global Iter: 1178900 training acc: 0.15625
Global Iter: 1179000 training loss: 2.03163
Global Iter: 1179000 training acc: 0.09375
Global Iter: 1179100 training loss: 1.95268
Global Iter: 1179100 training acc: 0.1875
Global Iter: 1179200 training loss: 1.97179
Global Iter: 1179200 training acc: 0.25
Global Iter: 1179300 training loss: 2.00054
Global Iter: 1179300 training acc: 0.3125
Global Iter: 1179400 training loss: 1.93668
Global Iter: 1179400 training acc: 0.28125
Global Iter: 1179500 training loss: 1.97315
Global Iter: 1179500 training acc: 0.15625
Global Iter: 1179600 training loss: 1.96405
Global Iter: 1179600 training acc: 0.25
Global Iter: 1179700 training loss: 1.9273
Global Iter: 1179700 training acc: 0.15625
Global Iter: 1179800 training loss: 2.03339
Global Iter: 1179800 training acc: 0.28125
Global Iter: 1179900 training loss: 1.96252
Global Iter: 1179900 training acc: 0.25
Global Iter: 1180000 training loss: 1.928
Global Iter: 1180000 training acc: 0.25
Global Iter: 1180100 training loss: 2.00592
Global Iter: 1180100 training acc: 0.125
Global Iter: 1180200 training loss: 1.93933
Global Iter: 1180200 training acc: 0.125
Global Iter: 1180300 training loss: 1.88027
Global Iter: 1180300 training acc: 0.34375
Global Iter: 1180400 training loss: 1.96154
Global Iter: 1180400 training acc: 0.25
Global Iter: 1180500 training loss: 2.03866
Global Iter: 1180500 training acc: 0.09375
Global Iter: 1180600 training loss: 1.90111
Global Iter: 1180600 training acc: 0.25
Global Iter: 1180700 training loss: 1.97202
Global Iter: 1180700 training acc: 0.21875
Global Iter: 1180800 training loss: 1.93179
Global Iter: 1180800 training acc: 0.15625
Global Iter: 1180900 training loss: 1.91461
Global Iter: 1180900 training acc: 0.1875
Global Iter: 1181000 training loss: 1.95564
Global Iter: 1181000 training acc: 0.1875
Global Iter: 1181100 training loss: 2.02753
Global Iter: 1181100 training acc: 0.03125
Global Iter: 1181200 training loss: 1.98799
Global Iter: 1181200 training acc: 0.15625
Global Iter: 1181300 training loss: 1.9727
Global Iter: 1181300 training acc: 0.15625
Global Iter: 1181400 training loss: 1.93216
Global Iter: 1181400 training acc: 0.1875
Global Iter: 1181500 training loss: 1.9678
Global Iter: 1181500 training acc: 0.15625
Global Iter: 1181600 training loss: 1.94597
Global Iter: 1181600 training acc: 0.1875
Global Iter: 1181700 training loss: 1.95383
Global Iter: 1181700 training acc: 0.21875
Global Iter: 1181800 training loss: 2.15292
Global Iter: 1181800 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1181851
Number of Patches: 118360
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1181851
Global Iter: 1181900 training loss: 2.08737
Global Iter: 1181900 training acc: 0.125
Global Iter: 1182000 training loss: 2.08964
Global Iter: 1182000 training acc: 0.09375
Global Iter: 1182100 training loss: 1.98271
Global Iter: 1182100 training acc: 0.03125
Global Iter: 1182200 training loss: 2.07934
Global Iter: 1182200 training acc: 0.09375
Global Iter: 1182300 training loss: 1.99627
Global Iter: 1182300 training acc: 0.15625
Global Iter: 1182400 training loss: 2.01759
Global Iter: 1182400 training acc: 0.0625
Global Iter: 1182500 training loss: 2.02213
Global Iter: 1182500 training acc: 0.25
Global Iter: 1182600 training loss: 2.0298
Global Iter: 1182600 training acc: 0.09375
Global Iter: 1182700 training loss: 1.99641
Global Iter: 1182700 training acc: 0.25
Global Iter: 1182800 training loss: 2.05376
Global Iter: 1182800 training acc: 0.125
Global Iter: 1182900 training loss: 1.9555
Global Iter: 1182900 training acc: 0.21875
Global Iter: 1183000 training loss: 1.92735
Global Iter: 1183000 training acc: 0.15625
Global Iter: 1183100 training loss: 1.98968
Global Iter: 1183100 training acc: 0.1875
Global Iter: 1183200 training loss: 1.98007
Global Iter: 1183200 training acc: 0.125
Global Iter: 1183300 training loss: 1.97957
Global Iter: 1183300 training acc: 0.21875
Global Iter: 1183400 training loss: 1.97449
Global Iter: 1183400 training acc: 0.15625
Global Iter: 1183500 training loss: 2.02647
Global Iter: 1183500 training acc: 0.1875
Global Iter: 1183600 training loss: 1.95928
Global Iter: 1183600 training acc: 0.28125
Global Iter: 1183700 training loss: 1.96933
Global Iter: 1183700 training acc: 0.21875
Global Iter: 1183800 training loss: 2.012
Global Iter: 1183800 training acc: 0.15625
Global Iter: 1183900 training loss: 2.06683
Global Iter: 1183900 training acc: 0.21875
Global Iter: 1184000 training loss: 2.03077
Global Iter: 1184000 training acc: 0.15625
Global Iter: 1184100 training loss: 2.00102
Global Iter: 1184100 training acc: 0.0625
Global Iter: 1184200 training loss: 1.94873
Global Iter: 1184200 training acc: 0.1875
Global Iter: 1184300 training loss: 2.02418
Global Iter: 1184300 training acc: 0.15625
Global Iter: 1184400 training loss: 2.02045
Global Iter: 1184400 training acc: 0.125
Global Iter: 1184500 training loss: 1.9684
Global Iter: 1184500 training acc: 0.1875
Global Iter: 1184600 training loss: 2.08575
Global Iter: 1184600 training acc: 0.15625
Global Iter: 1184700 training loss: 1.99812
Global Iter: 1184700 training acc: 0.1875
Global Iter: 1184800 training loss: 1.98549
Global Iter: 1184800 training acc: 0.21875
Global Iter: 1184900 training loss: 1.97832
Global Iter: 1184900 training acc: 0.125
Global Iter: 1185000 training loss: 2.03794
Global Iter: 1185000 training acc: 0.1875
Global Iter: 1185100 training loss: 2.06325
Global Iter: 1185100 training acc: 0.21875
Global Iter: 1185200 training loss: 2.16254
Global Iter: 1185200 training acc: 0.0625
Global Iter: 1185300 training loss: 2.01489
Global Iter: 1185300 training acc: 0.09375
Global Iter: 1185400 training loss: 1.99667
Global Iter: 1185400 training acc: 0.125
Global Iter: 1185500 training loss: 1.94522
Global Iter: 1185500 training acc: 0.3125
Global Iter: 1185600 training loss: 2.00667
Global Iter: 1185600 training acc: 0.15625
Global Iter: 1185700 training loss: 1.90757
Global Iter: 1185700 training acc: 0.3125
Global Iter: 1185800 training loss: 2.07385
Global Iter: 1185800 training acc: 0.25
Global Iter: 1185900 training loss: 1.9955
Global Iter: 1185900 training acc: 0.15625
Global Iter: 1186000 training loss: 1.8695
Global Iter: 1186000 training acc: 0.34375
Global Iter: 1186100 training loss: 1.96454
Global Iter: 1186100 training acc: 0.28125
Global Iter: 1186200 training loss: 2.03107
Global Iter: 1186200 training acc: 0.125
Global Iter: 1186300 training loss: 2.07848
Global Iter: 1186300 training acc: 0.1875
Global Iter: 1186400 training loss: 1.91501
Global Iter: 1186400 training acc: 0.21875
Global Iter: 1186500 training loss: 2.01401
Global Iter: 1186500 training acc: 0.21875
Global Iter: 1186600 training loss: 1.95518
Global Iter: 1186600 training acc: 0.21875
Global Iter: 1186700 training loss: 2.097
Global Iter: 1182017-06-22 15:01:49.797416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1189249
6700 training acc: 0.3125
Global Iter: 1186800 training loss: 2.05485
Global Iter: 1186800 training acc: 0.09375
Global Iter: 1186900 training loss: 2.00079
Global Iter: 1186900 training acc: 0.125
Global Iter: 1187000 training loss: 2.04439
Global Iter: 1187000 training acc: 0.09375
Global Iter: 1187100 training loss: 1.90227
Global Iter: 1187100 training acc: 0.125
Global Iter: 1187200 training loss: 2.00679
Global Iter: 1187200 training acc: 0.125
Global Iter: 1187300 training loss: 1.92893
Global Iter: 1187300 training acc: 0.15625
Global Iter: 1187400 training loss: 1.97981
Global Iter: 1187400 training acc: 0.25
Global Iter: 1187500 training loss: 1.96619
Global Iter: 1187500 training acc: 0.1875
Global Iter: 1187600 training loss: 1.97479
Global Iter: 1187600 training acc: 0.1875
Global Iter: 1187700 training loss: 2.066
Global Iter: 1187700 training acc: 0.25
Global Iter: 1187800 training loss: 2.13428
Global Iter: 1187800 training acc: 0.1875
Global Iter: 1187900 training loss: 2.14142
Global Iter: 1187900 training acc: 0.1875
Global Iter: 1188000 training loss: 2.00613
Global Iter: 1188000 training acc: 0.21875
Global Iter: 1188100 training loss: 2.03693
Global Iter: 1188100 training acc: 0.1875
Global Iter: 1188200 training loss: 2.07275
Global Iter: 1188200 training acc: 0.1875
Global Iter: 1188300 training loss: 2.00646
Global Iter: 1188300 training acc: 0.25
Global Iter: 1188400 training loss: 1.88617
Global Iter: 1188400 training acc: 0.3125
Global Iter: 1188500 training loss: 2.03961
Global Iter: 1188500 training acc: 0.1875
Global Iter: 1188600 training loss: 1.86363
Global Iter: 1188600 training acc: 0.34375
Global Iter: 1188700 training loss: 2.13166
Global Iter: 1188700 training acc: 0.09375
Global Iter: 1188800 training loss: 2.11855
Global Iter: 1188800 training acc: 0.15625
Global Iter: 1188900 training loss: 2.18326
Global Iter: 1188900 training acc: 0.0625
Global Iter: 1189000 training loss: 2.0081
Global Iter: 1189000 training acc: 0.21875
Global Iter: 1189100 training loss: 2.00516
Global Iter: 1189100 training acc: 0.15625
Global Iter: 1189200 training loss: 1.92952
Global Iter: 1189200 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1189249
Number of Patches: 117177
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1189249
Global Iter: 1189300 training loss: 1.89548
Global Iter: 1189300 training acc: 0.1875
Global Iter: 1189400 training loss: 1.90162
Global Iter: 1189400 training acc: 0.15625
Global Iter: 1189500 training loss: 2.06105
Global Iter: 1189500 training acc: 0.15625
Global Iter: 1189600 training loss: 1.94356
Global Iter: 1189600 training acc: 0.1875
Global Iter: 1189700 training loss: 2.02254
Global Iter: 1189700 training acc: 0.09375
Global Iter: 1189800 training loss: 1.92792
Global Iter: 1189800 training acc: 0.1875
Global Iter: 1189900 training loss: 1.9605
Global Iter: 1189900 training acc: 0.15625
Global Iter: 1190000 training loss: 2.02437
Global Iter: 1190000 training acc: 0.25
Global Iter: 1190100 training loss: 1.91597
Global Iter: 1190100 training acc: 0.15625
Global Iter: 1190200 training loss: 2.0551
Global Iter: 1190200 training acc: 0.25
Global Iter: 1190300 training loss: 1.99529
Global Iter: 1190300 training acc: 0.21875
Global Iter: 1190400 training loss: 1.9651
Global Iter: 1190400 training acc: 0.15625
Global Iter: 1190500 training loss: 1.95946
Global Iter: 1190500 training acc: 0.21875
Global Iter: 1190600 training loss: 1.96314
Global Iter: 1190600 training acc: 0.1875
Global Iter: 1190700 training loss: 1.97988
Global Iter: 1190700 training acc: 0.3125
Global Iter: 1190800 training loss: 2.06995
Global Iter: 1190800 training acc: 0.125
Global Iter: 1190900 training loss: 2.00367
Global Iter: 1190900 training acc: 0.09375
Global Iter: 1191000 training loss: 1.99227
Global Iter: 1191000 training acc: 0.3125
Global Iter: 1191100 training loss: 1.92135
Global Iter: 1191100 training acc: 0.15625
Global Iter: 1191200 training loss: 1.96555
Global Iter: 1191200 training acc: 0.1875
Global Iter: 1191300 training loss: 1.87406
Global Iter: 1191300 training acc: 0.25
Global Iter: 1191400 training loss: 2.01418
Global Iter: 1191400 training acc: 0.21875
Global Iter: 1191500 training loss: 2.07739
Global Iter: 1191500 training acc: 0.125
Global Iter: 1191600 training loss: 1.9971
Global Iter: 1191600 training acc: 0.125
Global Iter: 1191700 training loss: 1.97301
Global Iter: 1191700 training acc: 0.03125
Global Iter: 1191800 training loss: 2.04358
Global Iter: 1191800 training acc: 0.125
Global Iter: 1191900 training loss: 2.04393
Global Iter: 1191900 training acc: 0.125
Global Iter: 1192000 training loss: 1.99782
Global Iter: 1192000 training acc: 0.25
Global Iter: 1192100 training loss: 1.92836
Global Iter: 1192100 training acc: 0.28125
Global Iter: 1192200 training loss: 2.09112
Global Iter: 1192200 training acc: 0.0625
Global Iter: 1192300 training loss: 1.96086
Global Iter: 1192300 training acc: 0.1875
Global Iter: 1192400 training loss: 2.01867
Global Iter: 1192400 training acc: 0.15625
Global Iter: 1192500 training loss: 2.00381
Global Iter: 1192500 training acc: 0.1875
Global Iter: 1192600 training loss: 2.00525
Global Iter: 1192600 training acc: 0.1875
Global Iter: 1192700 training loss: 1.94355
Global Iter: 1192700 training acc: 0.25
Global Iter: 1192800 training loss: 2.02358
Global Iter: 1192800 training acc: 0.21875
Global Iter: 1192900 training loss: 2.02493
Global Iter: 1192900 training acc: 0.15625
Global Iter: 1193000 training loss: 1.9647
Global Iter: 1193000 training acc: 0.28125
Global Iter: 1193100 training loss: 1.91596
Global Iter: 1193100 training acc: 0.15625
Global Iter: 1193200 training loss: 2.02809
Global Iter: 1193200 training acc: 0.21875
Global Iter: 1193300 training loss: 1.98622
Global Iter: 1193300 training acc: 0.1875
Global Iter: 1193400 training loss: 2.02719
Global Iter: 1193400 training acc: 0.09375
Global Iter: 1193500 training loss: 1.97623
Global Iter: 1193500 training acc: 0.3125
Global Iter: 1193600 training loss: 1.98386
Global Iter: 1193600 training acc: 0.25
Global Iter: 1193700 training loss: 1.90895
Global Iter: 1193700 training acc: 0.3125
Global Iter: 1193800 training loss: 1.90101
Global Iter: 1193800 training acc: 0.1875
Global Iter: 1193900 training loss: 1.97764
Global Iter: 1193900 training acc: 0.09375
Global Iter: 1194000 training loss: 2.02365
Global Iter: 1194000 training acc: 0.15625
Global Iter: 1194100 training loss: 1.92725
Global Iter: 1194100 training acc: 0.21875
Global Iter: 1194200 training loss: 2.07545
Global Iter: 1194200 training acc: 0.125
Global Iter: 1194300 training loss: 2.02269
Global Iter: 1194300 training acc: 0.1875
Global Iter: 1194400 training loss: 1.99784
Global Iter: 1194400 training acc: 0.09375
Global Iter: 1194500 training loss: 2.01652
Global Iter: 1194500 training acc: 0.125
Global Iter: 1194600 training loss: 1.98782
Global Iter: 1194600 training acc: 0.09375
Global Iter: 1194700 training loss: 1.94013
Global Iter: 1194700 training acc: 0.28125
Global Iter: 1194800 training loss: 1.99033
Global Iter: 1194800 training acc: 0.15625
Global Iter: 1194900 training loss: 1.93893
Global Iter: 1194900 training acc: 0.25
Global Iter: 1195000 training loss: 1.9957
Global Iter: 1195000 training acc: 0.21875
Global Iter: 1195100 training loss: 1.96405
Global Iter: 1195100 training acc: 0.1875
Global Iter: 1195200 training loss: 2.0607
Global Iter: 1195200 training acc: 0.1875
Global Iter: 1195300 training loss: 2.06455
Global Iter: 1195300 training acc: 0.125
Global Iter: 1195400 training loss: 1.91632
Global Iter: 1195400 training acc: 0.21875
Global Iter: 1195500 training loss: 1.95259
Global Iter: 1195500 training acc: 0.21875
Global Iter: 1195600 training loss: 1.96192
Global Iter: 1195600 training acc: 0.09375
Global Iter: 1195700 training loss: 2.0783
Global Iter: 1195700 training acc: 0.15625
Global Iter: 1195800 training loss: 2.00745
Global Iter: 1195800 training acc: 0.15625
Global Iter: 1195900 training loss: 1.95817
Global Iter: 1195900 training acc: 0.09375
Global Iter: 1196000 traini2017-06-22 15:14:16.199944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1196573
ng loss: 1.99503
Global Iter: 1196000 training acc: 0.28125
Global Iter: 1196100 training loss: 2.04275
Global Iter: 1196100 training acc: 0.1875
Global Iter: 1196200 training loss: 2.15441
Global Iter: 1196200 training acc: 0.0625
Global Iter: 1196300 training loss: 1.91281
Global Iter: 1196300 training acc: 0.21875
Global Iter: 1196400 training loss: 1.97128
Global Iter: 1196400 training acc: 0.21875
Global Iter: 1196500 training loss: 2.02381
Global Iter: 1196500 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1196573
Number of Patches: 116006
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1196573
Global Iter: 1196600 training loss: 2.05874
Global Iter: 1196600 training acc: 0.0625
Global Iter: 1196700 training loss: 2.11321
Global Iter: 1196700 training acc: 0.1875
Global Iter: 1196800 training loss: 2.21914
Global Iter: 1196800 training acc: 0.125
Global Iter: 1196900 training loss: 1.9471
Global Iter: 1196900 training acc: 0.1875
Global Iter: 1197000 training loss: 2.0434
Global Iter: 1197000 training acc: 0.21875
Global Iter: 1197100 training loss: 2.00815
Global Iter: 1197100 training acc: 0.1875
Global Iter: 1197200 training loss: 2.08174
Global Iter: 1197200 training acc: 0.1875
Global Iter: 1197300 training loss: 2.22166
Global Iter: 1197300 training acc: 0.09375
Global Iter: 1197400 training loss: 2.03378
Global Iter: 1197400 training acc: 0.25
Global Iter: 1197500 training loss: 1.91595
Global Iter: 1197500 training acc: 0.3125
Global Iter: 1197600 training loss: 1.89922
Global Iter: 1197600 training acc: 0.34375
Global Iter: 1197700 training loss: 1.9214
Global Iter: 1197700 training acc: 0.375
Global Iter: 1197800 training loss: 1.9427
Global Iter: 1197800 training acc: 0.15625
Global Iter: 1197900 training loss: 2.02821
Global Iter: 1197900 training acc: 0.25
Global Iter: 1198000 training loss: 1.95401
Global Iter: 1198000 training acc: 0.09375
Global Iter: 1198100 training loss: 2.03517
Global Iter: 1198100 training acc: 0.25
Global Iter: 1198200 training loss: 2.02183
Global Iter: 1198200 training acc: 0.15625
Global Iter: 1198300 training loss: 2.05713
Global Iter: 1198300 training acc: 0.1875
Global Iter: 1198400 training loss: 1.92146
Global Iter: 1198400 training acc: 0.1875
Global Iter: 1198500 training loss: 1.97515
Global Iter: 1198500 training acc: 0.21875
Global Iter: 1198600 training loss: 2.07095
Global Iter: 1198600 training acc: 0.15625
Global Iter: 1198700 training loss: 2.06549
Global Iter: 1198700 training acc: 0.15625
Global Iter: 1198800 training loss: 1.99604
Global Iter: 1198800 training acc: 0.25
Global Iter: 1198900 training loss: 1.99644
Global Iter: 1198900 training acc: 0.1875
Global Iter: 1199000 training loss: 1.94958
Global Iter: 1199000 training acc: 0.125
Global Iter: 1199100 training loss: 1.93729
Global Iter: 1199100 training acc: 0.09375
Global Iter: 1199200 training loss: 2.06783
Global Iter: 1199200 training acc: 0.1875
Global Iter: 1199300 training loss: 1.99909
Global Iter: 1199300 training acc: 0.1875
Global Iter: 1199400 training loss: 1.97661
Global Iter: 1199400 training acc: 0.1875
Global Iter: 1199500 training loss: 2.10911
Global Iter: 1199500 training acc: 0.125
Global Iter: 1199600 training loss: 2.05255
Global Iter: 1199600 training acc: 0.34375
Global Iter: 1199700 training loss: 1.98284
Global Iter: 1199700 training acc: 0.15625
Global Iter: 1199800 training loss: 1.89106
Global Iter: 1199800 training acc: 0.28125
Global Iter: 1199900 training loss: 2.02591
Global Iter: 1199900 training acc: 0.25
Global Iter: 1200000 training loss: 2.03131
Global Iter: 1200000 training acc: 0.1875
Global Iter: 1200100 training loss: 1.96888
Global Iter: 1200100 training acc: 0.0625
Global Iter: 1200200 training loss: 1.97172
Global Iter: 1200200 training acc: 0.28125
Global Iter: 1200300 training loss: 1.96067
Global Iter: 1200300 training acc: 0.1875
Global Iter: 1200400 training loss: 1.9896
Global Iter: 1200400 training acc: 0.15625
Global Iter: 1200500 trainin2017-06-22 15:26:40.972844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1203824
g loss: 1.95045
Global Iter: 1200500 training acc: 0.15625
Global Iter: 1200600 training loss: 1.88825
Global Iter: 1200600 training acc: 0.3125
Global Iter: 1200700 training loss: 2.03814
Global Iter: 1200700 training acc: 0.125
Global Iter: 1200800 training loss: 2.0034
Global Iter: 1200800 training acc: 0.3125
Global Iter: 1200900 training loss: 1.94286
Global Iter: 1200900 training acc: 0.15625
Global Iter: 1201000 training loss: 1.97728
Global Iter: 1201000 training acc: 0.0625
Global Iter: 1201100 training loss: 1.98987
Global Iter: 1201100 training acc: 0.09375
Global Iter: 1201200 training loss: 1.9935
Global Iter: 1201200 training acc: 0.25
Global Iter: 1201300 training loss: 1.9523
Global Iter: 1201300 training acc: 0.28125
Global Iter: 1201400 training loss: 1.99157
Global Iter: 1201400 training acc: 0.09375
Global Iter: 1201500 training loss: 2.00418
Global Iter: 1201500 training acc: 0.125
Global Iter: 1201600 training loss: 1.98273
Global Iter: 1201600 training acc: 0.15625
Global Iter: 1201700 training loss: 2.03998
Global Iter: 1201700 training acc: 0.15625
Global Iter: 1201800 training loss: 1.9641
Global Iter: 1201800 training acc: 0.25
Global Iter: 1201900 training loss: 1.95263
Global Iter: 1201900 training acc: 0.15625
Global Iter: 1202000 training loss: 1.96594
Global Iter: 1202000 training acc: 0.125
Global Iter: 1202100 training loss: 1.99188
Global Iter: 1202100 training acc: 0.1875
Global Iter: 1202200 training loss: 2.03775
Global Iter: 1202200 training acc: 0.125
Global Iter: 1202300 training loss: 2.02443
Global Iter: 1202300 training acc: 0.03125
Global Iter: 1202400 training loss: 2.05547
Global Iter: 1202400 training acc: 0.15625
Global Iter: 1202500 training loss: 1.93796
Global Iter: 1202500 training acc: 0.09375
Global Iter: 1202600 training loss: 1.99543
Global Iter: 1202600 training acc: 0.03125
Global Iter: 1202700 training loss: 2.1832
Global Iter: 1202700 training acc: 0.0625
Global Iter: 1202800 training loss: 2.06249
Global Iter: 1202800 training acc: 0.09375
Global Iter: 1202900 training loss: 1.98918
Global Iter: 1202900 training acc: 0.09375
Global Iter: 1203000 training loss: 2.02266
Global Iter: 1203000 training acc: 0.15625
Global Iter: 1203100 training loss: 2.05079
Global Iter: 1203100 training acc: 0.1875
Global Iter: 1203200 training loss: 1.93194
Global Iter: 1203200 training acc: 0.1875
Global Iter: 1203300 training loss: 1.94983
Global Iter: 1203300 training acc: 0.0625
Global Iter: 1203400 training loss: 1.95739
Global Iter: 1203400 training acc: 0.21875
Global Iter: 1203500 training loss: 1.97689
Global Iter: 1203500 training acc: 0.21875
Global Iter: 1203600 training loss: 1.96646
Global Iter: 1203600 training acc: 0.1875
Global Iter: 1203700 training loss: 1.99979
Global Iter: 1203700 training acc: 0.125
Global Iter: 1203800 training loss: 1.90886
Global Iter: 1203800 training acc: 0.28125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1203824
Number of Patches: 114846
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1203824
Global Iter: 1203900 training loss: 1.89084
Global Iter: 1203900 training acc: 0.21875
Global Iter: 1204000 training loss: 2.07129
Global Iter: 1204000 training acc: 0.125
Global Iter: 1204100 training loss: 1.91287
Global Iter: 1204100 training acc: 0.25
Global Iter: 1204200 training loss: 2.01479
Global Iter: 1204200 training acc: 0.15625
Global Iter: 1204300 training loss: 1.9608
Global Iter: 1204300 training acc: 0.1875
Global Iter: 1204400 training loss: 1.98069
Global Iter: 1204400 training acc: 0.125
Global Iter: 1204500 training loss: 1.86886
Global Iter: 1204500 training acc: 0.34375
Global Iter: 1204600 training loss: 2.002
Global Iter: 1204600 training acc: 0.21875
Global Iter: 1204700 training loss: 1.9795
Global Iter: 1204700 training acc: 0.125
Global Iter: 1204800 training loss: 1.98155
Global Iter: 1204800 training acc: 0.25
Global Iter: 1204900 training loss: 1.91812
Global Iter: 1204900 training acc: 0.28125
Global Iter: 1205000 training loss: 1.9778
Global Iter: 1205000 training acc: 0.15625
Global Iter: 1205100 training loss: 1.97599
Global Iter: 1205100 training acc: 0.125
Global Iter: 1205200 training loss: 2.00154
Global Iter: 1205200 training acc: 0.21875
Global Iter: 1205300 training loss: 2.03938
Global Iter: 1205300 training acc: 0.125
Global Iter: 1205400 training loss: 2.13916
Global Iter: 1205400 training acc: 0.125
Global Iter: 1205500 training loss: 1.93158
Global Iter: 1205500 training acc: 0.21875
Global Iter: 1205600 training loss: 2.11922
Global Iter: 1205600 training acc: 0.15625
Global Iter: 1205700 training loss: 2.08611
Global Iter: 1205700 training acc: 0.0625
Global Iter: 1205800 training loss: 2.02963
Global Iter: 1205800 training acc: 0.1875
Global Iter: 1205900 training loss: 2.09615
Global Iter: 1205900 training acc: 0.15625
Global Iter: 1206000 training loss: 2.019
Global Iter: 1206000 training acc: 0.15625
Global Iter: 1206100 training loss: 2.1064
Global Iter: 1206100 training acc: 0.125
Global Iter: 1206200 training loss: 2.10594
Global Iter: 1206200 training acc: 0.15625
Global Iter: 1206300 training loss: 1.93541
Global Iter: 1206300 training acc: 0.1875
Global Iter: 1206400 training loss: 2.01533
Global Iter: 1206400 training acc: 0.21875
Global Iter: 1206500 training loss: 2.10023
Global Iter: 1206500 training acc: 0.09375
Global Iter: 1206600 training loss: 1.8829
Global Iter: 1206600 training acc: 0.09375
Global Iter: 1206700 training loss: 1.95643
Global Iter: 1206700 training acc: 0.125
Global Iter: 1206800 training loss: 2.04098
Global Iter: 1206800 training acc: 0.09375
Global Iter: 1206900 training loss: 1.98555
Global Iter: 1206900 training acc: 0.25
Global Iter: 1207000 training loss: 2.02985
Global Iter: 1207000 training acc: 0.15625
Global Iter: 1207100 training loss: 1.88695
Global Iter: 1207100 training acc: 0.34375
Global Iter: 1207200 training loss: 1.98324
Global Iter: 1207200 training acc: 0.0625
Global Iter: 1207300 training loss: 1.95895
Global Iter: 1207300 training acc: 0.1875
Global Iter: 1207400 training loss: 1.99124
Global Iter: 1207400 training acc: 0.21875
Global Iter: 1207500 training loss: 2.04934
Global Iter: 1207500 training acc: 0.0625
Global Iter: 1207600 training loss: 2.02508
Global Iter: 1207600 training acc: 0.125
Global Iter: 1207700 training loss: 2.12382
Global Iter: 1207700 training acc: 0.0625
Global Iter: 1207800 training loss: 1.93465
Global Iter: 1207800 training acc: 0.25
Global Iter: 1207900 training loss: 2.0011
Global Iter: 1207900 training acc: 0.09375
Global Iter: 1208000 training loss: 1.9577
Global Iter: 1208000 training acc: 0.21875
Global Iter: 1208100 training loss: 1.92094
Global Iter: 1208100 training acc: 0.21875
Global Iter: 1208200 training loss: 2.0042
Global Iter: 1208200 training acc: 0.1875
Global Iter: 1208300 training loss: 2.0118
Global Iter: 1208300 training acc: 0.09375
Global Iter: 1208400 training loss: 1.93126
Global Iter: 1208400 training acc: 0.1875
Global Iter: 1208500 training loss: 1.9626
Global Iter: 1208500 training acc: 0.25
Global Iter: 1208600 training loss: 1.95637
Global Iter: 1208600 training acc: 0.28125
Global Iter: 1208700 training loss: 2.02833
Global Iter: 1208700 training acc: 0.09375
Global Iter: 1208800 training loss: 2.01067
Global Iter: 1208800 training acc: 0.125
Global Iter: 1208900 training loss: 2.04255
Global Iter: 1208900 training acc: 0.21875
Global Iter: 1209000 training loss: 2.03104
Global Iter: 1209000 training acc: 0.1875
Global Iter: 1209100 training loss: 1.9656
Global Iter: 1209100 training acc: 0.125
Global Iter: 1209200 training loss: 2.03115
Global Iter: 1209200 training acc: 0.125
Global Iter: 1209300 training loss: 2.03268
Global Iter: 1209300 training acc: 0.09375
Global Iter: 1209400 training loss: 1.94483
Global Iter: 1209400 training acc: 0.21875
Global Iter: 1209500 training loss: 2.00955
Global Iter: 1209500 training acc: 0.15625
Global Iter: 1209600 training loss: 2.21073
Global Iter: 1209600 training acc: 0.125
Global Iter: 1209700 training loss: 2.06507
Global Iter: 1209700 training acc: 0.21875
G2017-06-22 15:38:57.260259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1211002
lobal Iter: 1209800 training loss: 1.91352
Global Iter: 1209800 training acc: 0.15625
Global Iter: 1209900 training loss: 1.96728
Global Iter: 1209900 training acc: 0.125
Global Iter: 1210000 training loss: 2.107
Global Iter: 1210000 training acc: 0.125
Global Iter: 1210100 training loss: 2.05871
Global Iter: 1210100 training acc: 0.28125
Global Iter: 1210200 training loss: 1.89645
Global Iter: 1210200 training acc: 0.21875
Global Iter: 1210300 training loss: 1.93827
Global Iter: 1210300 training acc: 0.25
Global Iter: 1210400 training loss: 2.01709
Global Iter: 1210400 training acc: 0.15625
Global Iter: 1210500 training loss: 1.95871
Global Iter: 1210500 training acc: 0.25
Global Iter: 1210600 training loss: 2.00551
Global Iter: 1210600 training acc: 0.15625
Global Iter: 1210700 training loss: 1.97676
Global Iter: 1210700 training acc: 0.375
Global Iter: 1210800 training loss: 2.05922
Global Iter: 1210800 training acc: 0.15625
Global Iter: 1210900 training loss: 2.03156
Global Iter: 1210900 training acc: 0.15625
Global Iter: 1211000 training loss: 1.96198
Global Iter: 1211000 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1211002
Number of Patches: 113698
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1211002
Global Iter: 1211100 training loss: 2.04534
Global Iter: 1211100 training acc: 0.1875
Global Iter: 1211200 training loss: 1.94018
Global Iter: 1211200 training acc: 0.3125
Global Iter: 1211300 training loss: 1.97351
Global Iter: 1211300 training acc: 0.09375
Global Iter: 1211400 training loss: 2.00621
Global Iter: 1211400 training acc: 0.125
Global Iter: 1211500 training loss: 2.05101
Global Iter: 1211500 training acc: 0.09375
Global Iter: 1211600 training loss: 1.90487
Global Iter: 1211600 training acc: 0.25
Global Iter: 1211700 training loss: 2.00847
Global Iter: 1211700 training acc: 0.03125
Global Iter: 1211800 training loss: 1.91829
Global Iter: 1211800 training acc: 0.15625
Global Iter: 1211900 training loss: 2.15249
Global Iter: 1211900 training acc: 0.21875
Global Iter: 1212000 training loss: 2.03548
Global Iter: 1212000 training acc: 0.125
Global Iter: 1212100 training loss: 2.17951
Global Iter: 1212100 training acc: 0.03125
Global Iter: 1212200 training loss: 2.00087
Global Iter: 1212200 training acc: 0.28125
Global Iter: 1212300 training loss: 1.95782
Global Iter: 1212300 training acc: 0.28125
Global Iter: 1212400 training loss: 1.96423
Global Iter: 1212400 training acc: 0.25
Global Iter: 1212500 training loss: 2.03463
Global Iter: 1212500 training acc: 0.15625
Global Iter: 1212600 training loss: 1.97833
Global Iter: 1212600 training acc: 0.25
Global Iter: 1212700 training loss: 1.90258
Global Iter: 1212700 training acc: 0.1875
Global Iter: 1212800 training loss: 1.9716
Global Iter: 1212800 training acc: 0.15625
Global Iter: 1212900 training loss: 2.02809
Global Iter: 1212900 training acc: 0.25
Global Iter: 1213000 training loss: 1.96137
Global Iter: 1213000 training acc: 0.1875
Global Iter: 1213100 training loss: 2.01441
Global Iter: 1213100 training acc: 0.28125
Global Iter: 1213200 training loss: 1.9702
Global Iter: 1213200 training acc: 0.1875
Global Iter: 1213300 training loss: 1.87556
Global Iter: 1213300 training acc: 0.15625
Global Iter: 1213400 training loss: 2.02793
Global Iter: 1213400 training acc: 0.21875
Global Iter: 1213500 training loss: 2.03302
Global Iter: 1213500 training acc: 0.1875
Global Iter: 1213600 training loss: 2.02097
Global Iter: 1213600 training acc: 0.09375
Global Iter: 1213700 training loss: 1.99521
Global Iter: 1213700 training acc: 0.125
Global Iter: 1213800 training loss: 2.04263
Global Iter: 1213800 training acc: 0.0625
Global Iter: 1213900 training loss: 1.94737
Global Iter: 1213900 training acc: 0.21875
Global Iter: 1214000 training loss: 1.99243
Global Iter: 1214000 training acc: 0.1875
Global Iter: 1214100 training loss: 1.95374
Global Iter: 1214100 training acc: 0.21875
Global Iter: 1214200 training loss: 1.95834
Global Iter: 1214200 training acc: 0.125
Gl2017-06-22 15:51:03.546251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1218109
obal Iter: 1214300 training loss: 2.02739
Global Iter: 1214300 training acc: 0.15625
Global Iter: 1214400 training loss: 2.08616
Global Iter: 1214400 training acc: 0.125
Global Iter: 1214500 training loss: 2.06064
Global Iter: 1214500 training acc: 0.125
Global Iter: 1214600 training loss: 1.9521
Global Iter: 1214600 training acc: 0.34375
Global Iter: 1214700 training loss: 2.00733
Global Iter: 1214700 training acc: 0.03125
Global Iter: 1214800 training loss: 1.94959
Global Iter: 1214800 training acc: 0.21875
Global Iter: 1214900 training loss: 1.90357
Global Iter: 1214900 training acc: 0.25
Global Iter: 1215000 training loss: 2.11677
Global Iter: 1215000 training acc: 0.15625
Global Iter: 1215100 training loss: 2.20327
Global Iter: 1215100 training acc: 0.03125
Global Iter: 1215200 training loss: 1.91069
Global Iter: 1215200 training acc: 0.1875
Global Iter: 1215300 training loss: 1.99648
Global Iter: 1215300 training acc: 0.25
Global Iter: 1215400 training loss: 2.07544
Global Iter: 1215400 training acc: 0.125
Global Iter: 1215500 training loss: 2.00869
Global Iter: 1215500 training acc: 0.15625
Global Iter: 1215600 training loss: 2.1274
Global Iter: 1215600 training acc: 0.09375
Global Iter: 1215700 training loss: 1.90041
Global Iter: 1215700 training acc: 0.15625
Global Iter: 1215800 training loss: 1.99525
Global Iter: 1215800 training acc: 0.25
Global Iter: 1215900 training loss: 1.96936
Global Iter: 1215900 training acc: 0.09375
Global Iter: 1216000 training loss: 1.91167
Global Iter: 1216000 training acc: 0.28125
Global Iter: 1216100 training loss: 1.96508
Global Iter: 1216100 training acc: 0.25
Global Iter: 1216200 training loss: 2.12169
Global Iter: 1216200 training acc: 0.25
Global Iter: 1216300 training loss: 2.04835
Global Iter: 1216300 training acc: 0.03125
Global Iter: 1216400 training loss: 2.06836
Global Iter: 1216400 training acc: 0.09375
Global Iter: 1216500 training loss: 1.94525
Global Iter: 1216500 training acc: 0.25
Global Iter: 1216600 training loss: 1.96008
Global Iter: 1216600 training acc: 0.15625
Global Iter: 1216700 training loss: 1.92511
Global Iter: 1216700 training acc: 0.21875
Global Iter: 1216800 training loss: 2.05018
Global Iter: 1216800 training acc: 0.1875
Global Iter: 1216900 training loss: 1.99006
Global Iter: 1216900 training acc: 0.25
Global Iter: 1217000 training loss: 2.0116
Global Iter: 1217000 training acc: 0.0625
Global Iter: 1217100 training loss: 1.92924
Global Iter: 1217100 training acc: 0.28125
Global Iter: 1217200 training loss: 2.07888
Global Iter: 1217200 training acc: 0.125
Global Iter: 1217300 training loss: 2.00067
Global Iter: 1217300 training acc: 0.25
Global Iter: 1217400 training loss: 1.95959
Global Iter: 1217400 training acc: 0.21875
Global Iter: 1217500 training loss: 1.90227
Global Iter: 1217500 training acc: 0.3125
Global Iter: 1217600 training loss: 1.95836
Global Iter: 1217600 training acc: 0.25
Global Iter: 1217700 training loss: 2.10579
Global Iter: 1217700 training acc: 0.125
Global Iter: 1217800 training loss: 1.91387
Global Iter: 1217800 training acc: 0.25
Global Iter: 1217900 training loss: 2.15181
Global Iter: 1217900 training acc: 0.0625
Global Iter: 1218000 training loss: 2.0701
Global Iter: 1218000 training acc: 0.15625
Global Iter: 1218100 training loss: 2.10322
Global Iter: 1218100 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1218109
Number of Patches: 112562
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1218109
Global Iter: 1218200 training loss: 1.92833
Global Iter: 1218200 training acc: 0.28125
Global Iter: 1218300 training loss: 1.9183
Global Iter: 1218300 training acc: 0.21875
Global Iter: 1218400 training loss: 1.97791
Global Iter: 1218400 training acc: 0.1875
Global Iter: 1218500 training loss: 2.10313
Global Iter: 1218500 training acc: 0.21875
Global Iter: 1218600 training loss: 1.90258
Global Iter: 1218600 training acc: 0.21875
Global Iter: 1218700 training loss: 1.90783
Global Iter: 1218700 training acc: 0.25
Global Iter: 1218800 training loss: 1.90724
Global Iter: 1218800 training acc: 0.125
Global Iter: 1218900 training loss: 1.86729
Global Iter: 1218900 training acc: 0.3125
Global Iter: 1219000 training loss: 2.01292
Global Iter: 1219000 training acc: 0.1875
Global Iter: 1219100 training loss: 1.93321
Global Iter: 1219100 training acc: 0.15625
Global Iter: 1219200 training loss: 1.94113
Global Iter: 1219200 training acc: 0.34375
Global Iter: 1219300 training loss: 1.92324
Global Iter: 1219300 training acc: 0.0625
Global Iter: 1219400 training loss: 2.10548
Global Iter: 1219400 training acc: 0.125
Global Iter: 1219500 training loss: 1.93292
Global Iter: 1219500 training acc: 0.28125
Global Iter: 1219600 training loss: 2.00667
Global Iter: 1219600 training acc: 0.09375
Global Iter: 1219700 training loss: 1.97986
Global Iter: 1219700 training acc: 0.21875
Global Iter: 1219800 training loss: 1.95579
Global Iter: 1219800 training acc: 0.0625
Global Iter: 1219900 training loss: 2.00449
Global Iter: 1219900 training acc: 0.1875
Global Iter: 1220000 training loss: 1.95146
Global Iter: 1220000 training acc: 0.1875
Global Iter: 1220100 training loss: 1.94912
Global Iter: 1220100 training acc: 0.1875
Global Iter: 1220200 training loss: 2.00225
Global Iter: 1220200 training acc: 0.1875
Global Iter: 1220300 training loss: 1.90326
Global Iter: 1220300 training acc: 0.15625
Global Iter: 1220400 training loss: 1.9926
Global Iter: 1220400 training acc: 0.15625
Global Iter: 1220500 training loss: 1.94189
Global Iter: 1220500 training acc: 0.1875
Global Iter: 1220600 training loss: 1.98747
Global Iter: 1220600 training acc: 0.21875
Global Iter: 1220700 training loss: 1.90503
Global Iter: 1220700 training acc: 0.34375
Global Iter: 1220800 training loss: 1.98529
Global Iter: 1220800 training acc: 0.21875
Global Iter: 1220900 training loss: 1.96176
Global Iter: 1220900 training acc: 0.125
Global Iter: 1221000 training loss: 1.99631
Global Iter: 1221000 training acc: 0.0625
Global Iter: 1221100 training loss: 2.03297
Global Iter: 1221100 training acc: 0.21875
Global Iter: 1221200 training loss: 1.92759
Global Iter: 1221200 training acc: 0.15625
Global Iter: 1221300 training loss: 1.96857
Global Iter: 1221300 training acc: 0.1875
Global Iter: 1221400 training loss: 2.04412
Global Iter: 1221400 training acc: 0.1875
Global Iter: 1221500 training loss: 1.93604
Global Iter: 1221500 training acc: 0.21875
Global Iter: 1221600 training loss: 1.99021
Global Iter: 1221600 training acc: 0.21875
Global Iter: 1221700 training loss: 2.02704
Global Iter: 1221700 training acc: 0.1875
Global Iter: 1221800 training loss: 1.99045
Global Iter: 1221800 training acc: 0.09375
Global Iter: 1221900 training loss: 1.86171
Global Iter: 1221900 training acc: 0.34375
Global Iter: 1222000 training loss: 2.02739
Global Iter: 1222000 training acc: 0.09375
Global Iter: 1222100 training loss: 1.8729
Global Iter: 1222100 training acc: 0.25
Global Iter: 1222200 training loss: 1.9076
Global Iter: 1222200 training acc: 0.28125
Global Iter: 1222300 training loss: 2.00594
Global Iter: 1222300 training acc: 0.15625
Global Iter: 1222400 training loss: 2.13527
Global Iter: 1222400 training acc: 0.125
Global Iter: 1222500 training loss: 1.94233
Global Iter: 1222500 training acc: 0.1875
Global Iter: 1222600 training loss: 2.07472
Global Iter: 1222600 training acc: 0.125
Global Iter: 1222700 training loss: 1.98246
Global Iter: 1222700 training acc: 0.25
Global Iter: 1222800 training loss: 1.91968
Global Iter: 1222800 training acc: 0.21875
Global Iter: 1222900 training loss: 2.06017
Global Iter: 1222900 training acc: 0.0625
Global Iter: 1223000 training loss: 1.95557
Global Iter: 1223000 training acc: 0.25
Global Iter: 1223100 training loss: 1.96014
Global Iter: 1223100 training acc: 0.125
Global Iter: 1223200 training loss: 2.00418
Global Iter: 1223200 training acc: 0.1875
Global Iter: 1223300 training loss: 1.94595
Global Iter: 1223300 training acc: 0.21875
Global Iter: 1223400 training loss: 1.93853
Global Iter: 1223400 training acc: 0.21875
Global Iter: 1223500 training loss: 2.23988
Global Iter: 122017-06-22 16:02:58.639202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1225145
23500 training acc: 0.15625
Global Iter: 1223600 training loss: 2.04442
Global Iter: 1223600 training acc: 0.125
Global Iter: 1223700 training loss: 1.97932
Global Iter: 1223700 training acc: 0.09375
Global Iter: 1223800 training loss: 1.95753
Global Iter: 1223800 training acc: 0.09375
Global Iter: 1223900 training loss: 2.01039
Global Iter: 1223900 training acc: 0.1875
Global Iter: 1224000 training loss: 2.08955
Global Iter: 1224000 training acc: 0.15625
Global Iter: 1224100 training loss: 2.00774
Global Iter: 1224100 training acc: 0.1875
Global Iter: 1224200 training loss: 1.99677
Global Iter: 1224200 training acc: 0.1875
Global Iter: 1224300 training loss: 1.95128
Global Iter: 1224300 training acc: 0.1875
Global Iter: 1224400 training loss: 2.04678
Global Iter: 1224400 training acc: 0.125
Global Iter: 1224500 training loss: 1.99429
Global Iter: 1224500 training acc: 0.21875
Global Iter: 1224600 training loss: 1.91782
Global Iter: 1224600 training acc: 0.21875
Global Iter: 1224700 training loss: 1.96042
Global Iter: 1224700 training acc: 0.1875
Global Iter: 1224800 training loss: 2.03011
Global Iter: 1224800 training acc: 0.1875
Global Iter: 1224900 training loss: 1.96828
Global Iter: 1224900 training acc: 0.28125
Global Iter: 1225000 training loss: 2.00587
Global Iter: 1225000 training acc: 0.1875
Global Iter: 1225100 training loss: 1.95958
Global Iter: 1225100 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1225145
Number of Patches: 111437
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1225145
Global Iter: 1225200 training loss: 2.06678
Global Iter: 1225200 training acc: 0.1875
Global Iter: 1225300 training loss: 2.1313
Global Iter: 1225300 training acc: 0.15625
Global Iter: 1225400 training loss: 1.96468
Global Iter: 1225400 training acc: 0.15625
Global Iter: 1225500 training loss: 1.89623
Global Iter: 1225500 training acc: 0.3125
Global Iter: 1225600 training loss: 2.04768
Global Iter: 1225600 training acc: 0.09375
Global Iter: 1225700 training loss: 2.02579
Global Iter: 1225700 training acc: 0.09375
Global Iter: 1225800 training loss: 1.99488
Global Iter: 1225800 training acc: 0.15625
Global Iter: 1225900 training loss: 1.94568
Global Iter: 1225900 training acc: 0.28125
Global Iter: 1226000 training loss: 2.02909
Global Iter: 1226000 training acc: 0.15625
Global Iter: 1226100 training loss: 1.92775
Global Iter: 1226100 training acc: 0.1875
Global Iter: 1226200 training loss: 2.04207
Global Iter: 1226200 training acc: 0.15625
Global Iter: 1226300 training loss: 1.95908
Global Iter: 1226300 training acc: 0.25
Global Iter: 1226400 training loss: 2.0722
Global Iter: 1226400 training acc: 0.125
Global Iter: 1226500 training loss: 1.95904
Global Iter: 1226500 training acc: 0.25
Global Iter: 1226600 training loss: 1.95043
Global Iter: 1226600 training acc: 0.28125
Global Iter: 1226700 training loss: 1.9509
Global Iter: 1226700 training acc: 0.15625
Global Iter: 1226800 training loss: 2.01319
Global Iter: 1226800 training acc: 0.1875
Global Iter: 1226900 training loss: 1.92568
Global Iter: 1226900 training acc: 0.25
Global Iter: 1227000 training loss: 1.94051
Global Iter: 1227000 training acc: 0.21875
Global Iter: 1227100 training loss: 1.95581
Global Iter: 1227100 training acc: 0.375
Global Iter: 1227200 training loss: 1.91574
Global Iter: 1227200 training acc: 0.15625
Global Iter: 1227300 training loss: 1.99303
Global Iter: 1227300 training acc: 0.1875
Global Iter: 1227400 training loss: 2.04284
Global Iter: 1227400 training acc: 0.15625
Global Iter: 1227500 training loss: 1.987
Global Iter: 1227500 training acc: 0.1875
Global Iter: 1227600 training loss: 1.9031
Global Iter: 1227600 training acc: 0.09375
Global Iter: 1227700 training loss: 1.89292
Global Iter: 1227700 training acc: 0.34375
Global Iter: 1227800 training loss: 1.98073
Global Iter: 1227800 training acc: 0.1875
Global Iter: 1227900 training loss: 2.00203
Global Iter: 1227900 training acc: 0.15625
Global Iter: 1228000 training loss: 1.97409
Global I2017-06-22 16:14:41.232827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1232110
ter: 1228000 training acc: 0.25
Global Iter: 1228100 training loss: 1.95869
Global Iter: 1228100 training acc: 0.125
Global Iter: 1228200 training loss: 2.00749
Global Iter: 1228200 training acc: 0.15625
Global Iter: 1228300 training loss: 2.08598
Global Iter: 1228300 training acc: 0.125
Global Iter: 1228400 training loss: 2.06589
Global Iter: 1228400 training acc: 0.21875
Global Iter: 1228500 training loss: 2.01171
Global Iter: 1228500 training acc: 0.15625
Global Iter: 1228600 training loss: 1.98047
Global Iter: 1228600 training acc: 0.09375
Global Iter: 1228700 training loss: 1.95533
Global Iter: 1228700 training acc: 0.09375
Global Iter: 1228800 training loss: 1.9805
Global Iter: 1228800 training acc: 0.0625
Global Iter: 1228900 training loss: 2.01388
Global Iter: 1228900 training acc: 0.09375
Global Iter: 1229000 training loss: 2.00574
Global Iter: 1229000 training acc: 0.15625
Global Iter: 1229100 training loss: 1.99565
Global Iter: 1229100 training acc: 0.3125
Global Iter: 1229200 training loss: 1.96825
Global Iter: 1229200 training acc: 0.1875
Global Iter: 1229300 training loss: 1.97738
Global Iter: 1229300 training acc: 0.25
Global Iter: 1229400 training loss: 1.98009
Global Iter: 1229400 training acc: 0.15625
Global Iter: 1229500 training loss: 2.01103
Global Iter: 1229500 training acc: 0.21875
Global Iter: 1229600 training loss: 2.01998
Global Iter: 1229600 training acc: 0.25
Global Iter: 1229700 training loss: 2.13097
Global Iter: 1229700 training acc: 0.21875
Global Iter: 1229800 training loss: 2.04832
Global Iter: 1229800 training acc: 0.09375
Global Iter: 1229900 training loss: 1.87821
Global Iter: 1229900 training acc: 0.375
Global Iter: 1230000 training loss: 2.13957
Global Iter: 1230000 training acc: 0.1875
Global Iter: 1230100 training loss: 2.04436
Global Iter: 1230100 training acc: 0.34375
Global Iter: 1230200 training loss: 1.84183
Global Iter: 1230200 training acc: 0.28125
Global Iter: 1230300 training loss: 1.92125
Global Iter: 1230300 training acc: 0.28125
Global Iter: 1230400 training loss: 2.00571
Global Iter: 1230400 training acc: 0.21875
Global Iter: 1230500 training loss: 1.97882
Global Iter: 1230500 training acc: 0.1875
Global Iter: 1230600 training loss: 1.95779
Global Iter: 1230600 training acc: 0.09375
Global Iter: 1230700 training loss: 2.02661
Global Iter: 1230700 training acc: 0.15625
Global Iter: 1230800 training loss: 2.01642
Global Iter: 1230800 training acc: 0.21875
Global Iter: 1230900 training loss: 2.04472
Global Iter: 1230900 training acc: 0.125
Global Iter: 1231000 training loss: 1.94559
Global Iter: 1231000 training acc: 0.21875
Global Iter: 1231100 training loss: 1.99907
Global Iter: 1231100 training acc: 0.1875
Global Iter: 1231200 training loss: 2.03903
Global Iter: 1231200 training acc: 0.0625
Global Iter: 1231300 training loss: 1.90803
Global Iter: 1231300 training acc: 0.21875
Global Iter: 1231400 training loss: 1.98092
Global Iter: 1231400 training acc: 0.125
Global Iter: 1231500 training loss: 1.99826
Global Iter: 1231500 training acc: 0.25
Global Iter: 1231600 training loss: 1.96644
Global Iter: 1231600 training acc: 0.1875
Global Iter: 1231700 training loss: 1.94285
Global Iter: 1231700 training acc: 0.25
Global Iter: 1231800 training loss: 2.00679
Global Iter: 1231800 training acc: 0.09375
Global Iter: 1231900 training loss: 1.97287
Global Iter: 1231900 training acc: 0.125
Global Iter: 1232000 training loss: 1.92102
Global Iter: 1232000 training acc: 0.25
Global Iter: 1232100 training loss: 1.93374
Global Iter: 1232100 training acc: 0.3125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1232110
Number of Patches: 110323
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1232110
Global Iter: 1232200 training loss: 1.87594
Global Iter: 1232200 training acc: 0.125
Global Iter: 1232300 training loss: 2.0501
Global Iter: 1232300 training acc: 0.1875
Global Iter: 1232400 training loss: 2.04981
Global Iter: 1232400 training acc: 0.125
Global Iter: 1232500 training loss: 1.94692
Global Iter: 1232500 training acc: 0.15625
Global Iter: 1232600 training loss: 1.88949
Global Iter: 1232600 training acc: 0.25
Global Iter: 1232700 training loss: 1.91855
Global Iter: 1232700 training acc: 0.1875
Global Iter: 1232800 training loss: 2.05126
Global Iter: 1232800 training acc: 0.15625
Global Iter: 1232900 training loss: 1.98389
Global Iter: 1232900 training acc: 0.1875
Global Iter: 1233000 training loss: 1.97515
Global Iter: 1233000 training acc: 0.09375
Global Iter: 1233100 training loss: 2.06431
Global Iter: 1233100 training acc: 0.1875
Global Iter: 1233200 training loss: 1.92015
Global Iter: 1233200 training acc: 0.15625
Global Iter: 1233300 training loss: 1.97306
Global Iter: 1233300 training acc: 0.21875
Global Iter: 1233400 training loss: 2.04246
Global Iter: 1233400 training acc: 0.21875
Global Iter: 1233500 training loss: 2.02625
Global Iter: 1233500 training acc: 0.0625
Global Iter: 1233600 training loss: 1.94766
Global Iter: 1233600 training acc: 0.25
Global Iter: 1233700 training loss: 1.98669
Global Iter: 1233700 training acc: 0.25
Global Iter: 1233800 training loss: 2.05322
Global Iter: 1233800 training acc: 0.1875
Global Iter: 1233900 training loss: 2.06214
Global Iter: 1233900 training acc: 0.1875
Global Iter: 1234000 training loss: 1.90362
Global Iter: 1234000 training acc: 0.21875
Global Iter: 1234100 training loss: 1.93904
Global Iter: 1234100 training acc: 0.15625
Global Iter: 1234200 training loss: 1.96404
Global Iter: 1234200 training acc: 0.125
Global Iter: 1234300 training loss: 1.90678
Global Iter: 1234300 training acc: 0.1875
Global Iter: 1234400 training loss: 1.96871
Global Iter: 1234400 training acc: 0.1875
Global Iter: 1234500 training loss: 2.06939
Global Iter: 1234500 training acc: 0.28125
Global Iter: 1234600 training loss: 1.99774
Global Iter: 1234600 training acc: 0.09375
Global Iter: 1234700 training loss: 1.94337
Global Iter: 1234700 training acc: 0.28125
Global Iter: 1234800 training loss: 1.97826
Global Iter: 1234800 training acc: 0.1875
Global Iter: 1234900 training loss: 2.01687
Global Iter: 1234900 training acc: 0.125
Global Iter: 1235000 training loss: 2.16403
Global Iter: 1235000 training acc: 0.09375
Global Iter: 1235100 training loss: 2.17592
Global Iter: 1235100 training acc: 0.1875
Global Iter: 1235200 training loss: 1.9774
Global Iter: 1235200 training acc: 0.125
Global Iter: 1235300 training loss: 1.97461
Global Iter: 1235300 training acc: 0.3125
Global Iter: 1235400 training loss: 1.9769
Global Iter: 1235400 training acc: 0.25
Global Iter: 1235500 training loss: 1.8665
Global Iter: 1235500 training acc: 0.3125
Global Iter: 1235600 training loss: 1.96731
Global Iter: 1235600 training acc: 0.1875
Global Iter: 1235700 training loss: 2.00894
Global Iter: 1235700 training acc: 0.1875
Global Iter: 1235800 training loss: 1.96211
Global Iter: 1235800 training acc: 0.28125
Global Iter: 1235900 training loss: 1.99338
Global Iter: 1235900 training acc: 0.21875
Global Iter: 1236000 training loss: 2.00912
Global Iter: 1236000 training acc: 0.1875
Global Iter: 1236100 training loss: 1.97412
Global Iter: 1236100 training acc: 0.21875
Global Iter: 1236200 training loss: 1.99017
Global Iter: 1236200 training acc: 0.125
Global Iter: 1236300 training loss: 1.94755
Global Iter: 1236300 training acc: 0.21875
Global Iter: 1236400 training loss: 2.15026
Global Iter: 1236400 training acc: 0.03125
Global Iter: 1236500 training loss: 2.09542
Global Iter: 1236500 training acc: 0.125
Global Iter: 1236600 training loss: 1.99199
Global Iter: 1236600 training acc: 0.125
Global Iter: 1236700 training loss: 2.07733
Global Iter: 1236700 training acc: 0.21875
Global Iter: 1236800 training loss: 1.9653
Global Iter: 1236800 training acc: 0.25
Global Iter: 1236900 training loss: 2.03098
Global Iter: 1236900 training acc: 0.1875
Global Iter: 1237000 training loss: 2.04978
Global Iter: 1237000 training acc: 0.28125
Global Iter: 1237100 training loss: 2.05529
Global Iter: 1237100 training acc: 0.125
Global Iter: 1237200 training loss: 2.03612
Global Iter: 1237200 training acc: 0.1875
Global Iter: 1237300 tra2017-06-22 16:26:22.386538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1239006
ining loss: 1.96045
Global Iter: 1237300 training acc: 0.25
Global Iter: 1237400 training loss: 1.93267
Global Iter: 1237400 training acc: 0.21875
Global Iter: 1237500 training loss: 1.96706
Global Iter: 1237500 training acc: 0.25
Global Iter: 1237600 training loss: 1.97034
Global Iter: 1237600 training acc: 0.21875
Global Iter: 1237700 training loss: 1.95452
Global Iter: 1237700 training acc: 0.15625
Global Iter: 1237800 training loss: 1.99511
Global Iter: 1237800 training acc: 0.34375
Global Iter: 1237900 training loss: 1.93285
Global Iter: 1237900 training acc: 0.375
Global Iter: 1238000 training loss: 2.09515
Global Iter: 1238000 training acc: 0.1875
Global Iter: 1238100 training loss: 1.9469
Global Iter: 1238100 training acc: 0.125
Global Iter: 1238200 training loss: 1.91442
Global Iter: 1238200 training acc: 0.1875
Global Iter: 1238300 training loss: 2.00324
Global Iter: 1238300 training acc: 0.09375
Global Iter: 1238400 training loss: 2.05191
Global Iter: 1238400 training acc: 0.1875
Global Iter: 1238500 training loss: 1.92849
Global Iter: 1238500 training acc: 0.25
Global Iter: 1238600 training loss: 1.88778
Global Iter: 1238600 training acc: 0.25
Global Iter: 1238700 training loss: 2.01521
Global Iter: 1238700 training acc: 0.1875
Global Iter: 1238800 training loss: 1.90799
Global Iter: 1238800 training acc: 0.3125
Global Iter: 1238900 training loss: 2.0227
Global Iter: 1238900 training acc: 0.21875
Global Iter: 1239000 training loss: 2.01141
Global Iter: 1239000 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1239006
Number of Patches: 109220
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1239006
Global Iter: 1239100 training loss: 1.97881
Global Iter: 1239100 training acc: 0.3125
Global Iter: 1239200 training loss: 2.1305
Global Iter: 1239200 training acc: 0.1875
Global Iter: 1239300 training loss: 2.08859
Global Iter: 1239300 training acc: 0.1875
Global Iter: 1239400 training loss: 2.02209
Global Iter: 1239400 training acc: 0.125
Global Iter: 1239500 training loss: 1.96077
Global Iter: 1239500 training acc: 0.21875
Global Iter: 1239600 training loss: 1.98615
Global Iter: 1239600 training acc: 0.09375
Global Iter: 1239700 training loss: 2.06898
Global Iter: 1239700 training acc: 0.09375
Global Iter: 1239800 training loss: 1.92261
Global Iter: 1239800 training acc: 0.125
Global Iter: 1239900 training loss: 2.02806
Global Iter: 1239900 training acc: 0.21875
Global Iter: 1240000 training loss: 2.05395
Global Iter: 1240000 training acc: 0.09375
Global Iter: 1240100 training loss: 1.95502
Global Iter: 1240100 training acc: 0.21875
Global Iter: 1240200 training loss: 2.01611
Global Iter: 1240200 training acc: 0.1875
Global Iter: 1240300 training loss: 1.93803
Global Iter: 1240300 training acc: 0.15625
Global Iter: 1240400 training loss: 1.92664
Global Iter: 1240400 training acc: 0.15625
Global Iter: 1240500 training loss: 2.09685
Global Iter: 1240500 training acc: 0.09375
Global Iter: 1240600 training loss: 1.95822
Global Iter: 1240600 training acc: 0.09375
Global Iter: 1240700 training loss: 1.99707
Global Iter: 1240700 training acc: 0.21875
Global Iter: 1240800 training loss: 2.02393
Global Iter: 1240800 training acc: 0.1875
Global Iter: 1240900 training loss: 2.06305
Global Iter: 1240900 training acc: 0.125
Global Iter: 1241000 training loss: 2.01673
Global Iter: 1241000 training acc: 0.1875
Global Iter: 1241100 training loss: 2.04966
Global Iter: 1241100 training acc: 0.15625
Global Iter: 1241200 training loss: 1.92362
Global Iter: 1241200 training acc: 0.09375
Global Iter: 1241300 training loss: 1.92857
Global Iter: 1241300 training acc: 0.1875
Global Iter: 1241400 training loss: 2.05354
Global Iter: 1241400 training acc: 0.25
Global Iter: 1241500 training loss: 2.01628
Global Iter: 1241500 training acc: 0.28125
Global Iter: 1241600 training loss: 1.93532
Global Iter: 1241600 training acc: 0.25
Global Iter: 1241700 training loss: 2.12835
Global Iter: 1241700 training acc: 0.0625
Global Iter: 1241800 tra2017-06-22 16:37:56.001633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1245833
ining loss: 2.0052
Global Iter: 1241800 training acc: 0.1875
Global Iter: 1241900 training loss: 1.89544
Global Iter: 1241900 training acc: 0.0625
Global Iter: 1242000 training loss: 1.98081
Global Iter: 1242000 training acc: 0.15625
Global Iter: 1242100 training loss: 1.884
Global Iter: 1242100 training acc: 0.40625
Global Iter: 1242200 training loss: 1.95383
Global Iter: 1242200 training acc: 0.25
Global Iter: 1242300 training loss: 2.02981
Global Iter: 1242300 training acc: 0.25
Global Iter: 1242400 training loss: 1.99201
Global Iter: 1242400 training acc: 0.1875
Global Iter: 1242500 training loss: 1.9998
Global Iter: 1242500 training acc: 0.21875
Global Iter: 1242600 training loss: 1.94205
Global Iter: 1242600 training acc: 0.21875
Global Iter: 1242700 training loss: 2.09526
Global Iter: 1242700 training acc: 0.0625
Global Iter: 1242800 training loss: 1.95345
Global Iter: 1242800 training acc: 0.125
Global Iter: 1242900 training loss: 1.91129
Global Iter: 1242900 training acc: 0.21875
Global Iter: 1243000 training loss: 1.89668
Global Iter: 1243000 training acc: 0.375
Global Iter: 1243100 training loss: 1.94758
Global Iter: 1243100 training acc: 0.1875
Global Iter: 1243200 training loss: 2.00811
Global Iter: 1243200 training acc: 0.15625
Global Iter: 1243300 training loss: 2.08075
Global Iter: 1243300 training acc: 0.21875
Global Iter: 1243400 training loss: 1.99659
Global Iter: 1243400 training acc: 0.15625
Global Iter: 1243500 training loss: 1.98106
Global Iter: 1243500 training acc: 0.15625
Global Iter: 1243600 training loss: 1.98229
Global Iter: 1243600 training acc: 0.25
Global Iter: 1243700 training loss: 1.99864
Global Iter: 1243700 training acc: 0.34375
Global Iter: 1243800 training loss: 1.9368
Global Iter: 1243800 training acc: 0.1875
Global Iter: 1243900 training loss: 1.96541
Global Iter: 1243900 training acc: 0.1875
Global Iter: 1244000 training loss: 2.1089
Global Iter: 1244000 training acc: 0.09375
Global Iter: 1244100 training loss: 1.9772
Global Iter: 1244100 training acc: 0.15625
Global Iter: 1244200 training loss: 2.0444
Global Iter: 1244200 training acc: 0.125
Global Iter: 1244300 training loss: 1.9664
Global Iter: 1244300 training acc: 0.28125
Global Iter: 1244400 training loss: 1.9277
Global Iter: 1244400 training acc: 0.21875
Global Iter: 1244500 training loss: 2.02386
Global Iter: 1244500 training acc: 0.21875
Global Iter: 1244600 training loss: 1.89478
Global Iter: 1244600 training acc: 0.21875
Global Iter: 1244700 training loss: 1.94091
Global Iter: 1244700 training acc: 0.15625
Global Iter: 1244800 training loss: 1.96866
Global Iter: 1244800 training acc: 0.15625
Global Iter: 1244900 training loss: 2.11134
Global Iter: 1244900 training acc: 0.125
Global Iter: 1245000 training loss: 1.90235
Global Iter: 1245000 training acc: 0.21875
Global Iter: 1245100 training loss: 1.98722
Global Iter: 1245100 training acc: 0.1875
Global Iter: 1245200 training loss: 1.91641
Global Iter: 1245200 training acc: 0.21875
Global Iter: 1245300 training loss: 1.90313
Global Iter: 1245300 training acc: 0.25
Global Iter: 1245400 training loss: 2.10613
Global Iter: 1245400 training acc: 0.1875
Global Iter: 1245500 training loss: 1.9471
Global Iter: 1245500 training acc: 0.21875
Global Iter: 1245600 training loss: 2.02446
Global Iter: 1245600 training acc: 0.21875
Global Iter: 1245700 training loss: 2.00449
Global Iter: 1245700 training acc: 0.125
Global Iter: 1245800 training loss: 1.94253
Global Iter: 1245800 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1245833
Number of Patches: 108128
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1245833
Global Iter: 1245900 training loss: 2.04559
Global Iter: 1245900 training acc: 0.21875
Global Iter: 1246000 training loss: 1.91656
Global Iter: 1246000 training acc: 0.1875
Global Iter: 1246100 training loss: 2.08656
Global Iter: 1246100 training acc: 0.09375
Global Iter: 1246200 training loss: 1.94157
Global Iter: 1246200 training acc: 0.1875
Global Iter: 1246300 training loss: 2.06576
Global Iter: 1246300 training acc: 0.09375
Global Iter: 1246400 training loss: 2.0608
Global Iter: 1246400 training acc: 0.3125
Global Iter: 1246500 training loss: 1.92224
Global Iter: 1246500 training acc: 0.28125
Global Iter: 1246600 training loss: 1.90649
Global Iter: 1246600 training acc: 0.15625
Global Iter: 1246700 training loss: 2.20909
Global Iter: 1246700 training acc: 0.125
Global Iter: 1246800 training loss: 2.02553
Global Iter: 1246800 training acc: 0.15625
Global Iter: 1246900 training loss: 1.96421
Global Iter: 1246900 training acc: 0.125
Global Iter: 1247000 training loss: 1.92183
Global Iter: 1247000 training acc: 0.1875
Global Iter: 1247100 training loss: 2.16591
Global Iter: 1247100 training acc: 0.125
Global Iter: 1247200 training loss: 1.92893
Global Iter: 1247200 training acc: 0.125
Global Iter: 1247300 training loss: 1.88322
Global Iter: 1247300 training acc: 0.25
Global Iter: 1247400 training loss: 1.92211
Global Iter: 1247400 training acc: 0.125
Global Iter: 1247500 training loss: 2.14551
Global Iter: 1247500 training acc: 0.25
Global Iter: 1247600 training loss: 1.967
Global Iter: 1247600 training acc: 0.21875
Global Iter: 1247700 training loss: 1.9882
Global Iter: 1247700 training acc: 0.15625
Global Iter: 1247800 training loss: 1.95639
Global Iter: 1247800 training acc: 0.15625
Global Iter: 1247900 training loss: 2.0296
Global Iter: 1247900 training acc: 0.09375
Global Iter: 1248000 training loss: 1.96149
Global Iter: 1248000 training acc: 0.1875
Global Iter: 1248100 training loss: 1.94912
Global Iter: 1248100 training acc: 0.0625
Global Iter: 1248200 training loss: 2.04411
Global Iter: 1248200 training acc: 0.125
Global Iter: 1248300 training loss: 2.07694
Global Iter: 1248300 training acc: 0.0625
Global Iter: 1248400 training loss: 1.96369
Global Iter: 1248400 training acc: 0.25
Global Iter: 1248500 training loss: 1.94404
Global Iter: 1248500 training acc: 0.125
Global Iter: 1248600 training loss: 1.88819
Global Iter: 1248600 training acc: 0.125
Global Iter: 1248700 training loss: 2.03232
Global Iter: 1248700 training acc: 0.125
Global Iter: 1248800 training loss: 2.03881
Global Iter: 1248800 training acc: 0.09375
Global Iter: 1248900 training loss: 1.94777
Global Iter: 1248900 training acc: 0.21875
Global Iter: 1249000 training loss: 1.98463
Global Iter: 1249000 training acc: 0.1875
Global Iter: 1249100 training loss: 2.00803
Global Iter: 1249100 training acc: 0.21875
Global Iter: 1249200 training loss: 2.04633
Global Iter: 1249200 training acc: 0.09375
Global Iter: 1249300 training loss: 2.01158
Global Iter: 1249300 training acc: 0.15625
Global Iter: 1249400 training loss: 1.99485
Global Iter: 1249400 training acc: 0.25
Global Iter: 1249500 training loss: 2.02119
Global Iter: 1249500 training acc: 0.1875
Global Iter: 1249600 training loss: 2.03555
Global Iter: 1249600 training acc: 0.15625
Global Iter: 1249700 training loss: 1.94298
Global Iter: 1249700 training acc: 0.21875
Global Iter: 1249800 training loss: 1.92366
Global Iter: 1249800 training acc: 0.1875
Global Iter: 1249900 training loss: 1.96292
Global Iter: 1249900 training acc: 0.125
Global Iter: 1250000 training loss: 1.91806
Global Iter: 1250000 training acc: 0.375
Global Iter: 1250100 training loss: 1.90487
Global Iter: 1250100 training acc: 0.125
Global Iter: 1250200 training loss: 1.98074
Global Iter: 1250200 training acc: 0.125
Global Iter: 1250300 training loss: 2.05364
Global Iter: 1250300 training acc: 0.09375
Global Iter: 1250400 training loss: 1.92202
Global Iter: 1250400 training acc: 0.21875
Global Iter: 1250500 training loss: 1.98832
Global Iter: 1250500 training acc: 0.09375
Global Iter: 1250600 training loss: 1.89499
Global Iter: 1250600 training acc: 0.28125
Global Iter: 1250700 training loss: 1.94145
Global Iter: 1250700 training acc: 0.25
Global Iter: 1250800 training loss: 1.94246
Global Iter: 1250800 training acc: 0.28125
Global Iter: 1250900 training loss: 1.94144
Global Iter: 1250900 training acc: 0.125
Global Iter: 1251000 training loss: 2.03695
Global Iter: 1251000 training acc: 0.0625
Gl2017-06-22 16:49:30.985475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1252591
obal Iter: 1251100 training loss: 1.98558
Global Iter: 1251100 training acc: 0.3125
Global Iter: 1251200 training loss: 1.96115
Global Iter: 1251200 training acc: 0.15625
Global Iter: 1251300 training loss: 1.93573
Global Iter: 1251300 training acc: 0.15625
Global Iter: 1251400 training loss: 2.02299
Global Iter: 1251400 training acc: 0.25
Global Iter: 1251500 training loss: 2.05374
Global Iter: 1251500 training acc: 0.09375
Global Iter: 1251600 training loss: 1.9629
Global Iter: 1251600 training acc: 0.25
Global Iter: 1251700 training loss: 2.10845
Global Iter: 1251700 training acc: 0.125
Global Iter: 1251800 training loss: 2.08764
Global Iter: 1251800 training acc: 0.25
Global Iter: 1251900 training loss: 1.91878
Global Iter: 1251900 training acc: 0.21875
Global Iter: 1252000 training loss: 2.0508
Global Iter: 1252000 training acc: 0.125
Global Iter: 1252100 training loss: 1.92996
Global Iter: 1252100 training acc: 0.28125
Global Iter: 1252200 training loss: 2.02056
Global Iter: 1252200 training acc: 0.125
Global Iter: 1252300 training loss: 2.09535
Global Iter: 1252300 training acc: 0.125
Global Iter: 1252400 training loss: 1.94639
Global Iter: 1252400 training acc: 0.21875
Global Iter: 1252500 training loss: 2.00274
Global Iter: 1252500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1252591
Number of Patches: 107047
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1252591
Global Iter: 1252600 training loss: 2.20255
Global Iter: 1252600 training acc: 0.1875
Global Iter: 1252700 training loss: 2.04387
Global Iter: 1252700 training acc: 0.21875
Global Iter: 1252800 training loss: 2.00006
Global Iter: 1252800 training acc: 0.28125
Global Iter: 1252900 training loss: 1.94714
Global Iter: 1252900 training acc: 0.21875
Global Iter: 1253000 training loss: 1.94981
Global Iter: 1253000 training acc: 0.1875
Global Iter: 1253100 training loss: 1.95087
Global Iter: 1253100 training acc: 0.15625
Global Iter: 1253200 training loss: 1.9757
Global Iter: 1253200 training acc: 0.15625
Global Iter: 1253300 training loss: 1.98841
Global Iter: 1253300 training acc: 0.28125
Global Iter: 1253400 training loss: 1.91175
Global Iter: 1253400 training acc: 0.21875
Global Iter: 1253500 training loss: 1.93336
Global Iter: 1253500 training acc: 0.21875
Global Iter: 1253600 training loss: 1.9938
Global Iter: 1253600 training acc: 0.15625
Global Iter: 1253700 training loss: 2.04588
Global Iter: 1253700 training acc: 0.28125
Global Iter: 1253800 training loss: 1.91782
Global Iter: 1253800 training acc: 0.21875
Global Iter: 1253900 training loss: 2.0077
Global Iter: 1253900 training acc: 0.09375
Global Iter: 1254000 training loss: 2.05675
Global Iter: 1254000 training acc: 0.125
Global Iter: 1254100 training loss: 1.98905
Global Iter: 1254100 training acc: 0.125
Global Iter: 1254200 training loss: 1.97108
Global Iter: 1254200 training acc: 0.25
Global Iter: 1254300 training loss: 2.1108
Global Iter: 1254300 training acc: 0.125
Global Iter: 1254400 training loss: 1.98457
Global Iter: 1254400 training acc: 0.25
Global Iter: 1254500 training loss: 2.07506
Global Iter: 1254500 training acc: 0.09375
Global Iter: 1254600 training loss: 2.01002
Global Iter: 1254600 training acc: 0.15625
Global Iter: 1254700 training loss: 1.96651
Global Iter: 1254700 training acc: 0.21875
Global Iter: 1254800 training loss: 2.04241
Global Iter: 1254800 training acc: 0.21875
Global Iter: 1254900 training loss: 2.13253
Global Iter: 1254900 training acc: 0.25
Global Iter: 1255000 training loss: 1.99383
Global Iter: 1255000 training acc: 0.125
Global Iter: 1255100 training loss: 1.94725
Global Iter: 1255100 training acc: 0.125
Global Iter: 1255200 training loss: 1.96704
Global Iter: 1255200 training acc: 0.25
Global Iter: 1255300 training loss: 1.94269
Global Iter: 1255300 training acc: 0.34375
Global Iter: 1255400 training loss: 2.0466
Global Iter: 1255400 training acc: 0.21875
Global Iter: 1255500 training loss: 1.89997
Global Iter: 1255500 training acc: 0.28125
Global I2017-06-22 17:00:54.055433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1259282
ter: 1255600 training loss: 2.1067
Global Iter: 1255600 training acc: 0.15625
Global Iter: 1255700 training loss: 2.00633
Global Iter: 1255700 training acc: 0.15625
Global Iter: 1255800 training loss: 1.91583
Global Iter: 1255800 training acc: 0.125
Global Iter: 1255900 training loss: 1.97894
Global Iter: 1255900 training acc: 0.21875
Global Iter: 1256000 training loss: 1.94504
Global Iter: 1256000 training acc: 0.15625
Global Iter: 1256100 training loss: 2.00592
Global Iter: 1256100 training acc: 0.1875
Global Iter: 1256200 training loss: 1.92737
Global Iter: 1256200 training acc: 0.21875
Global Iter: 1256300 training loss: 2.00224
Global Iter: 1256300 training acc: 0.21875
Global Iter: 1256400 training loss: 1.91839
Global Iter: 1256400 training acc: 0.21875
Global Iter: 1256500 training loss: 1.97423
Global Iter: 1256500 training acc: 0.15625
Global Iter: 1256600 training loss: 1.93647
Global Iter: 1256600 training acc: 0.21875
Global Iter: 1256700 training loss: 1.99977
Global Iter: 1256700 training acc: 0.15625
Global Iter: 1256800 training loss: 2.03221
Global Iter: 1256800 training acc: 0.21875
Global Iter: 1256900 training loss: 1.95231
Global Iter: 1256900 training acc: 0.1875
Global Iter: 1257000 training loss: 1.95325
Global Iter: 1257000 training acc: 0.25
Global Iter: 1257100 training loss: 2.03764
Global Iter: 1257100 training acc: 0.15625
Global Iter: 1257200 training loss: 1.86276
Global Iter: 1257200 training acc: 0.3125
Global Iter: 1257300 training loss: 2.03778
Global Iter: 1257300 training acc: 0.25
Global Iter: 1257400 training loss: 1.99486
Global Iter: 1257400 training acc: 0.15625
Global Iter: 1257500 training loss: 1.97582
Global Iter: 1257500 training acc: 0.1875
Global Iter: 1257600 training loss: 2.01766
Global Iter: 1257600 training acc: 0.3125
Global Iter: 1257700 training loss: 2.04973
Global Iter: 1257700 training acc: 0.21875
Global Iter: 1257800 training loss: 2.00405
Global Iter: 1257800 training acc: 0.21875
Global Iter: 1257900 training loss: 1.92095
Global Iter: 1257900 training acc: 0.125
Global Iter: 1258000 training loss: 2.01331
Global Iter: 1258000 training acc: 0.1875
Global Iter: 1258100 training loss: 2.03041
Global Iter: 1258100 training acc: 0.0625
Global Iter: 1258200 training loss: 2.09327
Global Iter: 1258200 training acc: 0.125
Global Iter: 1258300 training loss: 1.99157
Global Iter: 1258300 training acc: 0.28125
Global Iter: 1258400 training loss: 2.10443
Global Iter: 1258400 training acc: 0.1875
Global Iter: 1258500 training loss: 1.94896
Global Iter: 1258500 training acc: 0.34375
Global Iter: 1258600 training loss: 2.10305
Global Iter: 1258600 training acc: 0.0625
Global Iter: 1258700 training loss: 1.97752
Global Iter: 1258700 training acc: 0.3125
Global Iter: 1258800 training loss: 1.96742
Global Iter: 1258800 training acc: 0.25
Global Iter: 1258900 training loss: 2.05544
Global Iter: 1258900 training acc: 0.1875
Global Iter: 1259000 training loss: 2.00784
Global Iter: 1259000 training acc: 0.1875
Global Iter: 1259100 training loss: 2.16227
Global Iter: 1259100 training acc: 0.21875
Global Iter: 1259200 training loss: 1.98364
Global Iter: 1259200 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1259282
Number of Patches: 105977
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1259282
Global Iter: 1259300 training loss: 1.96485
Global Iter: 1259300 training acc: 0.1875
Global Iter: 1259400 training loss: 1.97096
Global Iter: 1259400 training acc: 0.21875
Global Iter: 1259500 training loss: 1.92216
Global Iter: 1259500 training acc: 0.21875
Global Iter: 1259600 training loss: 1.99083
Global Iter: 1259600 training acc: 0.21875
Global Iter: 1259700 training loss: 1.9097
Global Iter: 1259700 training acc: 0.1875
Global Iter: 1259800 training loss: 1.97556
Global Iter: 1259800 training acc: 0.09375
Global Iter: 1259900 training loss: 1.92515
Global Iter: 1259900 training acc: 0.21875
Global Iter: 1260000 training loss: 1.95376
Global Iter: 1260000 training acc: 0.15625
Global Iter: 1260100 training loss: 1.89748
Global Iter: 1260100 training acc: 0.21875
Global Iter: 1260200 training loss: 1.94577
Global Iter: 1260200 training acc: 0.21875
Global Iter: 1260300 training loss: 2.0956
Global Iter: 1260300 training acc: 0.09375
Global Iter: 1260400 training loss: 1.96914
Global Iter: 1260400 training acc: 0.21875
Global Iter: 1260500 training loss: 1.95547
Global Iter: 1260500 training acc: 0.15625
Global Iter: 1260600 training loss: 2.06046
Global Iter: 1260600 training acc: 0.125
Global Iter: 1260700 training loss: 1.95904
Global Iter: 1260700 training acc: 0.125
Global Iter: 1260800 training loss: 2.0354
Global Iter: 1260800 training acc: 0.125
Global Iter: 1260900 training loss: 2.04463
Global Iter: 1260900 training acc: 0.15625
Global Iter: 1261000 training loss: 2.06204
Global Iter: 1261000 training acc: 0.15625
Global Iter: 1261100 training loss: 2.01848
Global Iter: 1261100 training acc: 0.09375
Global Iter: 1261200 training loss: 1.99468
Global Iter: 1261200 training acc: 0.21875
Global Iter: 1261300 training loss: 1.90582
Global Iter: 1261300 training acc: 0.21875
Global Iter: 1261400 training loss: 1.91032
Global Iter: 1261400 training acc: 0.21875
Global Iter: 1261500 training loss: 1.92416
Global Iter: 1261500 training acc: 0.21875
Global Iter: 1261600 training loss: 1.88774
Global Iter: 1261600 training acc: 0.21875
Global Iter: 1261700 training loss: 2.01928
Global Iter: 1261700 training acc: 0.15625
Global Iter: 1261800 training loss: 1.94992
Global Iter: 1261800 training acc: 0.1875
Global Iter: 1261900 training loss: 2.02756
Global Iter: 1261900 training acc: 0.21875
Global Iter: 1262000 training loss: 1.99358
Global Iter: 1262000 training acc: 0.1875
Global Iter: 1262100 training loss: 1.95712
Global Iter: 1262100 training acc: 0.1875
Global Iter: 1262200 training loss: 2.00474
Global Iter: 1262200 training acc: 0.125
Global Iter: 1262300 training loss: 1.89248
Global Iter: 1262300 training acc: 0.21875
Global Iter: 1262400 training loss: 2.0228
Global Iter: 1262400 training acc: 0.15625
Global Iter: 1262500 training loss: 1.97232
Global Iter: 1262500 training acc: 0.3125
Global Iter: 1262600 training loss: 2.0151
Global Iter: 1262600 training acc: 0.1875
Global Iter: 1262700 training loss: 2.02844
Global Iter: 1262700 training acc: 0.125
Global Iter: 1262800 training loss: 1.93271
Global Iter: 1262800 training acc: 0.28125
Global Iter: 1262900 training loss: 2.06905
Global Iter: 1262900 training acc: 0.1875
Global Iter: 1263000 training loss: 2.05609
Global Iter: 1263000 training acc: 0.09375
Global Iter: 1263100 training loss: 2.03157
Global Iter: 1263100 training acc: 0.125
Global Iter: 1263200 training loss: 1.99344
Global Iter: 1263200 training acc: 0.21875
Global Iter: 1263300 training loss: 1.95243
Global Iter: 1263300 training acc: 0.25
Global Iter: 1263400 training loss: 1.95026
Global Iter: 1263400 training acc: 0.125
Global Iter: 1263500 training loss: 1.98896
Global Iter: 1263500 training acc: 0.09375
Global Iter: 1263600 training loss: 1.97422
Global Iter: 1263600 training acc: 0.15625
Global Iter: 1263700 training loss: 1.93288
Global Iter: 1263700 training acc: 0.15625
Global Iter: 1263800 training loss: 1.94208
Global Iter: 1263800 training acc: 0.1875
Global Iter: 1263900 training loss: 2.02735
Global Iter: 1263900 training acc: 0.25
Global Iter: 1264000 training loss: 1.93061
Global Iter: 1264000 training acc: 0.1875
Global Iter: 1264100 training loss: 1.98849
Global Iter: 1264100 training acc: 0.1875
Global Iter: 1264200 training loss: 1.91238
Global Iter: 1264200 training acc: 0.125
Global Iter: 1264300 training loss: 1.9465
Global Iter: 1264300 training acc: 0.125
Global Iter: 1264400 training loss: 2.01508
Global Iter: 1264400 training acc: 0.09375
Global Iter: 1264500 training loss: 1.99523
Global Iter: 1264500 training acc: 0.25
Global Iter: 1264600 training loss: 1.89794
Global Iter: 1264600 training acc: 0.25
Global Iter: 1264700 training loss: 2.00076
Global Iter: 1264700 training acc: 0.28125
Global Iter: 1264800 training loss: 1.9817
Glo2017-06-22 17:12:04.408079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1265906
bal Iter: 1264800 training acc: 0.09375
Global Iter: 1264900 training loss: 1.98184
Global Iter: 1264900 training acc: 0.1875
Global Iter: 1265000 training loss: 1.87732
Global Iter: 1265000 training acc: 0.25
Global Iter: 1265100 training loss: 2.01062
Global Iter: 1265100 training acc: 0.15625
Global Iter: 1265200 training loss: 1.99955
Global Iter: 1265200 training acc: 0.09375
Global Iter: 1265300 training loss: 2.01689
Global Iter: 1265300 training acc: 0.21875
Global Iter: 1265400 training loss: 1.98197
Global Iter: 1265400 training acc: 0.25
Global Iter: 1265500 training loss: 2.01943
Global Iter: 1265500 training acc: 0.1875
Global Iter: 1265600 training loss: 1.98418
Global Iter: 1265600 training acc: 0.125
Global Iter: 1265700 training loss: 2.00175
Global Iter: 1265700 training acc: 0.25
Global Iter: 1265800 training loss: 1.94018
Global Iter: 1265800 training acc: 0.03125
Global Iter: 1265900 training loss: 2.00378
Global Iter: 1265900 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1265906
Number of Patches: 104918
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1265906
Global Iter: 1266000 training loss: 2.0351
Global Iter: 1266000 training acc: 0.15625
Global Iter: 1266100 training loss: 2.06017
Global Iter: 1266100 training acc: 0.1875
Global Iter: 1266200 training loss: 1.9731
Global Iter: 1266200 training acc: 0.0625
Global Iter: 1266300 training loss: 1.95201
Global Iter: 1266300 training acc: 0.21875
Global Iter: 1266400 training loss: 1.93275
Global Iter: 1266400 training acc: 0.1875
Global Iter: 1266500 training loss: 2.12164
Global Iter: 1266500 training acc: 0.125
Global Iter: 1266600 training loss: 1.96544
Global Iter: 1266600 training acc: 0.25
Global Iter: 1266700 training loss: 2.08302
Global Iter: 1266700 training acc: 0.09375
Global Iter: 1266800 training loss: 1.90106
Global Iter: 1266800 training acc: 0.3125
Global Iter: 1266900 training loss: 1.99744
Global Iter: 1266900 training acc: 0.125
Global Iter: 1267000 training loss: 1.99605
Global Iter: 1267000 training acc: 0.15625
Global Iter: 1267100 training loss: 2.05571
Global Iter: 1267100 training acc: 0.1875
Global Iter: 1267200 training loss: 1.88104
Global Iter: 1267200 training acc: 0.1875
Global Iter: 1267300 training loss: 1.99311
Global Iter: 1267300 training acc: 0.15625
Global Iter: 1267400 training loss: 1.9428
Global Iter: 1267400 training acc: 0.21875
Global Iter: 1267500 training loss: 2.04559
Global Iter: 1267500 training acc: 0.1875
Global Iter: 1267600 training loss: 2.09793
Global Iter: 1267600 training acc: 0.21875
Global Iter: 1267700 training loss: 1.93661
Global Iter: 1267700 training acc: 0.25
Global Iter: 1267800 training loss: 2.0934
Global Iter: 1267800 training acc: 0.125
Global Iter: 1267900 training loss: 2.07409
Global Iter: 1267900 training acc: 0.21875
Global Iter: 1268000 training loss: 1.95933
Global Iter: 1268000 training acc: 0.125
Global Iter: 1268100 training loss: 2.02369
Global Iter: 1268100 training acc: 0.1875
Global Iter: 1268200 training loss: 2.21203
Global Iter: 1268200 training acc: 0.25
Global Iter: 1268300 training loss: 2.0095
Global Iter: 1268300 training acc: 0.28125
Global Iter: 1268400 training loss: 1.87225
Global Iter: 1268400 training acc: 0.3125
Global Iter: 1268500 training loss: 2.04673
Global Iter: 1268500 training acc: 0.1875
Global Iter: 1268600 training loss: 1.90296
Global Iter: 1268600 training acc: 0.25
Global Iter: 1268700 training loss: 1.99555
Global Iter: 1268700 training acc: 0.15625
Global Iter: 1268800 training loss: 1.97419
Global Iter: 1268800 training acc: 0.15625
Global Iter: 1268900 training loss: 2.00534
Global Iter: 1268900 training acc: 0.125
Global Iter: 1269000 training loss: 1.92459
Global Iter: 1269000 training acc: 0.3125
Global Iter: 1269100 training loss: 1.95218
Global Iter: 1269100 training acc: 0.28125
Global Iter: 1269200 training loss: 1.9913
Global Iter: 1269200 training acc: 0.125
Global Iter: 1269300 training loss: 1.93501
Global Iter: 2017-06-22 17:23:10.527387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1272464
1269300 training acc: 0.25
Global Iter: 1269400 training loss: 1.95761
Global Iter: 1269400 training acc: 0.15625
Global Iter: 1269500 training loss: 1.98294
Global Iter: 1269500 training acc: 0.1875
Global Iter: 1269600 training loss: 2.06031
Global Iter: 1269600 training acc: 0.125
Global Iter: 1269700 training loss: 1.9125
Global Iter: 1269700 training acc: 0.3125
Global Iter: 1269800 training loss: 2.00357
Global Iter: 1269800 training acc: 0.125
Global Iter: 1269900 training loss: 2.03329
Global Iter: 1269900 training acc: 0.1875
Global Iter: 1270000 training loss: 1.96062
Global Iter: 1270000 training acc: 0.09375
Global Iter: 1270100 training loss: 1.90874
Global Iter: 1270100 training acc: 0.15625
Global Iter: 1270200 training loss: 1.90136
Global Iter: 1270200 training acc: 0.1875
Global Iter: 1270300 training loss: 2.0124
Global Iter: 1270300 training acc: 0.1875
Global Iter: 1270400 training loss: 1.98096
Global Iter: 1270400 training acc: 0.21875
Global Iter: 1270500 training loss: 1.98391
Global Iter: 1270500 training acc: 0.1875
Global Iter: 1270600 training loss: 1.9829
Global Iter: 1270600 training acc: 0.3125
Global Iter: 1270700 training loss: 1.97725
Global Iter: 1270700 training acc: 0.15625
Global Iter: 1270800 training loss: 2.09745
Global Iter: 1270800 training acc: 0.125
Global Iter: 1270900 training loss: 1.98059
Global Iter: 1270900 training acc: 0.21875
Global Iter: 1271000 training loss: 2.17044
Global Iter: 1271000 training acc: 0.09375
Global Iter: 1271100 training loss: 2.06236
Global Iter: 1271100 training acc: 0.25
Global Iter: 1271200 training loss: 1.95862
Global Iter: 1271200 training acc: 0.125
Global Iter: 1271300 training loss: 2.03604
Global Iter: 1271300 training acc: 0.25
Global Iter: 1271400 training loss: 2.1251
Global Iter: 1271400 training acc: 0.1875
Global Iter: 1271500 training loss: 1.92391
Global Iter: 1271500 training acc: 0.375
Global Iter: 1271600 training loss: 2.01432
Global Iter: 1271600 training acc: 0.03125
Global Iter: 1271700 training loss: 1.89031
Global Iter: 1271700 training acc: 0.25
Global Iter: 1271800 training loss: 1.99699
Global Iter: 1271800 training acc: 0.125
Global Iter: 1271900 training loss: 2.1756
Global Iter: 1271900 training acc: 0.1875
Global Iter: 1272000 training loss: 2.00075
Global Iter: 1272000 training acc: 0.21875
Global Iter: 1272100 training loss: 2.04969
Global Iter: 1272100 training acc: 0.0625
Global Iter: 1272200 training loss: 1.99691
Global Iter: 1272200 training acc: 0.15625
Global Iter: 1272300 training loss: 1.99556
Global Iter: 1272300 training acc: 0.125
Global Iter: 1272400 training loss: 1.97681
Global Iter: 1272400 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1272464
Number of Patches: 103869
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1272464
Global Iter: 1272500 training loss: 1.91445
Global Iter: 1272500 training acc: 0.1875
Global Iter: 1272600 training loss: 1.96327
Global Iter: 1272600 training acc: 0.1875
Global Iter: 1272700 training loss: 1.94134
Global Iter: 1272700 training acc: 0.25
Global Iter: 1272800 training loss: 2.04323
Global Iter: 1272800 training acc: 0.1875
Global Iter: 1272900 training loss: 1.96944
Global Iter: 1272900 training acc: 0.1875
Global Iter: 1273000 training loss: 1.9249
Global Iter: 1273000 training acc: 0.125
Global Iter: 1273100 training loss: 2.03164
Global Iter: 1273100 training acc: 0.1875
Global Iter: 1273200 training loss: 1.99593
Global Iter: 1273200 training acc: 0.15625
Global Iter: 1273300 training loss: 1.90258
Global Iter: 1273300 training acc: 0.125
Global Iter: 1273400 training loss: 2.02372
Global Iter: 1273400 training acc: 0.125
Global Iter: 1273500 training loss: 2.03502
Global Iter: 1273500 training acc: 0.125
Global Iter: 1273600 training loss: 2.12858
Global Iter: 1273600 training acc: 0.125
Global Iter: 1273700 training loss: 1.97514
Global Iter: 1273700 training acc: 0.1875
Global Iter: 1273800 training loss: 1.98236
Global Iter: 1273800 training acc: 0.1875
Global Iter: 1273900 training loss: 1.84194
Global Iter: 1273900 training acc: 0.125
Global Iter: 1274000 training loss: 2.03693
Global Iter: 1274000 training acc: 0.15625
Global Iter: 1274100 training loss: 1.93042
Global Iter: 1274100 training acc: 0.1875
Global Iter: 1274200 training loss: 2.18535
Global Iter: 1274200 training acc: 0.15625
Global Iter: 1274300 training loss: 2.05919
Global Iter: 1274300 training acc: 0.21875
Global Iter: 1274400 training loss: 1.93995
Global Iter: 1274400 training acc: 0.21875
Global Iter: 1274500 training loss: 2.05837
Global Iter: 1274500 training acc: 0.1875
Global Iter: 1274600 training loss: 2.01203
Global Iter: 1274600 training acc: 0.1875
Global Iter: 1274700 training loss: 1.95494
Global Iter: 1274700 training acc: 0.15625
Global Iter: 1274800 training loss: 1.97266
Global Iter: 1274800 training acc: 0.1875
Global Iter: 1274900 training loss: 2.00119
Global Iter: 1274900 training acc: 0.21875
Global Iter: 1275000 training loss: 2.01377
Global Iter: 1275000 training acc: 0.1875
Global Iter: 1275100 training loss: 1.96379
Global Iter: 1275100 training acc: 0.3125
Global Iter: 1275200 training loss: 1.98826
Global Iter: 1275200 training acc: 0.1875
Global Iter: 1275300 training loss: 1.99758
Global Iter: 1275300 training acc: 0.09375
Global Iter: 1275400 training loss: 2.03373
Global Iter: 1275400 training acc: 0.15625
Global Iter: 1275500 training loss: 2.01403
Global Iter: 1275500 training acc: 0.15625
Global Iter: 1275600 training loss: 2.04326
Global Iter: 1275600 training acc: 0.15625
Global Iter: 1275700 training loss: 2.06749
Global Iter: 1275700 training acc: 0.0625
Global Iter: 1275800 training loss: 1.98181
Global Iter: 1275800 training acc: 0.15625
Global Iter: 1275900 training loss: 2.04813
Global Iter: 1275900 training acc: 0.15625
Global Iter: 1276000 training loss: 1.90185
Global Iter: 1276000 training acc: 0.25
Global Iter: 1276100 training loss: 2.02029
Global Iter: 1276100 training acc: 0.125
Global Iter: 1276200 training loss: 1.88881
Global Iter: 1276200 training acc: 0.3125
Global Iter: 1276300 training loss: 2.14054
Global Iter: 1276300 training acc: 0.15625
Global Iter: 1276400 training loss: 2.02851
Global Iter: 1276400 training acc: 0.15625
Global Iter: 1276500 training loss: 2.07228
Global Iter: 1276500 training acc: 0.1875
Global Iter: 1276600 training loss: 1.97319
Global Iter: 1276600 training acc: 0.0625
Global Iter: 1276700 training loss: 1.91657
Global Iter: 1276700 training acc: 0.25
Global Iter: 1276800 training loss: 2.0082
Global Iter: 1276800 training acc: 0.1875
Global Iter: 1276900 training loss: 1.93089
Global Iter: 1276900 training acc: 0.15625
Global Iter: 1277000 training loss: 1.9622
Global Iter: 1277000 training acc: 0.1875
Global Iter: 1277100 training loss: 2.0041
Global Iter: 1277100 training acc: 0.21875
Global Iter: 1277200 training loss: 1.98954
Global Iter: 1277200 training acc: 0.125
Global Iter: 1277300 training loss: 2.0004
Global Iter: 1277300 training acc: 0.125
Global Iter: 1277400 training loss: 2.02258
Global Iter: 1277400 training acc: 0.21875
Global Iter: 1277500 training loss: 2.13181
Global Iter: 1277500 training acc: 0.21875
Global Iter: 1277600 training loss: 2.09489
Global Iter: 1277600 training acc: 0.125
Global Iter: 1277700 training loss: 2.11826
Global Iter: 1277700 training acc: 0.09375
Global Iter: 1277800 training loss: 1.95009
Global Iter: 1277800 training acc: 0.15625
Global Iter: 1277900 training loss: 1.87899
Global Iter: 1277900 training acc: 0.21875
Global Iter: 1278000 training loss: 1.95468
Global Iter: 1278000 training acc: 0.1875
Global Iter: 1278100 training loss: 2.0308
Global Iter: 1278100 training acc: 0.15625
Global Iter: 1278200 training loss: 2.02481
Global Iter: 1278200 training acc: 0.1875
Global Iter: 1278300 training loss: 1.93673
Global Iter: 1278300 training acc: 0.21875
Global Iter: 1278400 training loss: 2.08104
Global Iter: 1278400 training acc: 0.09375
Global Iter: 1278500 training loss: 1.90449
Global Iter: 1278500 training acc: 0.1875
Global Iter: 1278600 training lo2017-06-22 17:34:13.266007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1278956
ss: 2.02153
Global Iter: 1278600 training acc: 0.1875
Global Iter: 1278700 training loss: 1.97812
Global Iter: 1278700 training acc: 0.25
Global Iter: 1278800 training loss: 2.01586
Global Iter: 1278800 training acc: 0.125
Global Iter: 1278900 training loss: 2.04839
Global Iter: 1278900 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1278956
Number of Patches: 102831
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1278956
Global Iter: 1279000 training loss: 2.06054
Global Iter: 1279000 training acc: 0.15625
Global Iter: 1279100 training loss: 1.97093
Global Iter: 1279100 training acc: 0.125
Global Iter: 1279200 training loss: 1.99142
Global Iter: 1279200 training acc: 0.1875
Global Iter: 1279300 training loss: 2.0194
Global Iter: 1279300 training acc: 0.125
Global Iter: 1279400 training loss: 1.9329
Global Iter: 1279400 training acc: 0.125
Global Iter: 1279500 training loss: 1.89618
Global Iter: 1279500 training acc: 0.28125
Global Iter: 1279600 training loss: 2.06512
Global Iter: 1279600 training acc: 0.21875
Global Iter: 1279700 training loss: 2.02294
Global Iter: 1279700 training acc: 0.25
Global Iter: 1279800 training loss: 1.97947
Global Iter: 1279800 training acc: 0.15625
Global Iter: 1279900 training loss: 2.05876
Global Iter: 1279900 training acc: 0.21875
Global Iter: 1280000 training loss: 1.93453
Global Iter: 1280000 training acc: 0.25
Global Iter: 1280100 training loss: 1.8564
Global Iter: 1280100 training acc: 0.28125
Global Iter: 1280200 training loss: 1.99948
Global Iter: 1280200 training acc: 0.09375
Global Iter: 1280300 training loss: 2.03076
Global Iter: 1280300 training acc: 0.09375
Global Iter: 1280400 training loss: 1.9572
Global Iter: 1280400 training acc: 0.125
Global Iter: 1280500 training loss: 1.91371
Global Iter: 1280500 training acc: 0.25
Global Iter: 1280600 training loss: 1.93037
Global Iter: 1280600 training acc: 0.125
Global Iter: 1280700 training loss: 1.92572
Global Iter: 1280700 training acc: 0.25
Global Iter: 1280800 training loss: 1.9645
Global Iter: 1280800 training acc: 0.125
Global Iter: 1280900 training loss: 1.89828
Global Iter: 1280900 training acc: 0.1875
Global Iter: 1281000 training loss: 1.98829
Global Iter: 1281000 training acc: 0.21875
Global Iter: 1281100 training loss: 1.92677
Global Iter: 1281100 training acc: 0.15625
Global Iter: 1281200 training loss: 1.97176
Global Iter: 1281200 training acc: 0.09375
Global Iter: 1281300 training loss: 2.01393
Global Iter: 1281300 training acc: 0.09375
Global Iter: 1281400 training loss: 2.07108
Global Iter: 1281400 training acc: 0.15625
Global Iter: 1281500 training loss: 1.94754
Global Iter: 1281500 training acc: 0.3125
Global Iter: 1281600 training loss: 1.94583
Global Iter: 1281600 training acc: 0.3125
Global Iter: 1281700 training loss: 2.04216
Global Iter: 1281700 training acc: 0.1875
Global Iter: 1281800 training loss: 2.13603
Global Iter: 1281800 training acc: 0.0625
Global Iter: 1281900 training loss: 1.88312
Global Iter: 1281900 training acc: 0.34375
Global Iter: 1282000 training loss: 1.97731
Global Iter: 1282000 training acc: 0.1875
Global Iter: 1282100 training loss: 1.89476
Global Iter: 1282100 training acc: 0.15625
Global Iter: 1282200 training loss: 1.95496
Global Iter: 1282200 training acc: 0.125
Global Iter: 1282300 training loss: 1.9337
Global Iter: 1282300 training acc: 0.1875
Global Iter: 1282400 training loss: 1.97305
Global Iter: 1282400 training acc: 0.21875
Global Iter: 1282500 training loss: 2.11748
Global Iter: 1282500 training acc: 0.125
Global Iter: 1282600 training loss: 1.89915
Global Iter: 1282600 training acc: 0.1875
Global Iter: 1282700 training loss: 2.03202
Global Iter: 1282700 training acc: 0.25
Global Iter: 1282800 training loss: 2.10387
Global Iter: 1282800 training acc: 0.125
Global Iter: 1282900 training loss: 2.00772
Global Iter: 1282900 training acc: 0.21875
Global Iter: 1283000 training loss: 1.95496
Global Iter: 1283000 training acc: 0.1875
Global Iter: 1283100 training loss: 1.989882017-06-22 17:45:03.024177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1285383

Global Iter: 1283100 training acc: 0.25
Global Iter: 1283200 training loss: 1.89871
Global Iter: 1283200 training acc: 0.25
Global Iter: 1283300 training loss: 2.00422
Global Iter: 1283300 training acc: 0.09375
Global Iter: 1283400 training loss: 1.89438
Global Iter: 1283400 training acc: 0.1875
Global Iter: 1283500 training loss: 1.93759
Global Iter: 1283500 training acc: 0.1875
Global Iter: 1283600 training loss: 1.88008
Global Iter: 1283600 training acc: 0.25
Global Iter: 1283700 training loss: 1.9886
Global Iter: 1283700 training acc: 0.125
Global Iter: 1283800 training loss: 2.01222
Global Iter: 1283800 training acc: 0.1875
Global Iter: 1283900 training loss: 1.95689
Global Iter: 1283900 training acc: 0.1875
Global Iter: 1284000 training loss: 2.05396
Global Iter: 1284000 training acc: 0.15625
Global Iter: 1284100 training loss: 1.94526
Global Iter: 1284100 training acc: 0.09375
Global Iter: 1284200 training loss: 1.94964
Global Iter: 1284200 training acc: 0.25
Global Iter: 1284300 training loss: 1.96226
Global Iter: 1284300 training acc: 0.28125
Global Iter: 1284400 training loss: 1.94945
Global Iter: 1284400 training acc: 0.28125
Global Iter: 1284500 training loss: 1.9645
Global Iter: 1284500 training acc: 0.21875
Global Iter: 1284600 training loss: 1.95454
Global Iter: 1284600 training acc: 0.21875
Global Iter: 1284700 training loss: 2.05504
Global Iter: 1284700 training acc: 0.15625
Global Iter: 1284800 training loss: 2.02078
Global Iter: 1284800 training acc: 0.1875
Global Iter: 1284900 training loss: 1.95147
Global Iter: 1284900 training acc: 0.28125
Global Iter: 1285000 training loss: 2.05335
Global Iter: 1285000 training acc: 0.15625
Global Iter: 1285100 training loss: 1.92055
Global Iter: 1285100 training acc: 0.1875
Global Iter: 1285200 training loss: 2.01129
Global Iter: 1285200 training acc: 0.0625
Global Iter: 1285300 training loss: 2.03627
Global Iter: 1285300 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1285383
Number of Patches: 101803
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1285383
Global Iter: 1285400 training loss: 2.03914
Global Iter: 1285400 training acc: 0.0625
Global Iter: 1285500 training loss: 1.99482
Global Iter: 1285500 training acc: 0.25
Global Iter: 1285600 training loss: 2.06214
Global Iter: 1285600 training acc: 0.1875
Global Iter: 1285700 training loss: 1.97102
Global Iter: 1285700 training acc: 0.25
Global Iter: 1285800 training loss: 1.99968
Global Iter: 1285800 training acc: 0.125
Global Iter: 1285900 training loss: 1.9621
Global Iter: 1285900 training acc: 0.125
Global Iter: 1286000 training loss: 2.04192
Global Iter: 1286000 training acc: 0.375
Global Iter: 1286100 training loss: 1.97092
Global Iter: 1286100 training acc: 0.125
Global Iter: 1286200 training loss: 1.97092
Global Iter: 1286200 training acc: 0.1875
Global Iter: 1286300 training loss: 1.93929
Global Iter: 1286300 training acc: 0.125
Global Iter: 1286400 training loss: 2.0153
Global Iter: 1286400 training acc: 0.15625
Global Iter: 1286500 training loss: 1.99779
Global Iter: 1286500 training acc: 0.1875
Global Iter: 1286600 training loss: 2.08967
Global Iter: 1286600 training acc: 0.125
Global Iter: 1286700 training loss: 1.92785
Global Iter: 1286700 training acc: 0.125
Global Iter: 1286800 training loss: 2.07692
Global Iter: 1286800 training acc: 0.125
Global Iter: 1286900 training loss: 1.96111
Global Iter: 1286900 training acc: 0.1875
Global Iter: 1287000 training loss: 1.95602
Global Iter: 1287000 training acc: 0.28125
Global Iter: 1287100 training loss: 1.92879
Global Iter: 1287100 training acc: 0.25
Global Iter: 1287200 training loss: 1.92816
Global Iter: 1287200 training acc: 0.25
Global Iter: 1287300 training loss: 2.13126
Global Iter: 1287300 training acc: 0.1875
Global Iter: 1287400 training loss: 1.95115
Global Iter: 1287400 training acc: 0.09375
Global Iter: 1287500 training loss: 1.99003
Global Iter: 1287500 training acc: 0.125
Global Iter: 1287600 training loss: 1.99405
Global Iter: 1282017-06-22 17:55:49.200750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1291746
7600 training acc: 0.21875
Global Iter: 1287700 training loss: 1.94632
Global Iter: 1287700 training acc: 0.21875
Global Iter: 1287800 training loss: 2.13151
Global Iter: 1287800 training acc: 0.125
Global Iter: 1287900 training loss: 1.95799
Global Iter: 1287900 training acc: 0.21875
Global Iter: 1288000 training loss: 1.97356
Global Iter: 1288000 training acc: 0.09375
Global Iter: 1288100 training loss: 1.9285
Global Iter: 1288100 training acc: 0.25
Global Iter: 1288200 training loss: 1.90524
Global Iter: 1288200 training acc: 0.3125
Global Iter: 1288300 training loss: 1.98156
Global Iter: 1288300 training acc: 0.1875
Global Iter: 1288400 training loss: 2.03459
Global Iter: 1288400 training acc: 0.1875
Global Iter: 1288500 training loss: 1.92834
Global Iter: 1288500 training acc: 0.1875
Global Iter: 1288600 training loss: 2.04728
Global Iter: 1288600 training acc: 0.25
Global Iter: 1288700 training loss: 2.06233
Global Iter: 1288700 training acc: 0.21875
Global Iter: 1288800 training loss: 1.97249
Global Iter: 1288800 training acc: 0.09375
Global Iter: 1288900 training loss: 2.04664
Global Iter: 1288900 training acc: 0.21875
Global Iter: 1289000 training loss: 2.02135
Global Iter: 1289000 training acc: 0.15625
Global Iter: 1289100 training loss: 1.94831
Global Iter: 1289100 training acc: 0.15625
Global Iter: 1289200 training loss: 1.98039
Global Iter: 1289200 training acc: 0.0625
Global Iter: 1289300 training loss: 1.90973
Global Iter: 1289300 training acc: 0.1875
Global Iter: 1289400 training loss: 2.08723
Global Iter: 1289400 training acc: 0.09375
Global Iter: 1289500 training loss: 1.948
Global Iter: 1289500 training acc: 0.28125
Global Iter: 1289600 training loss: 1.93633
Global Iter: 1289600 training acc: 0.21875
Global Iter: 1289700 training loss: 2.05658
Global Iter: 1289700 training acc: 0.1875
Global Iter: 1289800 training loss: 1.91777
Global Iter: 1289800 training acc: 0.1875
Global Iter: 1289900 training loss: 2.02816
Global Iter: 1289900 training acc: 0.09375
Global Iter: 1290000 training loss: 1.98491
Global Iter: 1290000 training acc: 0.09375
Global Iter: 1290100 training loss: 1.95821
Global Iter: 1290100 training acc: 0.125
Global Iter: 1290200 training loss: 1.99022
Global Iter: 1290200 training acc: 0.15625
Global Iter: 1290300 training loss: 1.92126
Global Iter: 1290300 training acc: 0.15625
Global Iter: 1290400 training loss: 2.01294
Global Iter: 1290400 training acc: 0.125
Global Iter: 1290500 training loss: 1.93946
Global Iter: 1290500 training acc: 0.15625
Global Iter: 1290600 training loss: 1.97244
Global Iter: 1290600 training acc: 0.15625
Global Iter: 1290700 training loss: 2.05694
Global Iter: 1290700 training acc: 0.1875
Global Iter: 1290800 training loss: 1.92219
Global Iter: 1290800 training acc: 0.21875
Global Iter: 1290900 training loss: 2.01277
Global Iter: 1290900 training acc: 0.09375
Global Iter: 1291000 training loss: 1.91119
Global Iter: 1291000 training acc: 0.3125
Global Iter: 1291100 training loss: 2.05206
Global Iter: 1291100 training acc: 0.21875
Global Iter: 1291200 training loss: 2.00873
Global Iter: 1291200 training acc: 0.09375
Global Iter: 1291300 training loss: 1.97249
Global Iter: 1291300 training acc: 0.28125
Global Iter: 1291400 training loss: 1.94511
Global Iter: 1291400 training acc: 0.28125
Global Iter: 1291500 training loss: 1.93293
Global Iter: 1291500 training acc: 0.21875
Global Iter: 1291600 training loss: 1.8882
Global Iter: 1291600 training acc: 0.1875
Global Iter: 1291700 training loss: 2.04234
Global Iter: 1291700 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1291746
Number of Patches: 100785
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1291746
Global Iter: 1291800 training loss: 1.97805
Global Iter: 1291800 training acc: 0.125
Global Iter: 1291900 training loss: 2.00531
Global Iter: 1291900 training acc: 0.125
Global Iter: 1292000 training loss: 1.88005
Global Iter: 1292000 training acc: 0.21875
Global Iter: 1292100 training loss: 1.95169
Global Iter: 1292100 training acc: 0.21875
Global Iter: 1292200 training loss: 2.06539
Global Iter: 1292200 training acc: 0.21875
Global Iter: 1292300 training loss: 1.92624
Global Iter: 1292300 training acc: 0.25
Global Iter: 1292400 training loss: 2.00694
Global Iter: 1292400 training acc: 0.21875
Global Iter: 1292500 training loss: 2.13048
Global Iter: 1292500 training acc: 0.125
Global Iter: 1292600 training loss: 2.02284
Global Iter: 1292600 training acc: 0.25
Global Iter: 1292700 training loss: 1.97836
Global Iter: 1292700 training acc: 0.1875
Global Iter: 1292800 training loss: 1.89028
Global Iter: 1292800 training acc: 0.25
Global Iter: 1292900 training loss: 1.91031
Global Iter: 1292900 training acc: 0.25
Global Iter: 1293000 training loss: 2.03273
Global Iter: 1293000 training acc: 0.1875
Global Iter: 1293100 training loss: 2.04413
Global Iter: 1293100 training acc: 0.15625
Global Iter: 1293200 training loss: 2.01277
Global Iter: 1293200 training acc: 0.03125
Global Iter: 1293300 training loss: 1.94768
Global Iter: 1293300 training acc: 0.1875
Global Iter: 1293400 training loss: 1.98346
Global Iter: 1293400 training acc: 0.1875
Global Iter: 1293500 training loss: 1.92907
Global Iter: 1293500 training acc: 0.15625
Global Iter: 1293600 training loss: 1.97907
Global Iter: 1293600 training acc: 0.15625
Global Iter: 1293700 training loss: 1.94258
Global Iter: 1293700 training acc: 0.1875
Global Iter: 1293800 training loss: 1.96693
Global Iter: 1293800 training acc: 0.21875
Global Iter: 1293900 training loss: 2.07294
Global Iter: 1293900 training acc: 0.1875
Global Iter: 1294000 training loss: 1.94837
Global Iter: 1294000 training acc: 0.25
Global Iter: 1294100 training loss: 1.98972
Global Iter: 1294100 training acc: 0.21875
Global Iter: 1294200 training loss: 2.00971
Global Iter: 1294200 training acc: 0.1875
Global Iter: 1294300 training loss: 1.92879
Global Iter: 1294300 training acc: 0.125
Global Iter: 1294400 training loss: 2.03765
Global Iter: 1294400 training acc: 0.09375
Global Iter: 1294500 training loss: 1.92961
Global Iter: 1294500 training acc: 0.15625
Global Iter: 1294600 training loss: 2.08361
Global Iter: 1294600 training acc: 0.0625
Global Iter: 1294700 training loss: 1.9844
Global Iter: 1294700 training acc: 0.15625
Global Iter: 1294800 training loss: 1.97815
Global Iter: 1294800 training acc: 0.28125
Global Iter: 1294900 training loss: 1.97619
Global Iter: 1294900 training acc: 0.25
Global Iter: 1295000 training loss: 1.97079
Global Iter: 1295000 training acc: 0.25
Global Iter: 1295100 training loss: 2.0567
Global Iter: 1295100 training acc: 0.125
Global Iter: 1295200 training loss: 2.01023
Global Iter: 1295200 training acc: 0.25
Global Iter: 1295300 training loss: 1.99193
Global Iter: 1295300 training acc: 0.15625
Global Iter: 1295400 training loss: 1.9295
Global Iter: 1295400 training acc: 0.21875
Global Iter: 1295500 training loss: 2.06788
Global Iter: 1295500 training acc: 0.1875
Global Iter: 1295600 training loss: 1.97831
Global Iter: 1295600 training acc: 0.0625
Global Iter: 1295700 training loss: 1.9922
Global Iter: 1295700 training acc: 0.0625
Global Iter: 1295800 training loss: 1.9799
Global Iter: 1295800 training acc: 0.1875
Global Iter: 1295900 training loss: 2.082
Global Iter: 1295900 training acc: 0.1875
Global Iter: 1296000 training loss: 1.9277
Global Iter: 1296000 training acc: 0.21875
Global Iter: 1296100 training loss: 2.01207
Global Iter: 1296100 training acc: 0.15625
Global Iter: 1296200 training loss: 1.99385
Global Iter: 1296200 training acc: 0.125
Global Iter: 1296300 training loss: 1.99814
Global Iter: 1296300 training acc: 0.1875
Global Iter: 1296400 training loss: 1.94037
Global Iter: 1296400 training acc: 0.21875
Global Iter: 1296500 training loss: 1.93874
Global Iter: 1296500 training acc: 0.1875
Global Iter: 1296600 training loss: 1.95235
Global Iter: 1296600 training acc: 0.1875
Global Iter: 1296700 training loss: 1.94541
Global Iter: 1296700 training acc: 0.09375
Global Iter: 1296800 training loss: 2.00224
Global Iter: 1296800 training acc: 0.125
Global Iter: 1296900 train2017-06-22 18:06:30.631628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1298046
ing loss: 1.98527
Global Iter: 1296900 training acc: 0.09375
Global Iter: 1297000 training loss: 1.90889
Global Iter: 1297000 training acc: 0.1875
Global Iter: 1297100 training loss: 1.97317
Global Iter: 1297100 training acc: 0.125
Global Iter: 1297200 training loss: 1.99122
Global Iter: 1297200 training acc: 0.28125
Global Iter: 1297300 training loss: 2.06438
Global Iter: 1297300 training acc: 0.15625
Global Iter: 1297400 training loss: 1.98221
Global Iter: 1297400 training acc: 0.15625
Global Iter: 1297500 training loss: 2.02412
Global Iter: 1297500 training acc: 0.15625
Global Iter: 1297600 training loss: 1.90382
Global Iter: 1297600 training acc: 0.21875
Global Iter: 1297700 training loss: 2.04143
Global Iter: 1297700 training acc: 0.0625
Global Iter: 1297800 training loss: 1.9383
Global Iter: 1297800 training acc: 0.15625
Global Iter: 1297900 training loss: 1.9962
Global Iter: 1297900 training acc: 0.09375
Global Iter: 1298000 training loss: 1.94295
Global Iter: 1298000 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1298046
Number of Patches: 99778
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1298046
Global Iter: 1298100 training loss: 2.00026
Global Iter: 1298100 training acc: 0.28125
Global Iter: 1298200 training loss: 2.00535
Global Iter: 1298200 training acc: 0.25
Global Iter: 1298300 training loss: 2.036
Global Iter: 1298300 training acc: 0.15625
Global Iter: 1298400 training loss: 1.99506
Global Iter: 1298400 training acc: 0.15625
Global Iter: 1298500 training loss: 1.98135
Global Iter: 1298500 training acc: 0.25
Global Iter: 1298600 training loss: 1.96757
Global Iter: 1298600 training acc: 0.1875
Global Iter: 1298700 training loss: 1.9825
Global Iter: 1298700 training acc: 0.15625
Global Iter: 1298800 training loss: 1.99066
Global Iter: 1298800 training acc: 0.125
Global Iter: 1298900 training loss: 1.99312
Global Iter: 1298900 training acc: 0.15625
Global Iter: 1299000 training loss: 2.08258
Global Iter: 1299000 training acc: 0.125
Global Iter: 1299100 training loss: 2.01515
Global Iter: 1299100 training acc: 0.15625
Global Iter: 1299200 training loss: 2.02507
Global Iter: 1299200 training acc: 0.1875
Global Iter: 1299300 training loss: 2.12422
Global Iter: 1299300 training acc: 0.125
Global Iter: 1299400 training loss: 2.02276
Global Iter: 1299400 training acc: 0.15625
Global Iter: 1299500 training loss: 1.90156
Global Iter: 1299500 training acc: 0.21875
Global Iter: 1299600 training loss: 2.07909
Global Iter: 1299600 training acc: 0.21875
Global Iter: 1299700 training loss: 2.02481
Global Iter: 1299700 training acc: 0.15625
Global Iter: 1299800 training loss: 2.16607
Global Iter: 1299800 training acc: 0.125
Global Iter: 1299900 training loss: 1.98999
Global Iter: 1299900 training acc: 0.21875
Global Iter: 1300000 training loss: 1.96919
Global Iter: 1300000 training acc: 0.125
Global Iter: 1300100 training loss: 2.06818
Global Iter: 1300100 training acc: 0.125
Global Iter: 1300200 training loss: 1.95538
Global Iter: 1300200 training acc: 0.15625
Global Iter: 1300300 training loss: 1.99483
Global Iter: 1300300 training acc: 0.15625
Global Iter: 1300400 training loss: 2.0211
Global Iter: 1300400 training acc: 0.21875
Global Iter: 1300500 training loss: 1.95422
Global Iter: 1300500 training acc: 0.1875
Global Iter: 1300600 training loss: 1.91456
Global Iter: 1300600 training acc: 0.15625
Global Iter: 1300700 training loss: 2.10544
Global Iter: 1300700 training acc: 0.1875
Global Iter: 1300800 training loss: 1.92317
Global Iter: 1300800 training acc: 0.1875
Global Iter: 1300900 training loss: 2.03244
Global Iter: 1300900 training acc: 0.09375
Global Iter: 1301000 training loss: 1.96315
Global Iter: 1301000 training acc: 0.15625
Global Iter: 1301100 training loss: 2.00397
Global Iter: 1301100 training acc: 0.125
Global Iter: 1301200 training loss: 1.962
Global Iter: 1301200 training acc: 0.15625
Global Iter: 1301300 training loss: 2.0337
Global Iter: 1301300 training acc: 0.1875
Global Iter: 1301400 t2017-06-22 18:17:08.428934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1304283
raining loss: 1.99712
Global Iter: 1301400 training acc: 0.1875
Global Iter: 1301500 training loss: 1.92124
Global Iter: 1301500 training acc: 0.15625
Global Iter: 1301600 training loss: 2.02844
Global Iter: 1301600 training acc: 0.125
Global Iter: 1301700 training loss: 2.00733
Global Iter: 1301700 training acc: 0.125
Global Iter: 1301800 training loss: 2.02047
Global Iter: 1301800 training acc: 0.1875
Global Iter: 1301900 training loss: 2.0371
Global Iter: 1301900 training acc: 0.25
Global Iter: 1302000 training loss: 1.98633
Global Iter: 1302000 training acc: 0.09375
Global Iter: 1302100 training loss: 2.01495
Global Iter: 1302100 training acc: 0.21875
Global Iter: 1302200 training loss: 2.10127
Global Iter: 1302200 training acc: 0.15625
Global Iter: 1302300 training loss: 2.01712
Global Iter: 1302300 training acc: 0.15625
Global Iter: 1302400 training loss: 2.01487
Global Iter: 1302400 training acc: 0.15625
Global Iter: 1302500 training loss: 1.94969
Global Iter: 1302500 training acc: 0.28125
Global Iter: 1302600 training loss: 2.07926
Global Iter: 1302600 training acc: 0.1875
Global Iter: 1302700 training loss: 1.96802
Global Iter: 1302700 training acc: 0.125
Global Iter: 1302800 training loss: 2.02649
Global Iter: 1302800 training acc: 0.21875
Global Iter: 1302900 training loss: 2.06086
Global Iter: 1302900 training acc: 0.15625
Global Iter: 1303000 training loss: 1.9541
Global Iter: 1303000 training acc: 0.25
Global Iter: 1303100 training loss: 2.09244
Global Iter: 1303100 training acc: 0.21875
Global Iter: 1303200 training loss: 1.96413
Global Iter: 1303200 training acc: 0.125
Global Iter: 1303300 training loss: 1.95132
Global Iter: 1303300 training acc: 0.21875
Global Iter: 1303400 training loss: 2.13978
Global Iter: 1303400 training acc: 0.0625
Global Iter: 1303500 training loss: 2.07659
Global Iter: 1303500 training acc: 0.15625
Global Iter: 1303600 training loss: 2.02799
Global Iter: 1303600 training acc: 0.1875
Global Iter: 1303700 training loss: 2.09557
Global Iter: 1303700 training acc: 0.09375
Global Iter: 1303800 training loss: 1.9394
Global Iter: 1303800 training acc: 0.125
Global Iter: 1303900 training loss: 1.92363
Global Iter: 1303900 training acc: 0.21875
Global Iter: 1304000 training loss: 2.02763
Global Iter: 1304000 training acc: 0.1875
Global Iter: 1304100 training loss: 2.00387
Global Iter: 1304100 training acc: 0.15625
Global Iter: 1304200 training loss: 2.01879
Global Iter: 1304200 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1304283
Number of Patches: 98781
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1304283
Global Iter: 1304300 training loss: 1.96143
Global Iter: 1304300 training acc: 0.25
Global Iter: 1304400 training loss: 1.88727
Global Iter: 1304400 training acc: 0.25
Global Iter: 1304500 training loss: 1.91364
Global Iter: 1304500 training acc: 0.125
Global Iter: 1304600 training loss: 1.9977
Global Iter: 1304600 training acc: 0.15625
Global Iter: 1304700 training loss: 2.07246
Global Iter: 1304700 training acc: 0.15625
Global Iter: 1304800 training loss: 1.92057
Global Iter: 1304800 training acc: 0.25
Global Iter: 1304900 training loss: 2.02296
Global Iter: 1304900 training acc: 0.1875
Global Iter: 1305000 training loss: 2.03766
Global Iter: 1305000 training acc: 0.125
Global Iter: 1305100 training loss: 2.15537
Global Iter: 1305100 training acc: 0.1875
Global Iter: 1305200 training loss: 2.02736
Global Iter: 1305200 training acc: 0.25
Global Iter: 1305300 training loss: 1.98659
Global Iter: 1305300 training acc: 0.21875
Global Iter: 1305400 training loss: 1.94544
Global Iter: 1305400 training acc: 0.15625
Global Iter: 1305500 training loss: 1.93645
Global Iter: 1305500 training acc: 0.15625
Global Iter: 1305600 training loss: 2.10521
Global Iter: 1305600 training acc: 0.25
Global Iter: 1305700 training loss: 1.93086
Global Iter: 1305700 training acc: 0.15625
Global Iter: 1305800 training loss: 2.03591
Global Iter: 1305800 training acc: 0.15625
Global Iter: 1305900 traini2017-06-22 18:27:40.748583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
ng loss: 1.97006
Global Iter: 1305900 training acc: 0.125
Global Iter: 1306000 training loss: 1.97921
Global Iter: 1306000 training acc: 0.25
Global Iter: 1306100 training loss: 1.99692
Global Iter: 1306100 training acc: 0.1875
Global Iter: 1306200 training loss: 1.98449
Global Iter: 1306200 training acc: 0.125
Global Iter: 1306300 training loss: 1.91444
Global Iter: 1306300 training acc: 0.25
Global Iter: 1306400 training loss: 2.08069
Global Iter: 1306400 training acc: 0.15625
Global Iter: 1306500 training loss: 2.07502
Global Iter: 1306500 training acc: 0.0625
Global Iter: 1306600 training loss: 1.90524
Global Iter: 1306600 training acc: 0.15625
Global Iter: 1306700 training loss: 2.00894
Global Iter: 1306700 training acc: 0.21875
Global Iter: 1306800 training loss: 1.93896
Global Iter: 1306800 training acc: 0.1875
Global Iter: 1306900 training loss: 2.06463
Global Iter: 1306900 training acc: 0.1875
Global Iter: 1307000 training loss: 1.95825
Global Iter: 1307000 training acc: 0.15625
Global Iter: 1307100 training loss: 1.93143
Global Iter: 1307100 training acc: 0.25
Global Iter: 1307200 training loss: 1.98323
Global Iter: 1307200 training acc: 0.1875
Global Iter: 1307300 training loss: 1.89616
Global Iter: 1307300 training acc: 0.1875
Global Iter: 1307400 training loss: 1.92374
Global Iter: 1307400 training acc: 0.1875
Global Iter: 1307500 training loss: 1.99887
Global Iter: 1307500 training acc: 0.125
Global Iter: 1307600 training loss: 2.06419
Global Iter: 1307600 training acc: 0.1875
Global Iter: 1307700 training loss: 2.07404
Global Iter: 1307700 training acc: 0.125
Global Iter: 1307800 training loss: 2.04479
Global Iter: 1307800 training acc: 0.21875
Global Iter: 1307900 training loss: 1.9636
Global Iter: 1307900 training acc: 0.28125
Global Iter: 1308000 training loss: 1.99178
Global Iter: 1308000 training acc: 0.21875
Global Iter: 1308100 training loss: 2.08431
Global Iter: 1308100 training acc: 0.0
Global Iter: 1308200 training loss: 1.91803
Global Iter: 1308200 training acc: 0.1875
Global Iter: 1308300 training loss: 1.97121
Global Iter: 1308300 training acc: 0.25
Global Iter: 1308400 training loss: 2.0965
Global Iter: 1308400 training acc: 0.0625
Global Iter: 1308500 training loss: 2.0251
Global Iter: 1308500 training acc: 0.15625
Global Iter: 1308600 training loss: 2.0018
Global Iter: 1308600 training acc: 0.125
Global Iter: 1308700 training loss: 2.02876
Global Iter: 1308700 training acc: 0.09375
Global Iter: 1308800 training loss: 1.9985
Global Iter: 1308800 training acc: 0.25
Global Iter: 1308900 training loss: 1.93819
Global Iter: 1308900 training acc: 0.15625
Global Iter: 1309000 training loss: 1.93186
Global Iter: 1309000 training acc: 0.25
Global Iter: 1309100 training loss: 2.01767
Global Iter: 1309100 training acc: 0.09375
Global Iter: 1309200 training loss: 1.95491
Global Iter: 1309200 training acc: 0.3125
Global Iter: 1309300 training loss: 1.90508
Global Iter: 1309300 training acc: 0.21875
Global Iter: 1309400 training loss: 1.99189
Global Iter: 1309400 training acc: 0.25
Global Iter: 1309500 training loss: 1.97866
Global Iter: 1309500 training acc: 0.125
Global Iter: 1309600 training loss: 2.12185
Global Iter: 1309600 training acc: 0.09375
Global Iter: 1309700 training loss: 2.04129
Global Iter: 1309700 training acc: 0.125
Global Iter: 1309800 training loss: 1.8955
Global Iter: 1309800 training acc: 0.25
Global Iter: 1309900 training loss: 1.93689
Global Iter: 1309900 training acc: 0.15625
Global Iter: 1310000 training loss: 1.96168
Global Iter: 1310000 training acc: 0.15625
Global Iter: 1310100 training loss: 1.99509
Global Iter: 1310100 training acc: 0.1875
Global Iter: 1310200 training loss: 1.99239
Global Iter: 1310200 training acc: 0.125
Global Iter: 1310300 training loss: 2.0445
Global Iter: 1310300 training acc: 0.125
Global Iter: 1310400 training loss: 2.00496
Global Iter: 1310400 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1310457
Number of Patches: 97794
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_aINFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1310457
lexnet_b256_lr0005/model.ckpt-1310457
Global Iter: 1310500 training loss: 2.01502
Global Iter: 1310500 training acc: 0.15625
Global Iter: 1310600 training loss: 1.95308
Global Iter: 1310600 training acc: 0.21875
Global Iter: 1310700 training loss: 2.02524
Global Iter: 1310700 training acc: 0.03125
Global Iter: 1310800 training loss: 1.98869
Global Iter: 1310800 training acc: 0.25
Global Iter: 1310900 training loss: 1.95575
Global Iter: 1310900 training acc: 0.25
Global Iter: 1311000 training loss: 1.95121
Global Iter: 1311000 training acc: 0.1875
Global Iter: 1311100 training loss: 1.99137
Global Iter: 1311100 training acc: 0.15625
Global Iter: 1311200 training loss: 2.03496
Global Iter: 1311200 training acc: 0.21875
Global Iter: 1311300 training loss: 1.9589
Global Iter: 1311300 training acc: 0.25
Global Iter: 1311400 training loss: 1.94955
Global Iter: 1311400 training acc: 0.25
Global Iter: 1311500 training loss: 1.92602
Global Iter: 1311500 training acc: 0.21875
Global Iter: 1311600 training loss: 1.97709
Global Iter: 1311600 training acc: 0.1875
Global Iter: 1311700 training loss: 1.98146
Global Iter: 1311700 training acc: 0.125
Global Iter: 1311800 training loss: 1.93004
Global Iter: 1311800 training acc: 0.25
Global Iter: 1311900 training loss: 2.17816
Global Iter: 1311900 training acc: 0.15625
Global Iter: 1312000 training loss: 1.98166
Global Iter: 1312000 training acc: 0.15625
Global Iter: 1312100 training loss: 2.07622
Global Iter: 1312100 training acc: 0.09375
Global Iter: 1312200 training loss: 2.01595
Global Iter: 1312200 training acc: 0.09375
Global Iter: 1312300 training loss: 1.88618
Global Iter: 1312300 training acc: 0.34375
Global Iter: 1312400 training loss: 2.01476
Global Iter: 1312400 training acc: 0.09375
Global Iter: 1312500 training loss: 2.03765
Global Iter: 1312500 training acc: 0.125
Global Iter: 1312600 training loss: 2.01095
Global Iter: 1312600 training acc: 0.15625
Global Iter: 1312700 training loss: 2.14571
Global Iter: 1312700 training acc: 0.03125
Global Iter: 1312800 training loss: 2.01121
Global Iter: 1312800 training acc: 0.0625
Global Iter: 1312900 training loss: 1.98045
Global Iter: 1312900 training acc: 0.1875
Global Iter: 1313000 training loss: 2.01942
Global Iter: 1313000 training acc: 0.1875
Global Iter: 1313100 training loss: 1.95478
Global Iter: 1313100 training acc: 0.1875
Global Iter: 1313200 training loss: 1.96523
Global Iter: 1313200 training acc: 0.28125
Global Iter: 1313300 training loss: 1.95974
Global Iter: 1313300 training acc: 0.21875
Global Iter: 1313400 training loss: 1.95095
Global Iter: 1313400 training acc: 0.09375
Global Iter: 1313500 training loss: 1.93887
Global Iter: 1313500 training acc: 0.25
Global Iter: 1313600 training loss: 1.9693
Global Iter: 1313600 training acc: 0.25
Global Iter: 1313700 training loss: 1.94853
Global Iter: 1313700 training acc: 0.125
Global Iter: 1313800 training loss: 2.05227
Global Iter: 1313800 training acc: 0.0625
Global Iter: 1313900 training loss: 1.95974
Global Iter: 1313900 training acc: 0.1875
Global Iter: 1314000 training loss: 1.96419
Global Iter: 1314000 training acc: 0.125
Global Iter: 1314100 training loss: 2.01575
Global Iter: 1314100 training acc: 0.125
Global Iter: 1314200 training loss: 1.91964
Global Iter: 1314200 training acc: 0.1875
Global Iter: 1314300 training loss: 1.93971
Global Iter: 1314300 training acc: 0.15625
Global Iter: 1314400 training loss: 1.90281
Global Iter: 1314400 training acc: 0.15625
Global Iter: 1314500 training loss: 1.98407
Global Iter: 1314500 training acc: 0.09375
Global Iter: 1314600 training loss: 2.10649
Global Iter: 1314600 training acc: 0.21875
Global Iter: 1314700 training loss: 1.96739
Global Iter: 1314700 training acc: 0.34375
Global Iter: 1314800 training loss: 1.96121
Global Iter: 1314800 training acc: 0.21875
Global Iter: 1314900 training loss: 1.86486
Global Iter: 1314900 training acc: 0.15625
Global Iter: 1315000 training loss: 1.94011
Global Iter: 1315000 training acc: 0.25
Global Iter: 1315100 training loss: 2.06848
Global Iter: 1315100 training acc: 0.15625
Global Iter: 12017-06-22 18:38:07.688133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1316570
315200 training loss: 2.01365
Global Iter: 1315200 training acc: 0.09375
Global Iter: 1315300 training loss: 1.91657
Global Iter: 1315300 training acc: 0.1875
Global Iter: 1315400 training loss: 1.96978
Global Iter: 1315400 training acc: 0.1875
Global Iter: 1315500 training loss: 2.0379
Global Iter: 1315500 training acc: 0.21875
Global Iter: 1315600 training loss: 1.92114
Global Iter: 1315600 training acc: 0.125
Global Iter: 1315700 training loss: 1.93864
Global Iter: 1315700 training acc: 0.25
Global Iter: 1315800 training loss: 1.91774
Global Iter: 1315800 training acc: 0.1875
Global Iter: 1315900 training loss: 1.98695
Global Iter: 1315900 training acc: 0.21875
Global Iter: 1316000 training loss: 1.96938
Global Iter: 1316000 training acc: 0.1875
Global Iter: 1316100 training loss: 1.94252
Global Iter: 1316100 training acc: 0.3125
Global Iter: 1316200 training loss: 1.9837
Global Iter: 1316200 training acc: 0.15625
Global Iter: 1316300 training loss: 2.08745
Global Iter: 1316300 training acc: 0.21875
Global Iter: 1316400 training loss: 1.95142
Global Iter: 1316400 training acc: 0.1875
Global Iter: 1316500 training loss: 1.92076
Global Iter: 1316500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1316570
Number of Patches: 96817
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1316570
Global Iter: 1316600 training loss: 2.07224
Global Iter: 1316600 training acc: 0.09375
Global Iter: 1316700 training loss: 1.99253
Global Iter: 1316700 training acc: 0.09375
Global Iter: 1316800 training loss: 1.96047
Global Iter: 1316800 training acc: 0.1875
Global Iter: 1316900 training loss: 2.09981
Global Iter: 1316900 training acc: 0.15625
Global Iter: 1317000 training loss: 2.0247
Global Iter: 1317000 training acc: 0.21875
Global Iter: 1317100 training loss: 2.05746
Global Iter: 1317100 training acc: 0.0625
Global Iter: 1317200 training loss: 2.0008
Global Iter: 1317200 training acc: 0.21875
Global Iter: 1317300 training loss: 1.95137
Global Iter: 1317300 training acc: 0.15625
Global Iter: 1317400 training loss: 2.00748
Global Iter: 1317400 training acc: 0.15625
Global Iter: 1317500 training loss: 2.06322
Global Iter: 1317500 training acc: 0.09375
Global Iter: 1317600 training loss: 2.0236
Global Iter: 1317600 training acc: 0.15625
Global Iter: 1317700 training loss: 2.0153
Global Iter: 1317700 training acc: 0.1875
Global Iter: 1317800 training loss: 2.03907
Global Iter: 1317800 training acc: 0.125
Global Iter: 1317900 training loss: 2.02457
Global Iter: 1317900 training acc: 0.1875
Global Iter: 1318000 training loss: 2.00057
Global Iter: 1318000 training acc: 0.15625
Global Iter: 1318100 training loss: 1.94932
Global Iter: 1318100 training acc: 0.21875
Global Iter: 1318200 training loss: 1.90574
Global Iter: 1318200 training acc: 0.1875
Global Iter: 1318300 training loss: 1.97031
Global Iter: 1318300 training acc: 0.15625
Global Iter: 1318400 training loss: 1.89623
Global Iter: 1318400 training acc: 0.15625
Global Iter: 1318500 training loss: 2.00619
Global Iter: 1318500 training acc: 0.15625
Global Iter: 1318600 training loss: 1.92247
Global Iter: 1318600 training acc: 0.1875
Global Iter: 1318700 training loss: 2.0572
Global Iter: 1318700 training acc: 0.03125
Global Iter: 1318800 training loss: 2.00565
Global Iter: 1318800 training acc: 0.09375
Global Iter: 1318900 training loss: 1.97722
Global Iter: 1318900 training acc: 0.15625
Global Iter: 1319000 training loss: 2.00836
Global Iter: 1319000 training acc: 0.125
Global Iter: 1319100 training loss: 2.04849
Global Iter: 1319100 training acc: 0.09375
Global Iter: 1319200 training loss: 1.96101
Global Iter: 1319200 training acc: 0.0625
Global Iter: 1319300 training loss: 1.972
Global Iter: 1319300 training acc: 0.1875
Global Iter: 1319400 training loss: 1.96043
Global Iter: 1319400 training acc: 0.125
Global Iter: 1319500 training loss: 1.9133
Global Iter: 1319500 training acc: 0.09375
Global Iter: 1319600 training loss: 1.89879
Global Iter: 1319600 training acc: 0.28125
Global 2017-06-22 18:48:16.268482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1322622
Iter: 1319700 training loss: 2.03682
Global Iter: 1319700 training acc: 0.125
Global Iter: 1319800 training loss: 2.0568
Global Iter: 1319800 training acc: 0.09375
Global Iter: 1319900 training loss: 1.96441
Global Iter: 1319900 training acc: 0.1875
Global Iter: 1320000 training loss: 1.9579
Global Iter: 1320000 training acc: 0.125
Global Iter: 1320100 training loss: 2.07087
Global Iter: 1320100 training acc: 0.1875
Global Iter: 1320200 training loss: 2.03423
Global Iter: 1320200 training acc: 0.125
Global Iter: 1320300 training loss: 2.04633
Global Iter: 1320300 training acc: 0.1875
Global Iter: 1320400 training loss: 1.86671
Global Iter: 1320400 training acc: 0.375
Global Iter: 1320500 training loss: 2.02425
Global Iter: 1320500 training acc: 0.125
Global Iter: 1320600 training loss: 1.99143
Global Iter: 1320600 training acc: 0.21875
Global Iter: 1320700 training loss: 2.15436
Global Iter: 1320700 training acc: 0.3125
Global Iter: 1320800 training loss: 1.9306
Global Iter: 1320800 training acc: 0.15625
Global Iter: 1320900 training loss: 1.88019
Global Iter: 1320900 training acc: 0.1875
Global Iter: 1321000 training loss: 1.99894
Global Iter: 1321000 training acc: 0.09375
Global Iter: 1321100 training loss: 1.9045
Global Iter: 1321100 training acc: 0.25
Global Iter: 1321200 training loss: 1.89775
Global Iter: 1321200 training acc: 0.3125
Global Iter: 1321300 training loss: 1.89105
Global Iter: 1321300 training acc: 0.34375
Global Iter: 1321400 training loss: 2.11417
Global Iter: 1321400 training acc: 0.21875
Global Iter: 1321500 training loss: 1.98543
Global Iter: 1321500 training acc: 0.21875
Global Iter: 1321600 training loss: 1.9536
Global Iter: 1321600 training acc: 0.09375
Global Iter: 1321700 training loss: 1.99893
Global Iter: 1321700 training acc: 0.09375
Global Iter: 1321800 training loss: 2.12854
Global Iter: 1321800 training acc: 0.09375
Global Iter: 1321900 training loss: 1.99229
Global Iter: 1321900 training acc: 0.1875
Global Iter: 1322000 training loss: 2.04023
Global Iter: 1322000 training acc: 0.15625
Global Iter: 1322100 training loss: 2.06618
Global Iter: 1322100 training acc: 0.15625
Global Iter: 1322200 training loss: 1.97149
Global Iter: 1322200 training acc: 0.125
Global Iter: 1322300 training loss: 2.04856
Global Iter: 1322300 training acc: 0.0625
Global Iter: 1322400 training loss: 2.01645
Global Iter: 1322400 training acc: 0.28125
Global Iter: 1322500 training loss: 2.00488
Global Iter: 1322500 training acc: 0.21875
Global Iter: 1322600 training loss: 1.96357
Global Iter: 1322600 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1322622
Number of Patches: 95849
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1322622
Global Iter: 1322700 training loss: 2.02755
Global Iter: 1322700 training acc: 0.15625
Global Iter: 1322800 training loss: 2.10304
Global Iter: 1322800 training acc: 0.15625
Global Iter: 1322900 training loss: 1.88922
Global Iter: 1322900 training acc: 0.40625
Global Iter: 1323000 training loss: 1.96324
Global Iter: 1323000 training acc: 0.25
Global Iter: 1323100 training loss: 2.03671
Global Iter: 1323100 training acc: 0.15625
Global Iter: 1323200 training loss: 1.90222
Global Iter: 1323200 training acc: 0.25
Global Iter: 1323300 training loss: 1.9513
Global Iter: 1323300 training acc: 0.21875
Global Iter: 1323400 training loss: 2.00386
Global Iter: 1323400 training acc: 0.28125
Global Iter: 1323500 training loss: 1.96248
Global Iter: 1323500 training acc: 0.1875
Global Iter: 1323600 training loss: 1.94872
Global Iter: 1323600 training acc: 0.3125
Global Iter: 1323700 training loss: 1.96904
Global Iter: 1323700 training acc: 0.1875
Global Iter: 1323800 training loss: 2.01851
Global Iter: 1323800 training acc: 0.1875
Global Iter: 1323900 training loss: 2.08559
Global Iter: 1323900 training acc: 0.0625
Global Iter: 1324000 training loss: 1.97391
Global Iter: 1324000 training acc: 0.21875
Global Iter: 1324100 training loss: 2.06082
Global Iter: 1324100 training acc: 0.125
Global2017-06-22 18:58:26.761512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1328613
 Iter: 1324200 training loss: 1.95718
Global Iter: 1324200 training acc: 0.21875
Global Iter: 1324300 training loss: 2.04946
Global Iter: 1324300 training acc: 0.1875
Global Iter: 1324400 training loss: 1.99363
Global Iter: 1324400 training acc: 0.15625
Global Iter: 1324500 training loss: 2.0134
Global Iter: 1324500 training acc: 0.125
Global Iter: 1324600 training loss: 1.96152
Global Iter: 1324600 training acc: 0.15625
Global Iter: 1324700 training loss: 1.98435
Global Iter: 1324700 training acc: 0.21875
Global Iter: 1324800 training loss: 1.96067
Global Iter: 1324800 training acc: 0.1875
Global Iter: 1324900 training loss: 1.91025
Global Iter: 1324900 training acc: 0.21875
Global Iter: 1325000 training loss: 2.02981
Global Iter: 1325000 training acc: 0.1875
Global Iter: 1325100 training loss: 1.97331
Global Iter: 1325100 training acc: 0.15625
Global Iter: 1325200 training loss: 1.94768
Global Iter: 1325200 training acc: 0.1875
Global Iter: 1325300 training loss: 2.06252
Global Iter: 1325300 training acc: 0.125
Global Iter: 1325400 training loss: 1.97088
Global Iter: 1325400 training acc: 0.28125
Global Iter: 1325500 training loss: 1.95804
Global Iter: 1325500 training acc: 0.21875
Global Iter: 1325600 training loss: 1.9253
Global Iter: 1325600 training acc: 0.25
Global Iter: 1325700 training loss: 2.00571
Global Iter: 1325700 training acc: 0.125
Global Iter: 1325800 training loss: 1.97985
Global Iter: 1325800 training acc: 0.3125
Global Iter: 1325900 training loss: 1.94301
Global Iter: 1325900 training acc: 0.25
Global Iter: 1326000 training loss: 2.07888
Global Iter: 1326000 training acc: 0.28125
Global Iter: 1326100 training loss: 1.96548
Global Iter: 1326100 training acc: 0.25
Global Iter: 1326200 training loss: 1.92782
Global Iter: 1326200 training acc: 0.15625
Global Iter: 1326300 training loss: 2.08935
Global Iter: 1326300 training acc: 0.125
Global Iter: 1326400 training loss: 2.10469
Global Iter: 1326400 training acc: 0.21875
Global Iter: 1326500 training loss: 2.10277
Global Iter: 1326500 training acc: 0.03125
Global Iter: 1326600 training loss: 2.08732
Global Iter: 1326600 training acc: 0.03125
Global Iter: 1326700 training loss: 2.01777
Global Iter: 1326700 training acc: 0.28125
Global Iter: 1326800 training loss: 1.95788
Global Iter: 1326800 training acc: 0.25
Global Iter: 1326900 training loss: 2.12805
Global Iter: 1326900 training acc: 0.125
Global Iter: 1327000 training loss: 1.88882
Global Iter: 1327000 training acc: 0.21875
Global Iter: 1327100 training loss: 1.92491
Global Iter: 1327100 training acc: 0.15625
Global Iter: 1327200 training loss: 1.98215
Global Iter: 1327200 training acc: 0.21875
Global Iter: 1327300 training loss: 2.04538
Global Iter: 1327300 training acc: 0.0
Global Iter: 1327400 training loss: 1.96156
Global Iter: 1327400 training acc: 0.21875
Global Iter: 1327500 training loss: 2.05988
Global Iter: 1327500 training acc: 0.25
Global Iter: 1327600 training loss: 1.95044
Global Iter: 1327600 training acc: 0.21875
Global Iter: 1327700 training loss: 2.01874
Global Iter: 1327700 training acc: 0.09375
Global Iter: 1327800 training loss: 1.95981
Global Iter: 1327800 training acc: 0.1875
Global Iter: 1327900 training loss: 1.89644
Global Iter: 1327900 training acc: 0.28125
Global Iter: 1328000 training loss: 1.94276
Global Iter: 1328000 training acc: 0.25
Global Iter: 1328100 training loss: 1.94048
Global Iter: 1328100 training acc: 0.125
Global Iter: 1328200 training loss: 1.97711
Global Iter: 1328200 training acc: 0.15625
Global Iter: 1328300 training loss: 1.91765
Global Iter: 1328300 training acc: 0.1875
Global Iter: 1328400 training loss: 1.98715
Global Iter: 1328400 training acc: 0.0625
Global Iter: 1328500 training loss: 1.90946
Global Iter: 1328500 training acc: 0.0625
Global Iter: 1328600 training loss: 1.98129
Global Iter: 1328600 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1328613
Number of Patches: 94891
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1328613
Global Iter: 1328700 training loss: 2.01615
Global Iter: 1328700 training acc: 0.25
Global Iter: 1328800 training loss: 1.94928
Global Iter: 1328800 training acc: 0.3125
Global Iter: 1328900 training loss: 1.98979
Global Iter: 1328900 training acc: 0.1875
Global Iter: 1329000 training loss: 2.03158
Global Iter: 1329000 training acc: 0.1875
Global Iter: 1329100 training loss: 1.9045
Global Iter: 1329100 training acc: 0.1875
Global Iter: 1329200 training loss: 1.95143
Global Iter: 1329200 training acc: 0.1875
Global Iter: 1329300 training loss: 1.97135
Global Iter: 1329300 training acc: 0.15625
Global Iter: 1329400 training loss: 2.01745
Global Iter: 1329400 training acc: 0.21875
Global Iter: 1329500 training loss: 2.01229
Global Iter: 1329500 training acc: 0.21875
Global Iter: 1329600 training loss: 1.9836
Global Iter: 1329600 training acc: 0.15625
Global Iter: 1329700 training loss: 1.99315
Global Iter: 1329700 training acc: 0.09375
Global Iter: 1329800 training loss: 1.91084
Global Iter: 1329800 training acc: 0.3125
Global Iter: 1329900 training loss: 1.9519
Global Iter: 1329900 training acc: 0.25
Global Iter: 1330000 training loss: 2.07105
Global Iter: 1330000 training acc: 0.125
Global Iter: 1330100 training loss: 1.94642
Global Iter: 1330100 training acc: 0.25
Global Iter: 1330200 training loss: 1.95936
Global Iter: 1330200 training acc: 0.125
Global Iter: 1330300 training loss: 1.93682
Global Iter: 1330300 training acc: 0.375
Global Iter: 1330400 training loss: 1.97622
Global Iter: 1330400 training acc: 0.09375
Global Iter: 1330500 training loss: 2.14576
Global Iter: 1330500 training acc: 0.09375
Global Iter: 1330600 training loss: 1.87413
Global Iter: 1330600 training acc: 0.3125
Global Iter: 1330700 training loss: 1.96628
Global Iter: 1330700 training acc: 0.3125
Global Iter: 1330800 training loss: 2.04943
Global Iter: 1330800 training acc: 0.1875
Global Iter: 1330900 training loss: 2.09488
Global Iter: 1330900 training acc: 0.15625
Global Iter: 1331000 training loss: 1.99314
Global Iter: 1331000 training acc: 0.125
Global Iter: 1331100 training loss: 1.96762
Global Iter: 1331100 training acc: 0.15625
Global Iter: 1331200 training loss: 2.00373
Global Iter: 1331200 training acc: 0.09375
Global Iter: 1331300 training loss: 2.00674
Global Iter: 1331300 training acc: 0.25
Global Iter: 1331400 training loss: 2.05679
Global Iter: 1331400 training acc: 0.1875
Global Iter: 1331500 training loss: 2.0838
Global Iter: 1331500 training acc: 0.09375
Global Iter: 1331600 training loss: 1.98663
Global Iter: 1331600 training acc: 0.09375
Global Iter: 1331700 training loss: 1.93425
Global Iter: 1331700 training acc: 0.21875
Global Iter: 1331800 training loss: 2.14443
Global Iter: 1331800 training acc: 0.09375
Global Iter: 1331900 training loss: 2.1378
Global Iter: 1331900 training acc: 0.15625
Global Iter: 1332000 training loss: 2.0232
Global Iter: 1332000 training acc: 0.09375
Global Iter: 1332100 training loss: 1.91788
Global Iter: 1332100 training acc: 0.28125
Global Iter: 1332200 training loss: 1.99329
Global Iter: 1332200 training acc: 0.15625
Global Iter: 1332300 training loss: 1.98711
Global Iter: 1332300 training acc: 0.125
Global Iter: 1332400 training loss: 1.88424
Global Iter: 1332400 training acc: 0.3125
Global Iter: 1332500 training loss: 2.04856
Global Iter: 1332500 training acc: 0.03125
Global Iter: 1332600 training loss: 1.92787
Global Iter: 1332600 training acc: 0.15625
Global Iter: 1332700 training loss: 2.09197
Global Iter: 1332700 training acc: 0.21875
Global Iter: 1332800 training loss: 1.92083
Global Iter: 1332800 training acc: 0.25
Global Iter: 1332900 training loss: 2.11577
Global Iter: 1332900 training acc: 0.09375
Global Iter: 1333000 training loss: 1.94477
Global Iter: 1333000 training acc: 0.28125
Global Iter: 1333100 training loss: 1.99447
Global Iter: 1333100 training acc: 0.0625
Global Iter: 1333200 training loss: 2.01468
Global Iter: 1333200 training acc: 0.09375
Global Iter: 1333300 training loss: 2.02403
Global Iter: 1333300 training acc: 0.125
Global Iter: 1333400 training loss: 1.9167
Global Iter: 13332017-06-22 19:08:28.060882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1334544
400 training acc: 0.1875
Global Iter: 1333500 training loss: 1.90199
Global Iter: 1333500 training acc: 0.28125
Global Iter: 1333600 training loss: 2.03568
Global Iter: 1333600 training acc: 0.125
Global Iter: 1333700 training loss: 1.94259
Global Iter: 1333700 training acc: 0.125
Global Iter: 1333800 training loss: 1.97863
Global Iter: 1333800 training acc: 0.28125
Global Iter: 1333900 training loss: 1.97901
Global Iter: 1333900 training acc: 0.1875
Global Iter: 1334000 training loss: 2.04903
Global Iter: 1334000 training acc: 0.09375
Global Iter: 1334100 training loss: 1.96461
Global Iter: 1334100 training acc: 0.25
Global Iter: 1334200 training loss: 2.02421
Global Iter: 1334200 training acc: 0.28125
Global Iter: 1334300 training loss: 2.0714
Global Iter: 1334300 training acc: 0.09375
Global Iter: 1334400 training loss: 2.01356
Global Iter: 1334400 training acc: 0.125
Global Iter: 1334500 training loss: 2.14644
Global Iter: 1334500 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1334544
Number of Patches: 93943
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1334544
Global Iter: 1334600 training loss: 2.16751
Global Iter: 1334600 training acc: 0.0625
Global Iter: 1334700 training loss: 1.90415
Global Iter: 1334700 training acc: 0.21875
Global Iter: 1334800 training loss: 1.92414
Global Iter: 1334800 training acc: 0.21875
Global Iter: 1334900 training loss: 2.05005
Global Iter: 1334900 training acc: 0.09375
Global Iter: 1335000 training loss: 1.95771
Global Iter: 1335000 training acc: 0.15625
Global Iter: 1335100 training loss: 2.02905
Global Iter: 1335100 training acc: 0.15625
Global Iter: 1335200 training loss: 2.03601
Global Iter: 1335200 training acc: 0.15625
Global Iter: 1335300 training loss: 1.91796
Global Iter: 1335300 training acc: 0.1875
Global Iter: 1335400 training loss: 1.97693
Global Iter: 1335400 training acc: 0.125
Global Iter: 1335500 training loss: 1.96912
Global Iter: 1335500 training acc: 0.09375
Global Iter: 1335600 training loss: 1.94159
Global Iter: 1335600 training acc: 0.28125
Global Iter: 1335700 training loss: 2.01193
Global Iter: 1335700 training acc: 0.21875
Global Iter: 1335800 training loss: 1.97007
Global Iter: 1335800 training acc: 0.1875
Global Iter: 1335900 training loss: 2.01086
Global Iter: 1335900 training acc: 0.125
Global Iter: 1336000 training loss: 1.96431
Global Iter: 1336000 training acc: 0.15625
Global Iter: 1336100 training loss: 2.008
Global Iter: 1336100 training acc: 0.1875
Global Iter: 1336200 training loss: 1.9925
Global Iter: 1336200 training acc: 0.1875
Global Iter: 1336300 training loss: 2.05978
Global Iter: 1336300 training acc: 0.125
Global Iter: 1336400 training loss: 2.08578
Global Iter: 1336400 training acc: 0.125
Global Iter: 1336500 training loss: 1.96644
Global Iter: 1336500 training acc: 0.125
Global Iter: 1336600 training loss: 2.049
Global Iter: 1336600 training acc: 0.125
Global Iter: 1336700 training loss: 2.16567
Global Iter: 1336700 training acc: 0.125
Global Iter: 1336800 training loss: 2.03703
Global Iter: 1336800 training acc: 0.03125
Global Iter: 1336900 training loss: 2.03907
Global Iter: 1336900 training acc: 0.1875
Global Iter: 1337000 training loss: 1.96353
Global Iter: 1337000 training acc: 0.21875
Global Iter: 1337100 training loss: 1.89735
Global Iter: 1337100 training acc: 0.28125
Global Iter: 1337200 training loss: 1.95109
Global Iter: 1337200 training acc: 0.15625
Global Iter: 1337300 training loss: 1.92908
Global Iter: 1337300 training acc: 0.15625
Global Iter: 1337400 training loss: 2.02981
Global Iter: 1337400 training acc: 0.09375
Global Iter: 1337500 training loss: 2.0077
Global Iter: 1337500 training acc: 0.21875
Global Iter: 1337600 training loss: 1.93895
Global Iter: 1337600 training acc: 0.1875
Global Iter: 1337700 training loss: 2.03354
Global Iter: 1337700 training acc: 0.1875
Global Iter: 1337800 training loss: 2.00705
Global Iter: 1337800 training acc: 0.15625
Global Iter: 1337900 training loss: 2.12698
Global Iter: 12017-06-22 19:18:25.612661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1340416
337900 training acc: 0.0625
Global Iter: 1338000 training loss: 1.91441
Global Iter: 1338000 training acc: 0.15625
Global Iter: 1338100 training loss: 2.03296
Global Iter: 1338100 training acc: 0.25
Global Iter: 1338200 training loss: 1.96144
Global Iter: 1338200 training acc: 0.03125
Global Iter: 1338300 training loss: 2.00732
Global Iter: 1338300 training acc: 0.15625
Global Iter: 1338400 training loss: 1.95531
Global Iter: 1338400 training acc: 0.21875
Global Iter: 1338500 training loss: 1.96013
Global Iter: 1338500 training acc: 0.1875
Global Iter: 1338600 training loss: 1.96609
Global Iter: 1338600 training acc: 0.21875
Global Iter: 1338700 training loss: 1.88377
Global Iter: 1338700 training acc: 0.28125
Global Iter: 1338800 training loss: 2.05597
Global Iter: 1338800 training acc: 0.125
Global Iter: 1338900 training loss: 2.09735
Global Iter: 1338900 training acc: 0.21875
Global Iter: 1339000 training loss: 1.94359
Global Iter: 1339000 training acc: 0.28125
Global Iter: 1339100 training loss: 2.00062
Global Iter: 1339100 training acc: 0.1875
Global Iter: 1339200 training loss: 1.8933
Global Iter: 1339200 training acc: 0.3125
Global Iter: 1339300 training loss: 2.19632
Global Iter: 1339300 training acc: 0.0
Global Iter: 1339400 training loss: 1.87966
Global Iter: 1339400 training acc: 0.15625
Global Iter: 1339500 training loss: 2.01635
Global Iter: 1339500 training acc: 0.1875
Global Iter: 1339600 training loss: 2.17431
Global Iter: 1339600 training acc: 0.125
Global Iter: 1339700 training loss: 2.1612
Global Iter: 1339700 training acc: 0.09375
Global Iter: 1339800 training loss: 2.01345
Global Iter: 1339800 training acc: 0.21875
Global Iter: 1339900 training loss: 1.91219
Global Iter: 1339900 training acc: 0.25
Global Iter: 1340000 training loss: 2.05619
Global Iter: 1340000 training acc: 0.0625
Global Iter: 1340100 training loss: 1.97808
Global Iter: 1340100 training acc: 0.1875
Global Iter: 1340200 training loss: 2.06846
Global Iter: 1340200 training acc: 0.1875
Global Iter: 1340300 training loss: 1.92523
Global Iter: 1340300 training acc: 0.3125
Global Iter: 1340400 training loss: 1.99565
Global Iter: 1340400 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1340416
Number of Patches: 93004
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1340416
Global Iter: 1340500 training loss: 2.00139
Global Iter: 1340500 training acc: 0.21875
Global Iter: 1340600 training loss: 2.01685
Global Iter: 1340600 training acc: 0.25
Global Iter: 1340700 training loss: 1.90466
Global Iter: 1340700 training acc: 0.3125
Global Iter: 1340800 training loss: 1.89047
Global Iter: 1340800 training acc: 0.125
Global Iter: 1340900 training loss: 2.07061
Global Iter: 1340900 training acc: 0.21875
Global Iter: 1341000 training loss: 1.9329
Global Iter: 1341000 training acc: 0.09375
Global Iter: 1341100 training loss: 1.87974
Global Iter: 1341100 training acc: 0.34375
Global Iter: 1341200 training loss: 1.95225
Global Iter: 1341200 training acc: 0.21875
Global Iter: 1341300 training loss: 2.03735
Global Iter: 1341300 training acc: 0.1875
Global Iter: 1341400 training loss: 1.97905
Global Iter: 1341400 training acc: 0.1875
Global Iter: 1341500 training loss: 1.96733
Global Iter: 1341500 training acc: 0.21875
Global Iter: 1341600 training loss: 1.99143
Global Iter: 1341600 training acc: 0.15625
Global Iter: 1341700 training loss: 1.99856
Global Iter: 1341700 training acc: 0.15625
Global Iter: 1341800 training loss: 1.90377
Global Iter: 1341800 training acc: 0.21875
Global Iter: 1341900 training loss: 1.99973
Global Iter: 1341900 training acc: 0.09375
Global Iter: 1342000 training loss: 2.05262
Global Iter: 1342000 training acc: 0.25
Global Iter: 1342100 training loss: 1.90758
Global Iter: 1342100 training acc: 0.3125
Global Iter: 1342200 training loss: 2.04746
Global Iter: 1342200 training acc: 0.21875
Global Iter: 1342300 training loss: 2.06132
Global Iter: 1342300 training acc: 0.1875
Global Iter: 1342400 training loss: 1.8934
Global Iter:2017-06-22 19:28:21.799102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1346229
 1342400 training acc: 0.28125
Global Iter: 1342500 training loss: 2.04353
Global Iter: 1342500 training acc: 0.0625
Global Iter: 1342600 training loss: 2.01795
Global Iter: 1342600 training acc: 0.125
Global Iter: 1342700 training loss: 1.96763
Global Iter: 1342700 training acc: 0.1875
Global Iter: 1342800 training loss: 1.98499
Global Iter: 1342800 training acc: 0.1875
Global Iter: 1342900 training loss: 1.93934
Global Iter: 1342900 training acc: 0.15625
Global Iter: 1343000 training loss: 1.98471
Global Iter: 1343000 training acc: 0.28125
Global Iter: 1343100 training loss: 1.99755
Global Iter: 1343100 training acc: 0.21875
Global Iter: 1343200 training loss: 1.9077
Global Iter: 1343200 training acc: 0.1875
Global Iter: 1343300 training loss: 2.05755
Global Iter: 1343300 training acc: 0.125
Global Iter: 1343400 training loss: 1.95765
Global Iter: 1343400 training acc: 0.1875
Global Iter: 1343500 training loss: 2.05712
Global Iter: 1343500 training acc: 0.15625
Global Iter: 1343600 training loss: 1.95191
Global Iter: 1343600 training acc: 0.1875
Global Iter: 1343700 training loss: 1.9467
Global Iter: 1343700 training acc: 0.21875
Global Iter: 1343800 training loss: 1.92266
Global Iter: 1343800 training acc: 0.21875
Global Iter: 1343900 training loss: 1.92936
Global Iter: 1343900 training acc: 0.21875
Global Iter: 1344000 training loss: 1.98921
Global Iter: 1344000 training acc: 0.15625
Global Iter: 1344100 training loss: 2.13499
Global Iter: 1344100 training acc: 0.25
Global Iter: 1344200 training loss: 1.91252
Global Iter: 1344200 training acc: 0.15625
Global Iter: 1344300 training loss: 1.90511
Global Iter: 1344300 training acc: 0.1875
Global Iter: 1344400 training loss: 1.94025
Global Iter: 1344400 training acc: 0.25
Global Iter: 1344500 training loss: 2.04216
Global Iter: 1344500 training acc: 0.125
Global Iter: 1344600 training loss: 1.99275
Global Iter: 1344600 training acc: 0.09375
Global Iter: 1344700 training loss: 2.14328
Global Iter: 1344700 training acc: 0.0625
Global Iter: 1344800 training loss: 1.92813
Global Iter: 1344800 training acc: 0.15625
Global Iter: 1344900 training loss: 2.00073
Global Iter: 1344900 training acc: 0.21875
Global Iter: 1345000 training loss: 2.01037
Global Iter: 1345000 training acc: 0.21875
Global Iter: 1345100 training loss: 2.03086
Global Iter: 1345100 training acc: 0.1875
Global Iter: 1345200 training loss: 2.11903
Global Iter: 1345200 training acc: 0.09375
Global Iter: 1345300 training loss: 1.84196
Global Iter: 1345300 training acc: 0.21875
Global Iter: 1345400 training loss: 1.94805
Global Iter: 1345400 training acc: 0.1875
Global Iter: 1345500 training loss: 1.9696
Global Iter: 1345500 training acc: 0.21875
Global Iter: 1345600 training loss: 2.06766
Global Iter: 1345600 training acc: 0.1875
Global Iter: 1345700 training loss: 2.1842
Global Iter: 1345700 training acc: 0.28125
Global Iter: 1345800 training loss: 1.98215
Global Iter: 1345800 training acc: 0.1875
Global Iter: 1345900 training loss: 1.90635
Global Iter: 1345900 training acc: 0.1875
Global Iter: 1346000 training loss: 1.96548
Global Iter: 1346000 training acc: 0.28125
Global Iter: 1346100 training loss: 1.87615
Global Iter: 1346100 training acc: 0.1875
Global Iter: 1346200 training loss: 1.9559
Global Iter: 1346200 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1346229
Number of Patches: 92074
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1346229
Global Iter: 1346300 training loss: 1.98586
Global Iter: 1346300 training acc: 0.15625
Global Iter: 1346400 training loss: 2.11891
Global Iter: 1346400 training acc: 0.1875
Global Iter: 1346500 training loss: 2.03472
Global Iter: 1346500 training acc: 0.125
Global Iter: 1346600 training loss: 2.01469
Global Iter: 1346600 training acc: 0.125
Global Iter: 1346700 training loss: 2.10388
Global Iter: 1346700 training acc: 0.03125
Global Iter: 1346800 training loss: 1.87372
Global Iter: 1346800 training acc: 0.34375
Global Iter: 1346900 training loss: 2.03146
Global Iter: 1346900 training acc: 0.125
Global Iter: 1347000 training loss: 1.95589
Global Iter: 1347000 training acc: 0.1875
Global Iter: 1347100 training loss: 1.99521
Global Iter: 1347100 training acc: 0.09375
Global Iter: 1347200 training loss: 1.98263
Global Iter: 1347200 training acc: 0.15625
Global Iter: 1347300 training loss: 1.96199
Global Iter: 1347300 training acc: 0.1875
Global Iter: 1347400 training loss: 1.92243
Global Iter: 1347400 training acc: 0.15625
Global Iter: 1347500 training loss: 1.92781
Global Iter: 1347500 training acc: 0.28125
Global Iter: 1347600 training loss: 1.90854
Global Iter: 1347600 training acc: 0.28125
Global Iter: 1347700 training loss: 1.98845
Global Iter: 1347700 training acc: 0.15625
Global Iter: 1347800 training loss: 1.874
Global Iter: 1347800 training acc: 0.3125
Global Iter: 1347900 training loss: 1.98011
Global Iter: 1347900 training acc: 0.1875
Global Iter: 1348000 training loss: 2.0886
Global Iter: 1348000 training acc: 0.21875
Global Iter: 1348100 training loss: 1.9545
Global Iter: 1348100 training acc: 0.15625
Global Iter: 1348200 training loss: 2.03859
Global Iter: 1348200 training acc: 0.21875
Global Iter: 1348300 training loss: 1.9496
Global Iter: 1348300 training acc: 0.1875
Global Iter: 1348400 training loss: 1.95607
Global Iter: 1348400 training acc: 0.15625
Global Iter: 1348500 training loss: 1.94528
Global Iter: 1348500 training acc: 0.09375
Global Iter: 1348600 training loss: 2.10133
Global Iter: 1348600 training acc: 0.125
Global Iter: 1348700 training loss: 2.04775
Global Iter: 1348700 training acc: 0.09375
Global Iter: 1348800 training loss: 1.91725
Global Iter: 1348800 training acc: 0.34375
Global Iter: 1348900 training loss: 1.9469
Global Iter: 1348900 training acc: 0.1875
Global Iter: 1349000 training loss: 1.97383
Global Iter: 1349000 training acc: 0.25
Global Iter: 1349100 training loss: 2.03519
Global Iter: 1349100 training acc: 0.1875
Global Iter: 1349200 training loss: 2.04197
Global Iter: 1349200 training acc: 0.0625
Global Iter: 1349300 training loss: 1.97729
Global Iter: 1349300 training acc: 0.15625
Global Iter: 1349400 training loss: 1.96278
Global Iter: 1349400 training acc: 0.09375
Global Iter: 1349500 training loss: 1.96885
Global Iter: 1349500 training acc: 0.09375
Global Iter: 1349600 training loss: 1.96671
Global Iter: 1349600 training acc: 0.21875
Global Iter: 1349700 training loss: 2.04685
Global Iter: 1349700 training acc: 0.25
Global Iter: 1349800 training loss: 2.03355
Global Iter: 1349800 training acc: 0.21875
Global Iter: 1349900 training loss: 2.06143
Global Iter: 1349900 training acc: 0.1875
Global Iter: 1350000 training loss: 2.04144
Global Iter: 1350000 training acc: 0.1875
Global Iter: 1350100 training loss: 2.12354
Global Iter: 1350100 training acc: 0.1875
Global Iter: 1350200 training loss: 2.0319
Global Iter: 1350200 training acc: 0.15625
Global Iter: 1350300 training loss: 1.99017
Global Iter: 1350300 training acc: 0.125
Global Iter: 1350400 training loss: 2.09543
Global Iter: 1350400 training acc: 0.1875
Global Iter: 1350500 training loss: 1.89879
Global Iter: 1350500 training acc: 0.21875
Global Iter: 1350600 training loss: 2.01134
Global Iter: 1350600 training acc: 0.21875
Global Iter: 1350700 training loss: 1.94278
Global Iter: 1350700 training acc: 0.09375
Global Iter: 1350800 training loss: 1.95778
Global Iter: 1350800 training acc: 0.21875
Global Iter: 1350900 training loss: 1.99682
Global Iter: 1350900 training acc: 0.125
Global Iter: 1351000 training loss: 2.03541
Global Iter: 1351000 training acc: 0.15625
Global Iter: 1351100 training loss: 1.8882
Global Iter: 1351100 training acc: 0.28125
Global Iter: 1351200 training loss: 1.88983
Global Iter: 1351200 training acc: 0.1875
Global Iter: 1351300 training loss: 2.03075
Global Iter: 1351300 training acc: 0.25
Global Iter: 1351400 training loss: 1.99652
Global Iter: 1351400 training acc: 0.21875
Global Iter: 1351500 training loss: 1.99945
Global Iter: 1351500 training acc: 0.21875
Global Iter: 1351600 training loss: 1.93801
Global Iter: 1351600 training acc: 0.34375
Global I2017-06-22 19:38:12.325652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1351984
ter: 1351700 training loss: 1.94331
Global Iter: 1351700 training acc: 0.21875
Global Iter: 1351800 training loss: 1.99422
Global Iter: 1351800 training acc: 0.1875
Global Iter: 1351900 training loss: 1.9494
Global Iter: 1351900 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1351984
Number of Patches: 91154
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1351984
Global Iter: 1352000 training loss: 1.981
Global Iter: 1352000 training acc: 0.21875
Global Iter: 1352100 training loss: 2.06619
Global Iter: 1352100 training acc: 0.125
Global Iter: 1352200 training loss: 1.95733
Global Iter: 1352200 training acc: 0.1875
Global Iter: 1352300 training loss: 2.00675
Global Iter: 1352300 training acc: 0.0625
Global Iter: 1352400 training loss: 2.02666
Global Iter: 1352400 training acc: 0.0625
Global Iter: 1352500 training loss: 1.91777
Global Iter: 1352500 training acc: 0.21875
Global Iter: 1352600 training loss: 1.93948
Global Iter: 1352600 training acc: 0.125
Global Iter: 1352700 training loss: 1.91225
Global Iter: 1352700 training acc: 0.25
Global Iter: 1352800 training loss: 2.08896
Global Iter: 1352800 training acc: 0.125
Global Iter: 1352900 training loss: 1.90916
Global Iter: 1352900 training acc: 0.1875
Global Iter: 1353000 training loss: 1.95492
Global Iter: 1353000 training acc: 0.1875
Global Iter: 1353100 training loss: 1.97677
Global Iter: 1353100 training acc: 0.25
Global Iter: 1353200 training loss: 1.97738
Global Iter: 1353200 training acc: 0.21875
Global Iter: 1353300 training loss: 2.07434
Global Iter: 1353300 training acc: 0.125
Global Iter: 1353400 training loss: 2.11651
Global Iter: 1353400 training acc: 0.125
Global Iter: 1353500 training loss: 2.02788
Global Iter: 1353500 training acc: 0.15625
Global Iter: 1353600 training loss: 1.9889
Global Iter: 1353600 training acc: 0.1875
Global Iter: 1353700 training loss: 2.04457
Global Iter: 1353700 training acc: 0.125
Global Iter: 1353800 training loss: 1.91893
Global Iter: 1353800 training acc: 0.21875
Global Iter: 1353900 training loss: 1.98665
Global Iter: 1353900 training acc: 0.25
Global Iter: 1354000 training loss: 2.07597
Global Iter: 1354000 training acc: 0.25
Global Iter: 1354100 training loss: 1.98019
Global Iter: 1354100 training acc: 0.1875
Global Iter: 1354200 training loss: 1.97998
Global Iter: 1354200 training acc: 0.1875
Global Iter: 1354300 training loss: 2.18593
Global Iter: 1354300 training acc: 0.0625
Global Iter: 1354400 training loss: 2.02806
Global Iter: 1354400 training acc: 0.21875
Global Iter: 1354500 training loss: 2.11259
Global Iter: 1354500 training acc: 0.15625
Global Iter: 1354600 training loss: 2.03799
Global Iter: 1354600 training acc: 0.21875
Global Iter: 1354700 training loss: 2.04167
Global Iter: 1354700 training acc: 0.28125
Global Iter: 1354800 training loss: 2.04238
Global Iter: 1354800 training acc: 0.125
Global Iter: 1354900 training loss: 2.08773
Global Iter: 1354900 training acc: 0.0625
Global Iter: 1355000 training loss: 1.95777
Global Iter: 1355000 training acc: 0.3125
Global Iter: 1355100 training loss: 1.94885
Global Iter: 1355100 training acc: 0.25
Global Iter: 1355200 training loss: 1.90071
Global Iter: 1355200 training acc: 0.1875
Global Iter: 1355300 training loss: 2.02295
Global Iter: 1355300 training acc: 0.15625
Global Iter: 1355400 training loss: 1.90826
Global Iter: 1355400 training acc: 0.28125
Global Iter: 1355500 training loss: 2.06632
Global Iter: 1355500 training acc: 0.25
Global Iter: 1355600 training loss: 2.0522
Global Iter: 1355600 training acc: 0.1875
Global Iter: 1355700 training loss: 2.00835
Global Iter: 1355700 training acc: 0.15625
Global Iter: 1355800 training loss: 1.95015
Global Iter: 1355800 training acc: 0.21875
Global Iter: 1355900 training loss: 1.88357
Global Iter: 1355900 training acc: 0.28125
Global Iter: 1356000 training loss: 1.9322
Global Iter: 1356000 training acc: 0.1875
Global Iter: 1356100 training loss: 2.00468
Global Iter: 1356100 training acc: 0.21875
Global Iter: 1356200 2017-06-22 19:47:51.303814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1357682
training loss: 2.04864
Global Iter: 1356200 training acc: 0.28125
Global Iter: 1356300 training loss: 1.94778
Global Iter: 1356300 training acc: 0.15625
Global Iter: 1356400 training loss: 2.05447
Global Iter: 1356400 training acc: 0.15625
Global Iter: 1356500 training loss: 1.97423
Global Iter: 1356500 training acc: 0.1875
Global Iter: 1356600 training loss: 1.97591
Global Iter: 1356600 training acc: 0.28125
Global Iter: 1356700 training loss: 2.0139
Global Iter: 1356700 training acc: 0.15625
Global Iter: 1356800 training loss: 1.92841
Global Iter: 1356800 training acc: 0.1875
Global Iter: 1356900 training loss: 2.04731
Global Iter: 1356900 training acc: 0.21875
Global Iter: 1357000 training loss: 1.98194
Global Iter: 1357000 training acc: 0.0625
Global Iter: 1357100 training loss: 1.87508
Global Iter: 1357100 training acc: 0.375
Global Iter: 1357200 training loss: 2.02309
Global Iter: 1357200 training acc: 0.09375
Global Iter: 1357300 training loss: 2.14279
Global Iter: 1357300 training acc: 0.0625
Global Iter: 1357400 training loss: 1.96234
Global Iter: 1357400 training acc: 0.3125
Global Iter: 1357500 training loss: 2.02176
Global Iter: 1357500 training acc: 0.125
Global Iter: 1357600 training loss: 2.04567
Global Iter: 1357600 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1357682
Number of Patches: 90243
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1357682
Global Iter: 1357700 training loss: 1.96962
Global Iter: 1357700 training acc: 0.1875
Global Iter: 1357800 training loss: 1.97918
Global Iter: 1357800 training acc: 0.09375
Global Iter: 1357900 training loss: 1.9539
Global Iter: 1357900 training acc: 0.28125
Global Iter: 1358000 training loss: 2.01352
Global Iter: 1358000 training acc: 0.0625
Global Iter: 1358100 training loss: 1.96748
Global Iter: 1358100 training acc: 0.1875
Global Iter: 1358200 training loss: 2.02152
Global Iter: 1358200 training acc: 0.15625
Global Iter: 1358300 training loss: 1.99472
Global Iter: 1358300 training acc: 0.15625
Global Iter: 1358400 training loss: 2.00047
Global Iter: 1358400 training acc: 0.1875
Global Iter: 1358500 training loss: 2.00822
Global Iter: 1358500 training acc: 0.125
Global Iter: 1358600 training loss: 1.99284
Global Iter: 1358600 training acc: 0.15625
Global Iter: 1358700 training loss: 2.00421
Global Iter: 1358700 training acc: 0.34375
Global Iter: 1358800 training loss: 2.10931
Global Iter: 1358800 training acc: 0.0625
Global Iter: 1358900 training loss: 1.93354
Global Iter: 1358900 training acc: 0.25
Global Iter: 1359000 training loss: 2.08269
Global Iter: 1359000 training acc: 0.03125
Global Iter: 1359100 training loss: 1.89812
Global Iter: 1359100 training acc: 0.15625
Global Iter: 1359200 training loss: 1.99351
Global Iter: 1359200 training acc: 0.0625
Global Iter: 1359300 training loss: 2.01238
Global Iter: 1359300 training acc: 0.1875
Global Iter: 1359400 training loss: 1.90455
Global Iter: 1359400 training acc: 0.125
Global Iter: 1359500 training loss: 1.99178
Global Iter: 1359500 training acc: 0.125
Global Iter: 1359600 training loss: 1.95987
Global Iter: 1359600 training acc: 0.125
Global Iter: 1359700 training loss: 2.12806
Global Iter: 1359700 training acc: 0.21875
Global Iter: 1359800 training loss: 1.96898
Global Iter: 1359800 training acc: 0.3125
Global Iter: 1359900 training loss: 1.98252
Global Iter: 1359900 training acc: 0.125
Global Iter: 1360000 training loss: 2.02035
Global Iter: 1360000 training acc: 0.125
Global Iter: 1360100 training loss: 2.03797
Global Iter: 1360100 training acc: 0.1875
Global Iter: 1360200 training loss: 1.97179
Global Iter: 1360200 training acc: 0.1875
Global Iter: 1360300 training loss: 2.06441
Global Iter: 1360300 training acc: 0.21875
Global Iter: 1360400 training loss: 2.15968
Global Iter: 1360400 training acc: 0.28125
Global Iter: 1360500 training loss: 1.96121
Global Iter: 1360500 training acc: 0.25
Global Iter: 1360600 training loss: 1.99642
Global Iter: 1360600 training acc: 0.15625
Global Iter: 1360702017-06-22 19:57:22.047295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1363323
0 training loss: 1.95143
Global Iter: 1360700 training acc: 0.28125
Global Iter: 1360800 training loss: 2.09879
Global Iter: 1360800 training acc: 0.15625
Global Iter: 1360900 training loss: 1.87472
Global Iter: 1360900 training acc: 0.34375
Global Iter: 1361000 training loss: 1.94648
Global Iter: 1361000 training acc: 0.21875
Global Iter: 1361100 training loss: 1.91895
Global Iter: 1361100 training acc: 0.25
Global Iter: 1361200 training loss: 1.9744
Global Iter: 1361200 training acc: 0.09375
Global Iter: 1361300 training loss: 1.94504
Global Iter: 1361300 training acc: 0.15625
Global Iter: 1361400 training loss: 1.9102
Global Iter: 1361400 training acc: 0.125
Global Iter: 1361500 training loss: 2.07715
Global Iter: 1361500 training acc: 0.09375
Global Iter: 1361600 training loss: 1.93455
Global Iter: 1361600 training acc: 0.15625
Global Iter: 1361700 training loss: 1.97446
Global Iter: 1361700 training acc: 0.25
Global Iter: 1361800 training loss: 1.96064
Global Iter: 1361800 training acc: 0.15625
Global Iter: 1361900 training loss: 1.90078
Global Iter: 1361900 training acc: 0.1875
Global Iter: 1362000 training loss: 2.04879
Global Iter: 1362000 training acc: 0.1875
Global Iter: 1362100 training loss: 1.99063
Global Iter: 1362100 training acc: 0.1875
Global Iter: 1362200 training loss: 1.98863
Global Iter: 1362200 training acc: 0.09375
Global Iter: 1362300 training loss: 1.95039
Global Iter: 1362300 training acc: 0.15625
Global Iter: 1362400 training loss: 2.04556
Global Iter: 1362400 training acc: 0.15625
Global Iter: 1362500 training loss: 1.96516
Global Iter: 1362500 training acc: 0.25
Global Iter: 1362600 training loss: 1.97293
Global Iter: 1362600 training acc: 0.03125
Global Iter: 1362700 training loss: 1.94955
Global Iter: 1362700 training acc: 0.21875
Global Iter: 1362800 training loss: 1.96659
Global Iter: 1362800 training acc: 0.15625
Global Iter: 1362900 training loss: 2.07364
Global Iter: 1362900 training acc: 0.28125
Global Iter: 1363000 training loss: 2.00922
Global Iter: 1363000 training acc: 0.15625
Global Iter: 1363100 training loss: 2.05628
Global Iter: 1363100 training acc: 0.1875
Global Iter: 1363200 training loss: 1.93363
Global Iter: 1363200 training acc: 0.21875
Global Iter: 1363300 training loss: 1.94484
Global Iter: 1363300 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1363323
Number of Patches: 89341
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1363323
Global Iter: 1363400 training loss: 1.96993
Global Iter: 1363400 training acc: 0.1875
Global Iter: 1363500 training loss: 1.96561
Global Iter: 1363500 training acc: 0.21875
Global Iter: 1363600 training loss: 1.95344
Global Iter: 1363600 training acc: 0.21875
Global Iter: 1363700 training loss: 1.92114
Global Iter: 1363700 training acc: 0.25
Global Iter: 1363800 training loss: 1.96774
Global Iter: 1363800 training acc: 0.09375
Global Iter: 1363900 training loss: 2.0637
Global Iter: 1363900 training acc: 0.0625
Global Iter: 1364000 training loss: 1.94181
Global Iter: 1364000 training acc: 0.1875
Global Iter: 1364100 training loss: 2.02138
Global Iter: 1364100 training acc: 0.0625
Global Iter: 1364200 training loss: 1.92312
Global Iter: 1364200 training acc: 0.1875
Global Iter: 1364300 training loss: 1.97064
Global Iter: 1364300 training acc: 0.21875
Global Iter: 1364400 training loss: 1.95085
Global Iter: 1364400 training acc: 0.125
Global Iter: 1364500 training loss: 2.04212
Global Iter: 1364500 training acc: 0.09375
Global Iter: 1364600 training loss: 1.97001
Global Iter: 1364600 training acc: 0.25
Global Iter: 1364700 training loss: 2.08691
Global Iter: 1364700 training acc: 0.15625
Global Iter: 1364800 training loss: 1.96346
Global Iter: 1364800 training acc: 0.1875
Global Iter: 1364900 training loss: 2.00529
Global Iter: 1364900 training acc: 0.1875
Global Iter: 1365000 training loss: 2.0035
Global Iter: 1365000 training acc: 0.21875
Global Iter: 1365100 training loss: 2.04128
Global Iter: 1365100 training acc: 0.21875
Global Iter:2017-06-22 20:06:55.363561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1368907
 1365200 training loss: 1.89015
Global Iter: 1365200 training acc: 0.125
Global Iter: 1365300 training loss: 1.93093
Global Iter: 1365300 training acc: 0.34375
Global Iter: 1365400 training loss: 1.96735
Global Iter: 1365400 training acc: 0.1875
Global Iter: 1365500 training loss: 2.02263
Global Iter: 1365500 training acc: 0.125
Global Iter: 1365600 training loss: 2.06265
Global Iter: 1365600 training acc: 0.15625
Global Iter: 1365700 training loss: 1.90721
Global Iter: 1365700 training acc: 0.3125
Global Iter: 1365800 training loss: 1.97071
Global Iter: 1365800 training acc: 0.25
Global Iter: 1365900 training loss: 2.05955
Global Iter: 1365900 training acc: 0.125
Global Iter: 1366000 training loss: 1.93377
Global Iter: 1366000 training acc: 0.25
Global Iter: 1366100 training loss: 2.05916
Global Iter: 1366100 training acc: 0.09375
Global Iter: 1366200 training loss: 2.07535
Global Iter: 1366200 training acc: 0.09375
Global Iter: 1366300 training loss: 1.95376
Global Iter: 1366300 training acc: 0.125
Global Iter: 1366400 training loss: 2.09956
Global Iter: 1366400 training acc: 0.15625
Global Iter: 1366500 training loss: 1.96365
Global Iter: 1366500 training acc: 0.1875
Global Iter: 1366600 training loss: 2.00329
Global Iter: 1366600 training acc: 0.09375
Global Iter: 1366700 training loss: 2.08772
Global Iter: 1366700 training acc: 0.15625
Global Iter: 1366800 training loss: 2.11586
Global Iter: 1366800 training acc: 0.15625
Global Iter: 1366900 training loss: 1.9561
Global Iter: 1366900 training acc: 0.15625
Global Iter: 1367000 training loss: 2.00341
Global Iter: 1367000 training acc: 0.125
Global Iter: 1367100 training loss: 1.93425
Global Iter: 1367100 training acc: 0.21875
Global Iter: 1367200 training loss: 1.96847
Global Iter: 1367200 training acc: 0.1875
Global Iter: 1367300 training loss: 1.95965
Global Iter: 1367300 training acc: 0.0625
Global Iter: 1367400 training loss: 2.04788
Global Iter: 1367400 training acc: 0.21875
Global Iter: 1367500 training loss: 1.94568
Global Iter: 1367500 training acc: 0.1875
Global Iter: 1367600 training loss: 2.0874
Global Iter: 1367600 training acc: 0.1875
Global Iter: 1367700 training loss: 2.10058
Global Iter: 1367700 training acc: 0.21875
Global Iter: 1367800 training loss: 2.03141
Global Iter: 1367800 training acc: 0.25
Global Iter: 1367900 training loss: 2.09117
Global Iter: 1367900 training acc: 0.125
Global Iter: 1368000 training loss: 1.95844
Global Iter: 1368000 training acc: 0.15625
Global Iter: 1368100 training loss: 2.21949
Global Iter: 1368100 training acc: 0.125
Global Iter: 1368200 training loss: 2.01782
Global Iter: 1368200 training acc: 0.125
Global Iter: 1368300 training loss: 1.97113
Global Iter: 1368300 training acc: 0.1875
Global Iter: 1368400 training loss: 1.99502
Global Iter: 1368400 training acc: 0.09375
Global Iter: 1368500 training loss: 1.9971
Global Iter: 1368500 training acc: 0.1875
Global Iter: 1368600 training loss: 1.95507
Global Iter: 1368600 training acc: 0.25
Global Iter: 1368700 training loss: 1.95166
Global Iter: 1368700 training acc: 0.21875
Global Iter: 1368800 training loss: 1.91676
Global Iter: 1368800 training acc: 0.125
Global Iter: 1368900 training loss: 1.98478
Global Iter: 1368900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1368907
Number of Patches: 88448
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1368907
Global Iter: 1369000 training loss: 1.99881
Global Iter: 1369000 training acc: 0.25
Global Iter: 1369100 training loss: 2.02198
Global Iter: 1369100 training acc: 0.125
Global Iter: 1369200 training loss: 2.03213
Global Iter: 1369200 training acc: 0.21875
Global Iter: 1369300 training loss: 2.06061
Global Iter: 1369300 training acc: 0.125
Global Iter: 1369400 training loss: 1.96589
Global Iter: 1369400 training acc: 0.125
Global Iter: 1369500 training loss: 2.00022
Global Iter: 1369500 training acc: 0.15625
Global Iter: 1369600 training loss: 2.09853
Global Iter: 1369600 training acc: 0.28125
Global Iter: 1369700 training loss: 1.96816
Global Iter: 1369700 training acc: 0.15625
Global Iter: 1369800 training loss: 1.85233
Global Iter: 1369800 training acc: 0.21875
Global Iter: 1369900 training loss: 1.97062
Global Iter: 1369900 training acc: 0.1875
Global Iter: 1370000 training loss: 2.07175
Global Iter: 1370000 training acc: 0.1875
Global Iter: 1370100 training loss: 2.02489
Global Iter: 1370100 training acc: 0.21875
Global Iter: 1370200 training loss: 1.96214
Global Iter: 1370200 training acc: 0.15625
Global Iter: 1370300 training loss: 1.96209
Global Iter: 1370300 training acc: 0.1875
Global Iter: 1370400 training loss: 1.92353
Global Iter: 1370400 training acc: 0.28125
Global Iter: 1370500 training loss: 2.12502
Global Iter: 1370500 training acc: 0.21875
Global Iter: 1370600 training loss: 1.93579
Global Iter: 1370600 training acc: 0.28125
Global Iter: 1370700 training loss: 1.98607
Global Iter: 1370700 training acc: 0.125
Global Iter: 1370800 training loss: 1.98716
Global Iter: 1370800 training acc: 0.25
Global Iter: 1370900 training loss: 1.96343
Global Iter: 1370900 training acc: 0.28125
Global Iter: 1371000 training loss: 1.98119
Global Iter: 1371000 training acc: 0.28125
Global Iter: 1371100 training loss: 1.97018
Global Iter: 1371100 training acc: 0.15625
Global Iter: 1371200 training loss: 1.90715
Global Iter: 1371200 training acc: 0.125
Global Iter: 1371300 training loss: 1.98393
Global Iter: 1371300 training acc: 0.09375
Global Iter: 1371400 training loss: 1.99631
Global Iter: 1371400 training acc: 0.1875
Global Iter: 1371500 training loss: 2.16143
Global Iter: 1371500 training acc: 0.125
Global Iter: 1371600 training loss: 2.04941
Global Iter: 1371600 training acc: 0.1875
Global Iter: 1371700 training loss: 1.94671
Global Iter: 1371700 training acc: 0.1875
Global Iter: 1371800 training loss: 2.02217
Global Iter: 1371800 training acc: 0.09375
Global Iter: 1371900 training loss: 1.92887
Global Iter: 1371900 training acc: 0.3125
Global Iter: 1372000 training loss: 1.9829
Global Iter: 1372000 training acc: 0.15625
Global Iter: 1372100 training loss: 1.89037
Global Iter: 1372100 training acc: 0.21875
Global Iter: 1372200 training loss: 1.98512
Global Iter: 1372200 training acc: 0.1875
Global Iter: 1372300 training loss: 2.04901
Global Iter: 1372300 training acc: 0.1875
Global Iter: 1372400 training loss: 1.91016
Global Iter: 1372400 training acc: 0.15625
Global Iter: 1372500 training loss: 1.99554
Global Iter: 1372500 training acc: 0.1875
Global Iter: 1372600 training loss: 1.97986
Global Iter: 1372600 training acc: 0.0625
Global Iter: 1372700 training loss: 1.98242
Global Iter: 1372700 training acc: 0.125
Global Iter: 1372800 training loss: 1.92371
Global Iter: 1372800 training acc: 0.21875
Global Iter: 1372900 training loss: 2.03751
Global Iter: 1372900 training acc: 0.125
Global Iter: 1373000 training loss: 1.96025
Global Iter: 1373000 training acc: 0.25
Global Iter: 1373100 training loss: 2.07571
Global Iter: 1373100 training acc: 0.15625
Global Iter: 1373200 training loss: 1.97689
Global Iter: 1373200 training acc: 0.1875
Global Iter: 1373300 training loss: 2.05338
Global Iter: 1373300 training acc: 0.1875
Global Iter: 1373400 training loss: 2.01955
Global Iter: 1373400 training acc: 0.25
Global Iter: 1373500 training loss: 2.07636
Global Iter: 1373500 training acc: 0.125
Global Iter: 1373600 training loss: 2.00764
Global Iter: 1373600 training acc: 0.125
Global Iter: 1373700 training loss: 1.95929
Global Iter: 1373700 training acc: 0.125
Global Iter: 1373800 training loss: 1.9872
Global Iter: 1373800 training acc: 0.3125
Global Iter: 1373900 training loss: 1.91806
Global Iter: 1373900 training acc: 0.15625
Global Iter: 1374000 training loss: 1.92883
Global Iter: 1374000 training acc: 0.3125
Global Iter: 1374100 training loss: 1.99619
Global Iter: 1374100 training acc: 0.125
Global Iter: 1374200 training loss: 2.20559
Global Iter: 1374200 training acc: 0.0625
Global Iter: 1374300 training loss: 1.99908
Global Iter: 1374300 training acc: 0.125
Global Iter: 1374400 training loss: 2.09751
Global Iter: 1374400 training ac2017-06-22 20:16:27.096045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1374435
c: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1374435
Number of Patches: 87564
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1374435
Global Iter: 1374500 training loss: 1.99508
Global Iter: 1374500 training acc: 0.15625
Global Iter: 1374600 training loss: 1.91087
Global Iter: 1374600 training acc: 0.21875
Global Iter: 1374700 training loss: 1.99376
Global Iter: 1374700 training acc: 0.3125
Global Iter: 1374800 training loss: 2.01339
Global Iter: 1374800 training acc: 0.125
Global Iter: 1374900 training loss: 2.01083
Global Iter: 1374900 training acc: 0.3125
Global Iter: 1375000 training loss: 2.01564
Global Iter: 1375000 training acc: 0.09375
Global Iter: 1375100 training loss: 1.86672
Global Iter: 1375100 training acc: 0.1875
Global Iter: 1375200 training loss: 2.08053
Global Iter: 1375200 training acc: 0.03125
Global Iter: 1375300 training loss: 2.00743
Global Iter: 1375300 training acc: 0.21875
Global Iter: 1375400 training loss: 1.92807
Global Iter: 1375400 training acc: 0.125
Global Iter: 1375500 training loss: 2.16757
Global Iter: 1375500 training acc: 0.28125
Global Iter: 1375600 training loss: 2.10206
Global Iter: 1375600 training acc: 0.1875
Global Iter: 1375700 training loss: 1.9893
Global Iter: 1375700 training acc: 0.15625
Global Iter: 1375800 training loss: 1.93073
Global Iter: 1375800 training acc: 0.15625
Global Iter: 1375900 training loss: 1.97503
Global Iter: 1375900 training acc: 0.09375
Global Iter: 1376000 training loss: 1.96438
Global Iter: 1376000 training acc: 0.1875
Global Iter: 1376100 training loss: 1.91795
Global Iter: 1376100 training acc: 0.3125
Global Iter: 1376200 training loss: 2.01686
Global Iter: 1376200 training acc: 0.09375
Global Iter: 1376300 training loss: 2.00535
Global Iter: 1376300 training acc: 0.25
Global Iter: 1376400 training loss: 2.05333
Global Iter: 1376400 training acc: 0.09375
Global Iter: 1376500 training loss: 2.10924
Global Iter: 1376500 training acc: 0.125
Global Iter: 1376600 training loss: 1.9459
Global Iter: 1376600 training acc: 0.25
Global Iter: 1376700 training loss: 1.8741
Global Iter: 1376700 training acc: 0.28125
Global Iter: 1376800 training loss: 2.00926
Global Iter: 1376800 training acc: 0.15625
Global Iter: 1376900 training loss: 1.9424
Global Iter: 1376900 training acc: 0.21875
Global Iter: 1377000 training loss: 1.97055
Global Iter: 1377000 training acc: 0.1875
Global Iter: 1377100 training loss: 2.00316
Global Iter: 1377100 training acc: 0.0625
Global Iter: 1377200 training loss: 2.02305
Global Iter: 1377200 training acc: 0.09375
Global Iter: 1377300 training loss: 2.14157
Global Iter: 1377300 training acc: 0.09375
Global Iter: 1377400 training loss: 1.97942
Global Iter: 1377400 training acc: 0.15625
Global Iter: 1377500 training loss: 1.99064
Global Iter: 1377500 training acc: 0.21875
Global Iter: 1377600 training loss: 1.98296
Global Iter: 1377600 training acc: 0.125
Global Iter: 1377700 training loss: 1.95514
Global Iter: 1377700 training acc: 0.15625
Global Iter: 1377800 training loss: 1.97767
Global Iter: 1377800 training acc: 0.125
Global Iter: 1377900 training loss: 1.96593
Global Iter: 1377900 training acc: 0.125
Global Iter: 1378000 training loss: 1.91825
Global Iter: 1378000 training acc: 0.28125
Global Iter: 1378100 training loss: 1.98556
Global Iter: 1378100 training acc: 0.15625
Global Iter: 1378200 training loss: 2.02436
Global Iter: 1378200 training acc: 0.21875
Global Iter: 1378300 training loss: 1.99441
Global Iter: 1378300 training acc: 0.3125
Global Iter: 1378400 training loss: 1.98847
Global Iter: 1378400 training acc: 0.125
Global Iter: 1378500 training loss: 1.95637
Global Iter: 1378500 training acc: 0.25
Global Iter: 1378600 training loss: 1.88911
Global Iter: 1378600 training acc: 0.1875
Global Iter: 1378700 training loss: 1.9559
Global Iter: 1378700 training acc: 0.15625
Global Iter: 1378800 training loss: 2.02621
Global Iter: 1378800 training acc: 0.09375
Global Iter: 1378900 training loss: 1.99012
Global Iter: 1378900 trainin2017-06-22 20:25:47.019289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1379908
g acc: 0.09375
Global Iter: 1379000 training loss: 2.01596
Global Iter: 1379000 training acc: 0.09375
Global Iter: 1379100 training loss: 1.9463
Global Iter: 1379100 training acc: 0.25
Global Iter: 1379200 training loss: 1.98926
Global Iter: 1379200 training acc: 0.21875
Global Iter: 1379300 training loss: 1.93927
Global Iter: 1379300 training acc: 0.21875
Global Iter: 1379400 training loss: 1.96739
Global Iter: 1379400 training acc: 0.25
Global Iter: 1379500 training loss: 1.96079
Global Iter: 1379500 training acc: 0.21875
Global Iter: 1379600 training loss: 1.9457
Global Iter: 1379600 training acc: 0.34375
Global Iter: 1379700 training loss: 1.91
Global Iter: 1379700 training acc: 0.1875
Global Iter: 1379800 training loss: 1.98705
Global Iter: 1379800 training acc: 0.1875
Global Iter: 1379900 training loss: 1.90053
Global Iter: 1379900 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1379908
Number of Patches: 86689
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1379908
Global Iter: 1380000 training loss: 1.97627
Global Iter: 1380000 training acc: 0.1875
Global Iter: 1380100 training loss: 2.05858
Global Iter: 1380100 training acc: 0.25
Global Iter: 1380200 training loss: 2.04281
Global Iter: 1380200 training acc: 0.21875
Global Iter: 1380300 training loss: 1.9891
Global Iter: 1380300 training acc: 0.25
Global Iter: 1380400 training loss: 1.97989
Global Iter: 1380400 training acc: 0.1875
Global Iter: 1380500 training loss: 2.00024
Global Iter: 1380500 training acc: 0.09375
Global Iter: 1380600 training loss: 1.96352
Global Iter: 1380600 training acc: 0.25
Global Iter: 1380700 training loss: 1.92521
Global Iter: 1380700 training acc: 0.1875
Global Iter: 1380800 training loss: 2.19488
Global Iter: 1380800 training acc: 0.125
Global Iter: 1380900 training loss: 2.07196
Global Iter: 1380900 training acc: 0.15625
Global Iter: 1381000 training loss: 1.98966
Global Iter: 1381000 training acc: 0.15625
Global Iter: 1381100 training loss: 2.01539
Global Iter: 1381100 training acc: 0.09375
Global Iter: 1381200 training loss: 2.00221
Global Iter: 1381200 training acc: 0.25
Global Iter: 1381300 training loss: 1.9918
Global Iter: 1381300 training acc: 0.21875
Global Iter: 1381400 training loss: 1.92787
Global Iter: 1381400 training acc: 0.15625
Global Iter: 1381500 training loss: 1.97876
Global Iter: 1381500 training acc: 0.09375
Global Iter: 1381600 training loss: 1.96503
Global Iter: 1381600 training acc: 0.28125
Global Iter: 1381700 training loss: 1.96917
Global Iter: 1381700 training acc: 0.125
Global Iter: 1381800 training loss: 1.94514
Global Iter: 1381800 training acc: 0.21875
Global Iter: 1381900 training loss: 1.96836
Global Iter: 1381900 training acc: 0.15625
Global Iter: 1382000 training loss: 1.92551
Global Iter: 1382000 training acc: 0.15625
Global Iter: 1382100 training loss: 2.00868
Global Iter: 1382100 training acc: 0.1875
Global Iter: 1382200 training loss: 1.92211
Global Iter: 1382200 training acc: 0.34375
Global Iter: 1382300 training loss: 2.04436
Global Iter: 1382300 training acc: 0.09375
Global Iter: 1382400 training loss: 1.91338
Global Iter: 1382400 training acc: 0.28125
Global Iter: 1382500 training loss: 1.99751
Global Iter: 1382500 training acc: 0.21875
Global Iter: 1382600 training loss: 1.9838
Global Iter: 1382600 training acc: 0.25
Global Iter: 1382700 training loss: 2.01204
Global Iter: 1382700 training acc: 0.28125
Global Iter: 1382800 training loss: 2.06493
Global Iter: 1382800 training acc: 0.15625
Global Iter: 1382900 training loss: 2.06058
Global Iter: 1382900 training acc: 0.1875
Global Iter: 1383000 training loss: 2.07299
Global Iter: 1383000 training acc: 0.125
Global Iter: 1383100 training loss: 1.97475
Global Iter: 1383100 training acc: 0.3125
Global Iter: 1383200 training loss: 1.93631
Global Iter: 1383200 training acc: 0.09375
Global Iter: 1383300 training loss: 1.98724
Global Iter: 1383300 training acc: 0.1875
Global Iter: 1383400 training loss: 1.96732
Global Iter: 1383400 training2017-06-22 20:34:56.858632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1385327
 acc: 0.15625
Global Iter: 1383500 training loss: 1.96241
Global Iter: 1383500 training acc: 0.15625
Global Iter: 1383600 training loss: 1.92507
Global Iter: 1383600 training acc: 0.21875
Global Iter: 1383700 training loss: 2.01764
Global Iter: 1383700 training acc: 0.09375
Global Iter: 1383800 training loss: 2.02538
Global Iter: 1383800 training acc: 0.125
Global Iter: 1383900 training loss: 2.03826
Global Iter: 1383900 training acc: 0.125
Global Iter: 1384000 training loss: 1.94088
Global Iter: 1384000 training acc: 0.1875
Global Iter: 1384100 training loss: 2.04239
Global Iter: 1384100 training acc: 0.1875
Global Iter: 1384200 training loss: 1.98383
Global Iter: 1384200 training acc: 0.125
Global Iter: 1384300 training loss: 2.05003
Global Iter: 1384300 training acc: 0.15625
Global Iter: 1384400 training loss: 1.96021
Global Iter: 1384400 training acc: 0.125
Global Iter: 1384500 training loss: 1.96659
Global Iter: 1384500 training acc: 0.09375
Global Iter: 1384600 training loss: 2.09347
Global Iter: 1384600 training acc: 0.15625
Global Iter: 1384700 training loss: 1.97006
Global Iter: 1384700 training acc: 0.28125
Global Iter: 1384800 training loss: 2.05415
Global Iter: 1384800 training acc: 0.03125
Global Iter: 1384900 training loss: 1.95283
Global Iter: 1384900 training acc: 0.21875
Global Iter: 1385000 training loss: 2.04271
Global Iter: 1385000 training acc: 0.15625
Global Iter: 1385100 training loss: 1.98293
Global Iter: 1385100 training acc: 0.28125
Global Iter: 1385200 training loss: 2.14303
Global Iter: 1385200 training acc: 0.15625
Global Iter: 1385300 training loss: 1.98778
Global Iter: 1385300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1385327
Number of Patches: 85823
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1385327
Global Iter: 1385400 training loss: 2.1763
Global Iter: 1385400 training acc: 0.1875
Global Iter: 1385500 training loss: 2.00038
Global Iter: 1385500 training acc: 0.25
Global Iter: 1385600 training loss: 2.0695
Global Iter: 1385600 training acc: 0.1875
Global Iter: 1385700 training loss: 1.93776
Global Iter: 1385700 training acc: 0.21875
Global Iter: 1385800 training loss: 1.93706
Global Iter: 1385800 training acc: 0.15625
Global Iter: 1385900 training loss: 2.02461
Global Iter: 1385900 training acc: 0.21875
Global Iter: 1386000 training loss: 2.05187
Global Iter: 1386000 training acc: 0.21875
Global Iter: 1386100 training loss: 2.08953
Global Iter: 1386100 training acc: 0.1875
Global Iter: 1386200 training loss: 1.9284
Global Iter: 1386200 training acc: 0.15625
Global Iter: 1386300 training loss: 2.01575
Global Iter: 1386300 training acc: 0.21875
Global Iter: 1386400 training loss: 1.97013
Global Iter: 1386400 training acc: 0.28125
Global Iter: 1386500 training loss: 2.06247
Global Iter: 1386500 training acc: 0.125
Global Iter: 1386600 training loss: 1.92956
Global Iter: 1386600 training acc: 0.21875
Global Iter: 1386700 training loss: 2.07742
Global Iter: 1386700 training acc: 0.09375
Global Iter: 1386800 training loss: 1.98189
Global Iter: 1386800 training acc: 0.25
Global Iter: 1386900 training loss: 2.06304
Global Iter: 1386900 training acc: 0.125
Global Iter: 1387000 training loss: 2.02252
Global Iter: 1387000 training acc: 0.15625
Global Iter: 1387100 training loss: 2.15926
Global Iter: 1387100 training acc: 0.09375
Global Iter: 1387200 training loss: 1.97992
Global Iter: 1387200 training acc: 0.1875
Global Iter: 1387300 training loss: 2.13804
Global Iter: 1387300 training acc: 0.15625
Global Iter: 1387400 training loss: 2.04156
Global Iter: 1387400 training acc: 0.15625
Global Iter: 1387500 training loss: 1.97113
Global Iter: 1387500 training acc: 0.21875
Global Iter: 1387600 training loss: 2.00168
Global Iter: 1387600 training acc: 0.25
Global Iter: 1387700 training loss: 2.10097
Global Iter: 1387700 training acc: 0.15625
Global Iter: 1387800 training loss: 1.94202
Global Iter: 1387800 training acc: 0.15625
Global Iter: 1387900 training loss: 1.9351
Global Iter: 13872017-06-22 20:44:01.858973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1390691
900 training acc: 0.09375
Global Iter: 1388000 training loss: 1.94091
Global Iter: 1388000 training acc: 0.21875
Global Iter: 1388100 training loss: 2.01312
Global Iter: 1388100 training acc: 0.15625
Global Iter: 1388200 training loss: 2.15569
Global Iter: 1388200 training acc: 0.125
Global Iter: 1388300 training loss: 1.894
Global Iter: 1388300 training acc: 0.1875
Global Iter: 1388400 training loss: 1.93664
Global Iter: 1388400 training acc: 0.3125
Global Iter: 1388500 training loss: 1.93916
Global Iter: 1388500 training acc: 0.15625
Global Iter: 1388600 training loss: 1.96095
Global Iter: 1388600 training acc: 0.15625
Global Iter: 1388700 training loss: 2.00857
Global Iter: 1388700 training acc: 0.15625
Global Iter: 1388800 training loss: 2.07028
Global Iter: 1388800 training acc: 0.09375
Global Iter: 1388900 training loss: 1.9824
Global Iter: 1388900 training acc: 0.21875
Global Iter: 1389000 training loss: 1.92301
Global Iter: 1389000 training acc: 0.21875
Global Iter: 1389100 training loss: 1.97236
Global Iter: 1389100 training acc: 0.1875
Global Iter: 1389200 training loss: 2.06924
Global Iter: 1389200 training acc: 0.09375
Global Iter: 1389300 training loss: 1.90102
Global Iter: 1389300 training acc: 0.25
Global Iter: 1389400 training loss: 1.94112
Global Iter: 1389400 training acc: 0.1875
Global Iter: 1389500 training loss: 1.99475
Global Iter: 1389500 training acc: 0.125
Global Iter: 1389600 training loss: 2.0778
Global Iter: 1389600 training acc: 0.1875
Global Iter: 1389700 training loss: 2.0643
Global Iter: 1389700 training acc: 0.0625
Global Iter: 1389800 training loss: 1.92733
Global Iter: 1389800 training acc: 0.15625
Global Iter: 1389900 training loss: 2.00942
Global Iter: 1389900 training acc: 0.25
Global Iter: 1390000 training loss: 2.05821
Global Iter: 1390000 training acc: 0.15625
Global Iter: 1390100 training loss: 2.05717
Global Iter: 1390100 training acc: 0.25
Global Iter: 1390200 training loss: 2.0192
Global Iter: 1390200 training acc: 0.15625
Global Iter: 1390300 training loss: 1.99091
Global Iter: 1390300 training acc: 0.21875
Global Iter: 1390400 training loss: 1.98853
Global Iter: 1390400 training acc: 0.15625
Global Iter: 1390500 training loss: 2.05532
Global Iter: 1390500 training acc: 0.15625
Global Iter: 1390600 training loss: 1.97336
Global Iter: 1390600 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1390691
Number of Patches: 84965
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1390691
Global Iter: 1390700 training loss: 1.99757
Global Iter: 1390700 training acc: 0.21875
Global Iter: 1390800 training loss: 2.00268
Global Iter: 1390800 training acc: 0.15625
Global Iter: 1390900 training loss: 2.01856
Global Iter: 1390900 training acc: 0.34375
Global Iter: 1391000 training loss: 2.01776
Global Iter: 1391000 training acc: 0.15625
Global Iter: 1391100 training loss: 2.12941
Global Iter: 1391100 training acc: 0.125
Global Iter: 1391200 training loss: 1.8966
Global Iter: 1391200 training acc: 0.25
Global Iter: 1391300 training loss: 2.04504
Global Iter: 1391300 training acc: 0.09375
Global Iter: 1391400 training loss: 1.97125
Global Iter: 1391400 training acc: 0.21875
Global Iter: 1391500 training loss: 1.95843
Global Iter: 1391500 training acc: 0.21875
Global Iter: 1391600 training loss: 2.07268
Global Iter: 1391600 training acc: 0.09375
Global Iter: 1391700 training loss: 1.94309
Global Iter: 1391700 training acc: 0.25
Global Iter: 1391800 training loss: 1.93597
Global Iter: 1391800 training acc: 0.125
Global Iter: 1391900 training loss: 1.95746
Global Iter: 1391900 training acc: 0.1875
Global Iter: 1392000 training loss: 2.06554
Global Iter: 1392000 training acc: 0.125
Global Iter: 1392100 training loss: 2.09718
Global Iter: 1392100 training acc: 0.09375
Global Iter: 1392200 training loss: 2.00903
Global Iter: 1392200 training acc: 0.15625
Global Iter: 1392300 training loss: 1.9006
Global Iter: 1392300 training acc: 0.25
Global Iter: 1392400 training loss: 2.03017
Global Iter: 13922017-06-22 20:53:04.491112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1396002
400 training acc: 0.15625
Global Iter: 1392500 training loss: 2.1515
Global Iter: 1392500 training acc: 0.21875
Global Iter: 1392600 training loss: 2.02203
Global Iter: 1392600 training acc: 0.0
Global Iter: 1392700 training loss: 1.91444
Global Iter: 1392700 training acc: 0.15625
Global Iter: 1392800 training loss: 2.03836
Global Iter: 1392800 training acc: 0.1875
Global Iter: 1392900 training loss: 1.97325
Global Iter: 1392900 training acc: 0.21875
Global Iter: 1393000 training loss: 2.02028
Global Iter: 1393000 training acc: 0.125
Global Iter: 1393100 training loss: 1.89276
Global Iter: 1393100 training acc: 0.28125
Global Iter: 1393200 training loss: 1.88485
Global Iter: 1393200 training acc: 0.25
Global Iter: 1393300 training loss: 1.88414
Global Iter: 1393300 training acc: 0.34375
Global Iter: 1393400 training loss: 1.93515
Global Iter: 1393400 training acc: 0.125
Global Iter: 1393500 training loss: 1.98475
Global Iter: 1393500 training acc: 0.15625
Global Iter: 1393600 training loss: 2.08859
Global Iter: 1393600 training acc: 0.09375
Global Iter: 1393700 training loss: 2.12091
Global Iter: 1393700 training acc: 0.09375
Global Iter: 1393800 training loss: 1.94781
Global Iter: 1393800 training acc: 0.09375
Global Iter: 1393900 training loss: 1.9935
Global Iter: 1393900 training acc: 0.21875
Global Iter: 1394000 training loss: 1.88901
Global Iter: 1394000 training acc: 0.28125
Global Iter: 1394100 training loss: 1.95001
Global Iter: 1394100 training acc: 0.125
Global Iter: 1394200 training loss: 1.96952
Global Iter: 1394200 training acc: 0.21875
Global Iter: 1394300 training loss: 2.02772
Global Iter: 1394300 training acc: 0.15625
Global Iter: 1394400 training loss: 1.99859
Global Iter: 1394400 training acc: 0.1875
Global Iter: 1394500 training loss: 1.9776
Global Iter: 1394500 training acc: 0.15625
Global Iter: 1394600 training loss: 2.00454
Global Iter: 1394600 training acc: 0.1875
Global Iter: 1394700 training loss: 1.91764
Global Iter: 1394700 training acc: 0.28125
Global Iter: 1394800 training loss: 1.98408
Global Iter: 1394800 training acc: 0.15625
Global Iter: 1394900 training loss: 2.10463
Global Iter: 1394900 training acc: 0.0625
Global Iter: 1395000 training loss: 1.98102
Global Iter: 1395000 training acc: 0.1875
Global Iter: 1395100 training loss: 1.94717
Global Iter: 1395100 training acc: 0.25
Global Iter: 1395200 training loss: 2.00608
Global Iter: 1395200 training acc: 0.125
Global Iter: 1395300 training loss: 1.99021
Global Iter: 1395300 training acc: 0.1875
Global Iter: 1395400 training loss: 1.91165
Global Iter: 1395400 training acc: 0.21875
Global Iter: 1395500 training loss: 2.04827
Global Iter: 1395500 training acc: 0.0625
Global Iter: 1395600 training loss: 1.87905
Global Iter: 1395600 training acc: 0.125
Global Iter: 1395700 training loss: 1.96404
Global Iter: 1395700 training acc: 0.25
Global Iter: 1395800 training loss: 2.06239
Global Iter: 1395800 training acc: 0.09375
Global Iter: 1395900 training loss: 2.0331
Global Iter: 1395900 training acc: 0.25
Global Iter: 1396000 training loss: 1.93355
Global Iter: 1396000 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1396002
Number of Patches: 84116
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1396002
Global Iter: 1396100 training loss: 2.06979
Global Iter: 1396100 training acc: 0.09375
Global Iter: 1396200 training loss: 2.03969
Global Iter: 1396200 training acc: 0.15625
Global Iter: 1396300 training loss: 1.93872
Global Iter: 1396300 training acc: 0.21875
Global Iter: 1396400 training loss: 1.92374
Global Iter: 1396400 training acc: 0.21875
Global Iter: 1396500 training loss: 2.09478
Global Iter: 1396500 training acc: 0.0625
Global Iter: 1396600 training loss: 1.87176
Global Iter: 1396600 training acc: 0.34375
Global Iter: 1396700 training loss: 2.00939
Global Iter: 1396700 training acc: 0.15625
Global Iter: 1396800 training loss: 2.00859
Global Iter: 1396800 training acc: 0.125
Global Iter: 1396900 training loss: 2.0771
Global Iter: 13962017-06-22 21:01:59.731371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1401260
900 training acc: 0.03125
Global Iter: 1397000 training loss: 1.99722
Global Iter: 1397000 training acc: 0.125
Global Iter: 1397100 training loss: 2.00765
Global Iter: 1397100 training acc: 0.25
Global Iter: 1397200 training loss: 2.06714
Global Iter: 1397200 training acc: 0.125
Global Iter: 1397300 training loss: 1.95197
Global Iter: 1397300 training acc: 0.15625
Global Iter: 1397400 training loss: 1.99405
Global Iter: 1397400 training acc: 0.09375
Global Iter: 1397500 training loss: 2.0253
Global Iter: 1397500 training acc: 0.21875
Global Iter: 1397600 training loss: 2.02035
Global Iter: 1397600 training acc: 0.1875
Global Iter: 1397700 training loss: 1.92685
Global Iter: 1397700 training acc: 0.25
Global Iter: 1397800 training loss: 1.92656
Global Iter: 1397800 training acc: 0.28125
Global Iter: 1397900 training loss: 1.99718
Global Iter: 1397900 training acc: 0.1875
Global Iter: 1398000 training loss: 1.92599
Global Iter: 1398000 training acc: 0.25
Global Iter: 1398100 training loss: 2.09711
Global Iter: 1398100 training acc: 0.15625
Global Iter: 1398200 training loss: 2.03808
Global Iter: 1398200 training acc: 0.15625
Global Iter: 1398300 training loss: 1.97098
Global Iter: 1398300 training acc: 0.28125
Global Iter: 1398400 training loss: 1.97753
Global Iter: 1398400 training acc: 0.09375
Global Iter: 1398500 training loss: 1.9378
Global Iter: 1398500 training acc: 0.1875
Global Iter: 1398600 training loss: 1.93274
Global Iter: 1398600 training acc: 0.15625
Global Iter: 1398700 training loss: 1.99009
Global Iter: 1398700 training acc: 0.125
Global Iter: 1398800 training loss: 1.94043
Global Iter: 1398800 training acc: 0.21875
Global Iter: 1398900 training loss: 2.04618
Global Iter: 1398900 training acc: 0.15625
Global Iter: 1399000 training loss: 2.13825
Global Iter: 1399000 training acc: 0.15625
Global Iter: 1399100 training loss: 1.94877
Global Iter: 1399100 training acc: 0.1875
Global Iter: 1399200 training loss: 2.2026
Global Iter: 1399200 training acc: 0.15625
Global Iter: 1399300 training loss: 1.91598
Global Iter: 1399300 training acc: 0.125
Global Iter: 1399400 training loss: 1.97313
Global Iter: 1399400 training acc: 0.15625
Global Iter: 1399500 training loss: 2.06218
Global Iter: 1399500 training acc: 0.15625
Global Iter: 1399600 training loss: 1.9741
Global Iter: 1399600 training acc: 0.125
Global Iter: 1399700 training loss: 2.05302
Global Iter: 1399700 training acc: 0.1875
Global Iter: 1399800 training loss: 2.0661
Global Iter: 1399800 training acc: 0.15625
Global Iter: 1399900 training loss: 1.92315
Global Iter: 1399900 training acc: 0.15625
Global Iter: 1400000 training loss: 1.99954
Global Iter: 1400000 training acc: 0.15625
Global Iter: 1400100 training loss: 1.9577
Global Iter: 1400100 training acc: 0.21875
Global Iter: 1400200 training loss: 2.01236
Global Iter: 1400200 training acc: 0.21875
Global Iter: 1400300 training loss: 2.09169
Global Iter: 1400300 training acc: 0.15625
Global Iter: 1400400 training loss: 1.96411
Global Iter: 1400400 training acc: 0.21875
Global Iter: 1400500 training loss: 2.03859
Global Iter: 1400500 training acc: 0.1875
Global Iter: 1400600 training loss: 2.01223
Global Iter: 1400600 training acc: 0.15625
Global Iter: 1400700 training loss: 1.94817
Global Iter: 1400700 training acc: 0.09375
Global Iter: 1400800 training loss: 1.94925
Global Iter: 1400800 training acc: 0.125
Global Iter: 1400900 training loss: 2.01417
Global Iter: 1400900 training acc: 0.15625
Global Iter: 1401000 training loss: 2.03654
Global Iter: 1401000 training acc: 0.15625
Global Iter: 1401100 training loss: 1.99775
Global Iter: 1401100 training acc: 0.0625
Global Iter: 1401200 training loss: 1.98302
Global Iter: 1401200 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1401260
Number of Patches: 83275
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1401260
Global Iter: 1401300 training loss: 1.94143
Global Iter: 1401300 training acc: 0.25
Global Iter: 1401400 training loss: 2.02085
Global Iter: 1401400 training acc: 0.0625
Global Iter: 1401500 training loss: 2.00593
Global Iter: 1401500 training acc: 0.1875
Global Iter: 1401600 training loss: 1.92239
Global Iter: 1401600 training acc: 0.28125
Global Iter: 1401700 training loss: 1.92052
Global Iter: 1401700 training acc: 0.15625
Global Iter: 1401800 training loss: 2.09955
Global Iter: 1401800 training acc: 0.25
Global Iter: 1401900 training loss: 2.01092
Global Iter: 1401900 training acc: 0.15625
Global Iter: 1402000 training loss: 2.03098
Global Iter: 1402000 training acc: 0.125
Global Iter: 1402100 training loss: 1.92977
Global Iter: 1402100 training acc: 0.34375
Global Iter: 1402200 training loss: 1.96632
Global Iter: 1402200 training acc: 0.1875
Global Iter: 1402300 training loss: 2.03791
Global Iter: 1402300 training acc: 0.09375
Global Iter: 1402400 training loss: 2.01128
Global Iter: 1402400 training acc: 0.125
Global Iter: 1402500 training loss: 2.01276
Global Iter: 1402500 training acc: 0.09375
Global Iter: 1402600 training loss: 1.93833
Global Iter: 1402600 training acc: 0.25
Global Iter: 1402700 training loss: 1.88893
Global Iter: 1402700 training acc: 0.3125
Global Iter: 1402800 training loss: 1.98269
Global Iter: 1402800 training acc: 0.1875
Global Iter: 1402900 training loss: 2.10359
Global Iter: 1402900 training acc: 0.15625
Global Iter: 1403000 training loss: 2.02338
Global Iter: 1403000 training acc: 0.15625
Global Iter: 1403100 training loss: 2.02989
Global Iter: 1403100 training acc: 0.1875
Global Iter: 1403200 training loss: 2.01821
Global Iter: 1403200 training acc: 0.21875
Global Iter: 1403300 training loss: 1.98082
Global Iter: 1403300 training acc: 0.21875
Global Iter: 1403400 training loss: 1.98732
Global Iter: 1403400 training acc: 0.09375
Global Iter: 1403500 training loss: 1.93853
Global Iter: 1403500 training acc: 0.125
Global Iter: 1403600 training loss: 1.905
Global Iter: 1403600 training acc: 0.1875
Global Iter: 1403700 training loss: 2.08695
Global Iter: 1403700 training acc: 0.1875
Global Iter: 1403800 training loss: 2.01888
Global Iter: 1403800 training acc: 0.09375
Global Iter: 1403900 training loss: 2.04768
Global Iter: 1403900 training acc: 0.125
Global Iter: 1404000 training loss: 2.01341
Global Iter: 1404000 training acc: 0.1875
Global Iter: 1404100 training loss: 1.96262
Global Iter: 1404100 training acc: 0.21875
Global Iter: 1404200 training loss: 2.22898
Global Iter: 1404200 training acc: 0.125
Global Iter: 1404300 training loss: 1.97046
Global Iter: 1404300 training acc: 0.25
Global Iter: 1404400 training loss: 2.16445
Global Iter: 1404400 training acc: 0.21875
Global Iter: 1404500 training loss: 2.08008
Global Iter: 1404500 training acc: 0.125
Global Iter: 1404600 training loss: 1.88449
Global Iter: 1404600 training acc: 0.25
Global Iter: 1404700 training loss: 1.9293
Global Iter: 1404700 training acc: 0.28125
Global Iter: 1404800 training loss: 2.04258
Global Iter: 1404800 training acc: 0.1875
Global Iter: 1404900 training loss: 2.01826
Global Iter: 1404900 training acc: 0.125
Global Iter: 1405000 training loss: 2.01002
Global Iter: 1405000 training acc: 0.09375
Global Iter: 1405100 training loss: 2.01604
Global Iter: 1405100 training acc: 0.1875
Global Iter: 1405200 training loss: 2.00041
Global Iter: 1405200 training acc: 0.21875
Global Iter: 1405300 training loss: 1.94383
Global Iter: 1405300 training acc: 0.1875
Global Iter: 1405400 training loss: 1.88811
Global Iter: 1405400 training acc: 0.28125
Global Iter: 1405500 training loss: 2.03556
Global Iter: 1405500 training acc: 0.15625
Global Iter: 1405600 training loss: 2.0286
Global Iter: 1405600 training acc: 0.21875
Global Iter: 1405700 training loss: 2.00266
Global Iter: 1405700 training acc: 0.28125
Global Iter: 1405800 training loss: 1.91476
Global Iter: 1405800 training acc: 0.25
Global Iter: 1405900 training loss: 2.02323
Global Iter: 1405900 training acc: 0.09375
Global Iter: 1406000 training loss: 1.93323
Global Iter: 1406000 training acc: 0.1875
Global Iter: 1406100 training loss: 2.09007
Global Iter: 1406100 training acc: 0.09375
Global Iter: 1406200 2017-06-22 21:10:50.593580: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1406465
training loss: 2.03007
Global Iter: 1406200 training acc: 0.21875
Global Iter: 1406300 training loss: 2.05161
Global Iter: 1406300 training acc: 0.125
Global Iter: 1406400 training loss: 2.05552
Global Iter: 1406400 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1406465
Number of Patches: 82443
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1406465
Global Iter: 1406500 training loss: 1.96744
Global Iter: 1406500 training acc: 0.21875
Global Iter: 1406600 training loss: 2.00041
Global Iter: 1406600 training acc: 0.125
Global Iter: 1406700 training loss: 1.92277
Global Iter: 1406700 training acc: 0.28125
Global Iter: 1406800 training loss: 1.86967
Global Iter: 1406800 training acc: 0.1875
Global Iter: 1406900 training loss: 2.0136
Global Iter: 1406900 training acc: 0.21875
Global Iter: 1407000 training loss: 2.02098
Global Iter: 1407000 training acc: 0.15625
Global Iter: 1407100 training loss: 1.99009
Global Iter: 1407100 training acc: 0.15625
Global Iter: 1407200 training loss: 1.93749
Global Iter: 1407200 training acc: 0.15625
Global Iter: 1407300 training loss: 1.93591
Global Iter: 1407300 training acc: 0.21875
Global Iter: 1407400 training loss: 1.95203
Global Iter: 1407400 training acc: 0.125
Global Iter: 1407500 training loss: 2.03078
Global Iter: 1407500 training acc: 0.0625
Global Iter: 1407600 training loss: 2.04754
Global Iter: 1407600 training acc: 0.0625
Global Iter: 1407700 training loss: 1.97188
Global Iter: 1407700 training acc: 0.21875
Global Iter: 1407800 training loss: 1.93552
Global Iter: 1407800 training acc: 0.125
Global Iter: 1407900 training loss: 2.09717
Global Iter: 1407900 training acc: 0.0625
Global Iter: 1408000 training loss: 2.00199
Global Iter: 1408000 training acc: 0.15625
Global Iter: 1408100 training loss: 2.0178
Global Iter: 1408100 training acc: 0.0625
Global Iter: 1408200 training loss: 1.97888
Global Iter: 1408200 training acc: 0.125
Global Iter: 1408300 training loss: 2.00103
Global Iter: 1408300 training acc: 0.21875
Global Iter: 1408400 training loss: 2.0016
Global Iter: 1408400 training acc: 0.15625
Global Iter: 1408500 training loss: 1.93932
Global Iter: 1408500 training acc: 0.21875
Global Iter: 1408600 training loss: 1.97946
Global Iter: 1408600 training acc: 0.15625
Global Iter: 1408700 training loss: 1.96659
Global Iter: 1408700 training acc: 0.21875
Global Iter: 1408800 training loss: 1.93563
Global Iter: 1408800 training acc: 0.28125
Global Iter: 1408900 training loss: 2.15196
Global Iter: 1408900 training acc: 0.03125
Global Iter: 1409000 training loss: 2.0562
Global Iter: 1409000 training acc: 0.1875
Global Iter: 1409100 training loss: 2.02213
Global Iter: 1409100 training acc: 0.125
Global Iter: 1409200 training loss: 2.02354
Global Iter: 1409200 training acc: 0.25
Global Iter: 1409300 training loss: 2.10139
Global Iter: 1409300 training acc: 0.0625
Global Iter: 1409400 training loss: 1.94386
Global Iter: 1409400 training acc: 0.25
Global Iter: 1409500 training loss: 2.03045
Global Iter: 1409500 training acc: 0.125
Global Iter: 1409600 training loss: 1.97352
Global Iter: 1409600 training acc: 0.1875
Global Iter: 1409700 training loss: 2.02514
Global Iter: 1409700 training acc: 0.1875
Global Iter: 1409800 training loss: 1.97427
Global Iter: 1409800 training acc: 0.125
Global Iter: 1409900 training loss: 2.02176
Global Iter: 1409900 training acc: 0.125
Global Iter: 1410000 training loss: 1.99095
Global Iter: 1410000 training acc: 0.125
Global Iter: 1410100 training loss: 1.94823
Global Iter: 1410100 training acc: 0.21875
Global Iter: 1410200 training loss: 2.05112
Global Iter: 1410200 training acc: 0.09375
Global Iter: 1410300 training loss: 1.90455
Global Iter: 1410300 training acc: 0.1875
Global Iter: 1410400 training loss: 1.98646
Global Iter: 1410400 training acc: 0.25
Global Iter: 1410500 training loss: 1.99746
Global Iter: 1410500 training acc: 0.09375
Global Iter: 1410600 training loss: 2.01145
Global Iter: 1410600 training acc: 0.09375
Global Iter: 1410700 t2017-06-22 21:19:33.393384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1411618
raining loss: 1.97334
Global Iter: 1410700 training acc: 0.21875
Global Iter: 1410800 training loss: 2.00959
Global Iter: 1410800 training acc: 0.28125
Global Iter: 1410900 training loss: 2.0674
Global Iter: 1410900 training acc: 0.0625
Global Iter: 1411000 training loss: 2.08014
Global Iter: 1411000 training acc: 0.25
Global Iter: 1411100 training loss: 1.93276
Global Iter: 1411100 training acc: 0.25
Global Iter: 1411200 training loss: 1.97667
Global Iter: 1411200 training acc: 0.3125
Global Iter: 1411300 training loss: 2.0612
Global Iter: 1411300 training acc: 0.25
Global Iter: 1411400 training loss: 2.02839
Global Iter: 1411400 training acc: 0.15625
Global Iter: 1411500 training loss: 1.95799
Global Iter: 1411500 training acc: 0.1875
Global Iter: 1411600 training loss: 1.99409
Global Iter: 1411600 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1411618
Number of Patches: 81619
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1411618
Global Iter: 1411700 training loss: 1.96124
Global Iter: 1411700 training acc: 0.125
Global Iter: 1411800 training loss: 2.03828
Global Iter: 1411800 training acc: 0.1875
Global Iter: 1411900 training loss: 1.96113
Global Iter: 1411900 training acc: 0.25
Global Iter: 1412000 training loss: 2.07194
Global Iter: 1412000 training acc: 0.125
Global Iter: 1412100 training loss: 2.03242
Global Iter: 1412100 training acc: 0.21875
Global Iter: 1412200 training loss: 1.95135
Global Iter: 1412200 training acc: 0.25
Global Iter: 1412300 training loss: 1.95016
Global Iter: 1412300 training acc: 0.1875
Global Iter: 1412400 training loss: 1.97902
Global Iter: 1412400 training acc: 0.15625
Global Iter: 1412500 training loss: 1.86981
Global Iter: 1412500 training acc: 0.25
Global Iter: 1412600 training loss: 1.93896
Global Iter: 1412600 training acc: 0.28125
Global Iter: 1412700 training loss: 1.94572
Global Iter: 1412700 training acc: 0.28125
Global Iter: 1412800 training loss: 1.95854
Global Iter: 1412800 training acc: 0.21875
Global Iter: 1412900 training loss: 1.90393
Global Iter: 1412900 training acc: 0.21875
Global Iter: 1413000 training loss: 1.98022
Global Iter: 1413000 training acc: 0.25
Global Iter: 1413100 training loss: 1.97799
Global Iter: 1413100 training acc: 0.125
Global Iter: 1413200 training loss: 2.07926
Global Iter: 1413200 training acc: 0.09375
Global Iter: 1413300 training loss: 2.02397
Global Iter: 1413300 training acc: 0.15625
Global Iter: 1413400 training loss: 1.95682
Global Iter: 1413400 training acc: 0.1875
Global Iter: 1413500 training loss: 1.99369
Global Iter: 1413500 training acc: 0.21875
Global Iter: 1413600 training loss: 1.96845
Global Iter: 1413600 training acc: 0.21875
Global Iter: 1413700 training loss: 1.90099
Global Iter: 1413700 training acc: 0.15625
Global Iter: 1413800 training loss: 2.08901
Global Iter: 1413800 training acc: 0.09375
Global Iter: 1413900 training loss: 1.96856
Global Iter: 1413900 training acc: 0.25
Global Iter: 1414000 training loss: 2.01821
Global Iter: 1414000 training acc: 0.15625
Global Iter: 1414100 training loss: 1.95871
Global Iter: 1414100 training acc: 0.1875
Global Iter: 1414200 training loss: 1.9721
Global Iter: 1414200 training acc: 0.125
Global Iter: 1414300 training loss: 2.0977
Global Iter: 1414300 training acc: 0.125
Global Iter: 1414400 training loss: 1.93948
Global Iter: 1414400 training acc: 0.1875
Global Iter: 1414500 training loss: 1.88099
Global Iter: 1414500 training acc: 0.1875
Global Iter: 1414600 training loss: 1.98669
Global Iter: 1414600 training acc: 0.15625
Global Iter: 1414700 training loss: 1.9787
Global Iter: 1414700 training acc: 0.09375
Global Iter: 1414800 training loss: 2.0082
Global Iter: 1414800 training acc: 0.25
Global Iter: 1414900 training loss: 1.91026
Global Iter: 1414900 training acc: 0.28125
Global Iter: 1415000 training loss: 2.11065
Global Iter: 1415000 training acc: 0.09375
Global Iter: 1415100 training loss: 1.9897
Global Iter: 1415100 training acc: 0.3125
Global Iter: 1415200 training loss2017-06-22 21:27:57.782292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1416720
: 1.91948
Global Iter: 1415200 training acc: 0.125
Global Iter: 1415300 training loss: 1.91625
Global Iter: 1415300 training acc: 0.1875
Global Iter: 1415400 training loss: 2.01426
Global Iter: 1415400 training acc: 0.25
Global Iter: 1415500 training loss: 1.98188
Global Iter: 1415500 training acc: 0.15625
Global Iter: 1415600 training loss: 1.97934
Global Iter: 1415600 training acc: 0.1875
Global Iter: 1415700 training loss: 1.92327
Global Iter: 1415700 training acc: 0.1875
Global Iter: 1415800 training loss: 2.02458
Global Iter: 1415800 training acc: 0.21875
Global Iter: 1415900 training loss: 2.03634
Global Iter: 1415900 training acc: 0.125
Global Iter: 1416000 training loss: 1.94633
Global Iter: 1416000 training acc: 0.15625
Global Iter: 1416100 training loss: 1.94109
Global Iter: 1416100 training acc: 0.15625
Global Iter: 1416200 training loss: 1.98819
Global Iter: 1416200 training acc: 0.125
Global Iter: 1416300 training loss: 1.95813
Global Iter: 1416300 training acc: 0.25
Global Iter: 1416400 training loss: 2.1366
Global Iter: 1416400 training acc: 0.03125
Global Iter: 1416500 training loss: 2.05014
Global Iter: 1416500 training acc: 0.09375
Global Iter: 1416600 training loss: 1.99091
Global Iter: 1416600 training acc: 0.125
Global Iter: 1416700 training loss: 1.94046
Global Iter: 1416700 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1416720
Number of Patches: 80803
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1416720
Global Iter: 1416800 training loss: 2.00793
Global Iter: 1416800 training acc: 0.15625
Global Iter: 1416900 training loss: 2.08825
Global Iter: 1416900 training acc: 0.21875
Global Iter: 1417000 training loss: 1.9115
Global Iter: 1417000 training acc: 0.15625
Global Iter: 1417100 training loss: 2.0093
Global Iter: 1417100 training acc: 0.1875
Global Iter: 1417200 training loss: 2.00532
Global Iter: 1417200 training acc: 0.25
Global Iter: 1417300 training loss: 1.91955
Global Iter: 1417300 training acc: 0.1875
Global Iter: 1417400 training loss: 1.94364
Global Iter: 1417400 training acc: 0.1875
Global Iter: 1417500 training loss: 1.96997
Global Iter: 1417500 training acc: 0.25
Global Iter: 1417600 training loss: 1.96212
Global Iter: 1417600 training acc: 0.1875
Global Iter: 1417700 training loss: 2.06593
Global Iter: 1417700 training acc: 0.15625
Global Iter: 1417800 training loss: 1.94796
Global Iter: 1417800 training acc: 0.21875
Global Iter: 1417900 training loss: 2.05
Global Iter: 1417900 training acc: 0.125
Global Iter: 1418000 training loss: 1.94072
Global Iter: 1418000 training acc: 0.1875
Global Iter: 1418100 training loss: 1.88974
Global Iter: 1418100 training acc: 0.21875
Global Iter: 1418200 training loss: 2.03931
Global Iter: 1418200 training acc: 0.15625
Global Iter: 1418300 training loss: 2.02397
Global Iter: 1418300 training acc: 0.125
Global Iter: 1418400 training loss: 1.94701
Global Iter: 1418400 training acc: 0.15625
Global Iter: 1418500 training loss: 2.03916
Global Iter: 1418500 training acc: 0.1875
Global Iter: 1418600 training loss: 2.07171
Global Iter: 1418600 training acc: 0.125
Global Iter: 1418700 training loss: 2.03878
Global Iter: 1418700 training acc: 0.125
Global Iter: 1418800 training loss: 1.97668
Global Iter: 1418800 training acc: 0.125
Global Iter: 1418900 training loss: 2.02469
Global Iter: 1418900 training acc: 0.09375
Global Iter: 1419000 training loss: 1.92463
Global Iter: 1419000 training acc: 0.09375
Global Iter: 1419100 training loss: 2.00096
Global Iter: 1419100 training acc: 0.15625
Global Iter: 1419200 training loss: 2.03096
Global Iter: 1419200 training acc: 0.21875
Global Iter: 1419300 training loss: 1.98936
Global Iter: 1419300 training acc: 0.21875
Global Iter: 1419400 training loss: 2.07688
Global Iter: 1419400 training acc: 0.0625
Global Iter: 1419500 training loss: 1.9048
Global Iter: 1419500 training acc: 0.21875
Global Iter: 1419600 training loss: 2.00662
Global Iter: 1419600 training acc: 0.03125
Global Iter: 1419700 training loss: 2.2017-06-22 21:36:27.074783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1421771
07063
Global Iter: 1419700 training acc: 0.125
Global Iter: 1419800 training loss: 1.95395
Global Iter: 1419800 training acc: 0.1875
Global Iter: 1419900 training loss: 1.96743
Global Iter: 1419900 training acc: 0.1875
Global Iter: 1420000 training loss: 1.9462
Global Iter: 1420000 training acc: 0.28125
Global Iter: 1420100 training loss: 1.92879
Global Iter: 1420100 training acc: 0.21875
Global Iter: 1420200 training loss: 2.05121
Global Iter: 1420200 training acc: 0.15625
Global Iter: 1420300 training loss: 1.89163
Global Iter: 1420300 training acc: 0.1875
Global Iter: 1420400 training loss: 2.05923
Global Iter: 1420400 training acc: 0.125
Global Iter: 1420500 training loss: 1.96009
Global Iter: 1420500 training acc: 0.1875
Global Iter: 1420600 training loss: 1.89443
Global Iter: 1420600 training acc: 0.25
Global Iter: 1420700 training loss: 2.05432
Global Iter: 1420700 training acc: 0.1875
Global Iter: 1420800 training loss: 1.9902
Global Iter: 1420800 training acc: 0.15625
Global Iter: 1420900 training loss: 1.91456
Global Iter: 1420900 training acc: 0.1875
Global Iter: 1421000 training loss: 2.04492
Global Iter: 1421000 training acc: 0.125
Global Iter: 1421100 training loss: 2.05003
Global Iter: 1421100 training acc: 0.1875
Global Iter: 1421200 training loss: 2.03751
Global Iter: 1421200 training acc: 0.125
Global Iter: 1421300 training loss: 1.97192
Global Iter: 1421300 training acc: 0.125
Global Iter: 1421400 training loss: 2.02202
Global Iter: 1421400 training acc: 0.125
Global Iter: 1421500 training loss: 1.92971
Global Iter: 1421500 training acc: 0.09375
Global Iter: 1421600 training loss: 2.06022
Global Iter: 1421600 training acc: 0.09375
Global Iter: 1421700 training loss: 2.01627
Global Iter: 1421700 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1421771
Number of Patches: 80003
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1421771
Global Iter: 1421800 training loss: 1.95411
Global Iter: 1421800 training acc: 0.21875
Global Iter: 1421900 training loss: 1.98775
Global Iter: 1421900 training acc: 0.15625
Global Iter: 1422000 training loss: 1.957
Global Iter: 1422000 training acc: 0.09375
Global Iter: 1422100 training loss: 2.03534
Global Iter: 1422100 training acc: 0.21875
Global Iter: 1422200 training loss: 1.98621
Global Iter: 1422200 training acc: 0.15625
Global Iter: 1422300 training loss: 1.89052
Global Iter: 1422300 training acc: 0.28125
Global Iter: 1422400 training loss: 1.92696
Global Iter: 1422400 training acc: 0.15625
Global Iter: 1422500 training loss: 2.0569
Global Iter: 1422500 training acc: 0.1875
Global Iter: 1422600 training loss: 2.00981
Global Iter: 1422600 training acc: 0.09375
Global Iter: 1422700 training loss: 2.04999
Global Iter: 1422700 training acc: 0.21875
Global Iter: 1422800 training loss: 2.00932
Global Iter: 1422800 training acc: 0.40625
Global Iter: 1422900 training loss: 1.95467
Global Iter: 1422900 training acc: 0.21875
Global Iter: 1423000 training loss: 1.98926
Global Iter: 1423000 training acc: 0.15625
Global Iter: 1423100 training loss: 1.92704
Global Iter: 1423100 training acc: 0.1875
Global Iter: 1423200 training loss: 1.93778
Global Iter: 1423200 training acc: 0.21875
Global Iter: 1423300 training loss: 1.95194
Global Iter: 1423300 training acc: 0.0625
Global Iter: 1423400 training loss: 1.98781
Global Iter: 1423400 training acc: 0.15625
Global Iter: 1423500 training loss: 2.01655
Global Iter: 1423500 training acc: 0.1875
Global Iter: 1423600 training loss: 2.0836
Global Iter: 1423600 training acc: 0.03125
Global Iter: 1423700 training loss: 1.92538
Global Iter: 1423700 training acc: 0.1875
Global Iter: 1423800 training loss: 1.97611
Global Iter: 1423800 training acc: 0.21875
Global Iter: 1423900 training loss: 1.98025
Global Iter: 1423900 training acc: 0.125
Global Iter: 1424000 training loss: 2.13711
Global Iter: 1424000 training acc: 0.15625
Global Iter: 1424100 training loss: 2.06851
Global Iter: 1424100 training acc: 0.03125
Global Iter: 1424200 training los2017-06-22 21:45:01.302872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1426772
s: 1.91305
Global Iter: 1424200 training acc: 0.3125
Global Iter: 1424300 training loss: 1.99616
Global Iter: 1424300 training acc: 0.25
Global Iter: 1424400 training loss: 2.01914
Global Iter: 1424400 training acc: 0.125
Global Iter: 1424500 training loss: 2.00956
Global Iter: 1424500 training acc: 0.125
Global Iter: 1424600 training loss: 1.93422
Global Iter: 1424600 training acc: 0.21875
Global Iter: 1424700 training loss: 2.06661
Global Iter: 1424700 training acc: 0.15625
Global Iter: 1424800 training loss: 2.03975
Global Iter: 1424800 training acc: 0.125
Global Iter: 1424900 training loss: 2.02498
Global Iter: 1424900 training acc: 0.15625
Global Iter: 1425000 training loss: 2.0557
Global Iter: 1425000 training acc: 0.1875
Global Iter: 1425100 training loss: 1.88536
Global Iter: 1425100 training acc: 0.28125
Global Iter: 1425200 training loss: 2.01322
Global Iter: 1425200 training acc: 0.1875
Global Iter: 1425300 training loss: 2.04558
Global Iter: 1425300 training acc: 0.09375
Global Iter: 1425400 training loss: 2.0504
Global Iter: 1425400 training acc: 0.09375
Global Iter: 1425500 training loss: 1.97441
Global Iter: 1425500 training acc: 0.09375
Global Iter: 1425600 training loss: 2.00243
Global Iter: 1425600 training acc: 0.28125
Global Iter: 1425700 training loss: 2.06039
Global Iter: 1425700 training acc: 0.09375
Global Iter: 1425800 training loss: 2.01697
Global Iter: 1425800 training acc: 0.25
Global Iter: 1425900 training loss: 1.92013
Global Iter: 1425900 training acc: 0.21875
Global Iter: 1426000 training loss: 1.97674
Global Iter: 1426000 training acc: 0.375
Global Iter: 1426100 training loss: 1.99514
Global Iter: 1426100 training acc: 0.1875
Global Iter: 1426200 training loss: 2.05245
Global Iter: 1426200 training acc: 0.15625
Global Iter: 1426300 training loss: 1.95744
Global Iter: 1426300 training acc: 0.21875
Global Iter: 1426400 training loss: 2.06844
Global Iter: 1426400 training acc: 0.15625
Global Iter: 1426500 training loss: 1.91855
Global Iter: 1426500 training acc: 0.3125
Global Iter: 1426600 training loss: 1.98281
Global Iter: 1426600 training acc: 0.21875
Global Iter: 1426700 training loss: 1.98192
Global Iter: 1426700 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1426772
Number of Patches: 79203
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1426772
Global Iter: 1426800 training loss: 1.95259
Global Iter: 1426800 training acc: 0.09375
Global Iter: 1426900 training loss: 1.93397
Global Iter: 1426900 training acc: 0.25
Global Iter: 1427000 training loss: 2.00662
Global Iter: 1427000 training acc: 0.125
Global Iter: 1427100 training loss: 2.01241
Global Iter: 1427100 training acc: 0.1875
Global Iter: 1427200 training loss: 1.99038
Global Iter: 1427200 training acc: 0.15625
Global Iter: 1427300 training loss: 1.93245
Global Iter: 1427300 training acc: 0.21875
Global Iter: 1427400 training loss: 2.01591
Global Iter: 1427400 training acc: 0.15625
Global Iter: 1427500 training loss: 1.95499
Global Iter: 1427500 training acc: 0.125
Global Iter: 1427600 training loss: 2.06886
Global Iter: 1427600 training acc: 0.15625
Global Iter: 1427700 training loss: 1.90534
Global Iter: 1427700 training acc: 0.21875
Global Iter: 1427800 training loss: 2.11699
Global Iter: 1427800 training acc: 0.3125
Global Iter: 1427900 training loss: 1.88439
Global Iter: 1427900 training acc: 0.28125
Global Iter: 1428000 training loss: 1.91836
Global Iter: 1428000 training acc: 0.1875
Global Iter: 1428100 training loss: 2.0673
Global Iter: 1428100 training acc: 0.125
Global Iter: 1428200 training loss: 1.99224
Global Iter: 1428200 training acc: 0.125
Global Iter: 1428300 training loss: 1.99453
Global Iter: 1428300 training acc: 0.09375
Global Iter: 1428400 training loss: 1.97365
Global Iter: 1428400 training acc: 0.25
Global Iter: 1428500 training loss: 1.9056
Global Iter: 1428500 training acc: 0.25
Global Iter: 1428600 training loss: 1.91217
Global Iter: 1428600 training acc: 0.25
Global Iter: 1428700 training loss: 12017-06-22 21:53:25.267870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1431723
.96357
Global Iter: 1428700 training acc: 0.21875
Global Iter: 1428800 training loss: 2.01809
Global Iter: 1428800 training acc: 0.1875
Global Iter: 1428900 training loss: 1.95913
Global Iter: 1428900 training acc: 0.125
Global Iter: 1429000 training loss: 1.94856
Global Iter: 1429000 training acc: 0.09375
Global Iter: 1429100 training loss: 1.92248
Global Iter: 1429100 training acc: 0.25
Global Iter: 1429200 training loss: 1.99522
Global Iter: 1429200 training acc: 0.15625
Global Iter: 1429300 training loss: 1.96662
Global Iter: 1429300 training acc: 0.21875
Global Iter: 1429400 training loss: 2.06256
Global Iter: 1429400 training acc: 0.15625
Global Iter: 1429500 training loss: 1.96211
Global Iter: 1429500 training acc: 0.28125
Global Iter: 1429600 training loss: 1.87649
Global Iter: 1429600 training acc: 0.28125
Global Iter: 1429700 training loss: 1.95847
Global Iter: 1429700 training acc: 0.15625
Global Iter: 1429800 training loss: 2.0619
Global Iter: 1429800 training acc: 0.1875
Global Iter: 1429900 training loss: 1.99906
Global Iter: 1429900 training acc: 0.21875
Global Iter: 1430000 training loss: 1.97915
Global Iter: 1430000 training acc: 0.1875
Global Iter: 1430100 training loss: 1.98801
Global Iter: 1430100 training acc: 0.15625
Global Iter: 1430200 training loss: 1.91166
Global Iter: 1430200 training acc: 0.15625
Global Iter: 1430300 training loss: 2.01501
Global Iter: 1430300 training acc: 0.15625
Global Iter: 1430400 training loss: 2.06616
Global Iter: 1430400 training acc: 0.03125
Global Iter: 1430500 training loss: 2.12085
Global Iter: 1430500 training acc: 0.15625
Global Iter: 1430600 training loss: 1.97746
Global Iter: 1430600 training acc: 0.125
Global Iter: 1430700 training loss: 2.03431
Global Iter: 1430700 training acc: 0.15625
Global Iter: 1430800 training loss: 2.0196
Global Iter: 1430800 training acc: 0.125
Global Iter: 1430900 training loss: 1.97289
Global Iter: 1430900 training acc: 0.25
Global Iter: 1431000 training loss: 1.99185
Global Iter: 1431000 training acc: 0.1875
Global Iter: 1431100 training loss: 1.9192
Global Iter: 1431100 training acc: 0.25
Global Iter: 1431200 training loss: 1.97753
Global Iter: 1431200 training acc: 0.125
Global Iter: 1431300 training loss: 1.97611
Global Iter: 1431300 training acc: 0.1875
Global Iter: 1431400 training loss: 2.0164
Global Iter: 1431400 training acc: 0.03125
Global Iter: 1431500 training loss: 2.02006
Global Iter: 1431500 training acc: 0.1875
Global Iter: 1431600 training loss: 2.01493
Global Iter: 1431600 training acc: 0.1875
Global Iter: 1431700 training loss: 2.01637
Global Iter: 1431700 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1431723
Number of Patches: 78411
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1431723
Global Iter: 1431800 training loss: 2.05693
Global Iter: 1431800 training acc: 0.03125
Global Iter: 1431900 training loss: 2.07262
Global Iter: 1431900 training acc: 0.21875
Global Iter: 1432000 training loss: 1.93899
Global Iter: 1432000 training acc: 0.125
Global Iter: 1432100 training loss: 1.93139
Global Iter: 1432100 training acc: 0.21875
Global Iter: 1432200 training loss: 2.04878
Global Iter: 1432200 training acc: 0.15625
Global Iter: 1432300 training loss: 2.02488
Global Iter: 1432300 training acc: 0.125
Global Iter: 1432400 training loss: 2.00892
Global Iter: 1432400 training acc: 0.125
Global Iter: 1432500 training loss: 1.944
Global Iter: 1432500 training acc: 0.25
Global Iter: 1432600 training loss: 1.99548
Global Iter: 1432600 training acc: 0.3125
Global Iter: 1432700 training loss: 1.927
Global Iter: 1432700 training acc: 0.34375
Global Iter: 1432800 training loss: 2.0328
Global Iter: 1432800 training acc: 0.1875
Global Iter: 1432900 training loss: 1.97609
Global Iter: 1432900 training acc: 0.125
Global Iter: 1433000 training loss: 2.0974
Global Iter: 1433000 training acc: 0.1875
Global Iter: 1433100 training loss: 1.90205
Global Iter: 1433100 training acc: 0.28125
Global Iter: 1433200 training loss: 1.973352017-06-22 22:01:43.127502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1436624

Global Iter: 1433200 training acc: 0.15625
Global Iter: 1433300 training loss: 1.94135
Global Iter: 1433300 training acc: 0.09375
Global Iter: 1433400 training loss: 2.16687
Global Iter: 1433400 training acc: 0.09375
Global Iter: 1433500 training loss: 2.07322
Global Iter: 1433500 training acc: 0.0625
Global Iter: 1433600 training loss: 2.00644
Global Iter: 1433600 training acc: 0.09375
Global Iter: 1433700 training loss: 2.02432
Global Iter: 1433700 training acc: 0.09375
Global Iter: 1433800 training loss: 1.94327
Global Iter: 1433800 training acc: 0.3125
Global Iter: 1433900 training loss: 2.06781
Global Iter: 1433900 training acc: 0.15625
Global Iter: 1434000 training loss: 2.10667
Global Iter: 1434000 training acc: 0.15625
Global Iter: 1434100 training loss: 2.00994
Global Iter: 1434100 training acc: 0.21875
Global Iter: 1434200 training loss: 1.99336
Global Iter: 1434200 training acc: 0.125
Global Iter: 1434300 training loss: 2.00865
Global Iter: 1434300 training acc: 0.21875
Global Iter: 1434400 training loss: 1.91322
Global Iter: 1434400 training acc: 0.15625
Global Iter: 1434500 training loss: 2.00899
Global Iter: 1434500 training acc: 0.125
Global Iter: 1434600 training loss: 1.91144
Global Iter: 1434600 training acc: 0.34375
Global Iter: 1434700 training loss: 1.99517
Global Iter: 1434700 training acc: 0.25
Global Iter: 1434800 training loss: 1.96538
Global Iter: 1434800 training acc: 0.25
Global Iter: 1434900 training loss: 1.92531
Global Iter: 1434900 training acc: 0.21875
Global Iter: 1435000 training loss: 1.93588
Global Iter: 1435000 training acc: 0.21875
Global Iter: 1435100 training loss: 1.95829
Global Iter: 1435100 training acc: 0.0625
Global Iter: 1435200 training loss: 1.98415
Global Iter: 1435200 training acc: 0.21875
Global Iter: 1435300 training loss: 1.89883
Global Iter: 1435300 training acc: 0.21875
Global Iter: 1435400 training loss: 1.89668
Global Iter: 1435400 training acc: 0.25
Global Iter: 1435500 training loss: 1.94788
Global Iter: 1435500 training acc: 0.25
Global Iter: 1435600 training loss: 1.97475
Global Iter: 1435600 training acc: 0.1875
Global Iter: 1435700 training loss: 1.91802
Global Iter: 1435700 training acc: 0.25
Global Iter: 1435800 training loss: 1.9795
Global Iter: 1435800 training acc: 0.09375
Global Iter: 1435900 training loss: 2.05509
Global Iter: 1435900 training acc: 0.09375
Global Iter: 1436000 training loss: 2.05663
Global Iter: 1436000 training acc: 0.125
Global Iter: 1436100 training loss: 1.97793
Global Iter: 1436100 training acc: 0.125
Global Iter: 1436200 training loss: 1.93048
Global Iter: 1436200 training acc: 0.1875
Global Iter: 1436300 training loss: 2.04502
Global Iter: 1436300 training acc: 0.1875
Global Iter: 1436400 training loss: 1.98252
Global Iter: 1436400 training acc: 0.15625
Global Iter: 1436500 training loss: 2.14706
Global Iter: 1436500 training acc: 0.15625
Global Iter: 1436600 training loss: 2.05916
Global Iter: 1436600 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1436624
Number of Patches: 77627
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1436624
Global Iter: 1436700 training loss: 2.07673
Global Iter: 1436700 training acc: 0.25
Global Iter: 1436800 training loss: 2.06778
Global Iter: 1436800 training acc: 0.09375
Global Iter: 1436900 training loss: 1.88465
Global Iter: 1436900 training acc: 0.28125
Global Iter: 1437000 training loss: 1.97338
Global Iter: 1437000 training acc: 0.125
Global Iter: 1437100 training loss: 1.97863
Global Iter: 1437100 training acc: 0.1875
Global Iter: 1437200 training loss: 2.01605
Global Iter: 1437200 training acc: 0.21875
Global Iter: 1437300 training loss: 1.93731
Global Iter: 1437300 training acc: 0.21875
Global Iter: 1437400 training loss: 1.96963
Global Iter: 1437400 training acc: 0.09375
Global Iter: 1437500 training loss: 2.04111
Global Iter: 1437500 training acc: 0.15625
Global Iter: 1437600 training loss: 2.03985
Global Iter: 1437600 training acc: 0.09375
Global Iter: 1437700 training loss: 2017-06-22 22:09:58.896954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1441476
2.04528
Global Iter: 1437700 training acc: 0.15625
Global Iter: 1437800 training loss: 2.04013
Global Iter: 1437800 training acc: 0.21875
Global Iter: 1437900 training loss: 2.06843
Global Iter: 1437900 training acc: 0.15625
Global Iter: 1438000 training loss: 1.95975
Global Iter: 1438000 training acc: 0.1875
Global Iter: 1438100 training loss: 2.02167
Global Iter: 1438100 training acc: 0.15625
Global Iter: 1438200 training loss: 2.01465
Global Iter: 1438200 training acc: 0.15625
Global Iter: 1438300 training loss: 1.94823
Global Iter: 1438300 training acc: 0.1875
Global Iter: 1438400 training loss: 2.0443
Global Iter: 1438400 training acc: 0.09375
Global Iter: 1438500 training loss: 1.95991
Global Iter: 1438500 training acc: 0.25
Global Iter: 1438600 training loss: 1.9475
Global Iter: 1438600 training acc: 0.15625
Global Iter: 1438700 training loss: 1.95429
Global Iter: 1438700 training acc: 0.09375
Global Iter: 1438800 training loss: 1.99496
Global Iter: 1438800 training acc: 0.25
Global Iter: 1438900 training loss: 1.89878
Global Iter: 1438900 training acc: 0.25
Global Iter: 1439000 training loss: 1.95742
Global Iter: 1439000 training acc: 0.21875
Global Iter: 1439100 training loss: 1.93003
Global Iter: 1439100 training acc: 0.21875
Global Iter: 1439200 training loss: 2.0575
Global Iter: 1439200 training acc: 0.15625
Global Iter: 1439300 training loss: 2.00271
Global Iter: 1439300 training acc: 0.15625
Global Iter: 1439400 training loss: 1.94592
Global Iter: 1439400 training acc: 0.125
Global Iter: 1439500 training loss: 1.98187
Global Iter: 1439500 training acc: 0.125
Global Iter: 1439600 training loss: 1.99588
Global Iter: 1439600 training acc: 0.1875
Global Iter: 1439700 training loss: 2.04029
Global Iter: 1439700 training acc: 0.1875
Global Iter: 1439800 training loss: 1.94386
Global Iter: 1439800 training acc: 0.09375
Global Iter: 1439900 training loss: 2.07561
Global Iter: 1439900 training acc: 0.09375
Global Iter: 1440000 training loss: 2.0414
Global Iter: 1440000 training acc: 0.21875
Global Iter: 1440100 training loss: 1.98539
Global Iter: 1440100 training acc: 0.09375
Global Iter: 1440200 training loss: 2.09475
Global Iter: 1440200 training acc: 0.15625
Global Iter: 1440300 training loss: 2.00663
Global Iter: 1440300 training acc: 0.125
Global Iter: 1440400 training loss: 1.92794
Global Iter: 1440400 training acc: 0.125
Global Iter: 1440500 training loss: 1.9954
Global Iter: 1440500 training acc: 0.125
Global Iter: 1440600 training loss: 1.97102
Global Iter: 1440600 training acc: 0.1875
Global Iter: 1440700 training loss: 1.93397
Global Iter: 1440700 training acc: 0.28125
Global Iter: 1440800 training loss: 1.9598
Global Iter: 1440800 training acc: 0.1875
Global Iter: 1440900 training loss: 1.95396
Global Iter: 1440900 training acc: 0.15625
Global Iter: 1441000 training loss: 2.0938
Global Iter: 1441000 training acc: 0.125
Global Iter: 1441100 training loss: 1.99861
Global Iter: 1441100 training acc: 0.09375
Global Iter: 1441200 training loss: 2.06796
Global Iter: 1441200 training acc: 0.125
Global Iter: 1441300 training loss: 1.92604
Global Iter: 1441300 training acc: 0.34375
Global Iter: 1441400 training loss: 1.87108
Global Iter: 1441400 training acc: 0.34375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1441476
Number of Patches: 76851
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1441476
Global Iter: 1441500 training loss: 1.93221
Global Iter: 1441500 training acc: 0.1875
Global Iter: 1441600 training loss: 1.95962
Global Iter: 1441600 training acc: 0.09375
Global Iter: 1441700 training loss: 2.17788
Global Iter: 1441700 training acc: 0.125
Global Iter: 1441800 training loss: 2.00788
Global Iter: 1441800 training acc: 0.28125
Global Iter: 1441900 training loss: 1.95226
Global Iter: 1441900 training acc: 0.21875
Global Iter: 1442000 training loss: 1.94458
Global Iter: 1442000 training acc: 0.09375
Global Iter: 1442100 training loss: 2.06517
Global Iter: 1442100 training acc: 0.0625
Global Iter: 1442200 training los2017-06-22 22:18:04.809784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1446280
s: 2.05936
Global Iter: 1442200 training acc: 0.15625
Global Iter: 1442300 training loss: 1.96463
Global Iter: 1442300 training acc: 0.25
Global Iter: 1442400 training loss: 2.07419
Global Iter: 1442400 training acc: 0.25
Global Iter: 1442500 training loss: 2.015
Global Iter: 1442500 training acc: 0.15625
Global Iter: 1442600 training loss: 1.99858
Global Iter: 1442600 training acc: 0.125
Global Iter: 1442700 training loss: 1.87015
Global Iter: 1442700 training acc: 0.1875
Global Iter: 1442800 training loss: 2.02615
Global Iter: 1442800 training acc: 0.25
Global Iter: 1442900 training loss: 2.02243
Global Iter: 1442900 training acc: 0.15625
Global Iter: 1443000 training loss: 1.98047
Global Iter: 1443000 training acc: 0.3125
Global Iter: 1443100 training loss: 2.07615
Global Iter: 1443100 training acc: 0.125
Global Iter: 1443200 training loss: 1.97771
Global Iter: 1443200 training acc: 0.15625
Global Iter: 1443300 training loss: 1.88105
Global Iter: 1443300 training acc: 0.25
Global Iter: 1443400 training loss: 2.00909
Global Iter: 1443400 training acc: 0.125
Global Iter: 1443500 training loss: 2.07504
Global Iter: 1443500 training acc: 0.125
Global Iter: 1443600 training loss: 1.98624
Global Iter: 1443600 training acc: 0.15625
Global Iter: 1443700 training loss: 1.99105
Global Iter: 1443700 training acc: 0.21875
Global Iter: 1443800 training loss: 2.02402
Global Iter: 1443800 training acc: 0.15625
Global Iter: 1443900 training loss: 2.00166
Global Iter: 1443900 training acc: 0.125
Global Iter: 1444000 training loss: 1.98066
Global Iter: 1444000 training acc: 0.125
Global Iter: 1444100 training loss: 2.01289
Global Iter: 1444100 training acc: 0.1875
Global Iter: 1444200 training loss: 2.03689
Global Iter: 1444200 training acc: 0.1875
Global Iter: 1444300 training loss: 2.01165
Global Iter: 1444300 training acc: 0.125
Global Iter: 1444400 training loss: 1.99088
Global Iter: 1444400 training acc: 0.3125
Global Iter: 1444500 training loss: 1.97968
Global Iter: 1444500 training acc: 0.125
Global Iter: 1444600 training loss: 2.05816
Global Iter: 1444600 training acc: 0.15625
Global Iter: 1444700 training loss: 1.98715
Global Iter: 1444700 training acc: 0.1875
Global Iter: 1444800 training loss: 1.94268
Global Iter: 1444800 training acc: 0.1875
Global Iter: 1444900 training loss: 1.90806
Global Iter: 1444900 training acc: 0.25
Global Iter: 1445000 training loss: 2.00179
Global Iter: 1445000 training acc: 0.21875
Global Iter: 1445100 training loss: 1.98775
Global Iter: 1445100 training acc: 0.09375
Global Iter: 1445200 training loss: 2.0019
Global Iter: 1445200 training acc: 0.21875
Global Iter: 1445300 training loss: 1.88905
Global Iter: 1445300 training acc: 0.25
Global Iter: 1445400 training loss: 2.01619
Global Iter: 1445400 training acc: 0.15625
Global Iter: 1445500 training loss: 2.20822
Global Iter: 1445500 training acc: 0.15625
Global Iter: 1445600 training loss: 2.11931
Global Iter: 1445600 training acc: 0.125
Global Iter: 1445700 training loss: 1.91133
Global Iter: 1445700 training acc: 0.3125
Global Iter: 1445800 training loss: 1.925
Global Iter: 1445800 training acc: 0.25
Global Iter: 1445900 training loss: 1.93313
Global Iter: 1445900 training acc: 0.15625
Global Iter: 1446000 training loss: 2.09105
Global Iter: 1446000 training acc: 0.1875
Global Iter: 1446100 training loss: 1.95506
Global Iter: 1446100 training acc: 0.25
Global Iter: 1446200 training loss: 1.97281
Global Iter: 1446200 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1446280
Number of Patches: 76083
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1446280
Global Iter: 1446300 training loss: 2.0134
Global Iter: 1446300 training acc: 0.25
Global Iter: 1446400 training loss: 1.99569
Global Iter: 1446400 training acc: 0.25
Global Iter: 1446500 training loss: 1.89812
Global Iter: 1446500 training acc: 0.28125
Global Iter: 1446600 training loss: 2.0302
Global Iter: 1446600 training acc: 0.1875
Global Iter: 1446700 training loss: 2.03571
Global Iter2017-06-22 22:26:10.728947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1451036
: 1446700 training acc: 0.1875
Global Iter: 1446800 training loss: 1.95237
Global Iter: 1446800 training acc: 0.15625
Global Iter: 1446900 training loss: 2.02387
Global Iter: 1446900 training acc: 0.15625
Global Iter: 1447000 training loss: 2.17741
Global Iter: 1447000 training acc: 0.09375
Global Iter: 1447100 training loss: 1.99558
Global Iter: 1447100 training acc: 0.15625
Global Iter: 1447200 training loss: 1.97692
Global Iter: 1447200 training acc: 0.125
Global Iter: 1447300 training loss: 1.90764
Global Iter: 1447300 training acc: 0.21875
Global Iter: 1447400 training loss: 2.01822
Global Iter: 1447400 training acc: 0.0625
Global Iter: 1447500 training loss: 1.91507
Global Iter: 1447500 training acc: 0.1875
Global Iter: 1447600 training loss: 2.06357
Global Iter: 1447600 training acc: 0.0625
Global Iter: 1447700 training loss: 1.97123
Global Iter: 1447700 training acc: 0.0625
Global Iter: 1447800 training loss: 1.92147
Global Iter: 1447800 training acc: 0.21875
Global Iter: 1447900 training loss: 1.94196
Global Iter: 1447900 training acc: 0.15625
Global Iter: 1448000 training loss: 1.97166
Global Iter: 1448000 training acc: 0.34375
Global Iter: 1448100 training loss: 1.94313
Global Iter: 1448100 training acc: 0.21875
Global Iter: 1448200 training loss: 1.95673
Global Iter: 1448200 training acc: 0.15625
Global Iter: 1448300 training loss: 2.01279
Global Iter: 1448300 training acc: 0.15625
Global Iter: 1448400 training loss: 2.04423
Global Iter: 1448400 training acc: 0.1875
Global Iter: 1448500 training loss: 1.93079
Global Iter: 1448500 training acc: 0.09375
Global Iter: 1448600 training loss: 1.98473
Global Iter: 1448600 training acc: 0.3125
Global Iter: 1448700 training loss: 2.03581
Global Iter: 1448700 training acc: 0.21875
Global Iter: 1448800 training loss: 1.99622
Global Iter: 1448800 training acc: 0.15625
Global Iter: 1448900 training loss: 2.02812
Global Iter: 1448900 training acc: 0.21875
Global Iter: 1449000 training loss: 1.90896
Global Iter: 1449000 training acc: 0.15625
Global Iter: 1449100 training loss: 1.89537
Global Iter: 1449100 training acc: 0.25
Global Iter: 1449200 training loss: 1.9193
Global Iter: 1449200 training acc: 0.09375
Global Iter: 1449300 training loss: 1.99412
Global Iter: 1449300 training acc: 0.15625
Global Iter: 1449400 training loss: 1.93385
Global Iter: 1449400 training acc: 0.15625
Global Iter: 1449500 training loss: 2.02268
Global Iter: 1449500 training acc: 0.15625
Global Iter: 1449600 training loss: 1.99783
Global Iter: 1449600 training acc: 0.15625
Global Iter: 1449700 training loss: 2.00318
Global Iter: 1449700 training acc: 0.09375
Global Iter: 1449800 training loss: 2.01958
Global Iter: 1449800 training acc: 0.0625
Global Iter: 1449900 training loss: 1.9694
Global Iter: 1449900 training acc: 0.1875
Global Iter: 1450000 training loss: 1.91594
Global Iter: 1450000 training acc: 0.125
Global Iter: 1450100 training loss: 1.91661
Global Iter: 1450100 training acc: 0.28125
Global Iter: 1450200 training loss: 1.94262
Global Iter: 1450200 training acc: 0.21875
Global Iter: 1450300 training loss: 2.00293
Global Iter: 1450300 training acc: 0.1875
Global Iter: 1450400 training loss: 2.0644
Global Iter: 1450400 training acc: 0.15625
Global Iter: 1450500 training loss: 1.91068
Global Iter: 1450500 training acc: 0.3125
Global Iter: 1450600 training loss: 2.00333
Global Iter: 1450600 training acc: 0.125
Global Iter: 1450700 training loss: 2.14464
Global Iter: 1450700 training acc: 0.09375
Global Iter: 1450800 training loss: 1.97583
Global Iter: 1450800 training acc: 0.0625
Global Iter: 1450900 training loss: 1.992
Global Iter: 1450900 training acc: 0.125
Global Iter: 1451000 training loss: 2.12687
Global Iter: 1451000 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1451036
Number of Patches: 75323
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1451036
Global Iter: 1451100 training loss: 1.95135
Global Iter: 1451100 training acc: 0.15625
Global Iter: 1451200 training loss: 1.912017-06-22 22:34:12.096114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
399
Global Iter: 1451200 training acc: 0.28125
Global Iter: 1451300 training loss: 2.04489
Global Iter: 1451300 training acc: 0.125
Global Iter: 1451400 training loss: 2.0202
Global Iter: 1451400 training acc: 0.21875
Global Iter: 1451500 training loss: 2.07479
Global Iter: 1451500 training acc: 0.15625
Global Iter: 1451600 training loss: 1.99956
Global Iter: 1451600 training acc: 0.09375
Global Iter: 1451700 training loss: 1.97305
Global Iter: 1451700 training acc: 0.1875
Global Iter: 1451800 training loss: 1.94721
Global Iter: 1451800 training acc: 0.09375
Global Iter: 1451900 training loss: 2.10255
Global Iter: 1451900 training acc: 0.09375
Global Iter: 1452000 training loss: 1.99375
Global Iter: 1452000 training acc: 0.1875
Global Iter: 1452100 training loss: 2.05143
Global Iter: 1452100 training acc: 0.125
Global Iter: 1452200 training loss: 1.97492
Global Iter: 1452200 training acc: 0.1875
Global Iter: 1452300 training loss: 1.92349
Global Iter: 1452300 training acc: 0.21875
Global Iter: 1452400 training loss: 2.05264
Global Iter: 1452400 training acc: 0.125
Global Iter: 1452500 training loss: 1.97994
Global Iter: 1452500 training acc: 0.15625
Global Iter: 1452600 training loss: 2.04509
Global Iter: 1452600 training acc: 0.1875
Global Iter: 1452700 training loss: 2.01092
Global Iter: 1452700 training acc: 0.15625
Global Iter: 1452800 training loss: 2.05263
Global Iter: 1452800 training acc: 0.3125
Global Iter: 1452900 training loss: 2.04987
Global Iter: 1452900 training acc: 0.21875
Global Iter: 1453000 training loss: 2.00602
Global Iter: 1453000 training acc: 0.21875
Global Iter: 1453100 training loss: 1.97015
Global Iter: 1453100 training acc: 0.1875
Global Iter: 1453200 training loss: 1.93118
Global Iter: 1453200 training acc: 0.125
Global Iter: 1453300 training loss: 2.00194
Global Iter: 1453300 training acc: 0.15625
Global Iter: 1453400 training loss: 2.02726
Global Iter: 1453400 training acc: 0.15625
Global Iter: 1453500 training loss: 1.88685
Global Iter: 1453500 training acc: 0.1875
Global Iter: 1453600 training loss: 2.01307
Global Iter: 1453600 training acc: 0.1875
Global Iter: 1453700 training loss: 2.01396
Global Iter: 1453700 training acc: 0.03125
Global Iter: 1453800 training loss: 2.04247
Global Iter: 1453800 training acc: 0.21875
Global Iter: 1453900 training loss: 1.94868
Global Iter: 1453900 training acc: 0.21875
Global Iter: 1454000 training loss: 1.9775
Global Iter: 1454000 training acc: 0.28125
Global Iter: 1454100 training loss: 2.04582
Global Iter: 1454100 training acc: 0.25
Global Iter: 1454200 training loss: 2.01647
Global Iter: 1454200 training acc: 0.15625
Global Iter: 1454300 training loss: 1.89653
Global Iter: 1454300 training acc: 0.25
Global Iter: 1454400 training loss: 2.05322
Global Iter: 1454400 training acc: 0.1875
Global Iter: 1454500 training loss: 1.96342
Global Iter: 1454500 training acc: 0.09375
Global Iter: 1454600 training loss: 1.92728
Global Iter: 1454600 training acc: 0.125
Global Iter: 1454700 training loss: 1.91208
Global Iter: 1454700 training acc: 0.09375
Global Iter: 1454800 training loss: 1.94709
Global Iter: 1454800 training acc: 0.28125
Global Iter: 1454900 training loss: 2.0372
Global Iter: 1454900 training acc: 0.15625
Global Iter: 1455000 training loss: 1.95889
Global Iter: 1455000 training acc: 0.21875
Global Iter: 1455100 training loss: 1.91566
Global Iter: 1455100 training acc: 0.21875
Global Iter: 1455200 training loss: 1.9477
Global Iter: 1455200 training acc: 0.21875
Global Iter: 1455300 training loss: 2.10913
Global Iter: 1455300 training acc: 0.09375
Global Iter: 1455400 training loss: 1.9518
Global Iter: 1455400 training acc: 0.21875
Global Iter: 1455500 training loss: 1.89387
Global Iter: 1455500 training acc: 0.15625
Global Iter: 1455600 training loss: 2.03205
Global Iter: 1455600 training acc: 0.125
Global Iter: 1455700 training loss: 1.95159
Global Iter: 1455700 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1455744
Number of Patches: 74570
checkpoint found: /home/ahmet/workspace/tINFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1455744
ensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1455744
Global Iter: 1455800 training loss: 2.13599
Global Iter: 1455800 training acc: 0.09375
Global Iter: 1455900 training loss: 2.00068
Global Iter: 1455900 training acc: 0.1875
Global Iter: 1456000 training loss: 1.98364
Global Iter: 1456000 training acc: 0.125
Global Iter: 1456100 training loss: 1.95179
Global Iter: 1456100 training acc: 0.28125
Global Iter: 1456200 training loss: 1.95242
Global Iter: 1456200 training acc: 0.15625
Global Iter: 1456300 training loss: 2.03061
Global Iter: 1456300 training acc: 0.0625
Global Iter: 1456400 training loss: 2.00593
Global Iter: 1456400 training acc: 0.3125
Global Iter: 1456500 training loss: 1.98946
Global Iter: 1456500 training acc: 0.125
Global Iter: 1456600 training loss: 2.04487
Global Iter: 1456600 training acc: 0.1875
Global Iter: 1456700 training loss: 1.94038
Global Iter: 1456700 training acc: 0.25
Global Iter: 1456800 training loss: 1.98237
Global Iter: 1456800 training acc: 0.0625
Global Iter: 1456900 training loss: 2.03233
Global Iter: 1456900 training acc: 0.21875
Global Iter: 1457000 training loss: 2.02759
Global Iter: 1457000 training acc: 0.1875
Global Iter: 1457100 training loss: 1.92542
Global Iter: 1457100 training acc: 0.125
Global Iter: 1457200 training loss: 2.03289
Global Iter: 1457200 training acc: 0.25
Global Iter: 1457300 training loss: 1.92201
Global Iter: 1457300 training acc: 0.21875
Global Iter: 1457400 training loss: 1.98171
Global Iter: 1457400 training acc: 0.125
Global Iter: 1457500 training loss: 1.94268
Global Iter: 1457500 training acc: 0.21875
Global Iter: 1457600 training loss: 2.00647
Global Iter: 1457600 training acc: 0.125
Global Iter: 1457700 training loss: 1.93254
Global Iter: 1457700 training acc: 0.15625
Global Iter: 1457800 training loss: 1.88752
Global Iter: 1457800 training acc: 0.21875
Global Iter: 1457900 training loss: 2.04603
Global Iter: 1457900 training acc: 0.03125
Global Iter: 1458000 training loss: 2.02885
Global Iter: 1458000 training acc: 0.1875
Global Iter: 1458100 training loss: 1.8789
Global Iter: 1458100 training acc: 0.34375
Global Iter: 1458200 training loss: 1.97569
Global Iter: 1458200 training acc: 0.09375
Global Iter: 1458300 training loss: 2.03437
Global Iter: 1458300 training acc: 0.3125
Global Iter: 1458400 training loss: 1.89198
Global Iter: 1458400 training acc: 0.34375
Global Iter: 1458500 training loss: 2.02703
Global Iter: 1458500 training acc: 0.21875
Global Iter: 1458600 training loss: 1.96131
Global Iter: 1458600 training acc: 0.3125
Global Iter: 1458700 training loss: 1.95627
Global Iter: 1458700 training acc: 0.15625
Global Iter: 1458800 training loss: 2.0223
Global Iter: 1458800 training acc: 0.15625
Global Iter: 1458900 training loss: 1.95208
Global Iter: 1458900 training acc: 0.21875
Global Iter: 1459000 training loss: 2.00011
Global Iter: 1459000 training acc: 0.0625
Global Iter: 1459100 training loss: 1.93672
Global Iter: 1459100 training acc: 0.15625
Global Iter: 1459200 training loss: 2.05965
Global Iter: 1459200 training acc: 0.125
Global Iter: 1459300 training loss: 2.00847
Global Iter: 1459300 training acc: 0.09375
Global Iter: 1459400 training loss: 2.04206
Global Iter: 1459400 training acc: 0.125
Global Iter: 1459500 training loss: 1.95596
Global Iter: 1459500 training acc: 0.15625
Global Iter: 1459600 training loss: 2.02268
Global Iter: 1459600 training acc: 0.0625
Global Iter: 1459700 training loss: 2.02989
Global Iter: 1459700 training acc: 0.25
Global Iter: 1459800 training loss: 2.01389
Global Iter: 1459800 training acc: 0.1875
Global Iter: 1459900 training loss: 1.95334
Global Iter: 1459900 training acc: 0.25
Global Iter: 1460000 training loss: 1.95223
Global Iter: 1460000 training acc: 0.25
Global Iter: 1460100 training loss: 1.95802
Global Iter: 1460100 training acc: 0.21875
Global Iter: 1460200 training loss: 2.04216
Global Iter: 1460200 training acc: 0.0625
Global Iter: 1460300 training loss: 1.91134
Global Iter: 1460300 training acc: 0.21875
Global Iter: 1460400 training loss: 1.98044
Global Iter: 1460400 training acc: 0.2017-06-22 22:42:03.554318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1460405
1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1460405
Number of Patches: 73825
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1460405
Global Iter: 1460500 training loss: 1.95821
Global Iter: 1460500 training acc: 0.125
Global Iter: 1460600 training loss: 1.95197
Global Iter: 1460600 training acc: 0.125
Global Iter: 1460700 training loss: 2.03369
Global Iter: 1460700 training acc: 0.25
Global Iter: 1460800 training loss: 2.02411
Global Iter: 1460800 training acc: 0.15625
Global Iter: 1460900 training loss: 2.05171
Global Iter: 1460900 training acc: 0.09375
Global Iter: 1461000 training loss: 1.99965
Global Iter: 1461000 training acc: 0.1875
Global Iter: 1461100 training loss: 2.06513
Global Iter: 1461100 training acc: 0.125
Global Iter: 1461200 training loss: 2.0011
Global Iter: 1461200 training acc: 0.09375
Global Iter: 1461300 training loss: 2.10391
Global Iter: 1461300 training acc: 0.25
Global Iter: 1461400 training loss: 2.05236
Global Iter: 1461400 training acc: 0.125
Global Iter: 1461500 training loss: 2.04045
Global Iter: 1461500 training acc: 0.125
Global Iter: 1461600 training loss: 2.0337
Global Iter: 1461600 training acc: 0.09375
Global Iter: 1461700 training loss: 1.99141
Global Iter: 1461700 training acc: 0.09375
Global Iter: 1461800 training loss: 2.07453
Global Iter: 1461800 training acc: 0.09375
Global Iter: 1461900 training loss: 2.05333
Global Iter: 1461900 training acc: 0.15625
Global Iter: 1462000 training loss: 1.95826
Global Iter: 1462000 training acc: 0.28125
Global Iter: 1462100 training loss: 1.93902
Global Iter: 1462100 training acc: 0.09375
Global Iter: 1462200 training loss: 1.98367
Global Iter: 1462200 training acc: 0.21875
Global Iter: 1462300 training loss: 2.05173
Global Iter: 1462300 training acc: 0.1875
Global Iter: 1462400 training loss: 1.92677
Global Iter: 1462400 training acc: 0.1875
Global Iter: 1462500 training loss: 1.95526
Global Iter: 1462500 training acc: 0.28125
Global Iter: 1462600 training loss: 2.09936
Global Iter: 1462600 training acc: 0.25
Global Iter: 1462700 training loss: 1.99468
Global Iter: 1462700 training acc: 0.21875
Global Iter: 1462800 training loss: 1.96753
Global Iter: 1462800 training acc: 0.21875
Global Iter: 1462900 training loss: 2.05446
Global Iter: 1462900 training acc: 0.15625
Global Iter: 1463000 training loss: 2.08479
Global Iter: 1463000 training acc: 0.09375
Global Iter: 1463100 training loss: 1.95687
Global Iter: 1463100 training acc: 0.125
Global Iter: 1463200 training loss: 1.96777
Global Iter: 1463200 training acc: 0.15625
Global Iter: 1463300 training loss: 1.94766
Global Iter: 1463300 training acc: 0.25
Global Iter: 1463400 training loss: 2.01945
Global Iter: 1463400 training acc: 0.15625
Global Iter: 1463500 training loss: 1.969
Global Iter: 1463500 training acc: 0.15625
Global Iter: 1463600 training loss: 1.99868
Global Iter: 1463600 training acc: 0.1875
Global Iter: 1463700 training loss: 2.03939
Global Iter: 1463700 training acc: 0.21875
Global Iter: 1463800 training loss: 1.97474
Global Iter: 1463800 training acc: 0.15625
Global Iter: 1463900 training loss: 1.9721
Global Iter: 1463900 training acc: 0.15625
Global Iter: 1464000 training loss: 1.97448
Global Iter: 1464000 training acc: 0.09375
Global Iter: 1464100 training loss: 2.00935
Global Iter: 1464100 training acc: 0.25
Global Iter: 1464200 training loss: 2.02818
Global Iter: 1464200 training acc: 0.1875
Global Iter: 1464300 training loss: 2.06413
Global Iter: 1464300 training acc: 0.15625
Global Iter: 1464400 training loss: 2.00958
Global Iter: 1464400 training acc: 0.21875
Global Iter: 1464500 training loss: 2.02252
Global Iter: 1464500 training acc: 0.09375
Global Iter: 1464600 training loss: 1.97147
Global Iter: 1464600 training acc: 0.15625
Global Iter: 1464700 training loss: 2.02617
Global Iter: 1464700 training acc: 0.125
Global Iter: 1464800 training loss: 2.19546
Global Iter: 1464800 training acc: 0.125
Global Iter: 1464900 training loss: 1.97962
Global Iter: 1464900 training acc: 02017-06-22 22:49:53.225815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1465020
.1875
Global Iter: 1465000 training loss: 2.08288
Global Iter: 1465000 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1465020
Number of Patches: 73087
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1465020
Global Iter: 1465100 training loss: 1.94789
Global Iter: 1465100 training acc: 0.15625
Global Iter: 1465200 training loss: 1.99011
Global Iter: 1465200 training acc: 0.09375
Global Iter: 1465300 training loss: 2.03142
Global Iter: 1465300 training acc: 0.25
Global Iter: 1465400 training loss: 1.89764
Global Iter: 1465400 training acc: 0.15625
Global Iter: 1465500 training loss: 1.99982
Global Iter: 1465500 training acc: 0.125
Global Iter: 1465600 training loss: 1.93425
Global Iter: 1465600 training acc: 0.15625
Global Iter: 1465700 training loss: 1.93379
Global Iter: 1465700 training acc: 0.1875
Global Iter: 1465800 training loss: 1.97863
Global Iter: 1465800 training acc: 0.09375
Global Iter: 1465900 training loss: 2.07074
Global Iter: 1465900 training acc: 0.09375
Global Iter: 1466000 training loss: 1.97741
Global Iter: 1466000 training acc: 0.09375
Global Iter: 1466100 training loss: 2.13101
Global Iter: 1466100 training acc: 0.0
Global Iter: 1466200 training loss: 1.91811
Global Iter: 1466200 training acc: 0.25
Global Iter: 1466300 training loss: 1.92441
Global Iter: 1466300 training acc: 0.15625
Global Iter: 1466400 training loss: 2.05073
Global Iter: 1466400 training acc: 0.21875
Global Iter: 1466500 training loss: 2.01427
Global Iter: 1466500 training acc: 0.125
Global Iter: 1466600 training loss: 2.04313
Global Iter: 1466600 training acc: 0.0625
Global Iter: 1466700 training loss: 2.01726
Global Iter: 1466700 training acc: 0.1875
Global Iter: 1466800 training loss: 1.90366
Global Iter: 1466800 training acc: 0.28125
Global Iter: 1466900 training loss: 2.02392
Global Iter: 1466900 training acc: 0.125
Global Iter: 1467000 training loss: 1.99335
Global Iter: 1467000 training acc: 0.09375
Global Iter: 1467100 training loss: 1.96064
Global Iter: 1467100 training acc: 0.125
Global Iter: 1467200 training loss: 2.01729
Global Iter: 1467200 training acc: 0.21875
Global Iter: 1467300 training loss: 1.9439
Global Iter: 1467300 training acc: 0.15625
Global Iter: 1467400 training loss: 2.01307
Global Iter: 1467400 training acc: 0.09375
Global Iter: 1467500 training loss: 2.02048
Global Iter: 1467500 training acc: 0.09375
Global Iter: 1467600 training loss: 2.02471
Global Iter: 1467600 training acc: 0.1875
Global Iter: 1467700 training loss: 1.93717
Global Iter: 1467700 training acc: 0.3125
Global Iter: 1467800 training loss: 2.1005
Global Iter: 1467800 training acc: 0.15625
Global Iter: 1467900 training loss: 1.95945
Global Iter: 1467900 training acc: 0.15625
Global Iter: 1468000 training loss: 2.02164
Global Iter: 1468000 training acc: 0.1875
Global Iter: 1468100 training loss: 2.09226
Global Iter: 1468100 training acc: 0.09375
Global Iter: 1468200 training loss: 2.08793
Global Iter: 1468200 training acc: 0.28125
Global Iter: 1468300 training loss: 2.00374
Global Iter: 1468300 training acc: 0.21875
Global Iter: 1468400 training loss: 1.98422
Global Iter: 1468400 training acc: 0.1875
Global Iter: 1468500 training loss: 1.96721
Global Iter: 1468500 training acc: 0.25
Global Iter: 1468600 training loss: 2.01054
Global Iter: 1468600 training acc: 0.1875
Global Iter: 1468700 training loss: 2.04621
Global Iter: 1468700 training acc: 0.125
Global Iter: 1468800 training loss: 1.96204
Global Iter: 1468800 training acc: 0.1875
Global Iter: 1468900 training loss: 2.0275
Global Iter: 1468900 training acc: 0.21875
Global Iter: 1469000 training loss: 2.13012
Global Iter: 1469000 training acc: 0.0625
Global Iter: 1469100 training loss: 1.9301
Global Iter: 1469100 training acc: 0.21875
Global Iter: 1469200 training loss: 1.9763
Global Iter: 1469200 training acc: 0.21875
Global Iter: 1469300 training loss: 1.99057
Global Iter: 1469300 training acc: 0.15625
Global Iter: 1469400 training loss: 2.00493
Global Iter: 1469400 training ac2017-06-22 22:57:29.671836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1469588
c: 0.15625
Global Iter: 1469500 training loss: 2.02986
Global Iter: 1469500 training acc: 0.03125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1469588
Number of Patches: 72357
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1469588
Global Iter: 1469600 training loss: 1.963
Global Iter: 1469600 training acc: 0.09375
Global Iter: 1469700 training loss: 2.03354
Global Iter: 1469700 training acc: 0.15625
Global Iter: 1469800 training loss: 1.97988
Global Iter: 1469800 training acc: 0.25
Global Iter: 1469900 training loss: 2.03499
Global Iter: 1469900 training acc: 0.1875
Global Iter: 1470000 training loss: 1.95248
Global Iter: 1470000 training acc: 0.125
Global Iter: 1470100 training loss: 2.01722
Global Iter: 1470100 training acc: 0.21875
Global Iter: 1470200 training loss: 2.04714
Global Iter: 1470200 training acc: 0.15625
Global Iter: 1470300 training loss: 2.01688
Global Iter: 1470300 training acc: 0.15625
Global Iter: 1470400 training loss: 2.00149
Global Iter: 1470400 training acc: 0.25
Global Iter: 1470500 training loss: 1.95864
Global Iter: 1470500 training acc: 0.125
Global Iter: 1470600 training loss: 1.98921
Global Iter: 1470600 training acc: 0.1875
Global Iter: 1470700 training loss: 2.00102
Global Iter: 1470700 training acc: 0.125
Global Iter: 1470800 training loss: 2.20333
Global Iter: 1470800 training acc: 0.21875
Global Iter: 1470900 training loss: 2.04494
Global Iter: 1470900 training acc: 0.125
Global Iter: 1471000 training loss: 1.98563
Global Iter: 1471000 training acc: 0.3125
Global Iter: 1471100 training loss: 1.94043
Global Iter: 1471100 training acc: 0.25
Global Iter: 1471200 training loss: 2.00738
Global Iter: 1471200 training acc: 0.1875
Global Iter: 1471300 training loss: 1.92304
Global Iter: 1471300 training acc: 0.15625
Global Iter: 1471400 training loss: 1.98789
Global Iter: 1471400 training acc: 0.125
Global Iter: 1471500 training loss: 2.04662
Global Iter: 1471500 training acc: 0.1875
Global Iter: 1471600 training loss: 1.91637
Global Iter: 1471600 training acc: 0.125
Global Iter: 1471700 training loss: 2.13654
Global Iter: 1471700 training acc: 0.125
Global Iter: 1471800 training loss: 2.02598
Global Iter: 1471800 training acc: 0.1875
Global Iter: 1471900 training loss: 1.9922
Global Iter: 1471900 training acc: 0.1875
Global Iter: 1472000 training loss: 2.00127
Global Iter: 1472000 training acc: 0.125
Global Iter: 1472100 training loss: 1.97901
Global Iter: 1472100 training acc: 0.125
Global Iter: 1472200 training loss: 2.0328
Global Iter: 1472200 training acc: 0.09375
Global Iter: 1472300 training loss: 1.99574
Global Iter: 1472300 training acc: 0.21875
Global Iter: 1472400 training loss: 2.07177
Global Iter: 1472400 training acc: 0.15625
Global Iter: 1472500 training loss: 2.06048
Global Iter: 1472500 training acc: 0.09375
Global Iter: 1472600 training loss: 1.99885
Global Iter: 1472600 training acc: 0.28125
Global Iter: 1472700 training loss: 1.89732
Global Iter: 1472700 training acc: 0.1875
Global Iter: 1472800 training loss: 1.99505
Global Iter: 1472800 training acc: 0.15625
Global Iter: 1472900 training loss: 1.965
Global Iter: 1472900 training acc: 0.125
Global Iter: 1473000 training loss: 2.10429
Global Iter: 1473000 training acc: 0.125
Global Iter: 1473100 training loss: 1.96595
Global Iter: 1473100 training acc: 0.21875
Global Iter: 1473200 training loss: 2.03872
Global Iter: 1473200 training acc: 0.1875
Global Iter: 1473300 training loss: 2.04353
Global Iter: 1473300 training acc: 0.09375
Global Iter: 1473400 training loss: 2.03155
Global Iter: 1473400 training acc: 0.21875
Global Iter: 1473500 training loss: 1.96752
Global Iter: 1473500 training acc: 0.1875
Global Iter: 1473600 training loss: 1.98733
Global Iter: 1473600 training acc: 0.21875
Global Iter: 1473700 training loss: 1.95561
Global Iter: 1473700 training acc: 0.15625
Global Iter: 1473800 training loss: 1.9965
Global Iter: 1473800 training acc: 0.21875
Global Iter: 1473900 training loss: 2.03427
Global Iter: 1473900 training acc: 0.2017-06-22 23:05:07.840896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1474111
0
Global Iter: 1474000 training loss: 2.17548
Global Iter: 1474000 training acc: 0.09375
Global Iter: 1474100 training loss: 1.96722
Global Iter: 1474100 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1474111
Number of Patches: 71634
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1474111
Global Iter: 1474200 training loss: 2.02312
Global Iter: 1474200 training acc: 0.09375
Global Iter: 1474300 training loss: 2.01013
Global Iter: 1474300 training acc: 0.09375
Global Iter: 1474400 training loss: 2.01346
Global Iter: 1474400 training acc: 0.21875
Global Iter: 1474500 training loss: 1.93032
Global Iter: 1474500 training acc: 0.21875
Global Iter: 1474600 training loss: 2.02214
Global Iter: 1474600 training acc: 0.15625
Global Iter: 1474700 training loss: 1.93481
Global Iter: 1474700 training acc: 0.25
Global Iter: 1474800 training loss: 2.04753
Global Iter: 1474800 training acc: 0.28125
Global Iter: 1474900 training loss: 1.9097
Global Iter: 1474900 training acc: 0.25
Global Iter: 1475000 training loss: 1.9211
Global Iter: 1475000 training acc: 0.25
Global Iter: 1475100 training loss: 2.00076
Global Iter: 1475100 training acc: 0.125
Global Iter: 1475200 training loss: 1.93895
Global Iter: 1475200 training acc: 0.28125
Global Iter: 1475300 training loss: 1.99048
Global Iter: 1475300 training acc: 0.21875
Global Iter: 1475400 training loss: 1.92427
Global Iter: 1475400 training acc: 0.09375
Global Iter: 1475500 training loss: 1.9444
Global Iter: 1475500 training acc: 0.3125
Global Iter: 1475600 training loss: 1.92754
Global Iter: 1475600 training acc: 0.25
Global Iter: 1475700 training loss: 2.04952
Global Iter: 1475700 training acc: 0.21875
Global Iter: 1475800 training loss: 2.01181
Global Iter: 1475800 training acc: 0.125
Global Iter: 1475900 training loss: 2.07578
Global Iter: 1475900 training acc: 0.15625
Global Iter: 1476000 training loss: 2.02634
Global Iter: 1476000 training acc: 0.21875
Global Iter: 1476100 training loss: 2.03535
Global Iter: 1476100 training acc: 0.15625
Global Iter: 1476200 training loss: 1.95079
Global Iter: 1476200 training acc: 0.15625
Global Iter: 1476300 training loss: 2.16364
Global Iter: 1476300 training acc: 0.0625
Global Iter: 1476400 training loss: 1.98112
Global Iter: 1476400 training acc: 0.15625
Global Iter: 1476500 training loss: 2.01132
Global Iter: 1476500 training acc: 0.1875
Global Iter: 1476600 training loss: 1.92009
Global Iter: 1476600 training acc: 0.125
Global Iter: 1476700 training loss: 2.01165
Global Iter: 1476700 training acc: 0.1875
Global Iter: 1476800 training loss: 1.976
Global Iter: 1476800 training acc: 0.15625
Global Iter: 1476900 training loss: 2.04459
Global Iter: 1476900 training acc: 0.1875
Global Iter: 1477000 training loss: 2.02278
Global Iter: 1477000 training acc: 0.125
Global Iter: 1477100 training loss: 1.9112
Global Iter: 1477100 training acc: 0.15625
Global Iter: 1477200 training loss: 1.90617
Global Iter: 1477200 training acc: 0.125
Global Iter: 1477300 training loss: 1.9686
Global Iter: 1477300 training acc: 0.09375
Global Iter: 1477400 training loss: 2.02154
Global Iter: 1477400 training acc: 0.21875
Global Iter: 1477500 training loss: 2.03859
Global Iter: 1477500 training acc: 0.0625
Global Iter: 1477600 training loss: 1.98417
Global Iter: 1477600 training acc: 0.1875
Global Iter: 1477700 training loss: 2.0666
Global Iter: 1477700 training acc: 0.09375
Global Iter: 1477800 training loss: 2.00506
Global Iter: 1477800 training acc: 0.15625
Global Iter: 1477900 training loss: 2.06966
Global Iter: 1477900 training acc: 0.1875
Global Iter: 1478000 training loss: 1.93939
Global Iter: 1478000 training acc: 0.125
Global Iter: 1478100 training loss: 2.05114
Global Iter: 1478100 training acc: 0.125
Global Iter: 1478200 training loss: 2.0297
Global Iter: 1478200 training acc: 0.1875
Global Iter: 1478300 training loss: 1.98594
Global Iter: 1478300 training acc: 0.21875
Global Iter: 1478400 training loss: 1.91038
Global Iter: 1478400 training acc: 0.3125
Glo2017-06-22 23:12:36.855975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1478589
bal Iter: 1478500 training loss: 1.99151
Global Iter: 1478500 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1478589
Number of Patches: 70918
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1478589
Global Iter: 1478600 training loss: 1.88528
Global Iter: 1478600 training acc: 0.15625
Global Iter: 1478700 training loss: 1.95074
Global Iter: 1478700 training acc: 0.15625
Global Iter: 1478800 training loss: 1.97703
Global Iter: 1478800 training acc: 0.15625
Global Iter: 1478900 training loss: 1.96176
Global Iter: 1478900 training acc: 0.25
Global Iter: 1479000 training loss: 1.99037
Global Iter: 1479000 training acc: 0.125
Global Iter: 1479100 training loss: 1.89822
Global Iter: 1479100 training acc: 0.09375
Global Iter: 1479200 training loss: 1.99772
Global Iter: 1479200 training acc: 0.15625
Global Iter: 1479300 training loss: 1.96272
Global Iter: 1479300 training acc: 0.34375
Global Iter: 1479400 training loss: 1.95783
Global Iter: 1479400 training acc: 0.25
Global Iter: 1479500 training loss: 1.92228
Global Iter: 1479500 training acc: 0.28125
Global Iter: 1479600 training loss: 1.97569
Global Iter: 1479600 training acc: 0.15625
Global Iter: 1479700 training loss: 1.89681
Global Iter: 1479700 training acc: 0.15625
Global Iter: 1479800 training loss: 1.93253
Global Iter: 1479800 training acc: 0.125
Global Iter: 1479900 training loss: 2.04984
Global Iter: 1479900 training acc: 0.09375
Global Iter: 1480000 training loss: 2.04408
Global Iter: 1480000 training acc: 0.1875
Global Iter: 1480100 training loss: 2.04064
Global Iter: 1480100 training acc: 0.125
Global Iter: 1480200 training loss: 2.01569
Global Iter: 1480200 training acc: 0.09375
Global Iter: 1480300 training loss: 2.03722
Global Iter: 1480300 training acc: 0.09375
Global Iter: 1480400 training loss: 1.98149
Global Iter: 1480400 training acc: 0.25
Global Iter: 1480500 training loss: 1.94419
Global Iter: 1480500 training acc: 0.15625
Global Iter: 1480600 training loss: 1.95747
Global Iter: 1480600 training acc: 0.15625
Global Iter: 1480700 training loss: 1.98287
Global Iter: 1480700 training acc: 0.15625
Global Iter: 1480800 training loss: 1.99943
Global Iter: 1480800 training acc: 0.25
Global Iter: 1480900 training loss: 1.96536
Global Iter: 1480900 training acc: 0.25
Global Iter: 1481000 training loss: 1.95287
Global Iter: 1481000 training acc: 0.21875
Global Iter: 1481100 training loss: 1.95954
Global Iter: 1481100 training acc: 0.1875
Global Iter: 1481200 training loss: 2.12228
Global Iter: 1481200 training acc: 0.15625
Global Iter: 1481300 training loss: 1.94332
Global Iter: 1481300 training acc: 0.15625
Global Iter: 1481400 training loss: 1.93602
Global Iter: 1481400 training acc: 0.15625
Global Iter: 1481500 training loss: 2.0781
Global Iter: 1481500 training acc: 0.15625
Global Iter: 1481600 training loss: 1.97012
Global Iter: 1481600 training acc: 0.21875
Global Iter: 1481700 training loss: 1.99477
Global Iter: 1481700 training acc: 0.1875
Global Iter: 1481800 training loss: 1.96155
Global Iter: 1481800 training acc: 0.15625
Global Iter: 1481900 training loss: 1.98258
Global Iter: 1481900 training acc: 0.21875
Global Iter: 1482000 training loss: 1.96467
Global Iter: 1482000 training acc: 0.28125
Global Iter: 1482100 training loss: 2.02071
Global Iter: 1482100 training acc: 0.25
Global Iter: 1482200 training loss: 1.91827
Global Iter: 1482200 training acc: 0.1875
Global Iter: 1482300 training loss: 2.01833
Global Iter: 1482300 training acc: 0.1875
Global Iter: 1482400 training loss: 1.91045
Global Iter: 1482400 training acc: 0.09375
Global Iter: 1482500 training loss: 1.99429
Global Iter: 1482500 training acc: 0.3125
Global Iter: 1482600 training loss: 2.0178
Global Iter: 1482600 training acc: 0.25
Global Iter: 1482700 training loss: 2.01019
Global Iter: 1482700 training acc: 0.21875
Global Iter: 1482800 training loss: 1.98953
Global Iter: 1482800 training acc: 0.1875
Global Iter: 1482900 training loss: 2.00509
Global Iter: 1482900 training acc: 0.062017-06-22 23:20:09.525132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1483022
25
Global Iter: 1483000 training loss: 1.88685
Global Iter: 1483000 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1483022
Number of Patches: 70209
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1483022
Global Iter: 1483100 training loss: 1.98364
Global Iter: 1483100 training acc: 0.25
Global Iter: 1483200 training loss: 1.88882
Global Iter: 1483200 training acc: 0.1875
Global Iter: 1483300 training loss: 2.01602
Global Iter: 1483300 training acc: 0.09375
Global Iter: 1483400 training loss: 1.96079
Global Iter: 1483400 training acc: 0.21875
Global Iter: 1483500 training loss: 1.99887
Global Iter: 1483500 training acc: 0.1875
Global Iter: 1483600 training loss: 2.00253
Global Iter: 1483600 training acc: 0.09375
Global Iter: 1483700 training loss: 2.03351
Global Iter: 1483700 training acc: 0.0625
Global Iter: 1483800 training loss: 2.01541
Global Iter: 1483800 training acc: 0.25
Global Iter: 1483900 training loss: 2.00658
Global Iter: 1483900 training acc: 0.15625
Global Iter: 1484000 training loss: 1.96572
Global Iter: 1484000 training acc: 0.21875
Global Iter: 1484100 training loss: 1.95255
Global Iter: 1484100 training acc: 0.1875
Global Iter: 1484200 training loss: 2.04796
Global Iter: 1484200 training acc: 0.03125
Global Iter: 1484300 training loss: 1.96207
Global Iter: 1484300 training acc: 0.15625
Global Iter: 1484400 training loss: 1.99337
Global Iter: 1484400 training acc: 0.125
Global Iter: 1484500 training loss: 1.93008
Global Iter: 1484500 training acc: 0.40625
Global Iter: 1484600 training loss: 1.96783
Global Iter: 1484600 training acc: 0.15625
Global Iter: 1484700 training loss: 1.91921
Global Iter: 1484700 training acc: 0.15625
Global Iter: 1484800 training loss: 1.98
Global Iter: 1484800 training acc: 0.1875
Global Iter: 1484900 training loss: 2.12372
Global Iter: 1484900 training acc: 0.0625
Global Iter: 1485000 training loss: 2.01737
Global Iter: 1485000 training acc: 0.15625
Global Iter: 1485100 training loss: 1.98795
Global Iter: 1485100 training acc: 0.09375
Global Iter: 1485200 training loss: 1.8981
Global Iter: 1485200 training acc: 0.25
Global Iter: 1485300 training loss: 1.99361
Global Iter: 1485300 training acc: 0.15625
Global Iter: 1485400 training loss: 1.97761
Global Iter: 1485400 training acc: 0.125
Global Iter: 1485500 training loss: 1.95497
Global Iter: 1485500 training acc: 0.25
Global Iter: 1485600 training loss: 1.99598
Global Iter: 1485600 training acc: 0.09375
Global Iter: 1485700 training loss: 2.03733
Global Iter: 1485700 training acc: 0.09375
Global Iter: 1485800 training loss: 2.11808
Global Iter: 1485800 training acc: 0.1875
Global Iter: 1485900 training loss: 1.94204
Global Iter: 1485900 training acc: 0.21875
Global Iter: 1486000 training loss: 2.08382
Global Iter: 1486000 training acc: 0.1875
Global Iter: 1486100 training loss: 1.96166
Global Iter: 1486100 training acc: 0.28125
Global Iter: 1486200 training loss: 1.981
Global Iter: 1486200 training acc: 0.1875
Global Iter: 1486300 training loss: 2.00668
Global Iter: 1486300 training acc: 0.09375
Global Iter: 1486400 training loss: 1.8897
Global Iter: 1486400 training acc: 0.28125
Global Iter: 1486500 training loss: 1.97873
Global Iter: 1486500 training acc: 0.15625
Global Iter: 1486600 training loss: 2.03534
Global Iter: 1486600 training acc: 0.09375
Global Iter: 1486700 training loss: 2.044
Global Iter: 1486700 training acc: 0.15625
Global Iter: 1486800 training loss: 2.00301
Global Iter: 1486800 training acc: 0.125
Global Iter: 1486900 training loss: 2.012
Global Iter: 1486900 training acc: 0.25
Global Iter: 1487000 training loss: 1.95149
Global Iter: 1487000 training acc: 0.1875
Global Iter: 1487100 training loss: 1.96827
Global Iter: 1487100 training acc: 0.09375
Global Iter: 1487200 training loss: 1.93904
Global Iter: 1487200 training acc: 0.21875
Global Iter: 1487300 training loss: 1.9472
Global Iter: 1487300 training acc: 0.21875
Global Iter: 1487400 training loss: 1.96404
Global Iter: 1487400 training acc: 0.1562017-06-22 23:27:30.474690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1487411
2017-06-22 23:34:50.956251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1487411
Number of Patches: 69507
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1487411
Global Iter: 1487500 training loss: 2.00143
Global Iter: 1487500 training acc: 0.1875
Global Iter: 1487600 training loss: 1.93289
Global Iter: 1487600 training acc: 0.125
Global Iter: 1487700 training loss: 2.01627
Global Iter: 1487700 training acc: 0.09375
Global Iter: 1487800 training loss: 1.94478
Global Iter: 1487800 training acc: 0.28125
Global Iter: 1487900 training loss: 1.93497
Global Iter: 1487900 training acc: 0.28125
Global Iter: 1488000 training loss: 1.91891
Global Iter: 1488000 training acc: 0.125
Global Iter: 1488100 training loss: 2.03412
Global Iter: 1488100 training acc: 0.21875
Global Iter: 1488200 training loss: 1.96341
Global Iter: 1488200 training acc: 0.28125
Global Iter: 1488300 training loss: 1.93479
Global Iter: 1488300 training acc: 0.28125
Global Iter: 1488400 training loss: 2.02994
Global Iter: 1488400 training acc: 0.15625
Global Iter: 1488500 training loss: 2.0822
Global Iter: 1488500 training acc: 0.1875
Global Iter: 1488600 training loss: 1.98465
Global Iter: 1488600 training acc: 0.25
Global Iter: 1488700 training loss: 2.01055
Global Iter: 1488700 training acc: 0.125
Global Iter: 1488800 training loss: 1.92892
Global Iter: 1488800 training acc: 0.21875
Global Iter: 1488900 training loss: 2.06834
Global Iter: 1488900 training acc: 0.1875
Global Iter: 1489000 training loss: 1.99375
Global Iter: 1489000 training acc: 0.09375
Global Iter: 1489100 training loss: 2.2036
Global Iter: 1489100 training acc: 0.21875
Global Iter: 1489200 training loss: 2.05162
Global Iter: 1489200 training acc: 0.125
Global Iter: 1489300 training loss: 1.92147
Global Iter: 1489300 training acc: 0.1875
Global Iter: 1489400 training loss: 1.91379
Global Iter: 1489400 training acc: 0.25
Global Iter: 1489500 training loss: 1.98736
Global Iter: 1489500 training acc: 0.1875
Global Iter: 1489600 training loss: 2.03438
Global Iter: 1489600 training acc: 0.125
Global Iter: 1489700 training loss: 1.97431
Global Iter: 1489700 training acc: 0.15625
Global Iter: 1489800 training loss: 1.94621
Global Iter: 1489800 training acc: 0.3125
Global Iter: 1489900 training loss: 1.99266
Global Iter: 1489900 training acc: 0.15625
Global Iter: 1490000 training loss: 1.87108
Global Iter: 1490000 training acc: 0.28125
Global Iter: 1490100 training loss: 2.02015
Global Iter: 1490100 training acc: 0.0625
Global Iter: 1490200 training loss: 2.00932
Global Iter: 1490200 training acc: 0.1875
Global Iter: 1490300 training loss: 1.99494
Global Iter: 1490300 training acc: 0.15625
Global Iter: 1490400 training loss: 2.00364
Global Iter: 1490400 training acc: 0.1875
Global Iter: 1490500 training loss: 2.03709
Global Iter: 1490500 training acc: 0.125
Global Iter: 1490600 training loss: 1.92741
Global Iter: 1490600 training acc: 0.21875
Global Iter: 1490700 training loss: 1.99071
Global Iter: 1490700 training acc: 0.21875
Global Iter: 1490800 training loss: 1.91897
Global Iter: 1490800 training acc: 0.1875
Global Iter: 1490900 training loss: 1.997
Global Iter: 1490900 training acc: 0.21875
Global Iter: 1491000 training loss: 2.10029
Global Iter: 1491000 training acc: 0.0625
Global Iter: 1491100 training loss: 2.02474
Global Iter: 1491100 training acc: 0.09375
Global Iter: 1491200 training loss: 2.05014
Global Iter: 1491200 training acc: 0.25
Global Iter: 1491300 training loss: 1.94913
Global Iter: 1491300 training acc: 0.1875
Global Iter: 1491400 training loss: 1.93054
Global Iter: 1491400 training acc: 0.125
Global Iter: 1491500 training loss: 1.98461
Global Iter: 1491500 training acc: 0.09375
Global Iter: 1491600 training loss: 2.01542
Global Iter: 1491600 training acc: 0.1875
Global Iter: 1491700 training loss: 1.99225
Global Iter: 1491700 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1491756
Number of Patches: 68812
checkpoint found: /home/ahmet/workspace/tenINFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1491756
2017-06-22 23:42:01.323725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1496057
sorboard/tissue_alexnet_b256_lr0005/model.ckpt-1491756
Global Iter: 1491800 training loss: 2.05747
Global Iter: 1491800 training acc: 0.125
Global Iter: 1491900 training loss: 1.96467
Global Iter: 1491900 training acc: 0.1875
Global Iter: 1492000 training loss: 1.9066
Global Iter: 1492000 training acc: 0.34375
Global Iter: 1492100 training loss: 2.08873
Global Iter: 1492100 training acc: 0.25
Global Iter: 1492200 training loss: 1.98122
Global Iter: 1492200 training acc: 0.1875
Global Iter: 1492300 training loss: 1.97552
Global Iter: 1492300 training acc: 0.1875
Global Iter: 1492400 training loss: 2.02928
Global Iter: 1492400 training acc: 0.0625
Global Iter: 1492500 training loss: 2.02989
Global Iter: 1492500 training acc: 0.21875
Global Iter: 1492600 training loss: 1.95108
Global Iter: 1492600 training acc: 0.1875
Global Iter: 1492700 training loss: 1.99342
Global Iter: 1492700 training acc: 0.1875
Global Iter: 1492800 training loss: 2.04091
Global Iter: 1492800 training acc: 0.09375
Global Iter: 1492900 training loss: 1.98952
Global Iter: 1492900 training acc: 0.28125
Global Iter: 1493000 training loss: 1.98914
Global Iter: 1493000 training acc: 0.21875
Global Iter: 1493100 training loss: 2.01767
Global Iter: 1493100 training acc: 0.125
Global Iter: 1493200 training loss: 1.9263
Global Iter: 1493200 training acc: 0.125
Global Iter: 1493300 training loss: 1.9624
Global Iter: 1493300 training acc: 0.15625
Global Iter: 1493400 training loss: 1.92192
Global Iter: 1493400 training acc: 0.15625
Global Iter: 1493500 training loss: 1.94203
Global Iter: 1493500 training acc: 0.28125
Global Iter: 1493600 training loss: 2.0111
Global Iter: 1493600 training acc: 0.25
Global Iter: 1493700 training loss: 2.00143
Global Iter: 1493700 training acc: 0.125
Global Iter: 1493800 training loss: 2.0235
Global Iter: 1493800 training acc: 0.25
Global Iter: 1493900 training loss: 2.07282
Global Iter: 1493900 training acc: 0.0625
Global Iter: 1494000 training loss: 1.99
Global Iter: 1494000 training acc: 0.15625
Global Iter: 1494100 training loss: 1.97818
Global Iter: 1494100 training acc: 0.125
Global Iter: 1494200 training loss: 1.96703
Global Iter: 1494200 training acc: 0.125
Global Iter: 1494300 training loss: 1.95835
Global Iter: 1494300 training acc: 0.15625
Global Iter: 1494400 training loss: 2.01575
Global Iter: 1494400 training acc: 0.15625
Global Iter: 1494500 training loss: 1.94339
Global Iter: 1494500 training acc: 0.125
Global Iter: 1494600 training loss: 2.11124
Global Iter: 1494600 training acc: 0.21875
Global Iter: 1494700 training loss: 2.06525
Global Iter: 1494700 training acc: 0.21875
Global Iter: 1494800 training loss: 1.93816
Global Iter: 1494800 training acc: 0.125
Global Iter: 1494900 training loss: 2.12285
Global Iter: 1494900 training acc: 0.15625
Global Iter: 1495000 training loss: 1.90635
Global Iter: 1495000 training acc: 0.125
Global Iter: 1495100 training loss: 1.96931
Global Iter: 1495100 training acc: 0.15625
Global Iter: 1495200 training loss: 1.91092
Global Iter: 1495200 training acc: 0.21875
Global Iter: 1495300 training loss: 2.02291
Global Iter: 1495300 training acc: 0.1875
Global Iter: 1495400 training loss: 2.03465
Global Iter: 1495400 training acc: 0.15625
Global Iter: 1495500 training loss: 1.96238
Global Iter: 1495500 training acc: 0.1875
Global Iter: 1495600 training loss: 1.9431
Global Iter: 1495600 training acc: 0.28125
Global Iter: 1495700 training loss: 1.98081
Global Iter: 1495700 training acc: 0.25
Global Iter: 1495800 training loss: 2.06501
Global Iter: 1495800 training acc: 0.125
Global Iter: 1495900 training loss: 1.97132
Global Iter: 1495900 training acc: 0.21875
Global Iter: 1496000 training loss: 1.96337
Global Iter: 1496000 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1496057
Number of Patches: 68124
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1496057
Global Iter: 1496100 training loss: 2.00502
Global Iter: 1496100 training acc: 0.1875
Global Iter: 1496200 training loss: 1.9882017-06-22 23:49:11.904422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1500315
27
Global Iter: 1496200 training acc: 0.28125
Global Iter: 1496300 training loss: 2.0832
Global Iter: 1496300 training acc: 0.21875
Global Iter: 1496400 training loss: 1.95006
Global Iter: 1496400 training acc: 0.25
Global Iter: 1496500 training loss: 1.98846
Global Iter: 1496500 training acc: 0.21875
Global Iter: 1496600 training loss: 1.9274
Global Iter: 1496600 training acc: 0.21875
Global Iter: 1496700 training loss: 1.94995
Global Iter: 1496700 training acc: 0.25
Global Iter: 1496800 training loss: 1.89752
Global Iter: 1496800 training acc: 0.25
Global Iter: 1496900 training loss: 1.96674
Global Iter: 1496900 training acc: 0.1875
Global Iter: 1497000 training loss: 1.95638
Global Iter: 1497000 training acc: 0.21875
Global Iter: 1497100 training loss: 1.99221
Global Iter: 1497100 training acc: 0.125
Global Iter: 1497200 training loss: 1.94813
Global Iter: 1497200 training acc: 0.1875
Global Iter: 1497300 training loss: 1.98049
Global Iter: 1497300 training acc: 0.25
Global Iter: 1497400 training loss: 2.02024
Global Iter: 1497400 training acc: 0.03125
Global Iter: 1497500 training loss: 2.12214
Global Iter: 1497500 training acc: 0.15625
Global Iter: 1497600 training loss: 1.93412
Global Iter: 1497600 training acc: 0.21875
Global Iter: 1497700 training loss: 1.90979
Global Iter: 1497700 training acc: 0.15625
Global Iter: 1497800 training loss: 1.99059
Global Iter: 1497800 training acc: 0.21875
Global Iter: 1497900 training loss: 1.95581
Global Iter: 1497900 training acc: 0.15625
Global Iter: 1498000 training loss: 1.95187
Global Iter: 1498000 training acc: 0.125
Global Iter: 1498100 training loss: 1.95475
Global Iter: 1498100 training acc: 0.21875
Global Iter: 1498200 training loss: 2.08108
Global Iter: 1498200 training acc: 0.125
Global Iter: 1498300 training loss: 2.02665
Global Iter: 1498300 training acc: 0.0625
Global Iter: 1498400 training loss: 1.96268
Global Iter: 1498400 training acc: 0.21875
Global Iter: 1498500 training loss: 1.9623
Global Iter: 1498500 training acc: 0.1875
Global Iter: 1498600 training loss: 2.03812
Global Iter: 1498600 training acc: 0.15625
Global Iter: 1498700 training loss: 2.01329
Global Iter: 1498700 training acc: 0.21875
Global Iter: 1498800 training loss: 1.91629
Global Iter: 1498800 training acc: 0.25
Global Iter: 1498900 training loss: 1.88993
Global Iter: 1498900 training acc: 0.21875
Global Iter: 1499000 training loss: 1.91627
Global Iter: 1499000 training acc: 0.25
Global Iter: 1499100 training loss: 2.00719
Global Iter: 1499100 training acc: 0.15625
Global Iter: 1499200 training loss: 1.97586
Global Iter: 1499200 training acc: 0.09375
Global Iter: 1499300 training loss: 2.06288
Global Iter: 1499300 training acc: 0.1875
Global Iter: 1499400 training loss: 1.96686
Global Iter: 1499400 training acc: 0.25
Global Iter: 1499500 training loss: 1.96661
Global Iter: 1499500 training acc: 0.25
Global Iter: 1499600 training loss: 1.98086
Global Iter: 1499600 training acc: 0.25
Global Iter: 1499700 training loss: 2.03576
Global Iter: 1499700 training acc: 0.15625
Global Iter: 1499800 training loss: 1.97884
Global Iter: 1499800 training acc: 0.125
Global Iter: 1499900 training loss: 1.86486
Global Iter: 1499900 training acc: 0.3125
Global Iter: 1500000 training loss: 2.08166
Global Iter: 1500000 training acc: 0.1875
Global Iter: 1500100 training loss: 1.97821
Global Iter: 1500100 training acc: 0.3125
Global Iter: 1500200 training loss: 2.14123
Global Iter: 1500200 training acc: 0.09375
Global Iter: 1500300 training loss: 2.02394
Global Iter: 1500300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1500315
Number of Patches: 67443
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1500315
Global Iter: 1500400 training loss: 2.00213
Global Iter: 1500400 training acc: 0.1875
Global Iter: 1500500 training loss: 2.0254
Global Iter: 1500500 training acc: 0.15625
Global Iter: 1500600 training loss: 1.99841
Global Iter: 1500600 training acc: 0.15625
Global Iter: 1500700 training loss: 1.94442
Gl2017-06-22 23:56:19.547697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1504531
obal Iter: 1500700 training acc: 0.15625
Global Iter: 1500800 training loss: 2.02371
Global Iter: 1500800 training acc: 0.1875
Global Iter: 1500900 training loss: 1.99761
Global Iter: 1500900 training acc: 0.1875
Global Iter: 1501000 training loss: 2.11912
Global Iter: 1501000 training acc: 0.09375
Global Iter: 1501100 training loss: 2.01131
Global Iter: 1501100 training acc: 0.09375
Global Iter: 1501200 training loss: 2.0381
Global Iter: 1501200 training acc: 0.15625
Global Iter: 1501300 training loss: 1.88528
Global Iter: 1501300 training acc: 0.34375
Global Iter: 1501400 training loss: 1.99435
Global Iter: 1501400 training acc: 0.15625
Global Iter: 1501500 training loss: 2.0418
Global Iter: 1501500 training acc: 0.15625
Global Iter: 1501600 training loss: 2.00764
Global Iter: 1501600 training acc: 0.125
Global Iter: 1501700 training loss: 1.9664
Global Iter: 1501700 training acc: 0.1875
Global Iter: 1501800 training loss: 1.94538
Global Iter: 1501800 training acc: 0.125
Global Iter: 1501900 training loss: 1.99559
Global Iter: 1501900 training acc: 0.15625
Global Iter: 1502000 training loss: 1.97786
Global Iter: 1502000 training acc: 0.125
Global Iter: 1502100 training loss: 2.00492
Global Iter: 1502100 training acc: 0.15625
Global Iter: 1502200 training loss: 1.92733
Global Iter: 1502200 training acc: 0.125
Global Iter: 1502300 training loss: 1.97368
Global Iter: 1502300 training acc: 0.21875
Global Iter: 1502400 training loss: 1.95518
Global Iter: 1502400 training acc: 0.28125
Global Iter: 1502500 training loss: 1.98712
Global Iter: 1502500 training acc: 0.25
Global Iter: 1502600 training loss: 1.919
Global Iter: 1502600 training acc: 0.1875
Global Iter: 1502700 training loss: 1.98059
Global Iter: 1502700 training acc: 0.15625
Global Iter: 1502800 training loss: 2.01286
Global Iter: 1502800 training acc: 0.15625
Global Iter: 1502900 training loss: 1.91638
Global Iter: 1502900 training acc: 0.25
Global Iter: 1503000 training loss: 1.96785
Global Iter: 1503000 training acc: 0.1875
Global Iter: 1503100 training loss: 2.12095
Global Iter: 1503100 training acc: 0.03125
Global Iter: 1503200 training loss: 1.93807
Global Iter: 1503200 training acc: 0.28125
Global Iter: 1503300 training loss: 1.99794
Global Iter: 1503300 training acc: 0.28125
Global Iter: 1503400 training loss: 1.98793
Global Iter: 1503400 training acc: 0.1875
Global Iter: 1503500 training loss: 2.09133
Global Iter: 1503500 training acc: 0.09375
Global Iter: 1503600 training loss: 1.94884
Global Iter: 1503600 training acc: 0.125
Global Iter: 1503700 training loss: 2.02338
Global Iter: 1503700 training acc: 0.125
Global Iter: 1503800 training loss: 1.8903
Global Iter: 1503800 training acc: 0.15625
Global Iter: 1503900 training loss: 1.99244
Global Iter: 1503900 training acc: 0.09375
Global Iter: 1504000 training loss: 2.04942
Global Iter: 1504000 training acc: 0.125
Global Iter: 1504100 training loss: 2.08526
Global Iter: 1504100 training acc: 0.0625
Global Iter: 1504200 training loss: 1.93289
Global Iter: 1504200 training acc: 0.25
Global Iter: 1504300 training loss: 1.96686
Global Iter: 1504300 training acc: 0.21875
Global Iter: 1504400 training loss: 1.93251
Global Iter: 1504400 training acc: 0.21875
Global Iter: 1504500 training loss: 2.00998
Global Iter: 1504500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1504531
Number of Patches: 66769
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1504531
Global Iter: 1504600 training loss: 1.95406
Global Iter: 1504600 training acc: 0.21875
Global Iter: 1504700 training loss: 1.94653
Global Iter: 1504700 training acc: 0.21875
Global Iter: 1504800 training loss: 1.92726
Global Iter: 1504800 training acc: 0.25
Global Iter: 1504900 training loss: 2.06962
Global Iter: 1504900 training acc: 0.125
Global Iter: 1505000 training loss: 1.95604
Global Iter: 1505000 training acc: 0.1875
Global Iter: 1505100 training loss: 1.98948
Global Iter: 1505100 training acc: 0.15625
Global Iter: 1505200 training loss: 1.95294
Gl2017-06-23 00:03:25.097104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1508705
obal Iter: 1505200 training acc: 0.15625
Global Iter: 1505300 training loss: 2.02666
Global Iter: 1505300 training acc: 0.0625
Global Iter: 1505400 training loss: 2.04917
Global Iter: 1505400 training acc: 0.21875
Global Iter: 1505500 training loss: 2.00369
Global Iter: 1505500 training acc: 0.21875
Global Iter: 1505600 training loss: 2.11656
Global Iter: 1505600 training acc: 0.0625
Global Iter: 1505700 training loss: 1.89889
Global Iter: 1505700 training acc: 0.15625
Global Iter: 1505800 training loss: 1.95126
Global Iter: 1505800 training acc: 0.375
Global Iter: 1505900 training loss: 2.08232
Global Iter: 1505900 training acc: 0.0625
Global Iter: 1506000 training loss: 2.05897
Global Iter: 1506000 training acc: 0.125
Global Iter: 1506100 training loss: 2.12019
Global Iter: 1506100 training acc: 0.1875
Global Iter: 1506200 training loss: 1.97076
Global Iter: 1506200 training acc: 0.1875
Global Iter: 1506300 training loss: 2.02989
Global Iter: 1506300 training acc: 0.1875
Global Iter: 1506400 training loss: 1.92295
Global Iter: 1506400 training acc: 0.25
Global Iter: 1506500 training loss: 2.00235
Global Iter: 1506500 training acc: 0.28125
Global Iter: 1506600 training loss: 1.92169
Global Iter: 1506600 training acc: 0.15625
Global Iter: 1506700 training loss: 2.0427
Global Iter: 1506700 training acc: 0.15625
Global Iter: 1506800 training loss: 1.91252
Global Iter: 1506800 training acc: 0.21875
Global Iter: 1506900 training loss: 1.99571
Global Iter: 1506900 training acc: 0.21875
Global Iter: 1507000 training loss: 2.02589
Global Iter: 1507000 training acc: 0.25
Global Iter: 1507100 training loss: 1.94395
Global Iter: 1507100 training acc: 0.125
Global Iter: 1507200 training loss: 1.91329
Global Iter: 1507200 training acc: 0.21875
Global Iter: 1507300 training loss: 1.96317
Global Iter: 1507300 training acc: 0.125
Global Iter: 1507400 training loss: 1.99077
Global Iter: 1507400 training acc: 0.1875
Global Iter: 1507500 training loss: 1.92888
Global Iter: 1507500 training acc: 0.28125
Global Iter: 1507600 training loss: 2.00015
Global Iter: 1507600 training acc: 0.25
Global Iter: 1507700 training loss: 2.03566
Global Iter: 1507700 training acc: 0.25
Global Iter: 1507800 training loss: 2.04723
Global Iter: 1507800 training acc: 0.09375
Global Iter: 1507900 training loss: 2.05444
Global Iter: 1507900 training acc: 0.125
Global Iter: 1508000 training loss: 2.05273
Global Iter: 1508000 training acc: 0.21875
Global Iter: 1508100 training loss: 1.98853
Global Iter: 1508100 training acc: 0.1875
Global Iter: 1508200 training loss: 2.05682
Global Iter: 1508200 training acc: 0.15625
Global Iter: 1508300 training loss: 2.05046
Global Iter: 1508300 training acc: 0.25
Global Iter: 1508400 training loss: 2.10132
Global Iter: 1508400 training acc: 0.0625
Global Iter: 1508500 training loss: 1.91823
Global Iter: 1508500 training acc: 0.3125
Global Iter: 1508600 training loss: 1.91604
Global Iter: 1508600 training acc: 0.1875
Global Iter: 1508700 training loss: 2.10439
Global Iter: 1508700 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1508705
Number of Patches: 66102
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1508705
Global Iter: 1508800 training loss: 1.95771
Global Iter: 1508800 training acc: 0.34375
Global Iter: 1508900 training loss: 1.90571
Global Iter: 1508900 training acc: 0.28125
Global Iter: 1509000 training loss: 2.02169
Global Iter: 1509000 training acc: 0.15625
Global Iter: 1509100 training loss: 1.96111
Global Iter: 1509100 training acc: 0.125
Global Iter: 1509200 training loss: 1.87815
Global Iter: 1509200 training acc: 0.15625
Global Iter: 1509300 training loss: 1.8667
Global Iter: 1509300 training acc: 0.3125
Global Iter: 1509400 training loss: 1.96455
Global Iter: 1509400 training acc: 0.15625
Global Iter: 1509500 training loss: 2.07666
Global Iter: 1509500 training acc: 0.15625
Global Iter: 1509600 training loss: 1.96849
Global Iter: 1509600 training acc: 0.15625
Global Iter: 1509700 training loss: 1.96398
Gl2017-06-23 00:10:20.055589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1512837
obal Iter: 1509700 training acc: 0.25
Global Iter: 1509800 training loss: 1.99929
Global Iter: 1509800 training acc: 0.15625
Global Iter: 1509900 training loss: 1.9716
Global Iter: 1509900 training acc: 0.25
Global Iter: 1510000 training loss: 2.04274
Global Iter: 1510000 training acc: 0.125
Global Iter: 1510100 training loss: 2.03858
Global Iter: 1510100 training acc: 0.125
Global Iter: 1510200 training loss: 2.03493
Global Iter: 1510200 training acc: 0.03125
Global Iter: 1510300 training loss: 1.93528
Global Iter: 1510300 training acc: 0.15625
Global Iter: 1510400 training loss: 2.10211
Global Iter: 1510400 training acc: 0.15625
Global Iter: 1510500 training loss: 1.96535
Global Iter: 1510500 training acc: 0.1875
Global Iter: 1510600 training loss: 2.10802
Global Iter: 1510600 training acc: 0.0625
Global Iter: 1510700 training loss: 1.94512
Global Iter: 1510700 training acc: 0.125
Global Iter: 1510800 training loss: 2.03356
Global Iter: 1510800 training acc: 0.1875
Global Iter: 1510900 training loss: 1.99695
Global Iter: 1510900 training acc: 0.03125
Global Iter: 1511000 training loss: 1.98076
Global Iter: 1511000 training acc: 0.15625
Global Iter: 1511100 training loss: 1.95381
Global Iter: 1511100 training acc: 0.09375
Global Iter: 1511200 training loss: 2.04673
Global Iter: 1511200 training acc: 0.1875
Global Iter: 1511300 training loss: 1.97499
Global Iter: 1511300 training acc: 0.1875
Global Iter: 1511400 training loss: 1.95304
Global Iter: 1511400 training acc: 0.1875
Global Iter: 1511500 training loss: 2.02676
Global Iter: 1511500 training acc: 0.125
Global Iter: 1511600 training loss: 2.00859
Global Iter: 1511600 training acc: 0.0625
Global Iter: 1511700 training loss: 1.95833
Global Iter: 1511700 training acc: 0.125
Global Iter: 1511800 training loss: 1.95594
Global Iter: 1511800 training acc: 0.1875
Global Iter: 1511900 training loss: 1.97742
Global Iter: 1511900 training acc: 0.125
Global Iter: 1512000 training loss: 2.0308
Global Iter: 1512000 training acc: 0.125
Global Iter: 1512100 training loss: 2.05178
Global Iter: 1512100 training acc: 0.125
Global Iter: 1512200 training loss: 1.88523
Global Iter: 1512200 training acc: 0.1875
Global Iter: 1512300 training loss: 2.08102
Global Iter: 1512300 training acc: 0.25
Global Iter: 1512400 training loss: 1.97809
Global Iter: 1512400 training acc: 0.21875
Global Iter: 1512500 training loss: 1.98811
Global Iter: 1512500 training acc: 0.21875
Global Iter: 1512600 training loss: 1.90256
Global Iter: 1512600 training acc: 0.3125
Global Iter: 1512700 training loss: 2.01665
Global Iter: 1512700 training acc: 0.25
Global Iter: 1512800 training loss: 2.02604
Global Iter: 1512800 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1512837
Number of Patches: 65441
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1512837
Global Iter: 1512900 training loss: 1.95239
Global Iter: 1512900 training acc: 0.15625
Global Iter: 1513000 training loss: 1.95089
Global Iter: 1513000 training acc: 0.28125
Global Iter: 1513100 training loss: 1.99178
Global Iter: 1513100 training acc: 0.125
Global Iter: 1513200 training loss: 2.09564
Global Iter: 1513200 training acc: 0.09375
Global Iter: 1513300 training loss: 2.03627
Global Iter: 1513300 training acc: 0.125
Global Iter: 1513400 training loss: 2.04941
Global Iter: 1513400 training acc: 0.1875
Global Iter: 1513500 training loss: 1.93119
Global Iter: 1513500 training acc: 0.28125
Global Iter: 1513600 training loss: 1.95527
Global Iter: 1513600 training acc: 0.15625
Global Iter: 1513700 training loss: 2.00762
Global Iter: 1513700 training acc: 0.125
Global Iter: 1513800 training loss: 1.9836
Global Iter: 1513800 training acc: 0.21875
Global Iter: 1513900 training loss: 1.94089
Global Iter: 1513900 training acc: 0.28125
Global Iter: 1514000 training loss: 1.99736
Global Iter: 1514000 training acc: 0.15625
Global Iter: 1514100 training loss: 1.93825
Global Iter: 1514100 training acc: 0.1875
Global Iter: 1514200 training loss: 1.95757
Global I2017-06-23 00:17:07.180247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1516928
ter: 1514200 training acc: 0.21875
Global Iter: 1514300 training loss: 1.99712
Global Iter: 1514300 training acc: 0.21875
Global Iter: 1514400 training loss: 1.95835
Global Iter: 1514400 training acc: 0.15625
Global Iter: 1514500 training loss: 1.95407
Global Iter: 1514500 training acc: 0.125
Global Iter: 1514600 training loss: 1.93281
Global Iter: 1514600 training acc: 0.21875
Global Iter: 1514700 training loss: 1.91664
Global Iter: 1514700 training acc: 0.21875
Global Iter: 1514800 training loss: 1.9636
Global Iter: 1514800 training acc: 0.15625
Global Iter: 1514900 training loss: 1.99664
Global Iter: 1514900 training acc: 0.15625
Global Iter: 1515000 training loss: 1.97398
Global Iter: 1515000 training acc: 0.09375
Global Iter: 1515100 training loss: 2.04175
Global Iter: 1515100 training acc: 0.09375
Global Iter: 1515200 training loss: 2.01243
Global Iter: 1515200 training acc: 0.25
Global Iter: 1515300 training loss: 2.0783
Global Iter: 1515300 training acc: 0.125
Global Iter: 1515400 training loss: 2.07947
Global Iter: 1515400 training acc: 0.34375
Global Iter: 1515500 training loss: 1.95354
Global Iter: 1515500 training acc: 0.15625
Global Iter: 1515600 training loss: 2.04364
Global Iter: 1515600 training acc: 0.1875
Global Iter: 1515700 training loss: 2.04897
Global Iter: 1515700 training acc: 0.21875
Global Iter: 1515800 training loss: 1.90665
Global Iter: 1515800 training acc: 0.21875
Global Iter: 1515900 training loss: 1.8979
Global Iter: 1515900 training acc: 0.15625
Global Iter: 1516000 training loss: 2.03871
Global Iter: 1516000 training acc: 0.09375
Global Iter: 1516100 training loss: 1.97562
Global Iter: 1516100 training acc: 0.28125
Global Iter: 1516200 training loss: 2.00081
Global Iter: 1516200 training acc: 0.1875
Global Iter: 1516300 training loss: 2.04178
Global Iter: 1516300 training acc: 0.125
Global Iter: 1516400 training loss: 2.17135
Global Iter: 1516400 training acc: 0.15625
Global Iter: 1516500 training loss: 2.0297
Global Iter: 1516500 training acc: 0.09375
Global Iter: 1516600 training loss: 2.01459
Global Iter: 1516600 training acc: 0.15625
Global Iter: 1516700 training loss: 1.9628
Global Iter: 1516700 training acc: 0.15625
Global Iter: 1516800 training loss: 1.94615
Global Iter: 1516800 training acc: 0.21875
Global Iter: 1516900 training loss: 2.00382
Global Iter: 1516900 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1516928
Number of Patches: 64787
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1516928
Global Iter: 1517000 training loss: 1.89315
Global Iter: 1517000 training acc: 0.21875
Global Iter: 1517100 training loss: 1.96522
Global Iter: 1517100 training acc: 0.21875
Global Iter: 1517200 training loss: 2.05074
Global Iter: 1517200 training acc: 0.03125
Global Iter: 1517300 training loss: 2.04371
Global Iter: 1517300 training acc: 0.15625
Global Iter: 1517400 training loss: 2.08136
Global Iter: 1517400 training acc: 0.21875
Global Iter: 1517500 training loss: 1.94284
Global Iter: 1517500 training acc: 0.15625
Global Iter: 1517600 training loss: 1.98435
Global Iter: 1517600 training acc: 0.09375
Global Iter: 1517700 training loss: 2.05254
Global Iter: 1517700 training acc: 0.1875
Global Iter: 1517800 training loss: 1.93314
Global Iter: 1517800 training acc: 0.15625
Global Iter: 1517900 training loss: 1.98397
Global Iter: 1517900 training acc: 0.15625
Global Iter: 1518000 training loss: 1.9185
Global Iter: 1518000 training acc: 0.1875
Global Iter: 1518100 training loss: 2.02705
Global Iter: 1518100 training acc: 0.15625
Global Iter: 1518200 training loss: 2.01043
Global Iter: 1518200 training acc: 0.125
Global Iter: 1518300 training loss: 1.89987
Global Iter: 1518300 training acc: 0.25
Global Iter: 1518400 training loss: 1.95251
Global Iter: 1518400 training acc: 0.03125
Global Iter: 1518500 training loss: 1.97606
Global Iter: 1518500 training acc: 0.15625
Global Iter: 1518600 training loss: 2.07799
Global Iter: 1518600 training acc: 0.09375
Global Iter: 1518700 training loss: 2017-06-23 00:23:54.367930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1520978
1.99541
Global Iter: 1518700 training acc: 0.0625
Global Iter: 1518800 training loss: 2.06397
Global Iter: 1518800 training acc: 0.09375
Global Iter: 1518900 training loss: 2.00332
Global Iter: 1518900 training acc: 0.15625
Global Iter: 1519000 training loss: 2.03603
Global Iter: 1519000 training acc: 0.1875
Global Iter: 1519100 training loss: 1.8366
Global Iter: 1519100 training acc: 0.3125
Global Iter: 1519200 training loss: 1.94113
Global Iter: 1519200 training acc: 0.15625
Global Iter: 1519300 training loss: 1.90012
Global Iter: 1519300 training acc: 0.21875
Global Iter: 1519400 training loss: 1.98637
Global Iter: 1519400 training acc: 0.25
Global Iter: 1519500 training loss: 1.98877
Global Iter: 1519500 training acc: 0.15625
Global Iter: 1519600 training loss: 1.96353
Global Iter: 1519600 training acc: 0.125
Global Iter: 1519700 training loss: 1.96811
Global Iter: 1519700 training acc: 0.125
Global Iter: 1519800 training loss: 1.99991
Global Iter: 1519800 training acc: 0.125
Global Iter: 1519900 training loss: 1.97986
Global Iter: 1519900 training acc: 0.09375
Global Iter: 1520000 training loss: 2.05012
Global Iter: 1520000 training acc: 0.1875
Global Iter: 1520100 training loss: 2.01554
Global Iter: 1520100 training acc: 0.21875
Global Iter: 1520200 training loss: 1.94828
Global Iter: 1520200 training acc: 0.15625
Global Iter: 1520300 training loss: 2.0406
Global Iter: 1520300 training acc: 0.1875
Global Iter: 1520400 training loss: 2.01143
Global Iter: 1520400 training acc: 0.15625
Global Iter: 1520500 training loss: 2.02905
Global Iter: 1520500 training acc: 0.15625
Global Iter: 1520600 training loss: 2.04302
Global Iter: 1520600 training acc: 0.1875
Global Iter: 1520700 training loss: 1.93191
Global Iter: 1520700 training acc: 0.28125
Global Iter: 1520800 training loss: 2.12325
Global Iter: 1520800 training acc: 0.21875
Global Iter: 1520900 training loss: 1.95354
Global Iter: 1520900 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1520978
Number of Patches: 64140
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1520978
Global Iter: 1521000 training loss: 1.94918
Global Iter: 1521000 training acc: 0.125
Global Iter: 1521100 training loss: 2.09512
Global Iter: 1521100 training acc: 0.25
Global Iter: 1521200 training loss: 1.97337
Global Iter: 1521200 training acc: 0.09375
Global Iter: 1521300 training loss: 1.93535
Global Iter: 1521300 training acc: 0.15625
Global Iter: 1521400 training loss: 2.04255
Global Iter: 1521400 training acc: 0.1875
Global Iter: 1521500 training loss: 1.92671
Global Iter: 1521500 training acc: 0.15625
Global Iter: 1521600 training loss: 2.02917
Global Iter: 1521600 training acc: 0.125
Global Iter: 1521700 training loss: 2.03925
Global Iter: 1521700 training acc: 0.125
Global Iter: 1521800 training loss: 1.8811
Global Iter: 1521800 training acc: 0.21875
Global Iter: 1521900 training loss: 2.00962
Global Iter: 1521900 training acc: 0.1875
Global Iter: 1522000 training loss: 2.12092
Global Iter: 1522000 training acc: 0.09375
Global Iter: 1522100 training loss: 1.97845
Global Iter: 1522100 training acc: 0.1875
Global Iter: 1522200 training loss: 2.08812
Global Iter: 1522200 training acc: 0.1875
Global Iter: 1522300 training loss: 1.93221
Global Iter: 1522300 training acc: 0.25
Global Iter: 1522400 training loss: 1.93279
Global Iter: 1522400 training acc: 0.28125
Global Iter: 1522500 training loss: 1.92084
Global Iter: 1522500 training acc: 0.25
Global Iter: 1522600 training loss: 1.95448
Global Iter: 1522600 training acc: 0.3125
Global Iter: 1522700 training loss: 1.99402
Global Iter: 1522700 training acc: 0.125
Global Iter: 1522800 training loss: 1.95195
Global Iter: 1522800 training acc: 0.25
Global Iter: 1522900 training loss: 1.93933
Global Iter: 1522900 training acc: 0.21875
Global Iter: 1523000 training loss: 2.1388
Global Iter: 1523000 training acc: 0.21875
Global Iter: 1523100 training loss: 2.02939
Global Iter: 1523100 training acc: 0.125
Global Iter: 1523200 training loss: 1.92017-06-23 00:30:36.964362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1524987
4782
Global Iter: 1523200 training acc: 0.21875
Global Iter: 1523300 training loss: 1.91101
Global Iter: 1523300 training acc: 0.25
Global Iter: 1523400 training loss: 2.0029
Global Iter: 1523400 training acc: 0.15625
Global Iter: 1523500 training loss: 2.17463
Global Iter: 1523500 training acc: 0.09375
Global Iter: 1523600 training loss: 2.00489
Global Iter: 1523600 training acc: 0.15625
Global Iter: 1523700 training loss: 2.02831
Global Iter: 1523700 training acc: 0.1875
Global Iter: 1523800 training loss: 2.0257
Global Iter: 1523800 training acc: 0.125
Global Iter: 1523900 training loss: 1.95454
Global Iter: 1523900 training acc: 0.21875
Global Iter: 1524000 training loss: 1.9246
Global Iter: 1524000 training acc: 0.28125
Global Iter: 1524100 training loss: 2.0896
Global Iter: 1524100 training acc: 0.125
Global Iter: 1524200 training loss: 1.95365
Global Iter: 1524200 training acc: 0.25
Global Iter: 1524300 training loss: 2.03162
Global Iter: 1524300 training acc: 0.15625
Global Iter: 1524400 training loss: 1.97045
Global Iter: 1524400 training acc: 0.21875
Global Iter: 1524500 training loss: 1.97323
Global Iter: 1524500 training acc: 0.09375
Global Iter: 1524600 training loss: 2.02761
Global Iter: 1524600 training acc: 0.125
Global Iter: 1524700 training loss: 2.05775
Global Iter: 1524700 training acc: 0.21875
Global Iter: 1524800 training loss: 2.01842
Global Iter: 1524800 training acc: 0.3125
Global Iter: 1524900 training loss: 1.93826
Global Iter: 1524900 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1524987
Number of Patches: 63499
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1524987
Global Iter: 1525000 training loss: 2.1127
Global Iter: 1525000 training acc: 0.125
Global Iter: 1525100 training loss: 1.90669
Global Iter: 1525100 training acc: 0.34375
Global Iter: 1525200 training loss: 2.01878
Global Iter: 1525200 training acc: 0.1875
Global Iter: 1525300 training loss: 1.98534
Global Iter: 1525300 training acc: 0.28125
Global Iter: 1525400 training loss: 1.91264
Global Iter: 1525400 training acc: 0.21875
Global Iter: 1525500 training loss: 2.05832
Global Iter: 1525500 training acc: 0.1875
Global Iter: 1525600 training loss: 2.02505
Global Iter: 1525600 training acc: 0.1875
Global Iter: 1525700 training loss: 1.93552
Global Iter: 1525700 training acc: 0.1875
Global Iter: 1525800 training loss: 1.99214
Global Iter: 1525800 training acc: 0.15625
Global Iter: 1525900 training loss: 1.95727
Global Iter: 1525900 training acc: 0.15625
Global Iter: 1526000 training loss: 1.9482
Global Iter: 1526000 training acc: 0.28125
Global Iter: 1526100 training loss: 1.99646
Global Iter: 1526100 training acc: 0.09375
Global Iter: 1526200 training loss: 2.03436
Global Iter: 1526200 training acc: 0.1875
Global Iter: 1526300 training loss: 1.99534
Global Iter: 1526300 training acc: 0.09375
Global Iter: 1526400 training loss: 2.05173
Global Iter: 1526400 training acc: 0.28125
Global Iter: 1526500 training loss: 1.97567
Global Iter: 1526500 training acc: 0.1875
Global Iter: 1526600 training loss: 1.98676
Global Iter: 1526600 training acc: 0.0625
Global Iter: 1526700 training loss: 2.04302
Global Iter: 1526700 training acc: 0.1875
Global Iter: 1526800 training loss: 2.02331
Global Iter: 1526800 training acc: 0.25
Global Iter: 1526900 training loss: 2.02015
Global Iter: 1526900 training acc: 0.1875
Global Iter: 1527000 training loss: 1.91748
Global Iter: 1527000 training acc: 0.21875
Global Iter: 1527100 training loss: 1.94202
Global Iter: 1527100 training acc: 0.15625
Global Iter: 1527200 training loss: 1.87386
Global Iter: 1527200 training acc: 0.25
Global Iter: 1527300 training loss: 1.95625
Global Iter: 1527300 training acc: 0.28125
Global Iter: 1527400 training loss: 1.95732
Global Iter: 1527400 training acc: 0.15625
Global Iter: 1527500 training loss: 2.01496
Global Iter: 1527500 training acc: 0.15625
Global Iter: 1527600 training loss: 1.91733
Global Iter: 1527600 training acc: 0.21875
Global Iter: 1527700 training los2017-06-23 00:37:18.102921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1528956
s: 2.14626
Global Iter: 1527700 training acc: 0.0625
Global Iter: 1527800 training loss: 2.02138
Global Iter: 1527800 training acc: 0.125
Global Iter: 1527900 training loss: 2.01191
Global Iter: 1527900 training acc: 0.09375
Global Iter: 1528000 training loss: 1.93234
Global Iter: 1528000 training acc: 0.25
Global Iter: 1528100 training loss: 2.00297
Global Iter: 1528100 training acc: 0.09375
Global Iter: 1528200 training loss: 1.9977
Global Iter: 1528200 training acc: 0.25
Global Iter: 1528300 training loss: 2.0675
Global Iter: 1528300 training acc: 0.09375
Global Iter: 1528400 training loss: 1.93629
Global Iter: 1528400 training acc: 0.09375
Global Iter: 1528500 training loss: 1.95295
Global Iter: 1528500 training acc: 0.1875
Global Iter: 1528600 training loss: 2.01268
Global Iter: 1528600 training acc: 0.1875
Global Iter: 1528700 training loss: 1.95102
Global Iter: 1528700 training acc: 0.1875
Global Iter: 1528800 training loss: 2.04326
Global Iter: 1528800 training acc: 0.28125
Global Iter: 1528900 training loss: 1.9488
Global Iter: 1528900 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1528956
Number of Patches: 62865
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1528956
Global Iter: 1529000 training loss: 1.8893
Global Iter: 1529000 training acc: 0.28125
Global Iter: 1529100 training loss: 1.98907
Global Iter: 1529100 training acc: 0.25
Global Iter: 1529200 training loss: 1.96785
Global Iter: 1529200 training acc: 0.25
Global Iter: 1529300 training loss: 1.91433
Global Iter: 1529300 training acc: 0.1875
Global Iter: 1529400 training loss: 1.93525
Global Iter: 1529400 training acc: 0.1875
Global Iter: 1529500 training loss: 1.95156
Global Iter: 1529500 training acc: 0.21875
Global Iter: 1529600 training loss: 2.01055
Global Iter: 1529600 training acc: 0.09375
Global Iter: 1529700 training loss: 1.97424
Global Iter: 1529700 training acc: 0.375
Global Iter: 1529800 training loss: 2.00936
Global Iter: 1529800 training acc: 0.15625
Global Iter: 1529900 training loss: 1.95649
Global Iter: 1529900 training acc: 0.15625
Global Iter: 1530000 training loss: 1.96348
Global Iter: 1530000 training acc: 0.09375
Global Iter: 1530100 training loss: 1.95083
Global Iter: 1530100 training acc: 0.1875
Global Iter: 1530200 training loss: 2.01125
Global Iter: 1530200 training acc: 0.125
Global Iter: 1530300 training loss: 1.99516
Global Iter: 1530300 training acc: 0.15625
Global Iter: 1530400 training loss: 1.94099
Global Iter: 1530400 training acc: 0.09375
Global Iter: 1530500 training loss: 2.03361
Global Iter: 1530500 training acc: 0.15625
Global Iter: 1530600 training loss: 1.93022
Global Iter: 1530600 training acc: 0.09375
Global Iter: 1530700 training loss: 1.93139
Global Iter: 1530700 training acc: 0.15625
Global Iter: 1530800 training loss: 2.02824
Global Iter: 1530800 training acc: 0.25
Global Iter: 1530900 training loss: 2.0293
Global Iter: 1530900 training acc: 0.125
Global Iter: 1531000 training loss: 1.99465
Global Iter: 1531000 training acc: 0.125
Global Iter: 1531100 training loss: 1.99868
Global Iter: 1531100 training acc: 0.09375
Global Iter: 1531200 training loss: 2.12292
Global Iter: 1531200 training acc: 0.21875
Global Iter: 1531300 training loss: 1.93924
Global Iter: 1531300 training acc: 0.1875
Global Iter: 1531400 training loss: 1.96964
Global Iter: 1531400 training acc: 0.21875
Global Iter: 1531500 training loss: 1.87348
Global Iter: 1531500 training acc: 0.125
Global Iter: 1531600 training loss: 2.04907
Global Iter: 1531600 training acc: 0.125
Global Iter: 1531700 training loss: 2.11467
Global Iter: 1531700 training acc: 0.15625
Global Iter: 1531800 training loss: 1.93785
Global Iter: 1531800 training acc: 0.15625
Global Iter: 1531900 training loss: 1.92429
Global Iter: 1531900 training acc: 0.1875
Global Iter: 1532000 training loss: 2.06348
Global Iter: 1532000 training acc: 0.15625
Global Iter: 1532100 training loss: 2.01888
Global Iter: 1532100 training acc: 0.09375
Global Iter: 1532200 training loss2017-06-23 00:44:00.590466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1532886
: 1.91964
Global Iter: 1532200 training acc: 0.375
Global Iter: 1532300 training loss: 1.97043
Global Iter: 1532300 training acc: 0.15625
Global Iter: 1532400 training loss: 2.04691
Global Iter: 1532400 training acc: 0.28125
Global Iter: 1532500 training loss: 1.95317
Global Iter: 1532500 training acc: 0.28125
Global Iter: 1532600 training loss: 1.92127
Global Iter: 1532600 training acc: 0.28125
Global Iter: 1532700 training loss: 1.94322
Global Iter: 1532700 training acc: 0.3125
Global Iter: 1532800 training loss: 1.98899
Global Iter: 1532800 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1532886
Number of Patches: 62237
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1532886
Global Iter: 1532900 training loss: 1.97502
Global Iter: 1532900 training acc: 0.125
Global Iter: 1533000 training loss: 1.98132
Global Iter: 1533000 training acc: 0.21875
Global Iter: 1533100 training loss: 2.00784
Global Iter: 1533100 training acc: 0.0625
Global Iter: 1533200 training loss: 2.03628
Global Iter: 1533200 training acc: 0.15625
Global Iter: 1533300 training loss: 1.92972
Global Iter: 1533300 training acc: 0.1875
Global Iter: 1533400 training loss: 1.97456
Global Iter: 1533400 training acc: 0.15625
Global Iter: 1533500 training loss: 1.93945
Global Iter: 1533500 training acc: 0.15625
Global Iter: 1533600 training loss: 2.0064
Global Iter: 1533600 training acc: 0.125
Global Iter: 1533700 training loss: 1.95189
Global Iter: 1533700 training acc: 0.09375
Global Iter: 1533800 training loss: 1.96617
Global Iter: 1533800 training acc: 0.09375
Global Iter: 1533900 training loss: 1.96603
Global Iter: 1533900 training acc: 0.125
Global Iter: 1534000 training loss: 1.98809
Global Iter: 1534000 training acc: 0.15625
Global Iter: 1534100 training loss: 2.13828
Global Iter: 1534100 training acc: 0.125
Global Iter: 1534200 training loss: 1.93728
Global Iter: 1534200 training acc: 0.1875
Global Iter: 1534300 training loss: 2.07257
Global Iter: 1534300 training acc: 0.15625
Global Iter: 1534400 training loss: 1.98336
Global Iter: 1534400 training acc: 0.21875
Global Iter: 1534500 training loss: 2.00056
Global Iter: 1534500 training acc: 0.15625
Global Iter: 1534600 training loss: 1.95522
Global Iter: 1534600 training acc: 0.21875
Global Iter: 1534700 training loss: 1.89217
Global Iter: 1534700 training acc: 0.15625
Global Iter: 1534800 training loss: 2.08322
Global Iter: 1534800 training acc: 0.125
Global Iter: 1534900 training loss: 2.00383
Global Iter: 1534900 training acc: 0.125
Global Iter: 1535000 training loss: 1.97221
Global Iter: 1535000 training acc: 0.125
Global Iter: 1535100 training loss: 1.90862
Global Iter: 1535100 training acc: 0.15625
Global Iter: 1535200 training loss: 1.95803
Global Iter: 1535200 training acc: 0.21875
Global Iter: 1535300 training loss: 1.99693
Global Iter: 1535300 training acc: 0.125
Global Iter: 1535400 training loss: 1.97709
Global Iter: 1535400 training acc: 0.15625
Global Iter: 1535500 training loss: 2.18262
Global Iter: 1535500 training acc: 0.15625
Global Iter: 1535600 training loss: 2.05178
Global Iter: 1535600 training acc: 0.0625
Global Iter: 1535700 training loss: 1.97056
Global Iter: 1535700 training acc: 0.3125
Global Iter: 1535800 training loss: 2.05719
Global Iter: 1535800 training acc: 0.15625
Global Iter: 1535900 training loss: 1.9334
Global Iter: 1535900 training acc: 0.15625
Global Iter: 1536000 training loss: 1.98437
Global Iter: 1536000 training acc: 0.09375
Global Iter: 1536100 training loss: 1.92714
Global Iter: 1536100 training acc: 0.21875
Global Iter: 1536200 training loss: 1.92576
Global Iter: 1536200 training acc: 0.125
Global Iter: 1536300 training loss: 2.07928
Global Iter: 1536300 training acc: 0.1875
Global Iter: 1536400 training loss: 1.90553
Global Iter: 1536400 training acc: 0.15625
Global Iter: 1536500 training loss: 1.96101
Global Iter: 1536500 training acc: 0.1875
Global Iter: 1536600 training loss: 2.10229
Global Iter: 1536600 training acc: 0.09375
Global Iter: 1536700 tr2017-06-23 00:50:28.828191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1536776
2017-06-23 00:56:56.835974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1540627
aining loss: 1.92069
Global Iter: 1536700 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1536776
Number of Patches: 61615
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1536776
Global Iter: 1536800 training loss: 2.00968
Global Iter: 1536800 training acc: 0.15625
Global Iter: 1536900 training loss: 2.12552
Global Iter: 1536900 training acc: 0.15625
Global Iter: 1537000 training loss: 1.99916
Global Iter: 1537000 training acc: 0.25
Global Iter: 1537100 training loss: 1.97466
Global Iter: 1537100 training acc: 0.15625
Global Iter: 1537200 training loss: 1.9991
Global Iter: 1537200 training acc: 0.1875
Global Iter: 1537300 training loss: 2.10402
Global Iter: 1537300 training acc: 0.03125
Global Iter: 1537400 training loss: 1.94403
Global Iter: 1537400 training acc: 0.09375
Global Iter: 1537500 training loss: 1.9517
Global Iter: 1537500 training acc: 0.125
Global Iter: 1537600 training loss: 2.03352
Global Iter: 1537600 training acc: 0.25
Global Iter: 1537700 training loss: 1.9995
Global Iter: 1537700 training acc: 0.125
Global Iter: 1537800 training loss: 2.04052
Global Iter: 1537800 training acc: 0.15625
Global Iter: 1537900 training loss: 1.89871
Global Iter: 1537900 training acc: 0.25
Global Iter: 1538000 training loss: 2.0365
Global Iter: 1538000 training acc: 0.1875
Global Iter: 1538100 training loss: 2.0154
Global Iter: 1538100 training acc: 0.25
Global Iter: 1538200 training loss: 2.02959
Global Iter: 1538200 training acc: 0.1875
Global Iter: 1538300 training loss: 1.94447
Global Iter: 1538300 training acc: 0.15625
Global Iter: 1538400 training loss: 1.97206
Global Iter: 1538400 training acc: 0.25
Global Iter: 1538500 training loss: 1.97267
Global Iter: 1538500 training acc: 0.1875
Global Iter: 1538600 training loss: 2.0123
Global Iter: 1538600 training acc: 0.125
Global Iter: 1538700 training loss: 1.97881
Global Iter: 1538700 training acc: 0.03125
Global Iter: 1538800 training loss: 1.95921
Global Iter: 1538800 training acc: 0.1875
Global Iter: 1538900 training loss: 1.94536
Global Iter: 1538900 training acc: 0.21875
Global Iter: 1539000 training loss: 1.98512
Global Iter: 1539000 training acc: 0.21875
Global Iter: 1539100 training loss: 1.9157
Global Iter: 1539100 training acc: 0.28125
Global Iter: 1539200 training loss: 2.0736
Global Iter: 1539200 training acc: 0.125
Global Iter: 1539300 training loss: 2.12345
Global Iter: 1539300 training acc: 0.09375
Global Iter: 1539400 training loss: 2.04605
Global Iter: 1539400 training acc: 0.125
Global Iter: 1539500 training loss: 1.95483
Global Iter: 1539500 training acc: 0.21875
Global Iter: 1539600 training loss: 2.07948
Global Iter: 1539600 training acc: 0.1875
Global Iter: 1539700 training loss: 2.0085
Global Iter: 1539700 training acc: 0.1875
Global Iter: 1539800 training loss: 1.9562
Global Iter: 1539800 training acc: 0.1875
Global Iter: 1539900 training loss: 2.13456
Global Iter: 1539900 training acc: 0.03125
Global Iter: 1540000 training loss: 1.94188
Global Iter: 1540000 training acc: 0.1875
Global Iter: 1540100 training loss: 1.91643
Global Iter: 1540100 training acc: 0.1875
Global Iter: 1540200 training loss: 2.06878
Global Iter: 1540200 training acc: 0.09375
Global Iter: 1540300 training loss: 1.97405
Global Iter: 1540300 training acc: 0.21875
Global Iter: 1540400 training loss: 1.96089
Global Iter: 1540400 training acc: 0.3125
Global Iter: 1540500 training loss: 1.97496
Global Iter: 1540500 training acc: 0.15625
Global Iter: 1540600 training loss: 1.97925
Global Iter: 1540600 training acc: 0.03125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1540627
Number of Patches: 60999
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1540627
Global Iter: 1540700 training loss: 2.03726
Global Iter: 1540700 training acc: 0.15625
Global Iter: 1540800 training loss: 2.05589
Global Iter: 1540800 training acc: 0.09375
Global Iter: 1540900 training loss: 1.92749
Global Iter: 1540902017-06-23 01:03:19.123887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1544440
0 training acc: 0.1875
Global Iter: 1541000 training loss: 1.95175
Global Iter: 1541000 training acc: 0.125
Global Iter: 1541100 training loss: 2.07169
Global Iter: 1541100 training acc: 0.0625
Global Iter: 1541200 training loss: 1.94998
Global Iter: 1541200 training acc: 0.09375
Global Iter: 1541300 training loss: 1.94244
Global Iter: 1541300 training acc: 0.28125
Global Iter: 1541400 training loss: 1.98123
Global Iter: 1541400 training acc: 0.125
Global Iter: 1541500 training loss: 2.05947
Global Iter: 1541500 training acc: 0.0625
Global Iter: 1541600 training loss: 1.97622
Global Iter: 1541600 training acc: 0.15625
Global Iter: 1541700 training loss: 2.01253
Global Iter: 1541700 training acc: 0.1875
Global Iter: 1541800 training loss: 1.91388
Global Iter: 1541800 training acc: 0.21875
Global Iter: 1541900 training loss: 2.04647
Global Iter: 1541900 training acc: 0.125
Global Iter: 1542000 training loss: 2.03055
Global Iter: 1542000 training acc: 0.15625
Global Iter: 1542100 training loss: 1.90659
Global Iter: 1542100 training acc: 0.21875
Global Iter: 1542200 training loss: 1.889
Global Iter: 1542200 training acc: 0.28125
Global Iter: 1542300 training loss: 1.99651
Global Iter: 1542300 training acc: 0.25
Global Iter: 1542400 training loss: 2.1108
Global Iter: 1542400 training acc: 0.21875
Global Iter: 1542500 training loss: 1.99188
Global Iter: 1542500 training acc: 0.25
Global Iter: 1542600 training loss: 2.0407
Global Iter: 1542600 training acc: 0.125
Global Iter: 1542700 training loss: 1.91019
Global Iter: 1542700 training acc: 0.1875
Global Iter: 1542800 training loss: 2.12094
Global Iter: 1542800 training acc: 0.09375
Global Iter: 1542900 training loss: 1.9846
Global Iter: 1542900 training acc: 0.09375
Global Iter: 1543000 training loss: 2.01215
Global Iter: 1543000 training acc: 0.15625
Global Iter: 1543100 training loss: 2.09432
Global Iter: 1543100 training acc: 0.125
Global Iter: 1543200 training loss: 1.9464
Global Iter: 1543200 training acc: 0.15625
Global Iter: 1543300 training loss: 2.06826
Global Iter: 1543300 training acc: 0.15625
Global Iter: 1543400 training loss: 1.99968
Global Iter: 1543400 training acc: 0.125
Global Iter: 1543500 training loss: 1.92531
Global Iter: 1543500 training acc: 0.1875
Global Iter: 1543600 training loss: 2.11765
Global Iter: 1543600 training acc: 0.09375
Global Iter: 1543700 training loss: 1.96495
Global Iter: 1543700 training acc: 0.15625
Global Iter: 1543800 training loss: 1.93084
Global Iter: 1543800 training acc: 0.09375
Global Iter: 1543900 training loss: 1.9541
Global Iter: 1543900 training acc: 0.25
Global Iter: 1544000 training loss: 2.04601
Global Iter: 1544000 training acc: 0.0625
Global Iter: 1544100 training loss: 1.89131
Global Iter: 1544100 training acc: 0.28125
Global Iter: 1544200 training loss: 1.9511
Global Iter: 1544200 training acc: 0.1875
Global Iter: 1544300 training loss: 1.98964
Global Iter: 1544300 training acc: 0.25
Global Iter: 1544400 training loss: 1.97351
Global Iter: 1544400 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1544440
Number of Patches: 60390
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1544440
Global Iter: 1544500 training loss: 1.9692
Global Iter: 1544500 training acc: 0.09375
Global Iter: 1544600 training loss: 2.09972
Global Iter: 1544600 training acc: 0.25
Global Iter: 1544700 training loss: 1.95637
Global Iter: 1544700 training acc: 0.34375
Global Iter: 1544800 training loss: 1.96984
Global Iter: 1544800 training acc: 0.25
Global Iter: 1544900 training loss: 1.87617
Global Iter: 1544900 training acc: 0.3125
Global Iter: 1545000 training loss: 1.98428
Global Iter: 1545000 training acc: 0.1875
Global Iter: 1545100 training loss: 2.05272
Global Iter: 1545100 training acc: 0.03125
Global Iter: 1545200 training loss: 1.92489
Global Iter: 1545200 training acc: 0.34375
Global Iter: 1545300 training loss: 2.04112
Global Iter: 1545300 training acc: 0.15625
Global Iter: 1545400 training loss: 1.98942
Global Iter: 1545400 traini2017-06-23 01:09:41.071346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1548215
ng acc: 0.125
Global Iter: 1545500 training loss: 1.87226
Global Iter: 1545500 training acc: 0.1875
Global Iter: 1545600 training loss: 1.95917
Global Iter: 1545600 training acc: 0.125
Global Iter: 1545700 training loss: 1.95245
Global Iter: 1545700 training acc: 0.21875
Global Iter: 1545800 training loss: 1.95307
Global Iter: 1545800 training acc: 0.21875
Global Iter: 1545900 training loss: 1.91237
Global Iter: 1545900 training acc: 0.4375
Global Iter: 1546000 training loss: 2.05232
Global Iter: 1546000 training acc: 0.25
Global Iter: 1546100 training loss: 1.94608
Global Iter: 1546100 training acc: 0.25
Global Iter: 1546200 training loss: 1.91043
Global Iter: 1546200 training acc: 0.1875
Global Iter: 1546300 training loss: 2.15581
Global Iter: 1546300 training acc: 0.0625
Global Iter: 1546400 training loss: 1.92041
Global Iter: 1546400 training acc: 0.25
Global Iter: 1546500 training loss: 2.05524
Global Iter: 1546500 training acc: 0.15625
Global Iter: 1546600 training loss: 2.12254
Global Iter: 1546600 training acc: 0.125
Global Iter: 1546700 training loss: 1.90618
Global Iter: 1546700 training acc: 0.25
Global Iter: 1546800 training loss: 2.03741
Global Iter: 1546800 training acc: 0.1875
Global Iter: 1546900 training loss: 1.90703
Global Iter: 1546900 training acc: 0.21875
Global Iter: 1547000 training loss: 2.14468
Global Iter: 1547000 training acc: 0.125
Global Iter: 1547100 training loss: 1.90699
Global Iter: 1547100 training acc: 0.25
Global Iter: 1547200 training loss: 2.02398
Global Iter: 1547200 training acc: 0.21875
Global Iter: 1547300 training loss: 1.99393
Global Iter: 1547300 training acc: 0.1875
Global Iter: 1547400 training loss: 1.95292
Global Iter: 1547400 training acc: 0.3125
Global Iter: 1547500 training loss: 2.04833
Global Iter: 1547500 training acc: 0.125
Global Iter: 1547600 training loss: 2.09181
Global Iter: 1547600 training acc: 0.15625
Global Iter: 1547700 training loss: 1.94555
Global Iter: 1547700 training acc: 0.25
Global Iter: 1547800 training loss: 1.95743
Global Iter: 1547800 training acc: 0.1875
Global Iter: 1547900 training loss: 1.97621
Global Iter: 1547900 training acc: 0.09375
Global Iter: 1548000 training loss: 1.88604
Global Iter: 1548000 training acc: 0.125
Global Iter: 1548100 training loss: 2.01996
Global Iter: 1548100 training acc: 0.125
Global Iter: 1548200 training loss: 1.98724
Global Iter: 1548200 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1548215
Number of Patches: 59787
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1548215
Global Iter: 1548300 training loss: 1.9318
Global Iter: 1548300 training acc: 0.15625
Global Iter: 1548400 training loss: 2.04473
Global Iter: 1548400 training acc: 0.21875
Global Iter: 1548500 training loss: 1.93847
Global Iter: 1548500 training acc: 0.1875
Global Iter: 1548600 training loss: 2.01254
Global Iter: 1548600 training acc: 0.125
Global Iter: 1548700 training loss: 1.97951
Global Iter: 1548700 training acc: 0.28125
Global Iter: 1548800 training loss: 2.10548
Global Iter: 1548800 training acc: 0.15625
Global Iter: 1548900 training loss: 1.94487
Global Iter: 1548900 training acc: 0.15625
Global Iter: 1549000 training loss: 1.99797
Global Iter: 1549000 training acc: 0.09375
Global Iter: 1549100 training loss: 2.04556
Global Iter: 1549100 training acc: 0.09375
Global Iter: 1549200 training loss: 1.98781
Global Iter: 1549200 training acc: 0.125
Global Iter: 1549300 training loss: 2.02739
Global Iter: 1549300 training acc: 0.09375
Global Iter: 1549400 training loss: 2.0968
Global Iter: 1549400 training acc: 0.25
Global Iter: 1549500 training loss: 1.98296
Global Iter: 1549500 training acc: 0.125
Global Iter: 1549600 training loss: 1.92846
Global Iter: 1549600 training acc: 0.21875
Global Iter: 1549700 training loss: 1.92598
Global Iter: 1549700 training acc: 0.28125
Global Iter: 1549800 training loss: 1.93354
Global Iter: 1549800 training acc: 0.15625
Global Iter: 1549900 training loss: 1.95629
Global Iter: 1549900 training acc: 02017-06-23 01:16:01.342348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1551952
.0625
Global Iter: 1550000 training loss: 1.96645
Global Iter: 1550000 training acc: 0.21875
Global Iter: 1550100 training loss: 1.97483
Global Iter: 1550100 training acc: 0.125
Global Iter: 1550200 training loss: 1.92176
Global Iter: 1550200 training acc: 0.15625
Global Iter: 1550300 training loss: 1.86286
Global Iter: 1550300 training acc: 0.21875
Global Iter: 1550400 training loss: 1.99459
Global Iter: 1550400 training acc: 0.09375
Global Iter: 1550500 training loss: 2.049
Global Iter: 1550500 training acc: 0.15625
Global Iter: 1550600 training loss: 2.03009
Global Iter: 1550600 training acc: 0.09375
Global Iter: 1550700 training loss: 1.98475
Global Iter: 1550700 training acc: 0.1875
Global Iter: 1550800 training loss: 2.05018
Global Iter: 1550800 training acc: 0.09375
Global Iter: 1550900 training loss: 1.99402
Global Iter: 1550900 training acc: 0.25
Global Iter: 1551000 training loss: 2.00536
Global Iter: 1551000 training acc: 0.15625
Global Iter: 1551100 training loss: 2.00917
Global Iter: 1551100 training acc: 0.125
Global Iter: 1551200 training loss: 1.92851
Global Iter: 1551200 training acc: 0.3125
Global Iter: 1551300 training loss: 2.01745
Global Iter: 1551300 training acc: 0.03125
Global Iter: 1551400 training loss: 2.01381
Global Iter: 1551400 training acc: 0.125
Global Iter: 1551500 training loss: 1.9552
Global Iter: 1551500 training acc: 0.34375
Global Iter: 1551600 training loss: 1.94576
Global Iter: 1551600 training acc: 0.25
Global Iter: 1551700 training loss: 2.00891
Global Iter: 1551700 training acc: 0.1875
Global Iter: 1551800 training loss: 1.95206
Global Iter: 1551800 training acc: 0.125
Global Iter: 1551900 training loss: 1.98752
Global Iter: 1551900 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1551952
Number of Patches: 59190
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1551952
Global Iter: 1552000 training loss: 1.97603
Global Iter: 1552000 training acc: 0.1875
Global Iter: 1552100 training loss: 2.03984
Global Iter: 1552100 training acc: 0.125
Global Iter: 1552200 training loss: 1.97301
Global Iter: 1552200 training acc: 0.125
Global Iter: 1552300 training loss: 2.05537
Global Iter: 1552300 training acc: 0.125
Global Iter: 1552400 training loss: 1.98291
Global Iter: 1552400 training acc: 0.21875
Global Iter: 1552500 training loss: 1.95894
Global Iter: 1552500 training acc: 0.15625
Global Iter: 1552600 training loss: 2.03257
Global Iter: 1552600 training acc: 0.09375
Global Iter: 1552700 training loss: 1.96335
Global Iter: 1552700 training acc: 0.09375
Global Iter: 1552800 training loss: 1.89736
Global Iter: 1552800 training acc: 0.1875
Global Iter: 1552900 training loss: 1.92424
Global Iter: 1552900 training acc: 0.25
Global Iter: 1553000 training loss: 1.95027
Global Iter: 1553000 training acc: 0.21875
Global Iter: 1553100 training loss: 2.11847
Global Iter: 1553100 training acc: 0.15625
Global Iter: 1553200 training loss: 1.90332
Global Iter: 1553200 training acc: 0.28125
Global Iter: 1553300 training loss: 1.90923
Global Iter: 1553300 training acc: 0.21875
Global Iter: 1553400 training loss: 2.02015
Global Iter: 1553400 training acc: 0.15625
Global Iter: 1553500 training loss: 1.97475
Global Iter: 1553500 training acc: 0.125
Global Iter: 1553600 training loss: 2.04268
Global Iter: 1553600 training acc: 0.21875
Global Iter: 1553700 training loss: 2.06591
Global Iter: 1553700 training acc: 0.21875
Global Iter: 1553800 training loss: 1.99173
Global Iter: 1553800 training acc: 0.15625
Global Iter: 1553900 training loss: 1.98289
Global Iter: 1553900 training acc: 0.25
Global Iter: 1554000 training loss: 1.98157
Global Iter: 1554000 training acc: 0.15625
Global Iter: 1554100 training loss: 1.87526
Global Iter: 1554100 training acc: 0.15625
Global Iter: 1554200 training loss: 2.03593
Global Iter: 1554200 training acc: 0.15625
Global Iter: 1554300 training loss: 2.10676
Global Iter: 1554300 training acc: 0.25
Global Iter: 1554400 training loss: 1.95665
Global Iter: 1554400 training acc2017-06-23 01:22:13.977117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1555652
: 0.15625
Global Iter: 1554500 training loss: 1.93064
Global Iter: 1554500 training acc: 0.28125
Global Iter: 1554600 training loss: 2.06862
Global Iter: 1554600 training acc: 0.125
Global Iter: 1554700 training loss: 1.95433
Global Iter: 1554700 training acc: 0.15625
Global Iter: 1554800 training loss: 2.13616
Global Iter: 1554800 training acc: 0.15625
Global Iter: 1554900 training loss: 1.95038
Global Iter: 1554900 training acc: 0.1875
Global Iter: 1555000 training loss: 1.91837
Global Iter: 1555000 training acc: 0.21875
Global Iter: 1555100 training loss: 1.91839
Global Iter: 1555100 training acc: 0.1875
Global Iter: 1555200 training loss: 1.92949
Global Iter: 1555200 training acc: 0.25
Global Iter: 1555300 training loss: 1.99754
Global Iter: 1555300 training acc: 0.21875
Global Iter: 1555400 training loss: 1.93595
Global Iter: 1555400 training acc: 0.125
Global Iter: 1555500 training loss: 1.94183
Global Iter: 1555500 training acc: 0.21875
Global Iter: 1555600 training loss: 1.97406
Global Iter: 1555600 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1555652
Number of Patches: 58599
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1555652
Global Iter: 1555700 training loss: 1.9297
Global Iter: 1555700 training acc: 0.21875
Global Iter: 1555800 training loss: 1.98867
Global Iter: 1555800 training acc: 0.125
Global Iter: 1555900 training loss: 2.02902
Global Iter: 1555900 training acc: 0.125
Global Iter: 1556000 training loss: 1.99382
Global Iter: 1556000 training acc: 0.21875
Global Iter: 1556100 training loss: 1.97286
Global Iter: 1556100 training acc: 0.15625
Global Iter: 1556200 training loss: 1.93126
Global Iter: 1556200 training acc: 0.28125
Global Iter: 1556300 training loss: 1.94448
Global Iter: 1556300 training acc: 0.1875
Global Iter: 1556400 training loss: 1.96593
Global Iter: 1556400 training acc: 0.1875
Global Iter: 1556500 training loss: 1.92017
Global Iter: 1556500 training acc: 0.21875
Global Iter: 1556600 training loss: 2.01948
Global Iter: 1556600 training acc: 0.15625
Global Iter: 1556700 training loss: 2.08084
Global Iter: 1556700 training acc: 0.125
Global Iter: 1556800 training loss: 2.07067
Global Iter: 1556800 training acc: 0.15625
Global Iter: 1556900 training loss: 2.03896
Global Iter: 1556900 training acc: 0.21875
Global Iter: 1557000 training loss: 1.96261
Global Iter: 1557000 training acc: 0.15625
Global Iter: 1557100 training loss: 1.98831
Global Iter: 1557100 training acc: 0.25
Global Iter: 1557200 training loss: 1.90849
Global Iter: 1557200 training acc: 0.25
Global Iter: 1557300 training loss: 2.00174
Global Iter: 1557300 training acc: 0.15625
Global Iter: 1557400 training loss: 1.93731
Global Iter: 1557400 training acc: 0.15625
Global Iter: 1557500 training loss: 1.9182
Global Iter: 1557500 training acc: 0.1875
Global Iter: 1557600 training loss: 1.94917
Global Iter: 1557600 training acc: 0.25
Global Iter: 1557700 training loss: 2.02698
Global Iter: 1557700 training acc: 0.15625
Global Iter: 1557800 training loss: 2.0478
Global Iter: 1557800 training acc: 0.1875
Global Iter: 1557900 training loss: 1.91843
Global Iter: 1557900 training acc: 0.1875
Global Iter: 1558000 training loss: 1.93198
Global Iter: 1558000 training acc: 0.21875
Global Iter: 1558100 training loss: 2.06518
Global Iter: 1558100 training acc: 0.09375
Global Iter: 1558200 training loss: 1.95249
Global Iter: 1558200 training acc: 0.25
Global Iter: 1558300 training loss: 1.97059
Global Iter: 1558300 training acc: 0.125
Global Iter: 1558400 training loss: 1.88625
Global Iter: 1558400 training acc: 0.28125
Global Iter: 1558500 training loss: 2.10207
Global Iter: 1558500 training acc: 0.09375
Global Iter: 1558600 training loss: 1.96462
Global Iter: 1558600 training acc: 0.0625
Global Iter: 1558700 training loss: 1.95951
Global Iter: 1558700 training acc: 0.125
Global Iter: 1558800 training loss: 1.99999
Global Iter: 1558800 training acc: 0.09375
Global Iter: 1558900 training loss: 2.00317
Global Iter: 1558900 training a2017-06-23 01:28:22.831710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1559315
2017-06-23 01:34:20.650234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1562941
cc: 0.25
Global Iter: 1559000 training loss: 1.98245
Global Iter: 1559000 training acc: 0.1875
Global Iter: 1559100 training loss: 1.96301
Global Iter: 1559100 training acc: 0.1875
Global Iter: 1559200 training loss: 1.96635
Global Iter: 1559200 training acc: 0.21875
Global Iter: 1559300 training loss: 2.01195
Global Iter: 1559300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1559315
Number of Patches: 58014
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1559315
Global Iter: 1559400 training loss: 1.96727
Global Iter: 1559400 training acc: 0.1875
Global Iter: 1559500 training loss: 1.99792
Global Iter: 1559500 training acc: 0.125
Global Iter: 1559600 training loss: 2.02879
Global Iter: 1559600 training acc: 0.125
Global Iter: 1559700 training loss: 1.99402
Global Iter: 1559700 training acc: 0.15625
Global Iter: 1559800 training loss: 1.97089
Global Iter: 1559800 training acc: 0.1875
Global Iter: 1559900 training loss: 1.92022
Global Iter: 1559900 training acc: 0.25
Global Iter: 1560000 training loss: 1.97982
Global Iter: 1560000 training acc: 0.1875
Global Iter: 1560100 training loss: 2.06928
Global Iter: 1560100 training acc: 0.15625
Global Iter: 1560200 training loss: 1.93424
Global Iter: 1560200 training acc: 0.28125
Global Iter: 1560300 training loss: 1.95859
Global Iter: 1560300 training acc: 0.21875
Global Iter: 1560400 training loss: 2.02388
Global Iter: 1560400 training acc: 0.125
Global Iter: 1560500 training loss: 1.97674
Global Iter: 1560500 training acc: 0.21875
Global Iter: 1560600 training loss: 2.03555
Global Iter: 1560600 training acc: 0.125
Global Iter: 1560700 training loss: 1.98158
Global Iter: 1560700 training acc: 0.15625
Global Iter: 1560800 training loss: 1.90859
Global Iter: 1560800 training acc: 0.15625
Global Iter: 1560900 training loss: 2.02532
Global Iter: 1560900 training acc: 0.21875
Global Iter: 1561000 training loss: 2.06076
Global Iter: 1561000 training acc: 0.09375
Global Iter: 1561100 training loss: 2.09924
Global Iter: 1561100 training acc: 0.15625
Global Iter: 1561200 training loss: 2.01102
Global Iter: 1561200 training acc: 0.25
Global Iter: 1561300 training loss: 2.07747
Global Iter: 1561300 training acc: 0.09375
Global Iter: 1561400 training loss: 1.94665
Global Iter: 1561400 training acc: 0.21875
Global Iter: 1561500 training loss: 1.99739
Global Iter: 1561500 training acc: 0.15625
Global Iter: 1561600 training loss: 1.9862
Global Iter: 1561600 training acc: 0.21875
Global Iter: 1561700 training loss: 1.98236
Global Iter: 1561700 training acc: 0.0625
Global Iter: 1561800 training loss: 1.94261
Global Iter: 1561800 training acc: 0.3125
Global Iter: 1561900 training loss: 2.05712
Global Iter: 1561900 training acc: 0.15625
Global Iter: 1562000 training loss: 2.05728
Global Iter: 1562000 training acc: 0.0625
Global Iter: 1562100 training loss: 2.05317
Global Iter: 1562100 training acc: 0.1875
Global Iter: 1562200 training loss: 2.03426
Global Iter: 1562200 training acc: 0.125
Global Iter: 1562300 training loss: 1.97963
Global Iter: 1562300 training acc: 0.09375
Global Iter: 1562400 training loss: 2.00956
Global Iter: 1562400 training acc: 0.125
Global Iter: 1562500 training loss: 2.03577
Global Iter: 1562500 training acc: 0.09375
Global Iter: 1562600 training loss: 2.02196
Global Iter: 1562600 training acc: 0.21875
Global Iter: 1562700 training loss: 2.04217
Global Iter: 1562700 training acc: 0.15625
Global Iter: 1562800 training loss: 1.95693
Global Iter: 1562800 training acc: 0.125
Global Iter: 1562900 training loss: 1.90829
Global Iter: 1562900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1562941
Number of Patches: 57434
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1562941
Global Iter: 1563000 training loss: 1.965
Global Iter: 1563000 training acc: 0.1875
Global Iter: 1563100 training loss: 1.90795
Global Iter: 1563100 training acc: 0.21875
Global Iter: 15632002017-06-23 01:40:23.080643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1566531
 training loss: 1.96324
Global Iter: 1563200 training acc: 0.1875
Global Iter: 1563300 training loss: 1.99222
Global Iter: 1563300 training acc: 0.28125
Global Iter: 1563400 training loss: 2.00591
Global Iter: 1563400 training acc: 0.34375
Global Iter: 1563500 training loss: 2.03363
Global Iter: 1563500 training acc: 0.09375
Global Iter: 1563600 training loss: 1.99107
Global Iter: 1563600 training acc: 0.03125
Global Iter: 1563700 training loss: 1.9713
Global Iter: 1563700 training acc: 0.09375
Global Iter: 1563800 training loss: 1.98555
Global Iter: 1563800 training acc: 0.25
Global Iter: 1563900 training loss: 1.94612
Global Iter: 1563900 training acc: 0.28125
Global Iter: 1564000 training loss: 2.0264
Global Iter: 1564000 training acc: 0.21875
Global Iter: 1564100 training loss: 1.91647
Global Iter: 1564100 training acc: 0.1875
Global Iter: 1564200 training loss: 2.10662
Global Iter: 1564200 training acc: 0.15625
Global Iter: 1564300 training loss: 1.99535
Global Iter: 1564300 training acc: 0.3125
Global Iter: 1564400 training loss: 2.00192
Global Iter: 1564400 training acc: 0.1875
Global Iter: 1564500 training loss: 1.99717
Global Iter: 1564500 training acc: 0.0625
Global Iter: 1564600 training loss: 1.97776
Global Iter: 1564600 training acc: 0.1875
Global Iter: 1564700 training loss: 2.02636
Global Iter: 1564700 training acc: 0.1875
Global Iter: 1564800 training loss: 1.95858
Global Iter: 1564800 training acc: 0.09375
Global Iter: 1564900 training loss: 2.01692
Global Iter: 1564900 training acc: 0.15625
Global Iter: 1565000 training loss: 1.96274
Global Iter: 1565000 training acc: 0.125
Global Iter: 1565100 training loss: 1.88692
Global Iter: 1565100 training acc: 0.125
Global Iter: 1565200 training loss: 1.95627
Global Iter: 1565200 training acc: 0.21875
Global Iter: 1565300 training loss: 2.14591
Global Iter: 1565300 training acc: 0.09375
Global Iter: 1565400 training loss: 2.02745
Global Iter: 1565400 training acc: 0.25
Global Iter: 1565500 training loss: 2.01086
Global Iter: 1565500 training acc: 0.0625
Global Iter: 1565600 training loss: 2.02024
Global Iter: 1565600 training acc: 0.1875
Global Iter: 1565700 training loss: 1.97224
Global Iter: 1565700 training acc: 0.21875
Global Iter: 1565800 training loss: 1.9924
Global Iter: 1565800 training acc: 0.15625
Global Iter: 1565900 training loss: 1.98398
Global Iter: 1565900 training acc: 0.25
Global Iter: 1566000 training loss: 1.95835
Global Iter: 1566000 training acc: 0.125
Global Iter: 1566100 training loss: 1.9822
Global Iter: 1566100 training acc: 0.25
Global Iter: 1566200 training loss: 2.01741
Global Iter: 1566200 training acc: 0.21875
Global Iter: 1566300 training loss: 2.03433
Global Iter: 1566300 training acc: 0.25
Global Iter: 1566400 training loss: 1.90652
Global Iter: 1566400 training acc: 0.25
Global Iter: 1566500 training loss: 2.02371
Global Iter: 1566500 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1566531
Number of Patches: 56860
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1566531
Global Iter: 1566600 training loss: 1.96087
Global Iter: 1566600 training acc: 0.125
Global Iter: 1566700 training loss: 2.05086
Global Iter: 1566700 training acc: 0.1875
Global Iter: 1566800 training loss: 1.99253
Global Iter: 1566800 training acc: 0.15625
Global Iter: 1566900 training loss: 2.01159
Global Iter: 1566900 training acc: 0.03125
Global Iter: 1567000 training loss: 1.88996
Global Iter: 1567000 training acc: 0.1875
Global Iter: 1567100 training loss: 2.16431
Global Iter: 1567100 training acc: 0.125
Global Iter: 1567200 training loss: 2.0205
Global Iter: 1567200 training acc: 0.1875
Global Iter: 1567300 training loss: 1.93699
Global Iter: 1567300 training acc: 0.125
Global Iter: 1567400 training loss: 1.97981
Global Iter: 1567400 training acc: 0.0625
Global Iter: 1567500 training loss: 1.98295
Global Iter: 1567500 training acc: 0.09375
Global Iter: 1567600 training loss: 1.9069
Global Iter: 1567600 training acc: 0.125
Global Iter: 1567700 training 2017-06-23 01:46:25.696362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1570085
loss: 1.95333
Global Iter: 1567700 training acc: 0.25
Global Iter: 1567800 training loss: 1.95265
Global Iter: 1567800 training acc: 0.15625
Global Iter: 1567900 training loss: 2.0341
Global Iter: 1567900 training acc: 0.125
Global Iter: 1568000 training loss: 1.99273
Global Iter: 1568000 training acc: 0.125
Global Iter: 1568100 training loss: 1.92257
Global Iter: 1568100 training acc: 0.1875
Global Iter: 1568200 training loss: 1.92208
Global Iter: 1568200 training acc: 0.125
Global Iter: 1568300 training loss: 1.94154
Global Iter: 1568300 training acc: 0.25
Global Iter: 1568400 training loss: 1.94338
Global Iter: 1568400 training acc: 0.21875
Global Iter: 1568500 training loss: 1.96962
Global Iter: 1568500 training acc: 0.125
Global Iter: 1568600 training loss: 1.9619
Global Iter: 1568600 training acc: 0.15625
Global Iter: 1568700 training loss: 1.92101
Global Iter: 1568700 training acc: 0.15625
Global Iter: 1568800 training loss: 1.91851
Global Iter: 1568800 training acc: 0.125
Global Iter: 1568900 training loss: 1.92939
Global Iter: 1568900 training acc: 0.25
Global Iter: 1569000 training loss: 1.95619
Global Iter: 1569000 training acc: 0.1875
Global Iter: 1569100 training loss: 1.93597
Global Iter: 1569100 training acc: 0.15625
Global Iter: 1569200 training loss: 1.97262
Global Iter: 1569200 training acc: 0.125
Global Iter: 1569300 training loss: 1.94076
Global Iter: 1569300 training acc: 0.21875
Global Iter: 1569400 training loss: 2.04067
Global Iter: 1569400 training acc: 0.0625
Global Iter: 1569500 training loss: 2.07514
Global Iter: 1569500 training acc: 0.15625
Global Iter: 1569600 training loss: 1.97397
Global Iter: 1569600 training acc: 0.21875
Global Iter: 1569700 training loss: 2.01782
Global Iter: 1569700 training acc: 0.1875
Global Iter: 1569800 training loss: 1.97891
Global Iter: 1569800 training acc: 0.21875
Global Iter: 1569900 training loss: 2.00811
Global Iter: 1569900 training acc: 0.09375
Global Iter: 1570000 training loss: 2.17006
Global Iter: 1570000 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1570085
Number of Patches: 56292
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1570085
Global Iter: 1570100 training loss: 2.01339
Global Iter: 1570100 training acc: 0.1875
Global Iter: 1570200 training loss: 2.07138
Global Iter: 1570200 training acc: 0.09375
Global Iter: 1570300 training loss: 2.02306
Global Iter: 1570300 training acc: 0.21875
Global Iter: 1570400 training loss: 1.99048
Global Iter: 1570400 training acc: 0.15625
Global Iter: 1570500 training loss: 2.02873
Global Iter: 1570500 training acc: 0.21875
Global Iter: 1570600 training loss: 1.94114
Global Iter: 1570600 training acc: 0.15625
Global Iter: 1570700 training loss: 1.98028
Global Iter: 1570700 training acc: 0.15625
Global Iter: 1570800 training loss: 1.94905
Global Iter: 1570800 training acc: 0.09375
Global Iter: 1570900 training loss: 2.05909
Global Iter: 1570900 training acc: 0.15625
Global Iter: 1571000 training loss: 1.88937
Global Iter: 1571000 training acc: 0.1875
Global Iter: 1571100 training loss: 1.91329
Global Iter: 1571100 training acc: 0.125
Global Iter: 1571200 training loss: 2.02325
Global Iter: 1571200 training acc: 0.28125
Global Iter: 1571300 training loss: 2.01413
Global Iter: 1571300 training acc: 0.125
Global Iter: 1571400 training loss: 1.95136
Global Iter: 1571400 training acc: 0.15625
Global Iter: 1571500 training loss: 1.92172
Global Iter: 1571500 training acc: 0.21875
Global Iter: 1571600 training loss: 1.94125
Global Iter: 1571600 training acc: 0.09375
Global Iter: 1571700 training loss: 2.00138
Global Iter: 1571700 training acc: 0.15625
Global Iter: 1571800 training loss: 2.03799
Global Iter: 1571800 training acc: 0.15625
Global Iter: 1571900 training loss: 1.99599
Global Iter: 1571900 training acc: 0.1875
Global Iter: 1572000 training loss: 2.02885
Global Iter: 1572000 training acc: 0.21875
Global Iter: 1572100 training loss: 1.9884
Global Iter: 1572100 training acc: 0.1875
Global Iter: 1572200 tr2017-06-23 01:52:21.802118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1573604
aining loss: 2.07249
Global Iter: 1572200 training acc: 0.0625
Global Iter: 1572300 training loss: 2.08409
Global Iter: 1572300 training acc: 0.1875
Global Iter: 1572400 training loss: 2.01656
Global Iter: 1572400 training acc: 0.09375
Global Iter: 1572500 training loss: 1.99005
Global Iter: 1572500 training acc: 0.125
Global Iter: 1572600 training loss: 2.00834
Global Iter: 1572600 training acc: 0.21875
Global Iter: 1572700 training loss: 2.01616
Global Iter: 1572700 training acc: 0.1875
Global Iter: 1572800 training loss: 2.02469
Global Iter: 1572800 training acc: 0.1875
Global Iter: 1572900 training loss: 1.98439
Global Iter: 1572900 training acc: 0.15625
Global Iter: 1573000 training loss: 1.97758
Global Iter: 1573000 training acc: 0.125
Global Iter: 1573100 training loss: 2.00268
Global Iter: 1573100 training acc: 0.125
Global Iter: 1573200 training loss: 1.94337
Global Iter: 1573200 training acc: 0.15625
Global Iter: 1573300 training loss: 1.94354
Global Iter: 1573300 training acc: 0.1875
Global Iter: 1573400 training loss: 1.98493
Global Iter: 1573400 training acc: 0.125
Global Iter: 1573500 training loss: 2.1468
Global Iter: 1573500 training acc: 0.125
Global Iter: 1573600 training loss: 1.95314
Global Iter: 1573600 training acc: 0.28125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1573604
Number of Patches: 55730
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1573604
Global Iter: 1573700 training loss: 2.18184
Global Iter: 1573700 training acc: 0.125
Global Iter: 1573800 training loss: 1.97646
Global Iter: 1573800 training acc: 0.15625
Global Iter: 1573900 training loss: 1.96824
Global Iter: 1573900 training acc: 0.25
Global Iter: 1574000 training loss: 2.01813
Global Iter: 1574000 training acc: 0.125
Global Iter: 1574100 training loss: 2.00647
Global Iter: 1574100 training acc: 0.1875
Global Iter: 1574200 training loss: 2.07626
Global Iter: 1574200 training acc: 0.21875
Global Iter: 1574300 training loss: 1.95121
Global Iter: 1574300 training acc: 0.25
Global Iter: 1574400 training loss: 2.01076
Global Iter: 1574400 training acc: 0.09375
Global Iter: 1574500 training loss: 1.9575
Global Iter: 1574500 training acc: 0.125
Global Iter: 1574600 training loss: 2.0388
Global Iter: 1574600 training acc: 0.125
Global Iter: 1574700 training loss: 2.16195
Global Iter: 1574700 training acc: 0.125
Global Iter: 1574800 training loss: 2.01483
Global Iter: 1574800 training acc: 0.15625
Global Iter: 1574900 training loss: 2.0827
Global Iter: 1574900 training acc: 0.1875
Global Iter: 1575000 training loss: 1.9852
Global Iter: 1575000 training acc: 0.125
Global Iter: 1575100 training loss: 1.94188
Global Iter: 1575100 training acc: 0.15625
Global Iter: 1575200 training loss: 1.98226
Global Iter: 1575200 training acc: 0.1875
Global Iter: 1575300 training loss: 2.05345
Global Iter: 1575300 training acc: 0.1875
Global Iter: 1575400 training loss: 2.02602
Global Iter: 1575400 training acc: 0.09375
Global Iter: 1575500 training loss: 1.96766
Global Iter: 1575500 training acc: 0.1875
Global Iter: 1575600 training loss: 1.97643
Global Iter: 1575600 training acc: 0.21875
Global Iter: 1575700 training loss: 2.00414
Global Iter: 1575700 training acc: 0.28125
Global Iter: 1575800 training loss: 1.95918
Global Iter: 1575800 training acc: 0.21875
Global Iter: 1575900 training loss: 2.00907
Global Iter: 1575900 training acc: 0.0625
Global Iter: 1576000 training loss: 1.98681
Global Iter: 1576000 training acc: 0.15625
Global Iter: 1576100 training loss: 2.11978
Global Iter: 1576100 training acc: 0.125
Global Iter: 1576200 training loss: 2.0684
Global Iter: 1576200 training acc: 0.15625
Global Iter: 1576300 training loss: 1.89444
Global Iter: 1576300 training acc: 0.25
Global Iter: 1576400 training loss: 1.97933
Global Iter: 1576400 training acc: 0.15625
Global Iter: 1576500 training loss: 1.97646
Global Iter: 1576500 training acc: 0.21875
Global Iter: 1576600 training loss: 1.96099
Global Iter: 1576600 training acc: 0.09375
Global Iter: 1576700 training 2017-06-23 01:58:16.784222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1577088
2017-06-23 02:04:07.751547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1580537
loss: 2.00712
Global Iter: 1576700 training acc: 0.125
Global Iter: 1576800 training loss: 2.10137
Global Iter: 1576800 training acc: 0.0625
Global Iter: 1576900 training loss: 2.05124
Global Iter: 1576900 training acc: 0.25
Global Iter: 1577000 training loss: 2.08782
Global Iter: 1577000 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1577088
Number of Patches: 55173
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1577088
Global Iter: 1577100 training loss: 1.97095
Global Iter: 1577100 training acc: 0.125
Global Iter: 1577200 training loss: 1.96654
Global Iter: 1577200 training acc: 0.1875
Global Iter: 1577300 training loss: 2.1113
Global Iter: 1577300 training acc: 0.15625
Global Iter: 1577400 training loss: 1.98675
Global Iter: 1577400 training acc: 0.1875
Global Iter: 1577500 training loss: 1.9222
Global Iter: 1577500 training acc: 0.21875
Global Iter: 1577600 training loss: 1.93636
Global Iter: 1577600 training acc: 0.15625
Global Iter: 1577700 training loss: 1.98189
Global Iter: 1577700 training acc: 0.21875
Global Iter: 1577800 training loss: 1.91432
Global Iter: 1577800 training acc: 0.21875
Global Iter: 1577900 training loss: 1.9977
Global Iter: 1577900 training acc: 0.15625
Global Iter: 1578000 training loss: 1.96328
Global Iter: 1578000 training acc: 0.125
Global Iter: 1578100 training loss: 2.00817
Global Iter: 1578100 training acc: 0.0625
Global Iter: 1578200 training loss: 1.94926
Global Iter: 1578200 training acc: 0.21875
Global Iter: 1578300 training loss: 1.98542
Global Iter: 1578300 training acc: 0.125
Global Iter: 1578400 training loss: 2.06463
Global Iter: 1578400 training acc: 0.25
Global Iter: 1578500 training loss: 1.98233
Global Iter: 1578500 training acc: 0.1875
Global Iter: 1578600 training loss: 1.99473
Global Iter: 1578600 training acc: 0.15625
Global Iter: 1578700 training loss: 2.00881
Global Iter: 1578700 training acc: 0.125
Global Iter: 1578800 training loss: 2.01344
Global Iter: 1578800 training acc: 0.15625
Global Iter: 1578900 training loss: 2.00874
Global Iter: 1578900 training acc: 0.21875
Global Iter: 1579000 training loss: 2.01161
Global Iter: 1579000 training acc: 0.25
Global Iter: 1579100 training loss: 2.0068
Global Iter: 1579100 training acc: 0.21875
Global Iter: 1579200 training loss: 1.95899
Global Iter: 1579200 training acc: 0.25
Global Iter: 1579300 training loss: 1.98348
Global Iter: 1579300 training acc: 0.1875
Global Iter: 1579400 training loss: 1.99755
Global Iter: 1579400 training acc: 0.21875
Global Iter: 1579500 training loss: 2.08857
Global Iter: 1579500 training acc: 0.21875
Global Iter: 1579600 training loss: 2.05013
Global Iter: 1579600 training acc: 0.3125
Global Iter: 1579700 training loss: 1.99894
Global Iter: 1579700 training acc: 0.21875
Global Iter: 1579800 training loss: 2.01164
Global Iter: 1579800 training acc: 0.21875
Global Iter: 1579900 training loss: 1.92448
Global Iter: 1579900 training acc: 0.21875
Global Iter: 1580000 training loss: 2.04422
Global Iter: 1580000 training acc: 0.125
Global Iter: 1580100 training loss: 1.97095
Global Iter: 1580100 training acc: 0.21875
Global Iter: 1580200 training loss: 1.98007
Global Iter: 1580200 training acc: 0.15625
Global Iter: 1580300 training loss: 1.97097
Global Iter: 1580300 training acc: 0.15625
Global Iter: 1580400 training loss: 1.98435
Global Iter: 1580400 training acc: 0.125
Global Iter: 1580500 training loss: 1.94776
Global Iter: 1580500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1580537
Number of Patches: 54622
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1580537
Global Iter: 1580600 training loss: 1.91793
Global Iter: 1580600 training acc: 0.25
Global Iter: 1580700 training loss: 1.96133
Global Iter: 1580700 training acc: 0.1875
Global Iter: 1580800 training loss: 1.97212
Global Iter: 1580800 training acc: 0.15625
Global Iter: 1580900 training loss: 2.04734
Global Iter: 1580900 tra2017-06-23 02:09:49.561712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1583951
ining acc: 0.25
Global Iter: 1581000 training loss: 1.96524
Global Iter: 1581000 training acc: 0.15625
Global Iter: 1581100 training loss: 2.02564
Global Iter: 1581100 training acc: 0.0625
Global Iter: 1581200 training loss: 1.94628
Global Iter: 1581200 training acc: 0.15625
Global Iter: 1581300 training loss: 2.02131
Global Iter: 1581300 training acc: 0.15625
Global Iter: 1581400 training loss: 1.99464
Global Iter: 1581400 training acc: 0.125
Global Iter: 1581500 training loss: 2.01807
Global Iter: 1581500 training acc: 0.25
Global Iter: 1581600 training loss: 1.97996
Global Iter: 1581600 training acc: 0.15625
Global Iter: 1581700 training loss: 2.02911
Global Iter: 1581700 training acc: 0.21875
Global Iter: 1581800 training loss: 1.93332
Global Iter: 1581800 training acc: 0.25
Global Iter: 1581900 training loss: 1.90515
Global Iter: 1581900 training acc: 0.375
Global Iter: 1582000 training loss: 1.89236
Global Iter: 1582000 training acc: 0.0625
Global Iter: 1582100 training loss: 1.9521
Global Iter: 1582100 training acc: 0.15625
Global Iter: 1582200 training loss: 1.9509
Global Iter: 1582200 training acc: 0.25
Global Iter: 1582300 training loss: 2.0279
Global Iter: 1582300 training acc: 0.125
Global Iter: 1582400 training loss: 1.83931
Global Iter: 1582400 training acc: 0.3125
Global Iter: 1582500 training loss: 1.94318
Global Iter: 1582500 training acc: 0.1875
Global Iter: 1582600 training loss: 2.12973
Global Iter: 1582600 training acc: 0.0
Global Iter: 1582700 training loss: 2.07315
Global Iter: 1582700 training acc: 0.21875
Global Iter: 1582800 training loss: 2.00702
Global Iter: 1582800 training acc: 0.09375
Global Iter: 1582900 training loss: 2.09214
Global Iter: 1582900 training acc: 0.21875
Global Iter: 1583000 training loss: 1.96373
Global Iter: 1583000 training acc: 0.15625
Global Iter: 1583100 training loss: 2.04193
Global Iter: 1583100 training acc: 0.21875
Global Iter: 1583200 training loss: 1.9256
Global Iter: 1583200 training acc: 0.25
Global Iter: 1583300 training loss: 2.04035
Global Iter: 1583300 training acc: 0.15625
Global Iter: 1583400 training loss: 2.01541
Global Iter: 1583400 training acc: 0.15625
Global Iter: 1583500 training loss: 1.88485
Global Iter: 1583500 training acc: 0.21875
Global Iter: 1583600 training loss: 2.04273
Global Iter: 1583600 training acc: 0.125
Global Iter: 1583700 training loss: 2.13554
Global Iter: 1583700 training acc: 0.09375
Global Iter: 1583800 training loss: 1.87841
Global Iter: 1583800 training acc: 0.25
Global Iter: 1583900 training loss: 1.97962
Global Iter: 1583900 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1583951
Number of Patches: 54076
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1583951
Global Iter: 1584000 training loss: 2.0495
Global Iter: 1584000 training acc: 0.1875
Global Iter: 1584100 training loss: 1.98121
Global Iter: 1584100 training acc: 0.25
Global Iter: 1584200 training loss: 1.95652
Global Iter: 1584200 training acc: 0.21875
Global Iter: 1584300 training loss: 1.92854
Global Iter: 1584300 training acc: 0.25
Global Iter: 1584400 training loss: 1.98506
Global Iter: 1584400 training acc: 0.34375
Global Iter: 1584500 training loss: 1.97038
Global Iter: 1584500 training acc: 0.3125
Global Iter: 1584600 training loss: 1.93643
Global Iter: 1584600 training acc: 0.09375
Global Iter: 1584700 training loss: 2.03343
Global Iter: 1584700 training acc: 0.09375
Global Iter: 1584800 training loss: 1.95476
Global Iter: 1584800 training acc: 0.1875
Global Iter: 1584900 training loss: 1.89055
Global Iter: 1584900 training acc: 0.25
Global Iter: 1585000 training loss: 2.06729
Global Iter: 1585000 training acc: 0.09375
Global Iter: 1585100 training loss: 2.07087
Global Iter: 1585100 training acc: 0.125
Global Iter: 1585200 training loss: 2.02291
Global Iter: 1585200 training acc: 0.15625
Global Iter: 1585300 training loss: 2.05488
Global Iter: 1585300 training acc: 0.25
Global Iter: 1585400 training loss: 1.93627
Global Iter: 1585400 training acc: 0.1562017-06-23 02:15:28.777809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1587331
25
Global Iter: 1585500 training loss: 1.95013
Global Iter: 1585500 training acc: 0.15625
Global Iter: 1585600 training loss: 2.01323
Global Iter: 1585600 training acc: 0.0625
Global Iter: 1585700 training loss: 2.05533
Global Iter: 1585700 training acc: 0.09375
Global Iter: 1585800 training loss: 2.0506
Global Iter: 1585800 training acc: 0.15625
Global Iter: 1585900 training loss: 1.86009
Global Iter: 1585900 training acc: 0.15625
Global Iter: 1586000 training loss: 2.02501
Global Iter: 1586000 training acc: 0.1875
Global Iter: 1586100 training loss: 1.94017
Global Iter: 1586100 training acc: 0.1875
Global Iter: 1586200 training loss: 2.06688
Global Iter: 1586200 training acc: 0.21875
Global Iter: 1586300 training loss: 1.94271
Global Iter: 1586300 training acc: 0.15625
Global Iter: 1586400 training loss: 2.01338
Global Iter: 1586400 training acc: 0.0625
Global Iter: 1586500 training loss: 1.98729
Global Iter: 1586500 training acc: 0.21875
Global Iter: 1586600 training loss: 2.0516
Global Iter: 1586600 training acc: 0.21875
Global Iter: 1586700 training loss: 1.99366
Global Iter: 1586700 training acc: 0.1875
Global Iter: 1586800 training loss: 2.05623
Global Iter: 1586800 training acc: 0.15625
Global Iter: 1586900 training loss: 1.99137
Global Iter: 1586900 training acc: 0.3125
Global Iter: 1587000 training loss: 2.12594
Global Iter: 1587000 training acc: 0.09375
Global Iter: 1587100 training loss: 1.97916
Global Iter: 1587100 training acc: 0.1875
Global Iter: 1587200 training loss: 2.01549
Global Iter: 1587200 training acc: 0.09375
Global Iter: 1587300 training loss: 1.97313
Global Iter: 1587300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1587331
Number of Patches: 53536
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1587331
Global Iter: 1587400 training loss: 1.94107
Global Iter: 1587400 training acc: 0.1875
Global Iter: 1587500 training loss: 2.08086
Global Iter: 1587500 training acc: 0.0625
Global Iter: 1587600 training loss: 1.98266
Global Iter: 1587600 training acc: 0.125
Global Iter: 1587700 training loss: 2.02944
Global Iter: 1587700 training acc: 0.25
Global Iter: 1587800 training loss: 1.9591
Global Iter: 1587800 training acc: 0.25
Global Iter: 1587900 training loss: 2.01006
Global Iter: 1587900 training acc: 0.09375
Global Iter: 1588000 training loss: 2.07806
Global Iter: 1588000 training acc: 0.125
Global Iter: 1588100 training loss: 1.9761
Global Iter: 1588100 training acc: 0.15625
Global Iter: 1588200 training loss: 1.96243
Global Iter: 1588200 training acc: 0.3125
Global Iter: 1588300 training loss: 1.94032
Global Iter: 1588300 training acc: 0.28125
Global Iter: 1588400 training loss: 1.96153
Global Iter: 1588400 training acc: 0.15625
Global Iter: 1588500 training loss: 1.9441
Global Iter: 1588500 training acc: 0.15625
Global Iter: 1588600 training loss: 1.96125
Global Iter: 1588600 training acc: 0.09375
Global Iter: 1588700 training loss: 2.11456
Global Iter: 1588700 training acc: 0.09375
Global Iter: 1588800 training loss: 2.05245
Global Iter: 1588800 training acc: 0.21875
Global Iter: 1588900 training loss: 2.11404
Global Iter: 1588900 training acc: 0.1875
Global Iter: 1589000 training loss: 1.87853
Global Iter: 1589000 training acc: 0.375
Global Iter: 1589100 training loss: 1.96746
Global Iter: 1589100 training acc: 0.1875
Global Iter: 1589200 training loss: 1.95684
Global Iter: 1589200 training acc: 0.21875
Global Iter: 1589300 training loss: 1.9339
Global Iter: 1589300 training acc: 0.1875
Global Iter: 1589400 training loss: 2.01753
Global Iter: 1589400 training acc: 0.21875
Global Iter: 1589500 training loss: 1.94642
Global Iter: 1589500 training acc: 0.21875
Global Iter: 1589600 training loss: 2.04762
Global Iter: 1589600 training acc: 0.09375
Global Iter: 1589700 training loss: 2.10231
Global Iter: 1589700 training acc: 0.1875
Global Iter: 1589800 training loss: 1.96397
Global Iter: 1589800 training acc: 0.21875
Global Iter: 1589900 training loss: 2.09388
Global Iter: 1589900 training2017-06-23 02:21:00.176328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1590677
2017-06-23 02:26:32.875647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1593990
 acc: 0.1875
Global Iter: 1590000 training loss: 1.93502
Global Iter: 1590000 training acc: 0.125
Global Iter: 1590100 training loss: 1.98489
Global Iter: 1590100 training acc: 0.15625
Global Iter: 1590200 training loss: 1.99422
Global Iter: 1590200 training acc: 0.125
Global Iter: 1590300 training loss: 1.94831
Global Iter: 1590300 training acc: 0.25
Global Iter: 1590400 training loss: 1.94863
Global Iter: 1590400 training acc: 0.1875
Global Iter: 1590500 training loss: 1.96589
Global Iter: 1590500 training acc: 0.125
Global Iter: 1590600 training loss: 2.11427
Global Iter: 1590600 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1590677
Number of Patches: 53001
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1590677
Global Iter: 1590700 training loss: 2.07554
Global Iter: 1590700 training acc: 0.09375
Global Iter: 1590800 training loss: 2.00116
Global Iter: 1590800 training acc: 0.15625
Global Iter: 1590900 training loss: 2.0245
Global Iter: 1590900 training acc: 0.15625
Global Iter: 1591000 training loss: 1.95695
Global Iter: 1591000 training acc: 0.125
Global Iter: 1591100 training loss: 1.87323
Global Iter: 1591100 training acc: 0.3125
Global Iter: 1591200 training loss: 1.99357
Global Iter: 1591200 training acc: 0.09375
Global Iter: 1591300 training loss: 1.96683
Global Iter: 1591300 training acc: 0.1875
Global Iter: 1591400 training loss: 1.99275
Global Iter: 1591400 training acc: 0.25
Global Iter: 1591500 training loss: 2.07647
Global Iter: 1591500 training acc: 0.125
Global Iter: 1591600 training loss: 2.07813
Global Iter: 1591600 training acc: 0.09375
Global Iter: 1591700 training loss: 1.94536
Global Iter: 1591700 training acc: 0.1875
Global Iter: 1591800 training loss: 2.0427
Global Iter: 1591800 training acc: 0.1875
Global Iter: 1591900 training loss: 2.00105
Global Iter: 1591900 training acc: 0.09375
Global Iter: 1592000 training loss: 2.04201
Global Iter: 1592000 training acc: 0.1875
Global Iter: 1592100 training loss: 1.92502
Global Iter: 1592100 training acc: 0.125
Global Iter: 1592200 training loss: 2.06945
Global Iter: 1592200 training acc: 0.15625
Global Iter: 1592300 training loss: 2.05614
Global Iter: 1592300 training acc: 0.09375
Global Iter: 1592400 training loss: 2.07379
Global Iter: 1592400 training acc: 0.21875
Global Iter: 1592500 training loss: 2.09033
Global Iter: 1592500 training acc: 0.15625
Global Iter: 1592600 training loss: 2.13183
Global Iter: 1592600 training acc: 0.21875
Global Iter: 1592700 training loss: 1.96571
Global Iter: 1592700 training acc: 0.1875
Global Iter: 1592800 training loss: 2.04313
Global Iter: 1592800 training acc: 0.0625
Global Iter: 1592900 training loss: 2.00671
Global Iter: 1592900 training acc: 0.1875
Global Iter: 1593000 training loss: 2.14588
Global Iter: 1593000 training acc: 0.15625
Global Iter: 1593100 training loss: 1.9991
Global Iter: 1593100 training acc: 0.125
Global Iter: 1593200 training loss: 1.9269
Global Iter: 1593200 training acc: 0.25
Global Iter: 1593300 training loss: 1.99783
Global Iter: 1593300 training acc: 0.25
Global Iter: 1593400 training loss: 2.00166
Global Iter: 1593400 training acc: 0.34375
Global Iter: 1593500 training loss: 2.05327
Global Iter: 1593500 training acc: 0.15625
Global Iter: 1593600 training loss: 2.05599
Global Iter: 1593600 training acc: 0.15625
Global Iter: 1593700 training loss: 2.0501
Global Iter: 1593700 training acc: 0.0625
Global Iter: 1593800 training loss: 1.93444
Global Iter: 1593800 training acc: 0.40625
Global Iter: 1593900 training loss: 1.96258
Global Iter: 1593900 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1593990
Number of Patches: 52471
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1593990
Global Iter: 1594000 training loss: 1.98486
Global Iter: 1594000 training acc: 0.125
Global Iter: 1594100 training loss: 1.94313
Global Iter: 1594100 training acc: 0.1875
Global Iter: 1594200 trai2017-06-23 02:32:03.898825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1597270
ning loss: 2.07232
Global Iter: 1594200 training acc: 0.21875
Global Iter: 1594300 training loss: 2.0151
Global Iter: 1594300 training acc: 0.0625
Global Iter: 1594400 training loss: 1.97986
Global Iter: 1594400 training acc: 0.1875
Global Iter: 1594500 training loss: 1.98923
Global Iter: 1594500 training acc: 0.1875
Global Iter: 1594600 training loss: 1.92254
Global Iter: 1594600 training acc: 0.21875
Global Iter: 1594700 training loss: 1.9774
Global Iter: 1594700 training acc: 0.34375
Global Iter: 1594800 training loss: 2.05218
Global Iter: 1594800 training acc: 0.125
Global Iter: 1594900 training loss: 1.98777
Global Iter: 1594900 training acc: 0.15625
Global Iter: 1595000 training loss: 1.92225
Global Iter: 1595000 training acc: 0.1875
Global Iter: 1595100 training loss: 2.03387
Global Iter: 1595100 training acc: 0.125
Global Iter: 1595200 training loss: 2.12112
Global Iter: 1595200 training acc: 0.09375
Global Iter: 1595300 training loss: 1.99325
Global Iter: 1595300 training acc: 0.125
Global Iter: 1595400 training loss: 2.02183
Global Iter: 1595400 training acc: 0.15625
Global Iter: 1595500 training loss: 2.00717
Global Iter: 1595500 training acc: 0.125
Global Iter: 1595600 training loss: 1.96078
Global Iter: 1595600 training acc: 0.125
Global Iter: 1595700 training loss: 2.1315
Global Iter: 1595700 training acc: 0.09375
Global Iter: 1595800 training loss: 1.9863
Global Iter: 1595800 training acc: 0.3125
Global Iter: 1595900 training loss: 1.9401
Global Iter: 1595900 training acc: 0.15625
Global Iter: 1596000 training loss: 1.963
Global Iter: 1596000 training acc: 0.1875
Global Iter: 1596100 training loss: 1.94335
Global Iter: 1596100 training acc: 0.21875
Global Iter: 1596200 training loss: 1.95126
Global Iter: 1596200 training acc: 0.15625
Global Iter: 1596300 training loss: 1.94779
Global Iter: 1596300 training acc: 0.21875
Global Iter: 1596400 training loss: 1.97623
Global Iter: 1596400 training acc: 0.125
Global Iter: 1596500 training loss: 1.92784
Global Iter: 1596500 training acc: 0.25
Global Iter: 1596600 training loss: 1.90327
Global Iter: 1596600 training acc: 0.34375
Global Iter: 1596700 training loss: 2.07452
Global Iter: 1596700 training acc: 0.1875
Global Iter: 1596800 training loss: 1.89205
Global Iter: 1596800 training acc: 0.21875
Global Iter: 1596900 training loss: 1.89546
Global Iter: 1596900 training acc: 0.25
Global Iter: 1597000 training loss: 2.08492
Global Iter: 1597000 training acc: 0.125
Global Iter: 1597100 training loss: 2.00475
Global Iter: 1597100 training acc: 0.28125
Global Iter: 1597200 training loss: 2.1589
Global Iter: 1597200 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1597270
Number of Patches: 51947
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1597270
Global Iter: 1597300 training loss: 2.07003
Global Iter: 1597300 training acc: 0.125
Global Iter: 1597400 training loss: 2.04778
Global Iter: 1597400 training acc: 0.125
Global Iter: 1597500 training loss: 1.96912
Global Iter: 1597500 training acc: 0.1875
Global Iter: 1597600 training loss: 1.9242
Global Iter: 1597600 training acc: 0.1875
Global Iter: 1597700 training loss: 1.91162
Global Iter: 1597700 training acc: 0.28125
Global Iter: 1597800 training loss: 2.00078
Global Iter: 1597800 training acc: 0.1875
Global Iter: 1597900 training loss: 2.00794
Global Iter: 1597900 training acc: 0.25
Global Iter: 1598000 training loss: 2.06953
Global Iter: 1598000 training acc: 0.1875
Global Iter: 1598100 training loss: 1.92979
Global Iter: 1598100 training acc: 0.09375
Global Iter: 1598200 training loss: 2.08444
Global Iter: 1598200 training acc: 0.0625
Global Iter: 1598300 training loss: 2.01213
Global Iter: 1598300 training acc: 0.1875
Global Iter: 1598400 training loss: 1.9583
Global Iter: 1598400 training acc: 0.21875
Global Iter: 1598500 training loss: 1.96934
Global Iter: 1598500 training acc: 0.1875
Global Iter: 1598600 training loss: 1.85471
Global Iter: 1598600 training acc: 0.25
Global Iter: 1598700 training loss: 2017-06-23 02:37:31.080124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1600517
1.95026
Global Iter: 1598700 training acc: 0.15625
Global Iter: 1598800 training loss: 2.0075
Global Iter: 1598800 training acc: 0.125
Global Iter: 1598900 training loss: 1.98625
Global Iter: 1598900 training acc: 0.1875
Global Iter: 1599000 training loss: 1.95502
Global Iter: 1599000 training acc: 0.1875
Global Iter: 1599100 training loss: 1.94936
Global Iter: 1599100 training acc: 0.09375
Global Iter: 1599200 training loss: 1.97478
Global Iter: 1599200 training acc: 0.34375
Global Iter: 1599300 training loss: 1.9455
Global Iter: 1599300 training acc: 0.1875
Global Iter: 1599400 training loss: 1.98115
Global Iter: 1599400 training acc: 0.15625
Global Iter: 1599500 training loss: 2.08824
Global Iter: 1599500 training acc: 0.0625
Global Iter: 1599600 training loss: 1.95204
Global Iter: 1599600 training acc: 0.25
Global Iter: 1599700 training loss: 1.97653
Global Iter: 1599700 training acc: 0.15625
Global Iter: 1599800 training loss: 1.99054
Global Iter: 1599800 training acc: 0.1875
Global Iter: 1599900 training loss: 2.09647
Global Iter: 1599900 training acc: 0.28125
Global Iter: 1600000 training loss: 1.91782
Global Iter: 1600000 training acc: 0.21875
Global Iter: 1600100 training loss: 1.93719
Global Iter: 1600100 training acc: 0.0625
Global Iter: 1600200 training loss: 1.95866
Global Iter: 1600200 training acc: 0.1875
Global Iter: 1600300 training loss: 1.9769
Global Iter: 1600300 training acc: 0.15625
Global Iter: 1600400 training loss: 1.97064
Global Iter: 1600400 training acc: 0.125
Global Iter: 1600500 training loss: 1.98145
Global Iter: 1600500 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1600517
Number of Patches: 51428
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1600517
Global Iter: 1600600 training loss: 2.05562
Global Iter: 1600600 training acc: 0.15625
Global Iter: 1600700 training loss: 2.01355
Global Iter: 1600700 training acc: 0.1875
Global Iter: 1600800 training loss: 2.05247
Global Iter: 1600800 training acc: 0.1875
Global Iter: 1600900 training loss: 2.03061
Global Iter: 1600900 training acc: 0.1875
Global Iter: 1601000 training loss: 2.16782
Global Iter: 1601000 training acc: 0.09375
Global Iter: 1601100 training loss: 1.99148
Global Iter: 1601100 training acc: 0.21875
Global Iter: 1601200 training loss: 2.09376
Global Iter: 1601200 training acc: 0.09375
Global Iter: 1601300 training loss: 1.99339
Global Iter: 1601300 training acc: 0.3125
Global Iter: 1601400 training loss: 1.917
Global Iter: 1601400 training acc: 0.15625
Global Iter: 1601500 training loss: 1.96391
Global Iter: 1601500 training acc: 0.25
Global Iter: 1601600 training loss: 1.96409
Global Iter: 1601600 training acc: 0.03125
Global Iter: 1601700 training loss: 2.03823
Global Iter: 1601700 training acc: 0.0625
Global Iter: 1601800 training loss: 1.98008
Global Iter: 1601800 training acc: 0.15625
Global Iter: 1601900 training loss: 2.00542
Global Iter: 1601900 training acc: 0.15625
Global Iter: 1602000 training loss: 1.93219
Global Iter: 1602000 training acc: 0.375
Global Iter: 1602100 training loss: 2.1225
Global Iter: 1602100 training acc: 0.1875
Global Iter: 1602200 training loss: 2.05839
Global Iter: 1602200 training acc: 0.1875
Global Iter: 1602300 training loss: 1.94909
Global Iter: 1602300 training acc: 0.15625
Global Iter: 1602400 training loss: 2.017
Global Iter: 1602400 training acc: 0.0625
Global Iter: 1602500 training loss: 2.03303
Global Iter: 1602500 training acc: 0.21875
Global Iter: 1602600 training loss: 2.05877
Global Iter: 1602600 training acc: 0.1875
Global Iter: 1602700 training loss: 2.0139
Global Iter: 1602700 training acc: 0.21875
Global Iter: 1602800 training loss: 1.986
Global Iter: 1602800 training acc: 0.21875
Global Iter: 1602900 training loss: 1.92911
Global Iter: 1602900 training acc: 0.125
Global Iter: 1603000 training loss: 1.91735
Global Iter: 1603000 training acc: 0.09375
Global Iter: 1603100 training loss: 2.01825
Global Iter: 1603100 training acc: 0.21875
Global Iter: 1603200 training loss2017-06-23 02:42:54.909190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1603732
2017-06-23 02:48:15.645624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1606915
: 2.04531
Global Iter: 1603200 training acc: 0.0625
Global Iter: 1603300 training loss: 2.0509
Global Iter: 1603300 training acc: 0.125
Global Iter: 1603400 training loss: 1.96795
Global Iter: 1603400 training acc: 0.1875
Global Iter: 1603500 training loss: 1.95958
Global Iter: 1603500 training acc: 0.21875
Global Iter: 1603600 training loss: 2.01694
Global Iter: 1603600 training acc: 0.125
Global Iter: 1603700 training loss: 2.0446
Global Iter: 1603700 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1603732
Number of Patches: 50914
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1603732
Global Iter: 1603800 training loss: 2.05736
Global Iter: 1603800 training acc: 0.15625
Global Iter: 1603900 training loss: 1.8937
Global Iter: 1603900 training acc: 0.21875
Global Iter: 1604000 training loss: 2.01911
Global Iter: 1604000 training acc: 0.25
Global Iter: 1604100 training loss: 1.97106
Global Iter: 1604100 training acc: 0.1875
Global Iter: 1604200 training loss: 1.98985
Global Iter: 1604200 training acc: 0.125
Global Iter: 1604300 training loss: 1.93202
Global Iter: 1604300 training acc: 0.1875
Global Iter: 1604400 training loss: 1.96484
Global Iter: 1604400 training acc: 0.03125
Global Iter: 1604500 training loss: 2.0225
Global Iter: 1604500 training acc: 0.21875
Global Iter: 1604600 training loss: 2.03786
Global Iter: 1604600 training acc: 0.15625
Global Iter: 1604700 training loss: 2.05777
Global Iter: 1604700 training acc: 0.1875
Global Iter: 1604800 training loss: 1.93842
Global Iter: 1604800 training acc: 0.15625
Global Iter: 1604900 training loss: 2.14218
Global Iter: 1604900 training acc: 0.1875
Global Iter: 1605000 training loss: 1.9917
Global Iter: 1605000 training acc: 0.09375
Global Iter: 1605100 training loss: 2.07138
Global Iter: 1605100 training acc: 0.03125
Global Iter: 1605200 training loss: 1.88867
Global Iter: 1605200 training acc: 0.28125
Global Iter: 1605300 training loss: 1.89165
Global Iter: 1605300 training acc: 0.125
Global Iter: 1605400 training loss: 1.98331
Global Iter: 1605400 training acc: 0.09375
Global Iter: 1605500 training loss: 2.03413
Global Iter: 1605500 training acc: 0.15625
Global Iter: 1605600 training loss: 1.9784
Global Iter: 1605600 training acc: 0.1875
Global Iter: 1605700 training loss: 2.03682
Global Iter: 1605700 training acc: 0.15625
Global Iter: 1605800 training loss: 1.94916
Global Iter: 1605800 training acc: 0.3125
Global Iter: 1605900 training loss: 1.953
Global Iter: 1605900 training acc: 0.15625
Global Iter: 1606000 training loss: 1.98233
Global Iter: 1606000 training acc: 0.1875
Global Iter: 1606100 training loss: 2.00099
Global Iter: 1606100 training acc: 0.375
Global Iter: 1606200 training loss: 2.00565
Global Iter: 1606200 training acc: 0.21875
Global Iter: 1606300 training loss: 2.08877
Global Iter: 1606300 training acc: 0.1875
Global Iter: 1606400 training loss: 1.97759
Global Iter: 1606400 training acc: 0.21875
Global Iter: 1606500 training loss: 1.93205
Global Iter: 1606500 training acc: 0.28125
Global Iter: 1606600 training loss: 1.91783
Global Iter: 1606600 training acc: 0.25
Global Iter: 1606700 training loss: 1.94792
Global Iter: 1606700 training acc: 0.09375
Global Iter: 1606800 training loss: 1.97226
Global Iter: 1606800 training acc: 0.125
Global Iter: 1606900 training loss: 1.94005
Global Iter: 1606900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1606915
Number of Patches: 50405
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1606915
Global Iter: 1607000 training loss: 1.98985
Global Iter: 1607000 training acc: 0.21875
Global Iter: 1607100 training loss: 1.98931
Global Iter: 1607100 training acc: 0.1875
Global Iter: 1607200 training loss: 2.07627
Global Iter: 1607200 training acc: 0.15625
Global Iter: 1607300 training loss: 1.99267
Global Iter: 1607300 training acc: 0.0625
Global Iter: 1607400 training loss: 2.02208
Global Iter: 1607400 trai2017-06-23 02:53:29.164158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1610066
ning acc: 0.15625
Global Iter: 1607500 training loss: 1.99169
Global Iter: 1607500 training acc: 0.34375
Global Iter: 1607600 training loss: 1.98145
Global Iter: 1607600 training acc: 0.15625
Global Iter: 1607700 training loss: 2.11329
Global Iter: 1607700 training acc: 0.28125
Global Iter: 1607800 training loss: 1.88364
Global Iter: 1607800 training acc: 0.25
Global Iter: 1607900 training loss: 2.06055
Global Iter: 1607900 training acc: 0.0625
Global Iter: 1608000 training loss: 2.07357
Global Iter: 1608000 training acc: 0.09375
Global Iter: 1608100 training loss: 2.09851
Global Iter: 1608100 training acc: 0.1875
Global Iter: 1608200 training loss: 2.14643
Global Iter: 1608200 training acc: 0.15625
Global Iter: 1608300 training loss: 1.9743
Global Iter: 1608300 training acc: 0.125
Global Iter: 1608400 training loss: 2.02186
Global Iter: 1608400 training acc: 0.15625
Global Iter: 1608500 training loss: 2.12812
Global Iter: 1608500 training acc: 0.3125
Global Iter: 1608600 training loss: 1.98719
Global Iter: 1608600 training acc: 0.21875
Global Iter: 1608700 training loss: 2.01343
Global Iter: 1608700 training acc: 0.21875
Global Iter: 1608800 training loss: 2.04433
Global Iter: 1608800 training acc: 0.1875
Global Iter: 1608900 training loss: 1.95152
Global Iter: 1608900 training acc: 0.21875
Global Iter: 1609000 training loss: 2.0038
Global Iter: 1609000 training acc: 0.1875
Global Iter: 1609100 training loss: 1.97534
Global Iter: 1609100 training acc: 0.0625
Global Iter: 1609200 training loss: 1.85771
Global Iter: 1609200 training acc: 0.28125
Global Iter: 1609300 training loss: 2.10163
Global Iter: 1609300 training acc: 0.125
Global Iter: 1609400 training loss: 1.91724
Global Iter: 1609400 training acc: 0.125
Global Iter: 1609500 training loss: 1.99065
Global Iter: 1609500 training acc: 0.125
Global Iter: 1609600 training loss: 2.05095
Global Iter: 1609600 training acc: 0.15625
Global Iter: 1609700 training loss: 1.95943
Global Iter: 1609700 training acc: 0.15625
Global Iter: 1609800 training loss: 1.92748
Global Iter: 1609800 training acc: 0.28125
Global Iter: 1609900 training loss: 1.99029
Global Iter: 1609900 training acc: 0.125
Global Iter: 1610000 training loss: 1.978
Global Iter: 1610000 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1610066
Number of Patches: 49901
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1610066
Global Iter: 1610100 training loss: 1.91481
Global Iter: 1610100 training acc: 0.34375
Global Iter: 1610200 training loss: 1.93841
Global Iter: 1610200 training acc: 0.21875
Global Iter: 1610300 training loss: 2.00042
Global Iter: 1610300 training acc: 0.21875
Global Iter: 1610400 training loss: 1.98539
Global Iter: 1610400 training acc: 0.15625
Global Iter: 1610500 training loss: 2.01745
Global Iter: 1610500 training acc: 0.1875
Global Iter: 1610600 training loss: 2.00591
Global Iter: 1610600 training acc: 0.15625
Global Iter: 1610700 training loss: 1.95443
Global Iter: 1610700 training acc: 0.1875
Global Iter: 1610800 training loss: 2.04948
Global Iter: 1610800 training acc: 0.09375
Global Iter: 1610900 training loss: 1.94399
Global Iter: 1610900 training acc: 0.21875
Global Iter: 1611000 training loss: 1.91493
Global Iter: 1611000 training acc: 0.1875
Global Iter: 1611100 training loss: 1.93814
Global Iter: 1611100 training acc: 0.21875
Global Iter: 1611200 training loss: 1.97426
Global Iter: 1611200 training acc: 0.09375
Global Iter: 1611300 training loss: 2.01059
Global Iter: 1611300 training acc: 0.125
Global Iter: 1611400 training loss: 1.90373
Global Iter: 1611400 training acc: 0.15625
Global Iter: 1611500 training loss: 1.98656
Global Iter: 1611500 training acc: 0.15625
Global Iter: 1611600 training loss: 1.95729
Global Iter: 1611600 training acc: 0.28125
Global Iter: 1611700 training loss: 1.98188
Global Iter: 1611700 training acc: 0.375
Global Iter: 1611800 training loss: 1.98811
Global Iter: 1611800 training acc: 0.28125
Global Iter: 1611900 training loss: 1.96808
Global Iter: 162017-06-23 02:58:41.069094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1613185
2017-06-23 03:03:51.147656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
11900 training acc: 0.125
Global Iter: 1612000 training loss: 2.0154
Global Iter: 1612000 training acc: 0.09375
Global Iter: 1612100 training loss: 1.92849
Global Iter: 1612100 training acc: 0.3125
Global Iter: 1612200 training loss: 1.97063
Global Iter: 1612200 training acc: 0.15625
Global Iter: 1612300 training loss: 1.99574
Global Iter: 1612300 training acc: 0.25
Global Iter: 1612400 training loss: 2.0037
Global Iter: 1612400 training acc: 0.09375
Global Iter: 1612500 training loss: 2.11893
Global Iter: 1612500 training acc: 0.1875
Global Iter: 1612600 training loss: 1.96739
Global Iter: 1612600 training acc: 0.15625
Global Iter: 1612700 training loss: 1.92178
Global Iter: 1612700 training acc: 0.25
Global Iter: 1612800 training loss: 1.95831
Global Iter: 1612800 training acc: 0.25
Global Iter: 1612900 training loss: 1.994
Global Iter: 1612900 training acc: 0.25
Global Iter: 1613000 training loss: 1.93411
Global Iter: 1613000 training acc: 0.25
Global Iter: 1613100 training loss: 1.94201
Global Iter: 1613100 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1613185
Number of Patches: 49402
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1613185
Global Iter: 1613200 training loss: 1.9914
Global Iter: 1613200 training acc: 0.15625
Global Iter: 1613300 training loss: 2.00951
Global Iter: 1613300 training acc: 0.1875
Global Iter: 1613400 training loss: 2.04855
Global Iter: 1613400 training acc: 0.15625
Global Iter: 1613500 training loss: 1.97617
Global Iter: 1613500 training acc: 0.1875
Global Iter: 1613600 training loss: 2.05309
Global Iter: 1613600 training acc: 0.25
Global Iter: 1613700 training loss: 2.02244
Global Iter: 1613700 training acc: 0.15625
Global Iter: 1613800 training loss: 2.08165
Global Iter: 1613800 training acc: 0.125
Global Iter: 1613900 training loss: 1.93646
Global Iter: 1613900 training acc: 0.1875
Global Iter: 1614000 training loss: 1.95461
Global Iter: 1614000 training acc: 0.3125
Global Iter: 1614100 training loss: 1.96442
Global Iter: 1614100 training acc: 0.1875
Global Iter: 1614200 training loss: 2.0521
Global Iter: 1614200 training acc: 0.15625
Global Iter: 1614300 training loss: 1.92165
Global Iter: 1614300 training acc: 0.15625
Global Iter: 1614400 training loss: 1.95788
Global Iter: 1614400 training acc: 0.125
Global Iter: 1614500 training loss: 1.9627
Global Iter: 1614500 training acc: 0.1875
Global Iter: 1614600 training loss: 1.94589
Global Iter: 1614600 training acc: 0.21875
Global Iter: 1614700 training loss: 1.97933
Global Iter: 1614700 training acc: 0.15625
Global Iter: 1614800 training loss: 1.92496
Global Iter: 1614800 training acc: 0.25
Global Iter: 1614900 training loss: 2.0211
Global Iter: 1614900 training acc: 0.15625
Global Iter: 1615000 training loss: 2.04645
Global Iter: 1615000 training acc: 0.125
Global Iter: 1615100 training loss: 1.94037
Global Iter: 1615100 training acc: 0.1875
Global Iter: 1615200 training loss: 2.10072
Global Iter: 1615200 training acc: 0.125
Global Iter: 1615300 training loss: 1.95046
Global Iter: 1615300 training acc: 0.28125
Global Iter: 1615400 training loss: 1.98254
Global Iter: 1615400 training acc: 0.1875
Global Iter: 1615500 training loss: 2.13653
Global Iter: 1615500 training acc: 0.15625
Global Iter: 1615600 training loss: 2.16396
Global Iter: 1615600 training acc: 0.1875
Global Iter: 1615700 training loss: 2.01046
Global Iter: 1615700 training acc: 0.1875
Global Iter: 1615800 training loss: 2.03112
Global Iter: 1615800 training acc: 0.15625
Global Iter: 1615900 training loss: 2.11711
Global Iter: 1615900 training acc: 0.1875
Global Iter: 1616000 training loss: 1.95351
Global Iter: 1616000 training acc: 0.28125
Global Iter: 1616100 training loss: 2.07221
Global Iter: 1616100 training acc: 0.0625
Global Iter: 1616200 training loss: 2.04832
Global Iter: 1616200 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1616273
Number of Patches: 48908
checkpoint found: /home/ahmet/worINFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1616273
2017-06-23 03:08:57.409471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1619330
kspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1616273
Global Iter: 1616300 training loss: 2.02905
Global Iter: 1616300 training acc: 0.125
Global Iter: 1616400 training loss: 2.10445
Global Iter: 1616400 training acc: 0.125
Global Iter: 1616500 training loss: 1.97382
Global Iter: 1616500 training acc: 0.1875
Global Iter: 1616600 training loss: 2.05859
Global Iter: 1616600 training acc: 0.09375
Global Iter: 1616700 training loss: 1.98384
Global Iter: 1616700 training acc: 0.28125
Global Iter: 1616800 training loss: 1.94896
Global Iter: 1616800 training acc: 0.15625
Global Iter: 1616900 training loss: 1.93626
Global Iter: 1616900 training acc: 0.28125
Global Iter: 1617000 training loss: 1.95684
Global Iter: 1617000 training acc: 0.1875
Global Iter: 1617100 training loss: 1.96355
Global Iter: 1617100 training acc: 0.09375
Global Iter: 1617200 training loss: 1.98478
Global Iter: 1617200 training acc: 0.0625
Global Iter: 1617300 training loss: 1.88991
Global Iter: 1617300 training acc: 0.15625
Global Iter: 1617400 training loss: 1.98396
Global Iter: 1617400 training acc: 0.25
Global Iter: 1617500 training loss: 1.93741
Global Iter: 1617500 training acc: 0.125
Global Iter: 1617600 training loss: 1.93327
Global Iter: 1617600 training acc: 0.21875
Global Iter: 1617700 training loss: 1.98202
Global Iter: 1617700 training acc: 0.125
Global Iter: 1617800 training loss: 2.03429
Global Iter: 1617800 training acc: 0.09375
Global Iter: 1617900 training loss: 1.91723
Global Iter: 1617900 training acc: 0.3125
Global Iter: 1618000 training loss: 1.97003
Global Iter: 1618000 training acc: 0.21875
Global Iter: 1618100 training loss: 2.0202
Global Iter: 1618100 training acc: 0.1875
Global Iter: 1618200 training loss: 1.91185
Global Iter: 1618200 training acc: 0.3125
Global Iter: 1618300 training loss: 2.04343
Global Iter: 1618300 training acc: 0.0625
Global Iter: 1618400 training loss: 1.96158
Global Iter: 1618400 training acc: 0.15625
Global Iter: 1618500 training loss: 1.95283
Global Iter: 1618500 training acc: 0.1875
Global Iter: 1618600 training loss: 2.00867
Global Iter: 1618600 training acc: 0.15625
Global Iter: 1618700 training loss: 1.91702
Global Iter: 1618700 training acc: 0.21875
Global Iter: 1618800 training loss: 2.00585
Global Iter: 1618800 training acc: 0.1875
Global Iter: 1618900 training loss: 1.98186
Global Iter: 1618900 training acc: 0.21875
Global Iter: 1619000 training loss: 1.99751
Global Iter: 1619000 training acc: 0.1875
Global Iter: 1619100 training loss: 1.9435
Global Iter: 1619100 training acc: 0.125
Global Iter: 1619200 training loss: 1.98784
Global Iter: 1619200 training acc: 0.34375
Global Iter: 1619300 training loss: 1.98332
Global Iter: 1619300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1619330
Number of Patches: 48419
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1619330
Global Iter: 1619400 training loss: 1.96082
Global Iter: 1619400 training acc: 0.125
Global Iter: 1619500 training loss: 2.08394
Global Iter: 1619500 training acc: 0.125
Global Iter: 1619600 training loss: 2.00802
Global Iter: 1619600 training acc: 0.125
Global Iter: 1619700 training loss: 1.9855
Global Iter: 1619700 training acc: 0.15625
Global Iter: 1619800 training loss: 2.03589
Global Iter: 1619800 training acc: 0.1875
Global Iter: 1619900 training loss: 2.11336
Global Iter: 1619900 training acc: 0.1875
Global Iter: 1620000 training loss: 2.02739
Global Iter: 1620000 training acc: 0.1875
Global Iter: 1620100 training loss: 2.05065
Global Iter: 1620100 training acc: 0.0625
Global Iter: 1620200 training loss: 2.05627
Global Iter: 1620200 training acc: 0.0625
Global Iter: 1620300 training loss: 1.99147
Global Iter: 1620300 training acc: 0.25
Global Iter: 1620400 training loss: 1.94736
Global Iter: 1620400 training acc: 0.375
Global Iter: 1620500 training loss: 2.04185
Global Iter: 1620500 training acc: 0.125
Global Iter: 1620600 training loss: 2.05291
Global Iter: 1620600 training acc: 0.09375
Global Iter: 1620700 tr2017-06-23 03:14:01.522263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1622357
aining loss: 2.07332
Global Iter: 1620700 training acc: 0.21875
Global Iter: 1620800 training loss: 1.98734
Global Iter: 1620800 training acc: 0.21875
Global Iter: 1620900 training loss: 1.98828
Global Iter: 1620900 training acc: 0.125
Global Iter: 1621000 training loss: 2.03599
Global Iter: 1621000 training acc: 0.1875
Global Iter: 1621100 training loss: 1.93768
Global Iter: 1621100 training acc: 0.1875
Global Iter: 1621200 training loss: 2.019
Global Iter: 1621200 training acc: 0.21875
Global Iter: 1621300 training loss: 1.99537
Global Iter: 1621300 training acc: 0.125
Global Iter: 1621400 training loss: 1.91577
Global Iter: 1621400 training acc: 0.28125
Global Iter: 1621500 training loss: 1.9861
Global Iter: 1621500 training acc: 0.21875
Global Iter: 1621600 training loss: 1.98974
Global Iter: 1621600 training acc: 0.03125
Global Iter: 1621700 training loss: 2.14565
Global Iter: 1621700 training acc: 0.0625
Global Iter: 1621800 training loss: 2.04586
Global Iter: 1621800 training acc: 0.1875
Global Iter: 1621900 training loss: 1.9414
Global Iter: 1621900 training acc: 0.15625
Global Iter: 1622000 training loss: 2.0131
Global Iter: 1622000 training acc: 0.21875
Global Iter: 1622100 training loss: 2.03817
Global Iter: 1622100 training acc: 0.15625
Global Iter: 1622200 training loss: 1.94665
Global Iter: 1622200 training acc: 0.125
Global Iter: 1622300 training loss: 1.96121
Global Iter: 1622300 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1622357
Number of Patches: 47935
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1622357
Global Iter: 1622400 training loss: 2.05166
Global Iter: 1622400 training acc: 0.15625
Global Iter: 1622500 training loss: 2.01368
Global Iter: 1622500 training acc: 0.09375
Global Iter: 1622600 training loss: 2.10185
Global Iter: 1622600 training acc: 0.1875
Global Iter: 1622700 training loss: 1.94965
Global Iter: 1622700 training acc: 0.25
Global Iter: 1622800 training loss: 2.04706
Global Iter: 1622800 training acc: 0.25
Global Iter: 1622900 training loss: 2.09859
Global Iter: 1622900 training acc: 0.15625
Global Iter: 1623000 training loss: 1.9832
Global Iter: 1623000 training acc: 0.1875
Global Iter: 1623100 training loss: 1.93106
Global Iter: 1623100 training acc: 0.15625
Global Iter: 1623200 training loss: 2.09045
Global Iter: 1623200 training acc: 0.1875
Global Iter: 1623300 training loss: 1.95694
Global Iter: 1623300 training acc: 0.25
Global Iter: 1623400 training loss: 1.92736
Global Iter: 1623400 training acc: 0.21875
Global Iter: 1623500 training loss: 1.95567
Global Iter: 1623500 training acc: 0.1875
Global Iter: 1623600 training loss: 2.02855
Global Iter: 1623600 training acc: 0.1875
Global Iter: 1623700 training loss: 2.04095
Global Iter: 1623700 training acc: 0.15625
Global Iter: 1623800 training loss: 2.04944
Global Iter: 1623800 training acc: 0.09375
Global Iter: 1623900 training loss: 1.97892
Global Iter: 1623900 training acc: 0.15625
Global Iter: 1624000 training loss: 2.01226
Global Iter: 1624000 training acc: 0.15625
Global Iter: 1624100 training loss: 2.01872
Global Iter: 1624100 training acc: 0.15625
Global Iter: 1624200 training loss: 2.00377
Global Iter: 1624200 training acc: 0.15625
Global Iter: 1624300 training loss: 1.95804
Global Iter: 1624300 training acc: 0.21875
Global Iter: 1624400 training loss: 1.92545
Global Iter: 1624400 training acc: 0.125
Global Iter: 1624500 training loss: 1.89835
Global Iter: 1624500 training acc: 0.34375
Global Iter: 1624600 training loss: 2.08211
Global Iter: 1624600 training acc: 0.0
Global Iter: 1624700 training loss: 2.0242
Global Iter: 1624700 training acc: 0.1875
Global Iter: 1624800 training loss: 2.01465
Global Iter: 1624800 training acc: 0.21875
Global Iter: 1624900 training loss: 1.99093
Global Iter: 1624900 training acc: 0.15625
Global Iter: 1625000 training loss: 1.85642
Global Iter: 1625000 training acc: 0.25
Global Iter: 1625100 training loss: 1.99091
Global Iter: 1625100 training acc: 0.25
Global Iter: 1625200 train2017-06-23 03:19:01.790378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1625353
2017-06-23 03:23:58.223776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1628319
ing loss: 1.92379
Global Iter: 1625200 training acc: 0.21875
Global Iter: 1625300 training loss: 2.09098
Global Iter: 1625300 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1625353
Number of Patches: 47456
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1625353
Global Iter: 1625400 training loss: 1.97978
Global Iter: 1625400 training acc: 0.25
Global Iter: 1625500 training loss: 2.06061
Global Iter: 1625500 training acc: 0.125
Global Iter: 1625600 training loss: 2.13701
Global Iter: 1625600 training acc: 0.21875
Global Iter: 1625700 training loss: 1.94533
Global Iter: 1625700 training acc: 0.3125
Global Iter: 1625800 training loss: 1.97461
Global Iter: 1625800 training acc: 0.21875
Global Iter: 1625900 training loss: 1.92545
Global Iter: 1625900 training acc: 0.21875
Global Iter: 1626000 training loss: 1.97327
Global Iter: 1626000 training acc: 0.28125
Global Iter: 1626100 training loss: 1.97255
Global Iter: 1626100 training acc: 0.15625
Global Iter: 1626200 training loss: 1.97732
Global Iter: 1626200 training acc: 0.21875
Global Iter: 1626300 training loss: 2.12809
Global Iter: 1626300 training acc: 0.28125
Global Iter: 1626400 training loss: 1.93957
Global Iter: 1626400 training acc: 0.1875
Global Iter: 1626500 training loss: 1.97201
Global Iter: 1626500 training acc: 0.1875
Global Iter: 1626600 training loss: 2.04016
Global Iter: 1626600 training acc: 0.15625
Global Iter: 1626700 training loss: 2.06232
Global Iter: 1626700 training acc: 0.125
Global Iter: 1626800 training loss: 1.91083
Global Iter: 1626800 training acc: 0.28125
Global Iter: 1626900 training loss: 1.95429
Global Iter: 1626900 training acc: 0.28125
Global Iter: 1627000 training loss: 1.96906
Global Iter: 1627000 training acc: 0.15625
Global Iter: 1627100 training loss: 2.11272
Global Iter: 1627100 training acc: 0.09375
Global Iter: 1627200 training loss: 1.92366
Global Iter: 1627200 training acc: 0.21875
Global Iter: 1627300 training loss: 1.94705
Global Iter: 1627300 training acc: 0.15625
Global Iter: 1627400 training loss: 2.09379
Global Iter: 1627400 training acc: 0.0625
Global Iter: 1627500 training loss: 2.00566
Global Iter: 1627500 training acc: 0.21875
Global Iter: 1627600 training loss: 1.99919
Global Iter: 1627600 training acc: 0.15625
Global Iter: 1627700 training loss: 2.00629
Global Iter: 1627700 training acc: 0.21875
Global Iter: 1627800 training loss: 2.09274
Global Iter: 1627800 training acc: 0.15625
Global Iter: 1627900 training loss: 1.93638
Global Iter: 1627900 training acc: 0.125
Global Iter: 1628000 training loss: 2.05232
Global Iter: 1628000 training acc: 0.15625
Global Iter: 1628100 training loss: 1.99223
Global Iter: 1628100 training acc: 0.0625
Global Iter: 1628200 training loss: 2.05076
Global Iter: 1628200 training acc: 0.28125
Global Iter: 1628300 training loss: 2.04849
Global Iter: 1628300 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1628319
Number of Patches: 46982
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1628319
Global Iter: 1628400 training loss: 2.04264
Global Iter: 1628400 training acc: 0.21875
Global Iter: 1628500 training loss: 1.96418
Global Iter: 1628500 training acc: 0.125
Global Iter: 1628600 training loss: 1.99878
Global Iter: 1628600 training acc: 0.15625
Global Iter: 1628700 training loss: 1.96303
Global Iter: 1628700 training acc: 0.1875
Global Iter: 1628800 training loss: 2.02406
Global Iter: 1628800 training acc: 0.21875
Global Iter: 1628900 training loss: 2.04424
Global Iter: 1628900 training acc: 0.1875
Global Iter: 1629000 training loss: 1.91388
Global Iter: 1629000 training acc: 0.21875
Global Iter: 1629100 training loss: 2.03972
Global Iter: 1629100 training acc: 0.1875
Global Iter: 1629200 training loss: 1.99377
Global Iter: 1629200 training acc: 0.09375
Global Iter: 1629300 training loss: 2.16394
Global Iter: 1629300 training acc: 0.125
Global Iter: 1629400 training loss: 1.9322017-06-23 03:28:49.560426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1631256
93
Global Iter: 1629400 training acc: 0.3125
Global Iter: 1629500 training loss: 1.98658
Global Iter: 1629500 training acc: 0.15625
Global Iter: 1629600 training loss: 1.93036
Global Iter: 1629600 training acc: 0.15625
Global Iter: 1629700 training loss: 1.99903
Global Iter: 1629700 training acc: 0.1875
Global Iter: 1629800 training loss: 2.07481
Global Iter: 1629800 training acc: 0.28125
Global Iter: 1629900 training loss: 2.03655
Global Iter: 1629900 training acc: 0.1875
Global Iter: 1630000 training loss: 1.9903
Global Iter: 1630000 training acc: 0.15625
Global Iter: 1630100 training loss: 2.02196
Global Iter: 1630100 training acc: 0.09375
Global Iter: 1630200 training loss: 2.19172
Global Iter: 1630200 training acc: 0.15625
Global Iter: 1630300 training loss: 2.05835
Global Iter: 1630300 training acc: 0.125
Global Iter: 1630400 training loss: 2.01481
Global Iter: 1630400 training acc: 0.21875
Global Iter: 1630500 training loss: 1.96526
Global Iter: 1630500 training acc: 0.125
Global Iter: 1630600 training loss: 1.99016
Global Iter: 1630600 training acc: 0.0625
Global Iter: 1630700 training loss: 1.99868
Global Iter: 1630700 training acc: 0.21875
Global Iter: 1630800 training loss: 1.94003
Global Iter: 1630800 training acc: 0.21875
Global Iter: 1630900 training loss: 2.07436
Global Iter: 1630900 training acc: 0.1875
Global Iter: 1631000 training loss: 1.95552
Global Iter: 1631000 training acc: 0.1875
Global Iter: 1631100 training loss: 1.919
Global Iter: 1631100 training acc: 0.25
Global Iter: 1631200 training loss: 1.94905
Global Iter: 1631200 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1631256
Number of Patches: 46513
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1631256
Global Iter: 1631300 training loss: 2.05456
Global Iter: 1631300 training acc: 0.1875
Global Iter: 1631400 training loss: 2.09308
Global Iter: 1631400 training acc: 0.09375
Global Iter: 1631500 training loss: 1.99781
Global Iter: 1631500 training acc: 0.125
Global Iter: 1631600 training loss: 2.00062
Global Iter: 1631600 training acc: 0.0625
Global Iter: 1631700 training loss: 2.04315
Global Iter: 1631700 training acc: 0.1875
Global Iter: 1631800 training loss: 1.94696
Global Iter: 1631800 training acc: 0.375
Global Iter: 1631900 training loss: 1.9897
Global Iter: 1631900 training acc: 0.125
Global Iter: 1632000 training loss: 1.96924
Global Iter: 1632000 training acc: 0.1875
Global Iter: 1632100 training loss: 2.05635
Global Iter: 1632100 training acc: 0.1875
Global Iter: 1632200 training loss: 1.97778
Global Iter: 1632200 training acc: 0.21875
Global Iter: 1632300 training loss: 1.93711
Global Iter: 1632300 training acc: 0.1875
Global Iter: 1632400 training loss: 2.02858
Global Iter: 1632400 training acc: 0.125
Global Iter: 1632500 training loss: 2.0226
Global Iter: 1632500 training acc: 0.15625
Global Iter: 1632600 training loss: 2.04274
Global Iter: 1632600 training acc: 0.09375
Global Iter: 1632700 training loss: 1.948
Global Iter: 1632700 training acc: 0.25
Global Iter: 1632800 training loss: 1.96259
Global Iter: 1632800 training acc: 0.15625
Global Iter: 1632900 training loss: 2.05109
Global Iter: 1632900 training acc: 0.125
Global Iter: 1633000 training loss: 2.00173
Global Iter: 1633000 training acc: 0.1875
Global Iter: 1633100 training loss: 1.97934
Global Iter: 1633100 training acc: 0.15625
Global Iter: 1633200 training loss: 1.99243
Global Iter: 1633200 training acc: 0.09375
Global Iter: 1633300 training loss: 1.99642
Global Iter: 1633300 training acc: 0.125
Global Iter: 1633400 training loss: 2.02361
Global Iter: 1633400 training acc: 0.125
Global Iter: 1633500 training loss: 2.03277
Global Iter: 1633500 training acc: 0.1875
Global Iter: 1633600 training loss: 1.92492
Global Iter: 1633600 training acc: 0.125
Global Iter: 1633700 training loss: 1.95889
Global Iter: 1633700 training acc: 0.15625
Global Iter: 1633800 training loss: 1.97899
Global Iter: 1633800 training acc: 0.15625
Global Iter: 1633900 training loss: 2.08183
G2017-06-23 03:33:39.395272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1634164
2017-06-23 03:38:32.860374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1637042
lobal Iter: 1633900 training acc: 0.125
Global Iter: 1634000 training loss: 2.11975
Global Iter: 1634000 training acc: 0.125
Global Iter: 1634100 training loss: 2.06783
Global Iter: 1634100 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1634164
Number of Patches: 46048
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1634164
Global Iter: 1634200 training loss: 1.97299
Global Iter: 1634200 training acc: 0.15625
Global Iter: 1634300 training loss: 1.98921
Global Iter: 1634300 training acc: 0.125
Global Iter: 1634400 training loss: 1.99568
Global Iter: 1634400 training acc: 0.09375
Global Iter: 1634500 training loss: 1.91472
Global Iter: 1634500 training acc: 0.21875
Global Iter: 1634600 training loss: 1.92859
Global Iter: 1634600 training acc: 0.125
Global Iter: 1634700 training loss: 1.91368
Global Iter: 1634700 training acc: 0.125
Global Iter: 1634800 training loss: 2.04049
Global Iter: 1634800 training acc: 0.125
Global Iter: 1634900 training loss: 1.98415
Global Iter: 1634900 training acc: 0.15625
Global Iter: 1635000 training loss: 2.03187
Global Iter: 1635000 training acc: 0.1875
Global Iter: 1635100 training loss: 2.043
Global Iter: 1635100 training acc: 0.15625
Global Iter: 1635200 training loss: 2.01633
Global Iter: 1635200 training acc: 0.15625
Global Iter: 1635300 training loss: 1.93992
Global Iter: 1635300 training acc: 0.15625
Global Iter: 1635400 training loss: 1.93966
Global Iter: 1635400 training acc: 0.21875
Global Iter: 1635500 training loss: 1.9283
Global Iter: 1635500 training acc: 0.1875
Global Iter: 1635600 training loss: 1.9787
Global Iter: 1635600 training acc: 0.1875
Global Iter: 1635700 training loss: 1.93325
Global Iter: 1635700 training acc: 0.1875
Global Iter: 1635800 training loss: 1.99003
Global Iter: 1635800 training acc: 0.125
Global Iter: 1635900 training loss: 1.96451
Global Iter: 1635900 training acc: 0.0625
Global Iter: 1636000 training loss: 2.1895
Global Iter: 1636000 training acc: 0.21875
Global Iter: 1636100 training loss: 1.98638
Global Iter: 1636100 training acc: 0.125
Global Iter: 1636200 training loss: 1.95385
Global Iter: 1636200 training acc: 0.21875
Global Iter: 1636300 training loss: 2.03318
Global Iter: 1636300 training acc: 0.21875
Global Iter: 1636400 training loss: 1.96933
Global Iter: 1636400 training acc: 0.1875
Global Iter: 1636500 training loss: 1.91317
Global Iter: 1636500 training acc: 0.15625
Global Iter: 1636600 training loss: 2.15954
Global Iter: 1636600 training acc: 0.15625
Global Iter: 1636700 training loss: 2.05453
Global Iter: 1636700 training acc: 0.125
Global Iter: 1636800 training loss: 1.97973
Global Iter: 1636800 training acc: 0.21875
Global Iter: 1636900 training loss: 2.00884
Global Iter: 1636900 training acc: 0.1875
Global Iter: 1637000 training loss: 2.03276
Global Iter: 1637000 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1637042
Number of Patches: 45588
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1637042
Global Iter: 1637100 training loss: 2.02288
Global Iter: 1637100 training acc: 0.1875
Global Iter: 1637200 training loss: 2.0957
Global Iter: 1637200 training acc: 0.15625
Global Iter: 1637300 training loss: 2.0222
Global Iter: 1637300 training acc: 0.15625
Global Iter: 1637400 training loss: 2.00459
Global Iter: 1637400 training acc: 0.1875
Global Iter: 1637500 training loss: 2.01576
Global Iter: 1637500 training acc: 0.15625
Global Iter: 1637600 training loss: 1.96476
Global Iter: 1637600 training acc: 0.21875
Global Iter: 1637700 training loss: 1.97169
Global Iter: 1637700 training acc: 0.15625
Global Iter: 1637800 training loss: 1.98795
Global Iter: 1637800 training acc: 0.21875
Global Iter: 1637900 training loss: 2.11396
Global Iter: 1637900 training acc: 0.125
Global Iter: 1638000 training loss: 2.07657
Global Iter: 1638000 training acc: 0.28125
Global Iter: 1638100 training loss: 2.10158
Global Iter: 1638100 training ac2017-06-23 03:43:18.372225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1639892
c: 0.125
Global Iter: 1638200 training loss: 2.00212
Global Iter: 1638200 training acc: 0.1875
Global Iter: 1638300 training loss: 1.89887
Global Iter: 1638300 training acc: 0.125
Global Iter: 1638400 training loss: 2.02325
Global Iter: 1638400 training acc: 0.09375
Global Iter: 1638500 training loss: 1.8771
Global Iter: 1638500 training acc: 0.375
Global Iter: 1638600 training loss: 1.91948
Global Iter: 1638600 training acc: 0.1875
Global Iter: 1638700 training loss: 2.05362
Global Iter: 1638700 training acc: 0.15625
Global Iter: 1638800 training loss: 1.96692
Global Iter: 1638800 training acc: 0.28125
Global Iter: 1638900 training loss: 1.94118
Global Iter: 1638900 training acc: 0.34375
Global Iter: 1639000 training loss: 1.90358
Global Iter: 1639000 training acc: 0.28125
Global Iter: 1639100 training loss: 1.94666
Global Iter: 1639100 training acc: 0.1875
Global Iter: 1639200 training loss: 2.0861
Global Iter: 1639200 training acc: 0.25
Global Iter: 1639300 training loss: 2.01953
Global Iter: 1639300 training acc: 0.21875
Global Iter: 1639400 training loss: 1.94124
Global Iter: 1639400 training acc: 0.25
Global Iter: 1639500 training loss: 1.93827
Global Iter: 1639500 training acc: 0.28125
Global Iter: 1639600 training loss: 2.01531
Global Iter: 1639600 training acc: 0.0625
Global Iter: 1639700 training loss: 2.12985
Global Iter: 1639700 training acc: 0.1875
Global Iter: 1639800 training loss: 1.98479
Global Iter: 1639800 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1639892
Number of Patches: 45133
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1639892
Global Iter: 1639900 training loss: 1.959
Global Iter: 1639900 training acc: 0.21875
Global Iter: 1640000 training loss: 1.9777
Global Iter: 1640000 training acc: 0.125
Global Iter: 1640100 training loss: 1.91498
Global Iter: 1640100 training acc: 0.15625
Global Iter: 1640200 training loss: 2.01934
Global Iter: 1640200 training acc: 0.15625
Global Iter: 1640300 training loss: 1.97173
Global Iter: 1640300 training acc: 0.15625
Global Iter: 1640400 training loss: 1.94435
Global Iter: 1640400 training acc: 0.1875
Global Iter: 1640500 training loss: 1.96826
Global Iter: 1640500 training acc: 0.25
Global Iter: 1640600 training loss: 1.94109
Global Iter: 1640600 training acc: 0.21875
Global Iter: 1640700 training loss: 2.04641
Global Iter: 1640700 training acc: 0.15625
Global Iter: 1640800 training loss: 1.9335
Global Iter: 1640800 training acc: 0.28125
Global Iter: 1640900 training loss: 1.95009
Global Iter: 1640900 training acc: 0.125
Global Iter: 1641000 training loss: 2.07481
Global Iter: 1641000 training acc: 0.21875
Global Iter: 1641100 training loss: 1.94934
Global Iter: 1641100 training acc: 0.125
Global Iter: 1641200 training loss: 1.93811
Global Iter: 1641200 training acc: 0.21875
Global Iter: 1641300 training loss: 1.98401
Global Iter: 1641300 training acc: 0.125
Global Iter: 1641400 training loss: 1.94477
Global Iter: 1641400 training acc: 0.375
Global Iter: 1641500 training loss: 2.09515
Global Iter: 1641500 training acc: 0.09375
Global Iter: 1641600 training loss: 2.06932
Global Iter: 1641600 training acc: 0.09375
Global Iter: 1641700 training loss: 1.96986
Global Iter: 1641700 training acc: 0.1875
Global Iter: 1641800 training loss: 2.01752
Global Iter: 1641800 training acc: 0.0625
Global Iter: 1641900 training loss: 2.0053
Global Iter: 1641900 training acc: 0.125
Global Iter: 1642000 training loss: 2.08733
Global Iter: 1642000 training acc: 0.0625
Global Iter: 1642100 training loss: 2.0145
Global Iter: 1642100 training acc: 0.15625
Global Iter: 1642200 training loss: 1.95446
Global Iter: 1642200 training acc: 0.0625
Global Iter: 1642300 training loss: 2.02998
Global Iter: 1642300 training acc: 0.125
Global Iter: 1642400 training loss: 2.02746
Global Iter: 1642400 training acc: 0.09375
Global Iter: 1642500 training loss: 2.01341
Global Iter: 1642500 training acc: 0.0625
Global Iter: 1642600 training loss: 2.08508
Global Iter: 1642600 training acc: 0.2017-06-23 03:48:04.756499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1642713
2017-06-23 03:52:47.895955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1645506
15625
Global Iter: 1642700 training loss: 2.00145
Global Iter: 1642700 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1642713
Number of Patches: 44682
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1642713
Global Iter: 1642800 training loss: 2.05968
Global Iter: 1642800 training acc: 0.25
Global Iter: 1642900 training loss: 2.07032
Global Iter: 1642900 training acc: 0.09375
Global Iter: 1643000 training loss: 1.97594
Global Iter: 1643000 training acc: 0.09375
Global Iter: 1643100 training loss: 2.10689
Global Iter: 1643100 training acc: 0.15625
Global Iter: 1643200 training loss: 2.00437
Global Iter: 1643200 training acc: 0.15625
Global Iter: 1643300 training loss: 2.0502
Global Iter: 1643300 training acc: 0.0625
Global Iter: 1643400 training loss: 1.94248
Global Iter: 1643400 training acc: 0.125
Global Iter: 1643500 training loss: 1.91462
Global Iter: 1643500 training acc: 0.375
Global Iter: 1643600 training loss: 1.97827
Global Iter: 1643600 training acc: 0.125
Global Iter: 1643700 training loss: 1.97778
Global Iter: 1643700 training acc: 0.0625
Global Iter: 1643800 training loss: 1.99207
Global Iter: 1643800 training acc: 0.0625
Global Iter: 1643900 training loss: 2.01768
Global Iter: 1643900 training acc: 0.09375
Global Iter: 1644000 training loss: 1.89582
Global Iter: 1644000 training acc: 0.1875
Global Iter: 1644100 training loss: 1.97835
Global Iter: 1644100 training acc: 0.03125
Global Iter: 1644200 training loss: 2.07756
Global Iter: 1644200 training acc: 0.09375
Global Iter: 1644300 training loss: 1.88879
Global Iter: 1644300 training acc: 0.21875
Global Iter: 1644400 training loss: 2.03058
Global Iter: 1644400 training acc: 0.1875
Global Iter: 1644500 training loss: 1.93266
Global Iter: 1644500 training acc: 0.15625
Global Iter: 1644600 training loss: 1.97523
Global Iter: 1644600 training acc: 0.1875
Global Iter: 1644700 training loss: 1.87472
Global Iter: 1644700 training acc: 0.3125
Global Iter: 1644800 training loss: 2.12918
Global Iter: 1644800 training acc: 0.09375
Global Iter: 1644900 training loss: 2.06016
Global Iter: 1644900 training acc: 0.15625
Global Iter: 1645000 training loss: 2.05409
Global Iter: 1645000 training acc: 0.125
Global Iter: 1645100 training loss: 2.01312
Global Iter: 1645100 training acc: 0.15625
Global Iter: 1645200 training loss: 1.95952
Global Iter: 1645200 training acc: 0.3125
Global Iter: 1645300 training loss: 1.95062
Global Iter: 1645300 training acc: 0.03125
Global Iter: 1645400 training loss: 1.90846
Global Iter: 1645400 training acc: 0.21875
Global Iter: 1645500 training loss: 1.91124
Global Iter: 1645500 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1645506
Number of Patches: 44236
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1645506
Global Iter: 1645600 training loss: 1.97325
Global Iter: 1645600 training acc: 0.125
Global Iter: 1645700 training loss: 2.00973
Global Iter: 1645700 training acc: 0.1875
Global Iter: 1645800 training loss: 2.06375
Global Iter: 1645800 training acc: 0.15625
Global Iter: 1645900 training loss: 2.01893
Global Iter: 1645900 training acc: 0.1875
Global Iter: 1646000 training loss: 2.01035
Global Iter: 1646000 training acc: 0.1875
Global Iter: 1646100 training loss: 1.88507
Global Iter: 1646100 training acc: 0.125
Global Iter: 1646200 training loss: 1.99171
Global Iter: 1646200 training acc: 0.125
Global Iter: 1646300 training loss: 2.14181
Global Iter: 1646300 training acc: 0.09375
Global Iter: 1646400 training loss: 2.02598
Global Iter: 1646400 training acc: 0.09375
Global Iter: 1646500 training loss: 2.0433
Global Iter: 1646500 training acc: 0.1875
Global Iter: 1646600 training loss: 2.02894
Global Iter: 1646600 training acc: 0.09375
Global Iter: 1646700 training loss: 1.97181
Global Iter: 1646700 training acc: 0.125
Global Iter: 1646800 training loss: 2.03043
Global Iter: 1646800 training acc: 0.25
Global Iter: 1646900 training2017-06-23 03:57:24.353888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1648271
2017-06-23 04:02:02.990556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1651009
 loss: 2.04386
Global Iter: 1646900 training acc: 0.09375
Global Iter: 1647000 training loss: 2.12532
Global Iter: 1647000 training acc: 0.125
Global Iter: 1647100 training loss: 1.99406
Global Iter: 1647100 training acc: 0.125
Global Iter: 1647200 training loss: 2.07
Global Iter: 1647200 training acc: 0.1875
Global Iter: 1647300 training loss: 1.9464
Global Iter: 1647300 training acc: 0.15625
Global Iter: 1647400 training loss: 2.10664
Global Iter: 1647400 training acc: 0.25
Global Iter: 1647500 training loss: 1.95879
Global Iter: 1647500 training acc: 0.15625
Global Iter: 1647600 training loss: 2.09804
Global Iter: 1647600 training acc: 0.125
Global Iter: 1647700 training loss: 2.02506
Global Iter: 1647700 training acc: 0.125
Global Iter: 1647800 training loss: 2.03729
Global Iter: 1647800 training acc: 0.03125
Global Iter: 1647900 training loss: 1.97121
Global Iter: 1647900 training acc: 0.125
Global Iter: 1648000 training loss: 1.93114
Global Iter: 1648000 training acc: 0.1875
Global Iter: 1648100 training loss: 2.00408
Global Iter: 1648100 training acc: 0.21875
Global Iter: 1648200 training loss: 2.01765
Global Iter: 1648200 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1648271
Number of Patches: 43794
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1648271
Global Iter: 1648300 training loss: 2.05017
Global Iter: 1648300 training acc: 0.21875
Global Iter: 1648400 training loss: 1.96519
Global Iter: 1648400 training acc: 0.15625
Global Iter: 1648500 training loss: 1.95625
Global Iter: 1648500 training acc: 0.15625
Global Iter: 1648600 training loss: 1.90454
Global Iter: 1648600 training acc: 0.1875
Global Iter: 1648700 training loss: 1.94313
Global Iter: 1648700 training acc: 0.28125
Global Iter: 1648800 training loss: 2.11257
Global Iter: 1648800 training acc: 0.0625
Global Iter: 1648900 training loss: 1.9794
Global Iter: 1648900 training acc: 0.21875
Global Iter: 1649000 training loss: 2.01635
Global Iter: 1649000 training acc: 0.15625
Global Iter: 1649100 training loss: 2.04308
Global Iter: 1649100 training acc: 0.15625
Global Iter: 1649200 training loss: 1.9223
Global Iter: 1649200 training acc: 0.15625
Global Iter: 1649300 training loss: 1.99548
Global Iter: 1649300 training acc: 0.1875
Global Iter: 1649400 training loss: 2.07247
Global Iter: 1649400 training acc: 0.03125
Global Iter: 1649500 training loss: 1.91348
Global Iter: 1649500 training acc: 0.21875
Global Iter: 1649600 training loss: 1.92664
Global Iter: 1649600 training acc: 0.09375
Global Iter: 1649700 training loss: 1.96548
Global Iter: 1649700 training acc: 0.09375
Global Iter: 1649800 training loss: 1.93814
Global Iter: 1649800 training acc: 0.28125
Global Iter: 1649900 training loss: 2.00555
Global Iter: 1649900 training acc: 0.15625
Global Iter: 1650000 training loss: 1.9372
Global Iter: 1650000 training acc: 0.1875
Global Iter: 1650100 training loss: 1.9956
Global Iter: 1650100 training acc: 0.15625
Global Iter: 1650200 training loss: 2.07661
Global Iter: 1650200 training acc: 0.15625
Global Iter: 1650300 training loss: 2.00789
Global Iter: 1650300 training acc: 0.21875
Global Iter: 1650400 training loss: 1.95431
Global Iter: 1650400 training acc: 0.0625
Global Iter: 1650500 training loss: 1.92367
Global Iter: 1650500 training acc: 0.21875
Global Iter: 1650600 training loss: 1.87924
Global Iter: 1650600 training acc: 0.15625
Global Iter: 1650700 training loss: 2.01125
Global Iter: 1650700 training acc: 0.125
Global Iter: 1650800 training loss: 1.89489
Global Iter: 1650800 training acc: 0.21875
Global Iter: 1650900 training loss: 1.98038
Global Iter: 1650900 training acc: 0.25
Global Iter: 1651000 training loss: 2.00436
Global Iter: 1651000 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1651009
Number of Patches: 43357
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1651009
Global Iter: 1651100 training loss: 1.95978
Global Iter: 12017-06-23 04:06:34.102843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1653719
651100 training acc: 0.1875
Global Iter: 1651200 training loss: 1.91588
Global Iter: 1651200 training acc: 0.25
Global Iter: 1651300 training loss: 1.98828
Global Iter: 1651300 training acc: 0.1875
Global Iter: 1651400 training loss: 1.97191
Global Iter: 1651400 training acc: 0.15625
Global Iter: 1651500 training loss: 1.94277
Global Iter: 1651500 training acc: 0.15625
Global Iter: 1651600 training loss: 1.97426
Global Iter: 1651600 training acc: 0.21875
Global Iter: 1651700 training loss: 2.04872
Global Iter: 1651700 training acc: 0.125
Global Iter: 1651800 training loss: 2.00083
Global Iter: 1651800 training acc: 0.28125
Global Iter: 1651900 training loss: 1.9255
Global Iter: 1651900 training acc: 0.28125
Global Iter: 1652000 training loss: 1.95491
Global Iter: 1652000 training acc: 0.21875
Global Iter: 1652100 training loss: 1.89021
Global Iter: 1652100 training acc: 0.28125
Global Iter: 1652200 training loss: 2.06368
Global Iter: 1652200 training acc: 0.03125
Global Iter: 1652300 training loss: 2.0701
Global Iter: 1652300 training acc: 0.15625
Global Iter: 1652400 training loss: 2.11258
Global Iter: 1652400 training acc: 0.1875
Global Iter: 1652500 training loss: 2.05056
Global Iter: 1652500 training acc: 0.15625
Global Iter: 1652600 training loss: 1.98581
Global Iter: 1652600 training acc: 0.1875
Global Iter: 1652700 training loss: 1.97983
Global Iter: 1652700 training acc: 0.28125
Global Iter: 1652800 training loss: 1.92162
Global Iter: 1652800 training acc: 0.1875
Global Iter: 1652900 training loss: 1.99386
Global Iter: 1652900 training acc: 0.15625
Global Iter: 1653000 training loss: 2.10406
Global Iter: 1653000 training acc: 0.15625
Global Iter: 1653100 training loss: 1.95959
Global Iter: 1653100 training acc: 0.09375
Global Iter: 1653200 training loss: 2.10761
Global Iter: 1653200 training acc: 0.15625
Global Iter: 1653300 training loss: 2.00441
Global Iter: 1653300 training acc: 0.1875
Global Iter: 1653400 training loss: 2.10258
Global Iter: 1653400 training acc: 0.1875
Global Iter: 1653500 training loss: 2.04221
Global Iter: 1653500 training acc: 0.15625
Global Iter: 1653600 training loss: 1.92484
Global Iter: 1653600 training acc: 0.28125
Global Iter: 1653700 training loss: 2.09642
Global Iter: 1653700 training acc: 0.28125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1653719
Number of Patches: 42924
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1653719
Global Iter: 1653800 training loss: 2.04861
Global Iter: 1653800 training acc: 0.125
Global Iter: 1653900 training loss: 2.01392
Global Iter: 1653900 training acc: 0.09375
Global Iter: 1654000 training loss: 1.99574
Global Iter: 1654000 training acc: 0.21875
Global Iter: 1654100 training loss: 2.05619
Global Iter: 1654100 training acc: 0.15625
Global Iter: 1654200 training loss: 2.06766
Global Iter: 1654200 training acc: 0.09375
Global Iter: 1654300 training loss: 2.00987
Global Iter: 1654300 training acc: 0.1875
Global Iter: 1654400 training loss: 1.92961
Global Iter: 1654400 training acc: 0.125
Global Iter: 1654500 training loss: 2.01224
Global Iter: 1654500 training acc: 0.21875
Global Iter: 1654600 training loss: 1.97458
Global Iter: 1654600 training acc: 0.15625
Global Iter: 1654700 training loss: 2.02402
Global Iter: 1654700 training acc: 0.125
Global Iter: 1654800 training loss: 1.96476
Global Iter: 1654800 training acc: 0.09375
Global Iter: 1654900 training loss: 1.91926
Global Iter: 1654900 training acc: 0.21875
Global Iter: 1655000 training loss: 2.04163
Global Iter: 1655000 training acc: 0.09375
Global Iter: 1655100 training loss: 2.15897
Global Iter: 1655100 training acc: 0.1875
Global Iter: 1655200 training loss: 1.94714
Global Iter: 1655200 training acc: 0.34375
Global Iter: 1655300 training loss: 1.92179
Global Iter: 1655300 training acc: 0.1875
Global Iter: 1655400 training loss: 1.97855
Global Iter: 1655400 training acc: 0.125
Global Iter: 1655500 training loss: 2.02737
Global Iter: 1655500 training acc: 0.15625
Global Iter: 1655600 training loss: 1.982017-06-23 04:11:09.177927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1656402
2017-06-23 04:15:36.453996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1659058
612
Global Iter: 1655600 training acc: 0.1875
Global Iter: 1655700 training loss: 1.96946
Global Iter: 1655700 training acc: 0.1875
Global Iter: 1655800 training loss: 1.96468
Global Iter: 1655800 training acc: 0.1875
Global Iter: 1655900 training loss: 2.00053
Global Iter: 1655900 training acc: 0.125
Global Iter: 1656000 training loss: 1.93102
Global Iter: 1656000 training acc: 0.21875
Global Iter: 1656100 training loss: 1.9775
Global Iter: 1656100 training acc: 0.15625
Global Iter: 1656200 training loss: 1.95219
Global Iter: 1656200 training acc: 0.15625
Global Iter: 1656300 training loss: 1.93254
Global Iter: 1656300 training acc: 0.15625
Global Iter: 1656400 training loss: 1.97865
Global Iter: 1656400 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1656402
Number of Patches: 42495
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1656402
Global Iter: 1656500 training loss: 1.9372
Global Iter: 1656500 training acc: 0.15625
Global Iter: 1656600 training loss: 2.03631
Global Iter: 1656600 training acc: 0.1875
Global Iter: 1656700 training loss: 2.12502
Global Iter: 1656700 training acc: 0.125
Global Iter: 1656800 training loss: 2.05748
Global Iter: 1656800 training acc: 0.125
Global Iter: 1656900 training loss: 1.97936
Global Iter: 1656900 training acc: 0.15625
Global Iter: 1657000 training loss: 1.96215
Global Iter: 1657000 training acc: 0.125
Global Iter: 1657100 training loss: 2.00533
Global Iter: 1657100 training acc: 0.03125
Global Iter: 1657200 training loss: 1.99058
Global Iter: 1657200 training acc: 0.09375
Global Iter: 1657300 training loss: 1.93461
Global Iter: 1657300 training acc: 0.0625
Global Iter: 1657400 training loss: 1.92838
Global Iter: 1657400 training acc: 0.25
Global Iter: 1657500 training loss: 1.94942
Global Iter: 1657500 training acc: 0.15625
Global Iter: 1657600 training loss: 2.05358
Global Iter: 1657600 training acc: 0.21875
Global Iter: 1657700 training loss: 2.00906
Global Iter: 1657700 training acc: 0.15625
Global Iter: 1657800 training loss: 2.09945
Global Iter: 1657800 training acc: 0.125
Global Iter: 1657900 training loss: 1.95312
Global Iter: 1657900 training acc: 0.28125
Global Iter: 1658000 training loss: 1.9884
Global Iter: 1658000 training acc: 0.25
Global Iter: 1658100 training loss: 2.087
Global Iter: 1658100 training acc: 0.1875
Global Iter: 1658200 training loss: 1.9819
Global Iter: 1658200 training acc: 0.15625
Global Iter: 1658300 training loss: 2.03098
Global Iter: 1658300 training acc: 0.09375
Global Iter: 1658400 training loss: 1.98102
Global Iter: 1658400 training acc: 0.125
Global Iter: 1658500 training loss: 2.11847
Global Iter: 1658500 training acc: 0.15625
Global Iter: 1658600 training loss: 2.00721
Global Iter: 1658600 training acc: 0.125
Global Iter: 1658700 training loss: 1.95884
Global Iter: 1658700 training acc: 0.1875
Global Iter: 1658800 training loss: 2.08318
Global Iter: 1658800 training acc: 0.125
Global Iter: 1658900 training loss: 1.99215
Global Iter: 1658900 training acc: 0.21875
Global Iter: 1659000 training loss: 1.97819
Global Iter: 1659000 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1659058
Number of Patches: 42071
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1659058
Global Iter: 1659100 training loss: 2.01859
Global Iter: 1659100 training acc: 0.21875
Global Iter: 1659200 training loss: 1.97189
Global Iter: 1659200 training acc: 0.21875
Global Iter: 1659300 training loss: 1.98374
Global Iter: 1659300 training acc: 0.3125
Global Iter: 1659400 training loss: 1.95766
Global Iter: 1659400 training acc: 0.1875
Global Iter: 1659500 training loss: 1.99123
Global Iter: 1659500 training acc: 0.21875
Global Iter: 1659600 training loss: 1.97737
Global Iter: 1659600 training acc: 0.1875
Global Iter: 1659700 training loss: 2.00106
Global Iter: 1659700 training acc: 0.09375
Global Iter: 1659800 training loss: 2.02003
Global Iter: 1659800 trainin2017-06-23 04:20:00.081734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1661688
g acc: 0.0625
Global Iter: 1659900 training loss: 1.94276
Global Iter: 1659900 training acc: 0.25
Global Iter: 1660000 training loss: 1.98299
Global Iter: 1660000 training acc: 0.0625
Global Iter: 1660100 training loss: 1.99237
Global Iter: 1660100 training acc: 0.21875
Global Iter: 1660200 training loss: 2.03365
Global Iter: 1660200 training acc: 0.28125
Global Iter: 1660300 training loss: 2.04769
Global Iter: 1660300 training acc: 0.125
Global Iter: 1660400 training loss: 1.932
Global Iter: 1660400 training acc: 0.25
Global Iter: 1660500 training loss: 2.04356
Global Iter: 1660500 training acc: 0.15625
Global Iter: 1660600 training loss: 2.02861
Global Iter: 1660600 training acc: 0.1875
Global Iter: 1660700 training loss: 1.94706
Global Iter: 1660700 training acc: 0.0625
Global Iter: 1660800 training loss: 1.94122
Global Iter: 1660800 training acc: 0.21875
Global Iter: 1660900 training loss: 1.94722
Global Iter: 1660900 training acc: 0.21875
Global Iter: 1661000 training loss: 1.90907
Global Iter: 1661000 training acc: 0.34375
Global Iter: 1661100 training loss: 2.03368
Global Iter: 1661100 training acc: 0.15625
Global Iter: 1661200 training loss: 2.00275
Global Iter: 1661200 training acc: 0.15625
Global Iter: 1661300 training loss: 2.14098
Global Iter: 1661300 training acc: 0.15625
Global Iter: 1661400 training loss: 1.95981
Global Iter: 1661400 training acc: 0.25
Global Iter: 1661500 training loss: 1.99822
Global Iter: 1661500 training acc: 0.15625
Global Iter: 1661600 training loss: 1.95653
Global Iter: 1661600 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1661688
Number of Patches: 41651
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1661688
Global Iter: 1661700 training loss: 2.02336
Global Iter: 1661700 training acc: 0.0625
Global Iter: 1661800 training loss: 2.07212
Global Iter: 1661800 training acc: 0.21875
Global Iter: 1661900 training loss: 1.95584
Global Iter: 1661900 training acc: 0.21875
Global Iter: 1662000 training loss: 1.97886
Global Iter: 1662000 training acc: 0.09375
Global Iter: 1662100 training loss: 2.02683
Global Iter: 1662100 training acc: 0.1875
Global Iter: 1662200 training loss: 1.90462
Global Iter: 1662200 training acc: 0.25
Global Iter: 1662300 training loss: 1.97944
Global Iter: 1662300 training acc: 0.25
Global Iter: 1662400 training loss: 2.03251
Global Iter: 1662400 training acc: 0.1875
Global Iter: 1662500 training loss: 2.01584
Global Iter: 1662500 training acc: 0.15625
Global Iter: 1662600 training loss: 2.0851
Global Iter: 1662600 training acc: 0.15625
Global Iter: 1662700 training loss: 1.97177
Global Iter: 1662700 training acc: 0.25
Global Iter: 1662800 training loss: 2.13063
Global Iter: 1662800 training acc: 0.3125
Global Iter: 1662900 training loss: 2.04186
Global Iter: 1662900 training acc: 0.25
Global Iter: 1663000 training loss: 1.91572
Global Iter: 1663000 training acc: 0.1875
Global Iter: 1663100 training loss: 1.92464
Global Iter: 1663100 training acc: 0.125
Global Iter: 1663200 training loss: 2.05533
Global Iter: 1663200 training acc: 0.125
Global Iter: 1663300 training loss: 2.01391
Global Iter: 1663300 training acc: 0.21875
Global Iter: 1663400 training loss: 2.00682
Global Iter: 1663400 training acc: 0.1875
Global Iter: 1663500 training loss: 2.00873
Global Iter: 1663500 training acc: 0.125
Global Iter: 1663600 training loss: 1.96762
Global Iter: 1663600 training acc: 0.21875
Global Iter: 1663700 training loss: 2.1072
Global Iter: 1663700 training acc: 0.125
Global Iter: 1663800 training loss: 2.07271
Global Iter: 1663800 training acc: 0.1875
Global Iter: 1663900 training loss: 1.91605
Global Iter: 1663900 training acc: 0.15625
Global Iter: 1664000 training loss: 2.02431
Global Iter: 1664000 training acc: 0.15625
Global Iter: 1664100 training loss: 1.93954
Global Iter: 1664100 training acc: 0.1875
Global Iter: 1664200 training loss: 1.97806
Global Iter: 1664200 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0002017-06-23 04:24:27.048978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1664292
2017-06-23 04:28:49.049355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1666870
5/model.ckpt-1664292
Number of Patches: 41235
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1664292
Global Iter: 1664300 training loss: 1.92445
Global Iter: 1664300 training acc: 0.125
Global Iter: 1664400 training loss: 2.00607
Global Iter: 1664400 training acc: 0.28125
Global Iter: 1664500 training loss: 1.96604
Global Iter: 1664500 training acc: 0.125
Global Iter: 1664600 training loss: 2.03118
Global Iter: 1664600 training acc: 0.0625
Global Iter: 1664700 training loss: 2.00802
Global Iter: 1664700 training acc: 0.15625
Global Iter: 1664800 training loss: 2.00513
Global Iter: 1664800 training acc: 0.21875
Global Iter: 1664900 training loss: 2.08577
Global Iter: 1664900 training acc: 0.15625
Global Iter: 1665000 training loss: 2.04661
Global Iter: 1665000 training acc: 0.1875
Global Iter: 1665100 training loss: 1.95855
Global Iter: 1665100 training acc: 0.1875
Global Iter: 1665200 training loss: 1.98199
Global Iter: 1665200 training acc: 0.25
Global Iter: 1665300 training loss: 2.09885
Global Iter: 1665300 training acc: 0.125
Global Iter: 1665400 training loss: 2.00183
Global Iter: 1665400 training acc: 0.28125
Global Iter: 1665500 training loss: 2.00709
Global Iter: 1665500 training acc: 0.21875
Global Iter: 1665600 training loss: 2.08505
Global Iter: 1665600 training acc: 0.125
Global Iter: 1665700 training loss: 1.95371
Global Iter: 1665700 training acc: 0.125
Global Iter: 1665800 training loss: 1.98046
Global Iter: 1665800 training acc: 0.21875
Global Iter: 1665900 training loss: 1.94124
Global Iter: 1665900 training acc: 0.21875
Global Iter: 1666000 training loss: 2.02094
Global Iter: 1666000 training acc: 0.25
Global Iter: 1666100 training loss: 1.93178
Global Iter: 1666100 training acc: 0.28125
Global Iter: 1666200 training loss: 1.99835
Global Iter: 1666200 training acc: 0.125
Global Iter: 1666300 training loss: 1.99855
Global Iter: 1666300 training acc: 0.1875
Global Iter: 1666400 training loss: 2.02847
Global Iter: 1666400 training acc: 0.15625
Global Iter: 1666500 training loss: 2.01366
Global Iter: 1666500 training acc: 0.21875
Global Iter: 1666600 training loss: 1.95459
Global Iter: 1666600 training acc: 0.21875
Global Iter: 1666700 training loss: 2.00797
Global Iter: 1666700 training acc: 0.09375
Global Iter: 1666800 training loss: 1.9826
Global Iter: 1666800 training acc: 0.28125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1666870
Number of Patches: 40823
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1666870
Global Iter: 1666900 training loss: 1.98108
Global Iter: 1666900 training acc: 0.125
Global Iter: 1667000 training loss: 1.98929
Global Iter: 1667000 training acc: 0.15625
Global Iter: 1667100 training loss: 2.05348
Global Iter: 1667100 training acc: 0.21875
Global Iter: 1667200 training loss: 2.12019
Global Iter: 1667200 training acc: 0.125
Global Iter: 1667300 training loss: 1.96698
Global Iter: 1667300 training acc: 0.1875
Global Iter: 1667400 training loss: 2.08364
Global Iter: 1667400 training acc: 0.1875
Global Iter: 1667500 training loss: 2.00194
Global Iter: 1667500 training acc: 0.125
Global Iter: 1667600 training loss: 2.03446
Global Iter: 1667600 training acc: 0.1875
Global Iter: 1667700 training loss: 1.99627
Global Iter: 1667700 training acc: 0.125
Global Iter: 1667800 training loss: 1.89899
Global Iter: 1667800 training acc: 0.1875
Global Iter: 1667900 training loss: 2.09494
Global Iter: 1667900 training acc: 0.125
Global Iter: 1668000 training loss: 1.97012
Global Iter: 1668000 training acc: 0.15625
Global Iter: 1668100 training loss: 1.94736
Global Iter: 1668100 training acc: 0.15625
Global Iter: 1668200 training loss: 1.99823
Global Iter: 1668200 training acc: 0.25
Global Iter: 1668300 training loss: 2.02059
Global Iter: 1668300 training acc: 0.15625
Global Iter: 1668400 training loss: 1.98332
Global Iter: 1668400 training acc: 0.09375
Global Iter: 1668500 training loss: 2.03444
Global Iter: 1668500 training acc: 0.15625
Global Iter: 1668600 traini2017-06-23 04:33:02.008961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1669422
2017-06-23 04:37:15.200743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1671948
ng loss: 2.01309
Global Iter: 1668600 training acc: 0.125
Global Iter: 1668700 training loss: 2.0678
Global Iter: 1668700 training acc: 0.15625
Global Iter: 1668800 training loss: 2.10701
Global Iter: 1668800 training acc: 0.09375
Global Iter: 1668900 training loss: 2.02935
Global Iter: 1668900 training acc: 0.28125
Global Iter: 1669000 training loss: 1.96235
Global Iter: 1669000 training acc: 0.125
Global Iter: 1669100 training loss: 2.04723
Global Iter: 1669100 training acc: 0.15625
Global Iter: 1669200 training loss: 2.02827
Global Iter: 1669200 training acc: 0.125
Global Iter: 1669300 training loss: 2.12085
Global Iter: 1669300 training acc: 0.15625
Global Iter: 1669400 training loss: 2.00036
Global Iter: 1669400 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1669422
Number of Patches: 40415
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1669422
Global Iter: 1669500 training loss: 1.95447
Global Iter: 1669500 training acc: 0.125
Global Iter: 1669600 training loss: 1.99158
Global Iter: 1669600 training acc: 0.21875
Global Iter: 1669700 training loss: 1.99197
Global Iter: 1669700 training acc: 0.15625
Global Iter: 1669800 training loss: 1.95561
Global Iter: 1669800 training acc: 0.1875
Global Iter: 1669900 training loss: 2.00781
Global Iter: 1669900 training acc: 0.0625
Global Iter: 1670000 training loss: 1.97359
Global Iter: 1670000 training acc: 0.21875
Global Iter: 1670100 training loss: 1.92453
Global Iter: 1670100 training acc: 0.125
Global Iter: 1670200 training loss: 1.94486
Global Iter: 1670200 training acc: 0.1875
Global Iter: 1670300 training loss: 1.99077
Global Iter: 1670300 training acc: 0.15625
Global Iter: 1670400 training loss: 2.09861
Global Iter: 1670400 training acc: 0.15625
Global Iter: 1670500 training loss: 1.96229
Global Iter: 1670500 training acc: 0.09375
Global Iter: 1670600 training loss: 2.0499
Global Iter: 1670600 training acc: 0.09375
Global Iter: 1670700 training loss: 2.0092
Global Iter: 1670700 training acc: 0.125
Global Iter: 1670800 training loss: 1.99097
Global Iter: 1670800 training acc: 0.125
Global Iter: 1670900 training loss: 2.08184
Global Iter: 1670900 training acc: 0.09375
Global Iter: 1671000 training loss: 2.07759
Global Iter: 1671000 training acc: 0.28125
Global Iter: 1671100 training loss: 1.97971
Global Iter: 1671100 training acc: 0.1875
Global Iter: 1671200 training loss: 1.96662
Global Iter: 1671200 training acc: 0.25
Global Iter: 1671300 training loss: 2.04342
Global Iter: 1671300 training acc: 0.15625
Global Iter: 1671400 training loss: 1.91363
Global Iter: 1671400 training acc: 0.21875
Global Iter: 1671500 training loss: 1.99864
Global Iter: 1671500 training acc: 0.09375
Global Iter: 1671600 training loss: 2.0863
Global Iter: 1671600 training acc: 0.25
Global Iter: 1671700 training loss: 2.03289
Global Iter: 1671700 training acc: 0.28125
Global Iter: 1671800 training loss: 1.92115
Global Iter: 1671800 training acc: 0.125
Global Iter: 1671900 training loss: 2.0522
Global Iter: 1671900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1671948
Number of Patches: 40011
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1671948
Global Iter: 1672000 training loss: 2.04524
Global Iter: 1672000 training acc: 0.125
Global Iter: 1672100 training loss: 2.0291
Global Iter: 1672100 training acc: 0.125
Global Iter: 1672200 training loss: 1.93452
Global Iter: 1672200 training acc: 0.21875
Global Iter: 1672300 training loss: 2.05834
Global Iter: 1672300 training acc: 0.0625
Global Iter: 1672400 training loss: 1.9625
Global Iter: 1672400 training acc: 0.09375
Global Iter: 1672500 training loss: 2.04098
Global Iter: 1672500 training acc: 0.0625
Global Iter: 1672600 training loss: 2.01354
Global Iter: 1672600 training acc: 0.15625
Global Iter: 1672700 training loss: 2.12313
Global Iter: 1672700 training acc: 0.21875
Global Iter: 1672800 training loss: 2.04921
Global Iter: 167282017-06-23 04:41:25.639316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1674449
2017-06-23 04:45:31.376345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1676925
00 training acc: 0.1875
Global Iter: 1672900 training loss: 1.91491
Global Iter: 1672900 training acc: 0.1875
Global Iter: 1673000 training loss: 2.03703
Global Iter: 1673000 training acc: 0.21875
Global Iter: 1673100 training loss: 2.09066
Global Iter: 1673100 training acc: 0.03125
Global Iter: 1673200 training loss: 2.05237
Global Iter: 1673200 training acc: 0.09375
Global Iter: 1673300 training loss: 2.01846
Global Iter: 1673300 training acc: 0.125
Global Iter: 1673400 training loss: 1.97982
Global Iter: 1673400 training acc: 0.125
Global Iter: 1673500 training loss: 2.08759
Global Iter: 1673500 training acc: 0.1875
Global Iter: 1673600 training loss: 1.93456
Global Iter: 1673600 training acc: 0.15625
Global Iter: 1673700 training loss: 2.10451
Global Iter: 1673700 training acc: 0.21875
Global Iter: 1673800 training loss: 1.96888
Global Iter: 1673800 training acc: 0.15625
Global Iter: 1673900 training loss: 1.95426
Global Iter: 1673900 training acc: 0.34375
Global Iter: 1674000 training loss: 1.98384
Global Iter: 1674000 training acc: 0.15625
Global Iter: 1674100 training loss: 1.97845
Global Iter: 1674100 training acc: 0.25
Global Iter: 1674200 training loss: 2.01233
Global Iter: 1674200 training acc: 0.15625
Global Iter: 1674300 training loss: 2.03117
Global Iter: 1674300 training acc: 0.125
Global Iter: 1674400 training loss: 1.96268
Global Iter: 1674400 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1674449
Number of Patches: 39611
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1674449
Global Iter: 1674500 training loss: 1.9684
Global Iter: 1674500 training acc: 0.21875
Global Iter: 1674600 training loss: 1.99223
Global Iter: 1674600 training acc: 0.0625
Global Iter: 1674700 training loss: 2.03105
Global Iter: 1674700 training acc: 0.15625
Global Iter: 1674800 training loss: 1.90465
Global Iter: 1674800 training acc: 0.21875
Global Iter: 1674900 training loss: 1.98592
Global Iter: 1674900 training acc: 0.21875
Global Iter: 1675000 training loss: 2.15487
Global Iter: 1675000 training acc: 0.0625
Global Iter: 1675100 training loss: 1.95872
Global Iter: 1675100 training acc: 0.15625
Global Iter: 1675200 training loss: 1.9822
Global Iter: 1675200 training acc: 0.0625
Global Iter: 1675300 training loss: 1.9799
Global Iter: 1675300 training acc: 0.09375
Global Iter: 1675400 training loss: 1.99491
Global Iter: 1675400 training acc: 0.25
Global Iter: 1675500 training loss: 1.89901
Global Iter: 1675500 training acc: 0.3125
Global Iter: 1675600 training loss: 1.94373
Global Iter: 1675600 training acc: 0.25
Global Iter: 1675700 training loss: 2.06805
Global Iter: 1675700 training acc: 0.21875
Global Iter: 1675800 training loss: 1.92296
Global Iter: 1675800 training acc: 0.125
Global Iter: 1675900 training loss: 1.95148
Global Iter: 1675900 training acc: 0.15625
Global Iter: 1676000 training loss: 1.94389
Global Iter: 1676000 training acc: 0.40625
Global Iter: 1676100 training loss: 1.90764
Global Iter: 1676100 training acc: 0.1875
Global Iter: 1676200 training loss: 2.08963
Global Iter: 1676200 training acc: 0.21875
Global Iter: 1676300 training loss: 2.09108
Global Iter: 1676300 training acc: 0.21875
Global Iter: 1676400 training loss: 1.94096
Global Iter: 1676400 training acc: 0.15625
Global Iter: 1676500 training loss: 1.98782
Global Iter: 1676500 training acc: 0.15625
Global Iter: 1676600 training loss: 2.20126
Global Iter: 1676600 training acc: 0.15625
Global Iter: 1676700 training loss: 2.08344
Global Iter: 1676700 training acc: 0.09375
Global Iter: 1676800 training loss: 1.98603
Global Iter: 1676800 training acc: 0.0625
Global Iter: 1676900 training loss: 1.92706
Global Iter: 1676900 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1676925
Number of Patches: 39215
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1676925
Global Iter: 1677000 training loss: 1.98563
Global Iter: 1677000 training acc: 0.25
Glob2017-06-23 04:49:34.686273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1679376
al Iter: 1677100 training loss: 1.96443
Global Iter: 1677100 training acc: 0.125
Global Iter: 1677200 training loss: 1.92801
Global Iter: 1677200 training acc: 0.25
Global Iter: 1677300 training loss: 1.91796
Global Iter: 1677300 training acc: 0.34375
Global Iter: 1677400 training loss: 1.99015
Global Iter: 1677400 training acc: 0.125
Global Iter: 1677500 training loss: 1.93103
Global Iter: 1677500 training acc: 0.34375
Global Iter: 1677600 training loss: 1.94999
Global Iter: 1677600 training acc: 0.1875
Global Iter: 1677700 training loss: 1.94541
Global Iter: 1677700 training acc: 0.15625
Global Iter: 1677800 training loss: 2.02361
Global Iter: 1677800 training acc: 0.125
Global Iter: 1677900 training loss: 1.92595
Global Iter: 1677900 training acc: 0.125
Global Iter: 1678000 training loss: 2.01957
Global Iter: 1678000 training acc: 0.21875
Global Iter: 1678100 training loss: 2.04496
Global Iter: 1678100 training acc: 0.03125
Global Iter: 1678200 training loss: 1.95889
Global Iter: 1678200 training acc: 0.15625
Global Iter: 1678300 training loss: 2.06396
Global Iter: 1678300 training acc: 0.125
Global Iter: 1678400 training loss: 2.0105
Global Iter: 1678400 training acc: 0.21875
Global Iter: 1678500 training loss: 2.04416
Global Iter: 1678500 training acc: 0.25
Global Iter: 1678600 training loss: 1.91589
Global Iter: 1678600 training acc: 0.21875
Global Iter: 1678700 training loss: 2.03007
Global Iter: 1678700 training acc: 0.15625
Global Iter: 1678800 training loss: 2.03753
Global Iter: 1678800 training acc: 0.09375
Global Iter: 1678900 training loss: 1.88016
Global Iter: 1678900 training acc: 0.1875
Global Iter: 1679000 training loss: 1.92918
Global Iter: 1679000 training acc: 0.1875
Global Iter: 1679100 training loss: 2.03981
Global Iter: 1679100 training acc: 0.1875
Global Iter: 1679200 training loss: 2.04117
Global Iter: 1679200 training acc: 0.1875
Global Iter: 1679300 training loss: 1.95751
Global Iter: 1679300 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1679376
Number of Patches: 38823
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1679376
Global Iter: 1679400 training loss: 1.97942
Global Iter: 1679400 training acc: 0.15625
Global Iter: 1679500 training loss: 1.97356
Global Iter: 1679500 training acc: 0.15625
Global Iter: 1679600 training loss: 1.97452
Global Iter: 1679600 training acc: 0.15625
Global Iter: 1679700 training loss: 2.06807
Global Iter: 1679700 training acc: 0.09375
Global Iter: 1679800 training loss: 1.87392
Global Iter: 1679800 training acc: 0.28125
Global Iter: 1679900 training loss: 1.97392
Global Iter: 1679900 training acc: 0.21875
Global Iter: 1680000 training loss: 2.08703
Global Iter: 1680000 training acc: 0.09375
Global Iter: 1680100 training loss: 2.11098
Global Iter: 1680100 training acc: 0.03125
Global Iter: 1680200 training loss: 1.97271
Global Iter: 1680200 training acc: 0.25
Global Iter: 1680300 training loss: 2.07595
Global Iter: 1680300 training acc: 0.125
Global Iter: 1680400 training loss: 2.07221
Global Iter: 1680400 training acc: 0.125
Global Iter: 1680500 training loss: 1.9472
Global Iter: 1680500 training acc: 0.21875
Global Iter: 1680600 training loss: 1.93745
Global Iter: 1680600 training acc: 0.1875
Global Iter: 1680700 training loss: 1.97399
Global Iter: 1680700 training acc: 0.15625
Global Iter: 1680800 training loss: 2.04811
Global Iter: 1680800 training acc: 0.15625
Global Iter: 1680900 training loss: 2.01559
Global Iter: 1680900 training acc: 0.0625
Global Iter: 1681000 training loss: 2.04965
Global Iter: 1681000 training acc: 0.15625
Global Iter: 1681100 training loss: 1.95114
Global Iter: 1681100 training acc: 0.1875
Global Iter: 1681200 training loss: 2.09744
Global Iter: 1681200 training acc: 0.09375
Global Iter: 1681300 training loss: 1.95983
Global Iter: 1681300 training acc: 0.21875
Global Iter: 1681400 training loss: 1.94341
Global Iter: 1681400 training acc: 0.125
Global Iter: 1681500 training loss: 2.00154
Global Iter: 1681500 training acc: 0.218752017-06-23 04:53:37.195676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1681803
2017-06-23 04:57:34.250224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1684206

Global Iter: 1681600 training loss: 2.09391
Global Iter: 1681600 training acc: 0.125
Global Iter: 1681700 training loss: 2.00253
Global Iter: 1681700 training acc: 0.15625
Global Iter: 1681800 training loss: 1.97828
Global Iter: 1681800 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1681803
Number of Patches: 38435
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1681803
Global Iter: 1681900 training loss: 2.03817
Global Iter: 1681900 training acc: 0.28125
Global Iter: 1682000 training loss: 2.00527
Global Iter: 1682000 training acc: 0.0625
Global Iter: 1682100 training loss: 1.95758
Global Iter: 1682100 training acc: 0.15625
Global Iter: 1682200 training loss: 1.94174
Global Iter: 1682200 training acc: 0.15625
Global Iter: 1682300 training loss: 1.9684
Global Iter: 1682300 training acc: 0.09375
Global Iter: 1682400 training loss: 2.00835
Global Iter: 1682400 training acc: 0.1875
Global Iter: 1682500 training loss: 2.05591
Global Iter: 1682500 training acc: 0.09375
Global Iter: 1682600 training loss: 2.00346
Global Iter: 1682600 training acc: 0.1875
Global Iter: 1682700 training loss: 2.17491
Global Iter: 1682700 training acc: 0.125
Global Iter: 1682800 training loss: 1.96606
Global Iter: 1682800 training acc: 0.3125
Global Iter: 1682900 training loss: 2.05854
Global Iter: 1682900 training acc: 0.09375
Global Iter: 1683000 training loss: 1.99916
Global Iter: 1683000 training acc: 0.125
Global Iter: 1683100 training loss: 2.01774
Global Iter: 1683100 training acc: 0.28125
Global Iter: 1683200 training loss: 2.02349
Global Iter: 1683200 training acc: 0.125
Global Iter: 1683300 training loss: 1.98204
Global Iter: 1683300 training acc: 0.15625
Global Iter: 1683400 training loss: 2.03211
Global Iter: 1683400 training acc: 0.15625
Global Iter: 1683500 training loss: 1.97102
Global Iter: 1683500 training acc: 0.09375
Global Iter: 1683600 training loss: 1.93527
Global Iter: 1683600 training acc: 0.15625
Global Iter: 1683700 training loss: 1.99058
Global Iter: 1683700 training acc: 0.09375
Global Iter: 1683800 training loss: 1.98148
Global Iter: 1683800 training acc: 0.1875
Global Iter: 1683900 training loss: 1.93251
Global Iter: 1683900 training acc: 0.21875
Global Iter: 1684000 training loss: 1.9916
Global Iter: 1684000 training acc: 0.1875
Global Iter: 1684100 training loss: 2.03455
Global Iter: 1684100 training acc: 0.1875
Global Iter: 1684200 training loss: 1.90782
Global Iter: 1684200 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1684206
Number of Patches: 38051
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1684206
Global Iter: 1684300 training loss: 1.92349
Global Iter: 1684300 training acc: 0.28125
Global Iter: 1684400 training loss: 2.05059
Global Iter: 1684400 training acc: 0.1875
Global Iter: 1684500 training loss: 2.13631
Global Iter: 1684500 training acc: 0.0625
Global Iter: 1684600 training loss: 1.94372
Global Iter: 1684600 training acc: 0.09375
Global Iter: 1684700 training loss: 1.96812
Global Iter: 1684700 training acc: 0.25
Global Iter: 1684800 training loss: 1.96547
Global Iter: 1684800 training acc: 0.125
Global Iter: 1684900 training loss: 2.12146
Global Iter: 1684900 training acc: 0.09375
Global Iter: 1685000 training loss: 2.01796
Global Iter: 1685000 training acc: 0.15625
Global Iter: 1685100 training loss: 1.9723
Global Iter: 1685100 training acc: 0.0625
Global Iter: 1685200 training loss: 2.07978
Global Iter: 1685200 training acc: 0.1875
Global Iter: 1685300 training loss: 2.02612
Global Iter: 1685300 training acc: 0.15625
Global Iter: 1685400 training loss: 1.941
Global Iter: 1685400 training acc: 0.25
Global Iter: 1685500 training loss: 2.00251
Global Iter: 1685500 training acc: 0.1875
Global Iter: 1685600 training loss: 1.95801
Global Iter: 1685600 training acc: 0.1875
Global Iter: 1685700 training loss: 1.99834
Global Iter: 1685700 training acc: 0.15625
Global Iter: 1685800 training 2017-06-23 05:01:35.890942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1686585
2017-06-23 05:05:31.789368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1688940
loss: 1.8623
Global Iter: 1685800 training acc: 0.28125
Global Iter: 1685900 training loss: 1.97872
Global Iter: 1685900 training acc: 0.1875
Global Iter: 1686000 training loss: 1.96171
Global Iter: 1686000 training acc: 0.125
Global Iter: 1686100 training loss: 1.98592
Global Iter: 1686100 training acc: 0.09375
Global Iter: 1686200 training loss: 1.97893
Global Iter: 1686200 training acc: 0.09375
Global Iter: 1686300 training loss: 1.9539
Global Iter: 1686300 training acc: 0.21875
Global Iter: 1686400 training loss: 2.09283
Global Iter: 1686400 training acc: 0.21875
Global Iter: 1686500 training loss: 1.99825
Global Iter: 1686500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1686585
Number of Patches: 37671
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1686585
Global Iter: 1686600 training loss: 1.99716
Global Iter: 1686600 training acc: 0.125
Global Iter: 1686700 training loss: 1.91477
Global Iter: 1686700 training acc: 0.15625
Global Iter: 1686800 training loss: 1.97451
Global Iter: 1686800 training acc: 0.15625
Global Iter: 1686900 training loss: 1.96924
Global Iter: 1686900 training acc: 0.125
Global Iter: 1687000 training loss: 2.06156
Global Iter: 1687000 training acc: 0.21875
Global Iter: 1687100 training loss: 1.96566
Global Iter: 1687100 training acc: 0.125
Global Iter: 1687200 training loss: 1.95805
Global Iter: 1687200 training acc: 0.1875
Global Iter: 1687300 training loss: 2.00222
Global Iter: 1687300 training acc: 0.15625
Global Iter: 1687400 training loss: 1.93935
Global Iter: 1687400 training acc: 0.1875
Global Iter: 1687500 training loss: 1.8709
Global Iter: 1687500 training acc: 0.21875
Global Iter: 1687600 training loss: 1.92967
Global Iter: 1687600 training acc: 0.25
Global Iter: 1687700 training loss: 1.94993
Global Iter: 1687700 training acc: 0.25
Global Iter: 1687800 training loss: 2.02007
Global Iter: 1687800 training acc: 0.125
Global Iter: 1687900 training loss: 1.98372
Global Iter: 1687900 training acc: 0.21875
Global Iter: 1688000 training loss: 2.04706
Global Iter: 1688000 training acc: 0.125
Global Iter: 1688100 training loss: 1.9552
Global Iter: 1688100 training acc: 0.125
Global Iter: 1688200 training loss: 1.97828
Global Iter: 1688200 training acc: 0.15625
Global Iter: 1688300 training loss: 1.84615
Global Iter: 1688300 training acc: 0.21875
Global Iter: 1688400 training loss: 1.99897
Global Iter: 1688400 training acc: 0.125
Global Iter: 1688500 training loss: 1.92861
Global Iter: 1688500 training acc: 0.15625
Global Iter: 1688600 training loss: 1.9765
Global Iter: 1688600 training acc: 0.1875
Global Iter: 1688700 training loss: 1.94849
Global Iter: 1688700 training acc: 0.1875
Global Iter: 1688800 training loss: 2.08898
Global Iter: 1688800 training acc: 0.0625
Global Iter: 1688900 training loss: 2.01347
Global Iter: 1688900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1688940
Number of Patches: 37295
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1688940
Global Iter: 1689000 training loss: 1.90686
Global Iter: 1689000 training acc: 0.1875
Global Iter: 1689100 training loss: 2.11618
Global Iter: 1689100 training acc: 0.125
Global Iter: 1689200 training loss: 1.97807
Global Iter: 1689200 training acc: 0.15625
Global Iter: 1689300 training loss: 1.95353
Global Iter: 1689300 training acc: 0.1875
Global Iter: 1689400 training loss: 1.9798
Global Iter: 1689400 training acc: 0.28125
Global Iter: 1689500 training loss: 1.9705
Global Iter: 1689500 training acc: 0.15625
Global Iter: 1689600 training loss: 1.96507
Global Iter: 1689600 training acc: 0.09375
Global Iter: 1689700 training loss: 1.98793
Global Iter: 1689700 training acc: 0.15625
Global Iter: 1689800 training loss: 2.10446
Global Iter: 1689800 training acc: 0.125
Global Iter: 1689900 training loss: 1.93922
Global Iter: 1689900 training acc: 0.25
Global Iter: 1690000 training loss: 1.95724
Global Iter: 1690000 traini2017-06-23 05:09:24.155484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1691271
2017-06-23 05:13:18.307724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1693579
ng acc: 0.21875
Global Iter: 1690100 training loss: 1.88902
Global Iter: 1690100 training acc: 0.3125
Global Iter: 1690200 training loss: 1.99345
Global Iter: 1690200 training acc: 0.21875
Global Iter: 1690300 training loss: 1.97237
Global Iter: 1690300 training acc: 0.28125
Global Iter: 1690400 training loss: 2.00305
Global Iter: 1690400 training acc: 0.0625
Global Iter: 1690500 training loss: 2.02695
Global Iter: 1690500 training acc: 0.125
Global Iter: 1690600 training loss: 2.0667
Global Iter: 1690600 training acc: 0.25
Global Iter: 1690700 training loss: 1.94265
Global Iter: 1690700 training acc: 0.15625
Global Iter: 1690800 training loss: 1.94961
Global Iter: 1690800 training acc: 0.15625
Global Iter: 1690900 training loss: 2.02164
Global Iter: 1690900 training acc: 0.125
Global Iter: 1691000 training loss: 1.98427
Global Iter: 1691000 training acc: 0.125
Global Iter: 1691100 training loss: 1.97557
Global Iter: 1691100 training acc: 0.15625
Global Iter: 1691200 training loss: 1.97299
Global Iter: 1691200 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1691271
Number of Patches: 36923
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1691271
Global Iter: 1691300 training loss: 1.97358
Global Iter: 1691300 training acc: 0.125
Global Iter: 1691400 training loss: 1.98394
Global Iter: 1691400 training acc: 0.125
Global Iter: 1691500 training loss: 2.09475
Global Iter: 1691500 training acc: 0.15625
Global Iter: 1691600 training loss: 1.9557
Global Iter: 1691600 training acc: 0.125
Global Iter: 1691700 training loss: 1.92716
Global Iter: 1691700 training acc: 0.3125
Global Iter: 1691800 training loss: 1.95564
Global Iter: 1691800 training acc: 0.15625
Global Iter: 1691900 training loss: 1.96047
Global Iter: 1691900 training acc: 0.125
Global Iter: 1692000 training loss: 1.97504
Global Iter: 1692000 training acc: 0.21875
Global Iter: 1692100 training loss: 2.00904
Global Iter: 1692100 training acc: 0.125
Global Iter: 1692200 training loss: 1.99647
Global Iter: 1692200 training acc: 0.15625
Global Iter: 1692300 training loss: 2.04841
Global Iter: 1692300 training acc: 0.375
Global Iter: 1692400 training loss: 2.08104
Global Iter: 1692400 training acc: 0.09375
Global Iter: 1692500 training loss: 1.96144
Global Iter: 1692500 training acc: 0.1875
Global Iter: 1692600 training loss: 2.01145
Global Iter: 1692600 training acc: 0.09375
Global Iter: 1692700 training loss: 1.95315
Global Iter: 1692700 training acc: 0.125
Global Iter: 1692800 training loss: 2.00557
Global Iter: 1692800 training acc: 0.125
Global Iter: 1692900 training loss: 2.10991
Global Iter: 1692900 training acc: 0.125
Global Iter: 1693000 training loss: 1.97783
Global Iter: 1693000 training acc: 0.25
Global Iter: 1693100 training loss: 1.91764
Global Iter: 1693100 training acc: 0.1875
Global Iter: 1693200 training loss: 1.8984
Global Iter: 1693200 training acc: 0.1875
Global Iter: 1693300 training loss: 2.01569
Global Iter: 1693300 training acc: 0.125
Global Iter: 1693400 training loss: 1.97251
Global Iter: 1693400 training acc: 0.09375
Global Iter: 1693500 training loss: 2.03348
Global Iter: 1693500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1693579
Number of Patches: 36554
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1693579
Global Iter: 1693600 training loss: 1.99826
Global Iter: 1693600 training acc: 0.0625
Global Iter: 1693700 training loss: 2.01573
Global Iter: 1693700 training acc: 0.1875
Global Iter: 1693800 training loss: 1.96902
Global Iter: 1693800 training acc: 0.125
Global Iter: 1693900 training loss: 2.0208
Global Iter: 1693900 training acc: 0.21875
Global Iter: 1694000 training loss: 1.99265
Global Iter: 1694000 training acc: 0.0625
Global Iter: 1694100 training loss: 1.91559
Global Iter: 1694100 training acc: 0.15625
Global Iter: 1694200 training loss: 2.02451
Global Iter: 1694200 training acc: 0.1875
Global Iter: 1694300 traini2017-06-23 05:17:06.031377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1695864
2017-06-23 05:20:51.457940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1698126
ng loss: 1.95273
Global Iter: 1694300 training acc: 0.21875
Global Iter: 1694400 training loss: 1.96508
Global Iter: 1694400 training acc: 0.125
Global Iter: 1694500 training loss: 2.04956
Global Iter: 1694500 training acc: 0.125
Global Iter: 1694600 training loss: 1.99362
Global Iter: 1694600 training acc: 0.15625
Global Iter: 1694700 training loss: 1.92839
Global Iter: 1694700 training acc: 0.21875
Global Iter: 1694800 training loss: 2.0161
Global Iter: 1694800 training acc: 0.28125
Global Iter: 1694900 training loss: 1.94815
Global Iter: 1694900 training acc: 0.09375
Global Iter: 1695000 training loss: 2.05448
Global Iter: 1695000 training acc: 0.09375
Global Iter: 1695100 training loss: 2.13063
Global Iter: 1695100 training acc: 0.15625
Global Iter: 1695200 training loss: 2.04616
Global Iter: 1695200 training acc: 0.25
Global Iter: 1695300 training loss: 1.98925
Global Iter: 1695300 training acc: 0.15625
Global Iter: 1695400 training loss: 2.02658
Global Iter: 1695400 training acc: 0.125
Global Iter: 1695500 training loss: 2.09093
Global Iter: 1695500 training acc: 0.15625
Global Iter: 1695600 training loss: 1.96467
Global Iter: 1695600 training acc: 0.125
Global Iter: 1695700 training loss: 1.98113
Global Iter: 1695700 training acc: 0.21875
Global Iter: 1695800 training loss: 2.01784
Global Iter: 1695800 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1695864
Number of Patches: 36189
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1695864
Global Iter: 1695900 training loss: 1.92506
Global Iter: 1695900 training acc: 0.28125
Global Iter: 1696000 training loss: 1.96231
Global Iter: 1696000 training acc: 0.1875
Global Iter: 1696100 training loss: 2.01606
Global Iter: 1696100 training acc: 0.21875
Global Iter: 1696200 training loss: 2.0458
Global Iter: 1696200 training acc: 0.125
Global Iter: 1696300 training loss: 2.11969
Global Iter: 1696300 training acc: 0.0625
Global Iter: 1696400 training loss: 2.02474
Global Iter: 1696400 training acc: 0.125
Global Iter: 1696500 training loss: 2.10025
Global Iter: 1696500 training acc: 0.15625
Global Iter: 1696600 training loss: 1.87333
Global Iter: 1696600 training acc: 0.1875
Global Iter: 1696700 training loss: 1.97942
Global Iter: 1696700 training acc: 0.1875
Global Iter: 1696800 training loss: 2.01461
Global Iter: 1696800 training acc: 0.15625
Global Iter: 1696900 training loss: 1.98027
Global Iter: 1696900 training acc: 0.21875
Global Iter: 1697000 training loss: 1.93962
Global Iter: 1697000 training acc: 0.09375
Global Iter: 1697100 training loss: 2.01468
Global Iter: 1697100 training acc: 0.125
Global Iter: 1697200 training loss: 2.0447
Global Iter: 1697200 training acc: 0.1875
Global Iter: 1697300 training loss: 2.04797
Global Iter: 1697300 training acc: 0.25
Global Iter: 1697400 training loss: 2.07126
Global Iter: 1697400 training acc: 0.0625
Global Iter: 1697500 training loss: 2.00024
Global Iter: 1697500 training acc: 0.15625
Global Iter: 1697600 training loss: 2.00971
Global Iter: 1697600 training acc: 0.15625
Global Iter: 1697700 training loss: 2.00652
Global Iter: 1697700 training acc: 0.0625
Global Iter: 1697800 training loss: 1.95108
Global Iter: 1697800 training acc: 0.1875
Global Iter: 1697900 training loss: 2.04854
Global Iter: 1697900 training acc: 0.15625
Global Iter: 1698000 training loss: 2.08584
Global Iter: 1698000 training acc: 0.0625
Global Iter: 1698100 training loss: 1.95559
Global Iter: 1698100 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1698126
Number of Patches: 35828
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1698126
Global Iter: 1698200 training loss: 1.98419
Global Iter: 1698200 training acc: 0.125
Global Iter: 1698300 training loss: 1.89293
Global Iter: 1698300 training acc: 0.28125
Global Iter: 1698400 training loss: 1.94422
Global Iter: 1698400 training acc: 0.21875
Global Iter: 1698500 training loss: 2.05621
Global Iter:2017-06-23 05:24:35.865092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1700366
2017-06-23 05:28:14.972968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1702583
 1698500 training acc: 0.15625
Global Iter: 1698600 training loss: 2.02217
Global Iter: 1698600 training acc: 0.1875
Global Iter: 1698700 training loss: 1.89617
Global Iter: 1698700 training acc: 0.25
Global Iter: 1698800 training loss: 2.14299
Global Iter: 1698800 training acc: 0.15625
Global Iter: 1698900 training loss: 1.91783
Global Iter: 1698900 training acc: 0.1875
Global Iter: 1699000 training loss: 1.92642
Global Iter: 1699000 training acc: 0.21875
Global Iter: 1699100 training loss: 2.04313
Global Iter: 1699100 training acc: 0.1875
Global Iter: 1699200 training loss: 1.90566
Global Iter: 1699200 training acc: 0.3125
Global Iter: 1699300 training loss: 1.91541
Global Iter: 1699300 training acc: 0.15625
Global Iter: 1699400 training loss: 1.91884
Global Iter: 1699400 training acc: 0.28125
Global Iter: 1699500 training loss: 2.00636
Global Iter: 1699500 training acc: 0.125
Global Iter: 1699600 training loss: 1.99501
Global Iter: 1699600 training acc: 0.1875
Global Iter: 1699700 training loss: 1.97806
Global Iter: 1699700 training acc: 0.21875
Global Iter: 1699800 training loss: 1.97643
Global Iter: 1699800 training acc: 0.09375
Global Iter: 1699900 training loss: 1.9923
Global Iter: 1699900 training acc: 0.15625
Global Iter: 1700000 training loss: 1.98368
Global Iter: 1700000 training acc: 0.21875
Global Iter: 1700100 training loss: 1.93556
Global Iter: 1700100 training acc: 0.25
Global Iter: 1700200 training loss: 2.02914
Global Iter: 1700200 training acc: 0.21875
Global Iter: 1700300 training loss: 1.99586
Global Iter: 1700300 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1700366
Number of Patches: 35470
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1700366
Global Iter: 1700400 training loss: 1.96457
Global Iter: 1700400 training acc: 0.125
Global Iter: 1700500 training loss: 2.02325
Global Iter: 1700500 training acc: 0.15625
Global Iter: 1700600 training loss: 1.95243
Global Iter: 1700600 training acc: 0.25
Global Iter: 1700700 training loss: 2.02639
Global Iter: 1700700 training acc: 0.25
Global Iter: 1700800 training loss: 1.95332
Global Iter: 1700800 training acc: 0.15625
Global Iter: 1700900 training loss: 1.99191
Global Iter: 1700900 training acc: 0.1875
Global Iter: 1701000 training loss: 2.0426
Global Iter: 1701000 training acc: 0.1875
Global Iter: 1701100 training loss: 1.96282
Global Iter: 1701100 training acc: 0.15625
Global Iter: 1701200 training loss: 1.99061
Global Iter: 1701200 training acc: 0.1875
Global Iter: 1701300 training loss: 2.03615
Global Iter: 1701300 training acc: 0.15625
Global Iter: 1701400 training loss: 1.93442
Global Iter: 1701400 training acc: 0.25
Global Iter: 1701500 training loss: 1.96184
Global Iter: 1701500 training acc: 0.09375
Global Iter: 1701600 training loss: 2.01265
Global Iter: 1701600 training acc: 0.1875
Global Iter: 1701700 training loss: 1.92268
Global Iter: 1701700 training acc: 0.40625
Global Iter: 1701800 training loss: 2.13461
Global Iter: 1701800 training acc: 0.25
Global Iter: 1701900 training loss: 2.05242
Global Iter: 1701900 training acc: 0.0625
Global Iter: 1702000 training loss: 1.96187
Global Iter: 1702000 training acc: 0.15625
Global Iter: 1702100 training loss: 1.9027
Global Iter: 1702100 training acc: 0.21875
Global Iter: 1702200 training loss: 2.02335
Global Iter: 1702200 training acc: 0.125
Global Iter: 1702300 training loss: 1.97212
Global Iter: 1702300 training acc: 0.125
Global Iter: 1702400 training loss: 1.90184
Global Iter: 1702400 training acc: 0.15625
Global Iter: 1702500 training loss: 1.95341
Global Iter: 1702500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1702583
Number of Patches: 35116
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1702583
Global Iter: 1702600 training loss: 1.99094
Global Iter: 1702600 training acc: 0.15625
Global Iter: 1702700 training loss: 2.00547
Global Iter: 1702700 training acc: 0.1875
Globa2017-06-23 05:31:52.830607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1704778
2017-06-23 05:35:33.827831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1706951
l Iter: 1702800 training loss: 1.98537
Global Iter: 1702800 training acc: 0.125
Global Iter: 1702900 training loss: 2.0009
Global Iter: 1702900 training acc: 0.09375
Global Iter: 1703000 training loss: 1.91487
Global Iter: 1703000 training acc: 0.125
Global Iter: 1703100 training loss: 1.97741
Global Iter: 1703100 training acc: 0.0625
Global Iter: 1703200 training loss: 1.97058
Global Iter: 1703200 training acc: 0.125
Global Iter: 1703300 training loss: 1.98682
Global Iter: 1703300 training acc: 0.1875
Global Iter: 1703400 training loss: 2.00643
Global Iter: 1703400 training acc: 0.1875
Global Iter: 1703500 training loss: 2.06025
Global Iter: 1703500 training acc: 0.125
Global Iter: 1703600 training loss: 1.9876
Global Iter: 1703600 training acc: 0.125
Global Iter: 1703700 training loss: 1.94973
Global Iter: 1703700 training acc: 0.25
Global Iter: 1703800 training loss: 1.97995
Global Iter: 1703800 training acc: 0.21875
Global Iter: 1703900 training loss: 1.99072
Global Iter: 1703900 training acc: 0.15625
Global Iter: 1704000 training loss: 1.96805
Global Iter: 1704000 training acc: 0.09375
Global Iter: 1704100 training loss: 1.96825
Global Iter: 1704100 training acc: 0.28125
Global Iter: 1704200 training loss: 2.12307
Global Iter: 1704200 training acc: 0.125
Global Iter: 1704300 training loss: 1.8925
Global Iter: 1704300 training acc: 0.1875
Global Iter: 1704400 training loss: 1.95811
Global Iter: 1704400 training acc: 0.21875
Global Iter: 1704500 training loss: 2.0426
Global Iter: 1704500 training acc: 0.1875
Global Iter: 1704600 training loss: 1.88358
Global Iter: 1704600 training acc: 0.21875
Global Iter: 1704700 training loss: 2.00865
Global Iter: 1704700 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1704778
Number of Patches: 34765
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1704778
Global Iter: 1704800 training loss: 2.02175
Global Iter: 1704800 training acc: 0.1875
Global Iter: 1704900 training loss: 2.12348
Global Iter: 1704900 training acc: 0.09375
Global Iter: 1705000 training loss: 2.06041
Global Iter: 1705000 training acc: 0.1875
Global Iter: 1705100 training loss: 1.93863
Global Iter: 1705100 training acc: 0.3125
Global Iter: 1705200 training loss: 2.03062
Global Iter: 1705200 training acc: 0.1875
Global Iter: 1705300 training loss: 1.89661
Global Iter: 1705300 training acc: 0.125
Global Iter: 1705400 training loss: 2.04816
Global Iter: 1705400 training acc: 0.15625
Global Iter: 1705500 training loss: 2.01936
Global Iter: 1705500 training acc: 0.34375
Global Iter: 1705600 training loss: 2.05922
Global Iter: 1705600 training acc: 0.28125
Global Iter: 1705700 training loss: 1.93829
Global Iter: 1705700 training acc: 0.09375
Global Iter: 1705800 training loss: 1.97778
Global Iter: 1705800 training acc: 0.1875
Global Iter: 1705900 training loss: 2.06966
Global Iter: 1705900 training acc: 0.15625
Global Iter: 1706000 training loss: 2.08237
Global Iter: 1706000 training acc: 0.09375
Global Iter: 1706100 training loss: 1.93109
Global Iter: 1706100 training acc: 0.09375
Global Iter: 1706200 training loss: 2.06884
Global Iter: 1706200 training acc: 0.1875
Global Iter: 1706300 training loss: 1.96599
Global Iter: 1706300 training acc: 0.125
Global Iter: 1706400 training loss: 1.95532
Global Iter: 1706400 training acc: 0.125
Global Iter: 1706500 training loss: 1.99816
Global Iter: 1706500 training acc: 0.25
Global Iter: 1706600 training loss: 1.96904
Global Iter: 1706600 training acc: 0.1875
Global Iter: 1706700 training loss: 2.01356
Global Iter: 1706700 training acc: 0.09375
Global Iter: 1706800 training loss: 1.90256
Global Iter: 1706800 training acc: 0.09375
Global Iter: 1706900 training loss: 1.99617
Global Iter: 1706900 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1706951
Number of Patches: 34418
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1706951
Global Iter: 1707000 training loss: 2.0462017-06-23 05:39:07.910658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1709103
2017-06-23 05:42:38.523171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
51
Global Iter: 1707000 training acc: 0.125
Global Iter: 1707100 training loss: 2.02792
Global Iter: 1707100 training acc: 0.09375
Global Iter: 1707200 training loss: 1.93613
Global Iter: 1707200 training acc: 0.125
Global Iter: 1707300 training loss: 1.91304
Global Iter: 1707300 training acc: 0.1875
Global Iter: 1707400 training loss: 1.98058
Global Iter: 1707400 training acc: 0.125
Global Iter: 1707500 training loss: 2.03436
Global Iter: 1707500 training acc: 0.28125
Global Iter: 1707600 training loss: 1.91273
Global Iter: 1707600 training acc: 0.21875
Global Iter: 1707700 training loss: 1.94781
Global Iter: 1707700 training acc: 0.15625
Global Iter: 1707800 training loss: 2.00464
Global Iter: 1707800 training acc: 0.1875
Global Iter: 1707900 training loss: 1.92439
Global Iter: 1707900 training acc: 0.3125
Global Iter: 1708000 training loss: 2.16009
Global Iter: 1708000 training acc: 0.125
Global Iter: 1708100 training loss: 2.00736
Global Iter: 1708100 training acc: 0.15625
Global Iter: 1708200 training loss: 2.06593
Global Iter: 1708200 training acc: 0.34375
Global Iter: 1708300 training loss: 2.08343
Global Iter: 1708300 training acc: 0.0
Global Iter: 1708400 training loss: 1.87759
Global Iter: 1708400 training acc: 0.28125
Global Iter: 1708500 training loss: 1.98029
Global Iter: 1708500 training acc: 0.15625
Global Iter: 1708600 training loss: 1.98176
Global Iter: 1708600 training acc: 0.25
Global Iter: 1708700 training loss: 1.94365
Global Iter: 1708700 training acc: 0.15625
Global Iter: 1708800 training loss: 2.03089
Global Iter: 1708800 training acc: 0.0625
Global Iter: 1708900 training loss: 1.95625
Global Iter: 1708900 training acc: 0.1875
Global Iter: 1709000 training loss: 1.96797
Global Iter: 1709000 training acc: 0.0625
Global Iter: 1709100 training loss: 2.05352
Global Iter: 1709100 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1709103
Number of Patches: 34074
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1709103
Global Iter: 1709200 training loss: 1.99985
Global Iter: 1709200 training acc: 0.25
Global Iter: 1709300 training loss: 1.93373
Global Iter: 1709300 training acc: 0.21875
Global Iter: 1709400 training loss: 2.07578
Global Iter: 1709400 training acc: 0.09375
Global Iter: 1709500 training loss: 1.96742
Global Iter: 1709500 training acc: 0.15625
Global Iter: 1709600 training loss: 1.97856
Global Iter: 1709600 training acc: 0.1875
Global Iter: 1709700 training loss: 2.08598
Global Iter: 1709700 training acc: 0.25
Global Iter: 1709800 training loss: 1.9582
Global Iter: 1709800 training acc: 0.1875
Global Iter: 1709900 training loss: 2.12804
Global Iter: 1709900 training acc: 0.09375
Global Iter: 1710000 training loss: 2.02017
Global Iter: 1710000 training acc: 0.21875
Global Iter: 1710100 training loss: 1.98767
Global Iter: 1710100 training acc: 0.21875
Global Iter: 1710200 training loss: 1.98938
Global Iter: 1710200 training acc: 0.21875
Global Iter: 1710300 training loss: 2.03087
Global Iter: 1710300 training acc: 0.125
Global Iter: 1710400 training loss: 1.89954
Global Iter: 1710400 training acc: 0.21875
Global Iter: 1710500 training loss: 1.9761
Global Iter: 1710500 training acc: 0.03125
Global Iter: 1710600 training loss: 1.95638
Global Iter: 1710600 training acc: 0.1875
Global Iter: 1710700 training loss: 2.01563
Global Iter: 1710700 training acc: 0.03125
Global Iter: 1710800 training loss: 1.93236
Global Iter: 1710800 training acc: 0.15625
Global Iter: 1710900 training loss: 2.1082
Global Iter: 1710900 training acc: 0.1875
Global Iter: 1711000 training loss: 1.92948
Global Iter: 1711000 training acc: 0.25
Global Iter: 1711100 training loss: 2.02621
Global Iter: 1711100 training acc: 0.125
Global Iter: 1711200 training loss: 1.98648
Global Iter: 1711200 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1711233
Number of Patches: 33734
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckINFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1711233
2017-06-23 05:46:08.141375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1713342
2017-06-23 05:49:35.206901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1715430
pt-1711233
Global Iter: 1711300 training loss: 2.06658
Global Iter: 1711300 training acc: 0.15625
Global Iter: 1711400 training loss: 2.00651
Global Iter: 1711400 training acc: 0.09375
Global Iter: 1711500 training loss: 1.96728
Global Iter: 1711500 training acc: 0.15625
Global Iter: 1711600 training loss: 2.00697
Global Iter: 1711600 training acc: 0.1875
Global Iter: 1711700 training loss: 2.02905
Global Iter: 1711700 training acc: 0.1875
Global Iter: 1711800 training loss: 1.93976
Global Iter: 1711800 training acc: 0.1875
Global Iter: 1711900 training loss: 2.00332
Global Iter: 1711900 training acc: 0.1875
Global Iter: 1712000 training loss: 1.97581
Global Iter: 1712000 training acc: 0.15625
Global Iter: 1712100 training loss: 2.05061
Global Iter: 1712100 training acc: 0.15625
Global Iter: 1712200 training loss: 1.93802
Global Iter: 1712200 training acc: 0.21875
Global Iter: 1712300 training loss: 1.96304
Global Iter: 1712300 training acc: 0.21875
Global Iter: 1712400 training loss: 2.15372
Global Iter: 1712400 training acc: 0.15625
Global Iter: 1712500 training loss: 1.95765
Global Iter: 1712500 training acc: 0.15625
Global Iter: 1712600 training loss: 1.9801
Global Iter: 1712600 training acc: 0.1875
Global Iter: 1712700 training loss: 2.06508
Global Iter: 1712700 training acc: 0.125
Global Iter: 1712800 training loss: 2.10395
Global Iter: 1712800 training acc: 0.25
Global Iter: 1712900 training loss: 1.94398
Global Iter: 1712900 training acc: 0.15625
Global Iter: 1713000 training loss: 2.01397
Global Iter: 1713000 training acc: 0.21875
Global Iter: 1713100 training loss: 2.0421
Global Iter: 1713100 training acc: 0.21875
Global Iter: 1713200 training loss: 1.98949
Global Iter: 1713200 training acc: 0.25
Global Iter: 1713300 training loss: 2.03637
Global Iter: 1713300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1713342
Number of Patches: 33397
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1713342
Global Iter: 1713400 training loss: 1.96383
Global Iter: 1713400 training acc: 0.15625
Global Iter: 1713500 training loss: 2.0539
Global Iter: 1713500 training acc: 0.09375
Global Iter: 1713600 training loss: 1.98093
Global Iter: 1713600 training acc: 0.15625
Global Iter: 1713700 training loss: 2.12982
Global Iter: 1713700 training acc: 0.1875
Global Iter: 1713800 training loss: 2.1295
Global Iter: 1713800 training acc: 0.125
Global Iter: 1713900 training loss: 1.95045
Global Iter: 1713900 training acc: 0.25
Global Iter: 1714000 training loss: 1.97728
Global Iter: 1714000 training acc: 0.09375
Global Iter: 1714100 training loss: 1.90568
Global Iter: 1714100 training acc: 0.21875
Global Iter: 1714200 training loss: 2.045
Global Iter: 1714200 training acc: 0.28125
Global Iter: 1714300 training loss: 2.01148
Global Iter: 1714300 training acc: 0.15625
Global Iter: 1714400 training loss: 2.00973
Global Iter: 1714400 training acc: 0.125
Global Iter: 1714500 training loss: 2.05713
Global Iter: 1714500 training acc: 0.125
Global Iter: 1714600 training loss: 1.95975
Global Iter: 1714600 training acc: 0.21875
Global Iter: 1714700 training loss: 1.93256
Global Iter: 1714700 training acc: 0.34375
Global Iter: 1714800 training loss: 1.97805
Global Iter: 1714800 training acc: 0.25
Global Iter: 1714900 training loss: 2.05397
Global Iter: 1714900 training acc: 0.125
Global Iter: 1715000 training loss: 2.03664
Global Iter: 1715000 training acc: 0.125
Global Iter: 1715100 training loss: 2.01965
Global Iter: 1715100 training acc: 0.125
Global Iter: 1715200 training loss: 1.93398
Global Iter: 1715200 training acc: 0.15625
Global Iter: 1715300 training loss: 1.99331
Global Iter: 1715300 training acc: 0.125
Global Iter: 1715400 training loss: 2.06808
Global Iter: 1715400 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1715430
Number of Patches: 33064
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1715430
Global Iter: 1715500 tr2017-06-23 05:53:02.318093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1717497
2017-06-23 05:56:26.973892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1719543
aining loss: 1.94182
Global Iter: 1715500 training acc: 0.125
Global Iter: 1715600 training loss: 2.01097
Global Iter: 1715600 training acc: 0.21875
Global Iter: 1715700 training loss: 2.00026
Global Iter: 1715700 training acc: 0.1875
Global Iter: 1715800 training loss: 1.97756
Global Iter: 1715800 training acc: 0.3125
Global Iter: 1715900 training loss: 1.94377
Global Iter: 1715900 training acc: 0.15625
Global Iter: 1716000 training loss: 1.95876
Global Iter: 1716000 training acc: 0.125
Global Iter: 1716100 training loss: 1.91068
Global Iter: 1716100 training acc: 0.1875
Global Iter: 1716200 training loss: 1.99512
Global Iter: 1716200 training acc: 0.125
Global Iter: 1716300 training loss: 2.02795
Global Iter: 1716300 training acc: 0.125
Global Iter: 1716400 training loss: 1.98011
Global Iter: 1716400 training acc: 0.09375
Global Iter: 1716500 training loss: 1.99669
Global Iter: 1716500 training acc: 0.15625
Global Iter: 1716600 training loss: 1.99491
Global Iter: 1716600 training acc: 0.34375
Global Iter: 1716700 training loss: 1.98359
Global Iter: 1716700 training acc: 0.21875
Global Iter: 1716800 training loss: 1.95667
Global Iter: 1716800 training acc: 0.21875
Global Iter: 1716900 training loss: 1.99726
Global Iter: 1716900 training acc: 0.1875
Global Iter: 1717000 training loss: 1.95563
Global Iter: 1717000 training acc: 0.1875
Global Iter: 1717100 training loss: 2.04993
Global Iter: 1717100 training acc: 0.28125
Global Iter: 1717200 training loss: 2.02795
Global Iter: 1717200 training acc: 0.15625
Global Iter: 1717300 training loss: 2.04949
Global Iter: 1717300 training acc: 0.3125
Global Iter: 1717400 training loss: 1.98628
Global Iter: 1717400 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1717497
Number of Patches: 32734
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1717497
Global Iter: 1717500 training loss: 2.0038
Global Iter: 1717500 training acc: 0.15625
Global Iter: 1717600 training loss: 2.03179
Global Iter: 1717600 training acc: 0.21875
Global Iter: 1717700 training loss: 1.9109
Global Iter: 1717700 training acc: 0.25
Global Iter: 1717800 training loss: 2.0208
Global Iter: 1717800 training acc: 0.15625
Global Iter: 1717900 training loss: 2.01462
Global Iter: 1717900 training acc: 0.125
Global Iter: 1718000 training loss: 2.05625
Global Iter: 1718000 training acc: 0.1875
Global Iter: 1718100 training loss: 2.00032
Global Iter: 1718100 training acc: 0.1875
Global Iter: 1718200 training loss: 1.94521
Global Iter: 1718200 training acc: 0.15625
Global Iter: 1718300 training loss: 1.96527
Global Iter: 1718300 training acc: 0.15625
Global Iter: 1718400 training loss: 1.96552
Global Iter: 1718400 training acc: 0.0625
Global Iter: 1718500 training loss: 2.00362
Global Iter: 1718500 training acc: 0.125
Global Iter: 1718600 training loss: 1.97863
Global Iter: 1718600 training acc: 0.125
Global Iter: 1718700 training loss: 1.91071
Global Iter: 1718700 training acc: 0.25
Global Iter: 1718800 training loss: 1.95072
Global Iter: 1718800 training acc: 0.15625
Global Iter: 1718900 training loss: 2.00803
Global Iter: 1718900 training acc: 0.0625
Global Iter: 1719000 training loss: 2.0664
Global Iter: 1719000 training acc: 0.125
Global Iter: 1719100 training loss: 2.00688
Global Iter: 1719100 training acc: 0.09375
Global Iter: 1719200 training loss: 2.10634
Global Iter: 1719200 training acc: 0.125
Global Iter: 1719300 training loss: 2.01005
Global Iter: 1719300 training acc: 0.09375
Global Iter: 1719400 training loss: 2.02362
Global Iter: 1719400 training acc: 0.15625
Global Iter: 1719500 training loss: 2.02948
Global Iter: 1719500 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1719543
Number of Patches: 32407
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1719543
Global Iter: 1719600 training loss: 1.90416
Global Iter: 1719600 training acc: 0.21875
Global Iter: 1719700 training loss: 1.94177
Global Iter:2017-06-23 05:59:56.056642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1721569
2017-06-23 06:03:16.313981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1723575
 1719700 training acc: 0.0625
Global Iter: 1719800 training loss: 1.98761
Global Iter: 1719800 training acc: 0.25
Global Iter: 1719900 training loss: 2.02343
Global Iter: 1719900 training acc: 0.21875
Global Iter: 1720000 training loss: 1.99979
Global Iter: 1720000 training acc: 0.21875
Global Iter: 1720100 training loss: 1.89561
Global Iter: 1720100 training acc: 0.1875
Global Iter: 1720200 training loss: 1.91216
Global Iter: 1720200 training acc: 0.28125
Global Iter: 1720300 training loss: 2.01738
Global Iter: 1720300 training acc: 0.1875
Global Iter: 1720400 training loss: 2.03957
Global Iter: 1720400 training acc: 0.125
Global Iter: 1720500 training loss: 1.93811
Global Iter: 1720500 training acc: 0.125
Global Iter: 1720600 training loss: 1.99004
Global Iter: 1720600 training acc: 0.25
Global Iter: 1720700 training loss: 2.01487
Global Iter: 1720700 training acc: 0.09375
Global Iter: 1720800 training loss: 2.08201
Global Iter: 1720800 training acc: 0.15625
Global Iter: 1720900 training loss: 2.03299
Global Iter: 1720900 training acc: 0.15625
Global Iter: 1721000 training loss: 1.94478
Global Iter: 1721000 training acc: 0.28125
Global Iter: 1721100 training loss: 2.01483
Global Iter: 1721100 training acc: 0.125
Global Iter: 1721200 training loss: 2.01365
Global Iter: 1721200 training acc: 0.1875
Global Iter: 1721300 training loss: 1.97176
Global Iter: 1721300 training acc: 0.15625
Global Iter: 1721400 training loss: 2.02068
Global Iter: 1721400 training acc: 0.21875
Global Iter: 1721500 training loss: 2.01156
Global Iter: 1721500 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1721569
Number of Patches: 32083
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1721569
Global Iter: 1721600 training loss: 2.04047
Global Iter: 1721600 training acc: 0.0625
Global Iter: 1721700 training loss: 1.96392
Global Iter: 1721700 training acc: 0.21875
Global Iter: 1721800 training loss: 2.00041
Global Iter: 1721800 training acc: 0.125
Global Iter: 1721900 training loss: 1.9426
Global Iter: 1721900 training acc: 0.125
Global Iter: 1722000 training loss: 1.96345
Global Iter: 1722000 training acc: 0.1875
Global Iter: 1722100 training loss: 1.97079
Global Iter: 1722100 training acc: 0.15625
Global Iter: 1722200 training loss: 1.97065
Global Iter: 1722200 training acc: 0.15625
Global Iter: 1722300 training loss: 1.94866
Global Iter: 1722300 training acc: 0.1875
Global Iter: 1722400 training loss: 1.91636
Global Iter: 1722400 training acc: 0.28125
Global Iter: 1722500 training loss: 2.01007
Global Iter: 1722500 training acc: 0.125
Global Iter: 1722600 training loss: 1.89842
Global Iter: 1722600 training acc: 0.21875
Global Iter: 1722700 training loss: 1.92897
Global Iter: 1722700 training acc: 0.03125
Global Iter: 1722800 training loss: 2.03827
Global Iter: 1722800 training acc: 0.1875
Global Iter: 1722900 training loss: 2.0108
Global Iter: 1722900 training acc: 0.1875
Global Iter: 1723000 training loss: 1.98452
Global Iter: 1723000 training acc: 0.21875
Global Iter: 1723100 training loss: 2.0381
Global Iter: 1723100 training acc: 0.0625
Global Iter: 1723200 training loss: 1.93594
Global Iter: 1723200 training acc: 0.15625
Global Iter: 1723300 training loss: 1.94009
Global Iter: 1723300 training acc: 0.125
Global Iter: 1723400 training loss: 2.05262
Global Iter: 1723400 training acc: 0.21875
Global Iter: 1723500 training loss: 1.92389
Global Iter: 1723500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1723575
Number of Patches: 31763
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1723575
Global Iter: 1723600 training loss: 1.93877
Global Iter: 1723600 training acc: 0.15625
Global Iter: 1723700 training loss: 2.02415
Global Iter: 1723700 training acc: 0.21875
Global Iter: 1723800 training loss: 1.95199
Global Iter: 1723800 training acc: 0.15625
Global Iter: 1723900 training loss: 2.05369
Global Iter: 1723900 training acc: 0.1875
2017-06-23 06:06:34.907515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1725561
2017-06-23 06:09:53.430561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1727527
Global Iter: 1724000 training loss: 2.00396
Global Iter: 1724000 training acc: 0.125
Global Iter: 1724100 training loss: 2.01697
Global Iter: 1724100 training acc: 0.0625
Global Iter: 1724200 training loss: 1.9744
Global Iter: 1724200 training acc: 0.15625
Global Iter: 1724300 training loss: 1.96981
Global Iter: 1724300 training acc: 0.25
Global Iter: 1724400 training loss: 2.0698
Global Iter: 1724400 training acc: 0.40625
Global Iter: 1724500 training loss: 1.97447
Global Iter: 1724500 training acc: 0.125
Global Iter: 1724600 training loss: 2.00486
Global Iter: 1724600 training acc: 0.15625
Global Iter: 1724700 training loss: 2.00196
Global Iter: 1724700 training acc: 0.09375
Global Iter: 1724800 training loss: 1.99066
Global Iter: 1724800 training acc: 0.0625
Global Iter: 1724900 training loss: 2.00059
Global Iter: 1724900 training acc: 0.25
Global Iter: 1725000 training loss: 1.99206
Global Iter: 1725000 training acc: 0.15625
Global Iter: 1725100 training loss: 1.93213
Global Iter: 1725100 training acc: 0.25
Global Iter: 1725200 training loss: 2.03481
Global Iter: 1725200 training acc: 0.125
Global Iter: 1725300 training loss: 2.02865
Global Iter: 1725300 training acc: 0.0625
Global Iter: 1725400 training loss: 1.94415
Global Iter: 1725400 training acc: 0.1875
Global Iter: 1725500 training loss: 2.01861
Global Iter: 1725500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1725561
Number of Patches: 31446
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1725561
Global Iter: 1725600 training loss: 1.97514
Global Iter: 1725600 training acc: 0.28125
Global Iter: 1725700 training loss: 2.01634
Global Iter: 1725700 training acc: 0.125
Global Iter: 1725800 training loss: 1.98799
Global Iter: 1725800 training acc: 0.125
Global Iter: 1725900 training loss: 2.02581
Global Iter: 1725900 training acc: 0.09375
Global Iter: 1726000 training loss: 2.06463
Global Iter: 1726000 training acc: 0.1875
Global Iter: 1726100 training loss: 1.9275
Global Iter: 1726100 training acc: 0.375
Global Iter: 1726200 training loss: 1.97384
Global Iter: 1726200 training acc: 0.3125
Global Iter: 1726300 training loss: 2.05546
Global Iter: 1726300 training acc: 0.125
Global Iter: 1726400 training loss: 1.9284
Global Iter: 1726400 training acc: 0.28125
Global Iter: 1726500 training loss: 1.98087
Global Iter: 1726500 training acc: 0.125
Global Iter: 1726600 training loss: 2.02906
Global Iter: 1726600 training acc: 0.09375
Global Iter: 1726700 training loss: 2.04829
Global Iter: 1726700 training acc: 0.21875
Global Iter: 1726800 training loss: 2.00174
Global Iter: 1726800 training acc: 0.125
Global Iter: 1726900 training loss: 2.0532
Global Iter: 1726900 training acc: 0.15625
Global Iter: 1727000 training loss: 1.95479
Global Iter: 1727000 training acc: 0.28125
Global Iter: 1727100 training loss: 1.9592
Global Iter: 1727100 training acc: 0.15625
Global Iter: 1727200 training loss: 2.02733
Global Iter: 1727200 training acc: 0.09375
Global Iter: 1727300 training loss: 2.01019
Global Iter: 1727300 training acc: 0.15625
Global Iter: 1727400 training loss: 1.93659
Global Iter: 1727400 training acc: 0.125
Global Iter: 1727500 training loss: 1.97302
Global Iter: 1727500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1727527
Number of Patches: 31132
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1727527
Global Iter: 1727600 training loss: 1.9245
Global Iter: 1727600 training acc: 0.21875
Global Iter: 1727700 training loss: 1.97389
Global Iter: 1727700 training acc: 0.21875
Global Iter: 1727800 training loss: 1.94431
Global Iter: 1727800 training acc: 0.28125
Global Iter: 1727900 training loss: 1.9078
Global Iter: 1727900 training acc: 0.28125
Global Iter: 1728000 training loss: 2.06631
Global Iter: 1728000 training acc: 0.1875
Global Iter: 1728100 training loss: 1.96157
Global Iter: 1728100 training acc: 0.1875
Global Iter: 1728200 training loss: 1.932017-06-23 06:13:06.138010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1729473
2017-06-23 06:16:15.076678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1731400
41
Global Iter: 1728200 training acc: 0.09375
Global Iter: 1728300 training loss: 1.9462
Global Iter: 1728300 training acc: 0.125
Global Iter: 1728400 training loss: 2.09165
Global Iter: 1728400 training acc: 0.15625
Global Iter: 1728500 training loss: 1.96752
Global Iter: 1728500 training acc: 0.125
Global Iter: 1728600 training loss: 2.08127
Global Iter: 1728600 training acc: 0.15625
Global Iter: 1728700 training loss: 1.96869
Global Iter: 1728700 training acc: 0.15625
Global Iter: 1728800 training loss: 2.07605
Global Iter: 1728800 training acc: 0.09375
Global Iter: 1728900 training loss: 1.99089
Global Iter: 1728900 training acc: 0.28125
Global Iter: 1729000 training loss: 2.01043
Global Iter: 1729000 training acc: 0.0625
Global Iter: 1729100 training loss: 2.00447
Global Iter: 1729100 training acc: 0.28125
Global Iter: 1729200 training loss: 1.97007
Global Iter: 1729200 training acc: 0.09375
Global Iter: 1729300 training loss: 2.0081
Global Iter: 1729300 training acc: 0.0625
Global Iter: 1729400 training loss: 1.97651
Global Iter: 1729400 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1729473
Number of Patches: 30821
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1729473
Global Iter: 1729500 training loss: 2.01375
Global Iter: 1729500 training acc: 0.0625
Global Iter: 1729600 training loss: 1.90549
Global Iter: 1729600 training acc: 0.1875
Global Iter: 1729700 training loss: 2.01946
Global Iter: 1729700 training acc: 0.15625
Global Iter: 1729800 training loss: 1.98933
Global Iter: 1729800 training acc: 0.1875
Global Iter: 1729900 training loss: 1.98257
Global Iter: 1729900 training acc: 0.21875
Global Iter: 1730000 training loss: 2.00915
Global Iter: 1730000 training acc: 0.125
Global Iter: 1730100 training loss: 1.99213
Global Iter: 1730100 training acc: 0.0625
Global Iter: 1730200 training loss: 1.96394
Global Iter: 1730200 training acc: 0.125
Global Iter: 1730300 training loss: 2.06039
Global Iter: 1730300 training acc: 0.0625
Global Iter: 1730400 training loss: 1.98469
Global Iter: 1730400 training acc: 0.125
Global Iter: 1730500 training loss: 2.16064
Global Iter: 1730500 training acc: 0.0625
Global Iter: 1730600 training loss: 2.01797
Global Iter: 1730600 training acc: 0.1875
Global Iter: 1730700 training loss: 2.00577
Global Iter: 1730700 training acc: 0.0625
Global Iter: 1730800 training loss: 1.95606
Global Iter: 1730800 training acc: 0.125
Global Iter: 1730900 training loss: 2.00719
Global Iter: 1730900 training acc: 0.15625
Global Iter: 1731000 training loss: 1.90902
Global Iter: 1731000 training acc: 0.25
Global Iter: 1731100 training loss: 1.98204
Global Iter: 1731100 training acc: 0.25
Global Iter: 1731200 training loss: 1.99664
Global Iter: 1731200 training acc: 0.15625
Global Iter: 1731300 training loss: 1.92308
Global Iter: 1731300 training acc: 0.3125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1731400
Number of Patches: 30513
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1731400
Global Iter: 1731500 training loss: 2.03145
Global Iter: 1731500 training acc: 0.1875
Global Iter: 1731600 training loss: 1.92612
Global Iter: 1731600 training acc: 0.1875
Global Iter: 1731700 training loss: 2.00482
Global Iter: 1731700 training acc: 0.1875
Global Iter: 1731800 training loss: 2.01716
Global Iter: 1731800 training acc: 0.0625
Global Iter: 1731900 training loss: 1.95663
Global Iter: 1731900 training acc: 0.1875
Global Iter: 1732000 training loss: 2.12634
Global Iter: 1732000 training acc: 0.125
Global Iter: 1732100 training loss: 2.02074
Global Iter: 1732100 training acc: 0.0625
Global Iter: 1732200 training loss: 2.06659
Global Iter: 1732200 training acc: 0.09375
Global Iter: 1732300 training loss: 1.97547
Global Iter: 1732300 training acc: 0.34375
Global Iter: 1732400 training loss: 2.10422
Global Iter: 1732400 training acc: 0.125
Global Iter: 1732500 training loss: 1.9655
Global Iter: 1732500 training acc2017-06-23 06:19:24.666860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1733308
2017-06-23 06:22:33.543961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1735196
: 0.1875
Global Iter: 1732600 training loss: 1.94271
Global Iter: 1732600 training acc: 0.1875
Global Iter: 1732700 training loss: 2.02813
Global Iter: 1732700 training acc: 0.21875
Global Iter: 1732800 training loss: 1.96384
Global Iter: 1732800 training acc: 0.15625
Global Iter: 1732900 training loss: 1.98292
Global Iter: 1732900 training acc: 0.125
Global Iter: 1733000 training loss: 2.07078
Global Iter: 1733000 training acc: 0.1875
Global Iter: 1733100 training loss: 2.05428
Global Iter: 1733100 training acc: 0.15625
Global Iter: 1733200 training loss: 1.92062
Global Iter: 1733200 training acc: 0.25
Global Iter: 1733300 training loss: 1.96402
Global Iter: 1733300 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1733308
Number of Patches: 30208
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1733308
Global Iter: 1733400 training loss: 1.93649
Global Iter: 1733400 training acc: 0.09375
Global Iter: 1733500 training loss: 1.94213
Global Iter: 1733500 training acc: 0.15625
Global Iter: 1733600 training loss: 2.01986
Global Iter: 1733600 training acc: 0.0625
Global Iter: 1733700 training loss: 2.03254
Global Iter: 1733700 training acc: 0.125
Global Iter: 1733800 training loss: 1.92304
Global Iter: 1733800 training acc: 0.25
Global Iter: 1733900 training loss: 2.14808
Global Iter: 1733900 training acc: 0.1875
Global Iter: 1734000 training loss: 1.98613
Global Iter: 1734000 training acc: 0.15625
Global Iter: 1734100 training loss: 1.9353
Global Iter: 1734100 training acc: 0.125
Global Iter: 1734200 training loss: 1.91089
Global Iter: 1734200 training acc: 0.21875
Global Iter: 1734300 training loss: 2.09304
Global Iter: 1734300 training acc: 0.0625
Global Iter: 1734400 training loss: 2.05773
Global Iter: 1734400 training acc: 0.15625
Global Iter: 1734500 training loss: 1.99782
Global Iter: 1734500 training acc: 0.125
Global Iter: 1734600 training loss: 1.96597
Global Iter: 1734600 training acc: 0.1875
Global Iter: 1734700 training loss: 1.94798
Global Iter: 1734700 training acc: 0.15625
Global Iter: 1734800 training loss: 1.9618
Global Iter: 1734800 training acc: 0.15625
Global Iter: 1734900 training loss: 1.95333
Global Iter: 1734900 training acc: 0.25
Global Iter: 1735000 training loss: 1.97655
Global Iter: 1735000 training acc: 0.25
Global Iter: 1735100 training loss: 1.96032
Global Iter: 1735100 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1735196
Number of Patches: 29906
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1735196
Global Iter: 1735200 training loss: 2.03873
Global Iter: 1735200 training acc: 0.1875
Global Iter: 1735300 training loss: 1.95482
Global Iter: 1735300 training acc: 0.1875
Global Iter: 1735400 training loss: 1.97046
Global Iter: 1735400 training acc: 0.125
Global Iter: 1735500 training loss: 2.09413
Global Iter: 1735500 training acc: 0.15625
Global Iter: 1735600 training loss: 1.98744
Global Iter: 1735600 training acc: 0.21875
Global Iter: 1735700 training loss: 2.01708
Global Iter: 1735700 training acc: 0.125
Global Iter: 1735800 training loss: 2.04752
Global Iter: 1735800 training acc: 0.25
Global Iter: 1735900 training loss: 1.995
Global Iter: 1735900 training acc: 0.125
Global Iter: 1736000 training loss: 1.9751
Global Iter: 1736000 training acc: 0.21875
Global Iter: 1736100 training loss: 1.94809
Global Iter: 1736100 training acc: 0.15625
Global Iter: 1736200 training loss: 2.03431
Global Iter: 1736200 training acc: 0.1875
Global Iter: 1736300 training loss: 1.97443
Global Iter: 1736300 training acc: 0.0625
Global Iter: 1736400 training loss: 2.05165
Global Iter: 1736400 training acc: 0.25
Global Iter: 1736500 training loss: 2.05882
Global Iter: 1736500 training acc: 0.15625
Global Iter: 1736600 training loss: 2.04467
Global Iter: 1736600 training acc: 0.09375
Global Iter: 1736700 training loss: 1.95581
Global Iter: 1736700 training acc: 0.21875
Global Iter: 1736800 training los2017-06-23 06:25:41.958962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1737066
2017-06-23 06:28:46.853527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1738917
2017-06-23 06:31:47.509852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1740749
s: 2.06754
Global Iter: 1736800 training acc: 0.1875
Global Iter: 1736900 training loss: 2.0377
Global Iter: 1736900 training acc: 0.15625
Global Iter: 1737000 training loss: 1.98273
Global Iter: 1737000 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1737066
Number of Patches: 29607
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1737066
Global Iter: 1737100 training loss: 1.94438
Global Iter: 1737100 training acc: 0.1875
Global Iter: 1737200 training loss: 1.96058
Global Iter: 1737200 training acc: 0.15625
Global Iter: 1737300 training loss: 1.96393
Global Iter: 1737300 training acc: 0.125
Global Iter: 1737400 training loss: 2.04759
Global Iter: 1737400 training acc: 0.125
Global Iter: 1737500 training loss: 2.03578
Global Iter: 1737500 training acc: 0.21875
Global Iter: 1737600 training loss: 2.0155
Global Iter: 1737600 training acc: 0.25
Global Iter: 1737700 training loss: 2.06166
Global Iter: 1737700 training acc: 0.125
Global Iter: 1737800 training loss: 2.01781
Global Iter: 1737800 training acc: 0.15625
Global Iter: 1737900 training loss: 1.97945
Global Iter: 1737900 training acc: 0.15625
Global Iter: 1738000 training loss: 1.94551
Global Iter: 1738000 training acc: 0.03125
Global Iter: 1738100 training loss: 2.17667
Global Iter: 1738100 training acc: 0.25
Global Iter: 1738200 training loss: 1.90919
Global Iter: 1738200 training acc: 0.21875
Global Iter: 1738300 training loss: 2.1496
Global Iter: 1738300 training acc: 0.15625
Global Iter: 1738400 training loss: 1.97196
Global Iter: 1738400 training acc: 0.0625
Global Iter: 1738500 training loss: 2.03766
Global Iter: 1738500 training acc: 0.125
Global Iter: 1738600 training loss: 2.00075
Global Iter: 1738600 training acc: 0.15625
Global Iter: 1738700 training loss: 1.99923
Global Iter: 1738700 training acc: 0.1875
Global Iter: 1738800 training loss: 1.92003
Global Iter: 1738800 training acc: 0.25
Global Iter: 1738900 training loss: 1.99851
Global Iter: 1738900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1738917
Number of Patches: 29311
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1738917
Global Iter: 1739000 training loss: 2.04989
Global Iter: 1739000 training acc: 0.21875
Global Iter: 1739100 training loss: 2.00028
Global Iter: 1739100 training acc: 0.125
Global Iter: 1739200 training loss: 2.0957
Global Iter: 1739200 training acc: 0.125
Global Iter: 1739300 training loss: 1.90119
Global Iter: 1739300 training acc: 0.21875
Global Iter: 1739400 training loss: 1.99743
Global Iter: 1739400 training acc: 0.125
Global Iter: 1739500 training loss: 2.09401
Global Iter: 1739500 training acc: 0.125
Global Iter: 1739600 training loss: 2.04274
Global Iter: 1739600 training acc: 0.03125
Global Iter: 1739700 training loss: 1.95339
Global Iter: 1739700 training acc: 0.28125
Global Iter: 1739800 training loss: 1.94699
Global Iter: 1739800 training acc: 0.25
Global Iter: 1739900 training loss: 2.16583
Global Iter: 1739900 training acc: 0.125
Global Iter: 1740000 training loss: 2.02006
Global Iter: 1740000 training acc: 0.1875
Global Iter: 1740100 training loss: 2.09028
Global Iter: 1740100 training acc: 0.0625
Global Iter: 1740200 training loss: 1.90198
Global Iter: 1740200 training acc: 0.21875
Global Iter: 1740300 training loss: 1.93717
Global Iter: 1740300 training acc: 0.15625
Global Iter: 1740400 training loss: 2.05074
Global Iter: 1740400 training acc: 0.0625
Global Iter: 1740500 training loss: 2.08817
Global Iter: 1740500 training acc: 0.1875
Global Iter: 1740600 training loss: 2.07708
Global Iter: 1740600 training acc: 0.1875
Global Iter: 1740700 training loss: 1.97167
Global Iter: 1740700 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1740749
Number of Patches: 29018
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1740749
Global Iter: 1740800 tr2017-06-23 06:34:49.142159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1742563
2017-06-23 06:37:46.512815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1744359
aining loss: 1.9839
Global Iter: 1740800 training acc: 0.125
Global Iter: 1740900 training loss: 2.00343
Global Iter: 1740900 training acc: 0.15625
Global Iter: 1741000 training loss: 1.98292
Global Iter: 1741000 training acc: 0.125
Global Iter: 1741100 training loss: 2.02426
Global Iter: 1741100 training acc: 0.09375
Global Iter: 1741200 training loss: 1.94935
Global Iter: 1741200 training acc: 0.15625
Global Iter: 1741300 training loss: 2.02548
Global Iter: 1741300 training acc: 0.09375
Global Iter: 1741400 training loss: 1.9278
Global Iter: 1741400 training acc: 0.1875
Global Iter: 1741500 training loss: 2.04164
Global Iter: 1741500 training acc: 0.1875
Global Iter: 1741600 training loss: 2.04538
Global Iter: 1741600 training acc: 0.09375
Global Iter: 1741700 training loss: 2.01672
Global Iter: 1741700 training acc: 0.1875
Global Iter: 1741800 training loss: 2.03622
Global Iter: 1741800 training acc: 0.125
Global Iter: 1741900 training loss: 2.07713
Global Iter: 1741900 training acc: 0.09375
Global Iter: 1742000 training loss: 2.06186
Global Iter: 1742000 training acc: 0.09375
Global Iter: 1742100 training loss: 2.07293
Global Iter: 1742100 training acc: 0.15625
Global Iter: 1742200 training loss: 1.94739
Global Iter: 1742200 training acc: 0.21875
Global Iter: 1742300 training loss: 1.96721
Global Iter: 1742300 training acc: 0.125
Global Iter: 1742400 training loss: 2.12423
Global Iter: 1742400 training acc: 0.15625
Global Iter: 1742500 training loss: 2.02627
Global Iter: 1742500 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1742563
Number of Patches: 28728
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1742563
Global Iter: 1742600 training loss: 1.95591
Global Iter: 1742600 training acc: 0.0625
Global Iter: 1742700 training loss: 2.00823
Global Iter: 1742700 training acc: 0.125
Global Iter: 1742800 training loss: 2.13845
Global Iter: 1742800 training acc: 0.03125
Global Iter: 1742900 training loss: 2.01435
Global Iter: 1742900 training acc: 0.28125
Global Iter: 1743000 training loss: 1.99306
Global Iter: 1743000 training acc: 0.09375
Global Iter: 1743100 training loss: 2.03292
Global Iter: 1743100 training acc: 0.15625
Global Iter: 1743200 training loss: 1.93008
Global Iter: 1743200 training acc: 0.125
Global Iter: 1743300 training loss: 2.08475
Global Iter: 1743300 training acc: 0.125
Global Iter: 1743400 training loss: 2.02266
Global Iter: 1743400 training acc: 0.0625
Global Iter: 1743500 training loss: 1.93977
Global Iter: 1743500 training acc: 0.15625
Global Iter: 1743600 training loss: 1.94901
Global Iter: 1743600 training acc: 0.25
Global Iter: 1743700 training loss: 2.00851
Global Iter: 1743700 training acc: 0.1875
Global Iter: 1743800 training loss: 2.08502
Global Iter: 1743800 training acc: 0.09375
Global Iter: 1743900 training loss: 1.97293
Global Iter: 1743900 training acc: 0.09375
Global Iter: 1744000 training loss: 1.9748
Global Iter: 1744000 training acc: 0.125
Global Iter: 1744100 training loss: 2.16284
Global Iter: 1744100 training acc: 0.15625
Global Iter: 1744200 training loss: 1.99555
Global Iter: 1744200 training acc: 0.15625
Global Iter: 1744300 training loss: 2.00987
Global Iter: 1744300 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1744359
Number of Patches: 28441
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1744359
Global Iter: 1744400 training loss: 2.00373
Global Iter: 1744400 training acc: 0.1875
Global Iter: 1744500 training loss: 2.02352
Global Iter: 1744500 training acc: 0.3125
Global Iter: 1744600 training loss: 1.98865
Global Iter: 1744600 training acc: 0.125
Global Iter: 1744700 training loss: 1.9407
Global Iter: 1744700 training acc: 0.09375
Global Iter: 1744800 training loss: 2.00761
Global Iter: 1744800 training acc: 0.09375
Global Iter: 1744900 training loss: 2.05814
Global Iter: 1744900 training acc: 0.0625
Global Iter: 1745000 training loss: 1.93772
Global It2017-06-23 06:40:43.885412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1746137
2017-06-23 06:43:37.225206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1747897
er: 1745000 training acc: 0.21875
Global Iter: 1745100 training loss: 1.91947
Global Iter: 1745100 training acc: 0.21875
Global Iter: 1745200 training loss: 1.97192
Global Iter: 1745200 training acc: 0.1875
Global Iter: 1745300 training loss: 1.97241
Global Iter: 1745300 training acc: 0.15625
Global Iter: 1745400 training loss: 2.00287
Global Iter: 1745400 training acc: 0.09375
Global Iter: 1745500 training loss: 2.19684
Global Iter: 1745500 training acc: 0.125
Global Iter: 1745600 training loss: 1.9965
Global Iter: 1745600 training acc: 0.0625
Global Iter: 1745700 training loss: 2.2358
Global Iter: 1745700 training acc: 0.15625
Global Iter: 1745800 training loss: 1.94269
Global Iter: 1745800 training acc: 0.3125
Global Iter: 1745900 training loss: 1.92709
Global Iter: 1745900 training acc: 0.09375
Global Iter: 1746000 training loss: 1.97703
Global Iter: 1746000 training acc: 0.21875
Global Iter: 1746100 training loss: 1.92718
Global Iter: 1746100 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1746137
Number of Patches: 28157
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1746137
Global Iter: 1746200 training loss: 1.93958
Global Iter: 1746200 training acc: 0.125
Global Iter: 1746300 training loss: 1.95781
Global Iter: 1746300 training acc: 0.125
Global Iter: 1746400 training loss: 2.10861
Global Iter: 1746400 training acc: 0.09375
Global Iter: 1746500 training loss: 1.96253
Global Iter: 1746500 training acc: 0.125
Global Iter: 1746600 training loss: 1.96472
Global Iter: 1746600 training acc: 0.25
Global Iter: 1746700 training loss: 2.12516
Global Iter: 1746700 training acc: 0.125
Global Iter: 1746800 training loss: 2.00266
Global Iter: 1746800 training acc: 0.1875
Global Iter: 1746900 training loss: 2.13391
Global Iter: 1746900 training acc: 0.03125
Global Iter: 1747000 training loss: 2.00933
Global Iter: 1747000 training acc: 0.1875
Global Iter: 1747100 training loss: 2.0024
Global Iter: 1747100 training acc: 0.125
Global Iter: 1747200 training loss: 2.00982
Global Iter: 1747200 training acc: 0.09375
Global Iter: 1747300 training loss: 2.0184
Global Iter: 1747300 training acc: 0.15625
Global Iter: 1747400 training loss: 1.99188
Global Iter: 1747400 training acc: 0.0625
Global Iter: 1747500 training loss: 1.99024
Global Iter: 1747500 training acc: 0.03125
Global Iter: 1747600 training loss: 1.91452
Global Iter: 1747600 training acc: 0.21875
Global Iter: 1747700 training loss: 2.03393
Global Iter: 1747700 training acc: 0.0625
Global Iter: 1747800 training loss: 2.02238
Global Iter: 1747800 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1747897
Number of Patches: 27876
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1747897
Global Iter: 1747900 training loss: 1.95977
Global Iter: 1747900 training acc: 0.125
Global Iter: 1748000 training loss: 1.9613
Global Iter: 1748000 training acc: 0.09375
Global Iter: 1748100 training loss: 2.00014
Global Iter: 1748100 training acc: 0.09375
Global Iter: 1748200 training loss: 2.05799
Global Iter: 1748200 training acc: 0.15625
Global Iter: 1748300 training loss: 2.01067
Global Iter: 1748300 training acc: 0.0625
Global Iter: 1748400 training loss: 2.02812
Global Iter: 1748400 training acc: 0.25
Global Iter: 1748500 training loss: 1.99935
Global Iter: 1748500 training acc: 0.25
Global Iter: 1748600 training loss: 2.03082
Global Iter: 1748600 training acc: 0.1875
Global Iter: 1748700 training loss: 1.96969
Global Iter: 1748700 training acc: 0.1875
Global Iter: 1748800 training loss: 1.95504
Global Iter: 1748800 training acc: 0.25
Global Iter: 1748900 training loss: 2.02101
Global Iter: 1748900 training acc: 0.125
Global Iter: 1749000 training loss: 2.07408
Global Iter: 1749000 training acc: 0.0625
Global Iter: 1749100 training loss: 1.93887
Global Iter: 1749100 training acc: 0.125
Global Iter: 1749200 training loss: 2.00333
Global Iter: 1749200 training acc: 0.21875
Global It2017-06-23 06:46:31.843705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1749640
2017-06-23 06:49:24.160827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1751365
2017-06-23 06:52:15.424912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1753073
er: 1749300 training loss: 2.08423
Global Iter: 1749300 training acc: 0.15625
Global Iter: 1749400 training loss: 2.09188
Global Iter: 1749400 training acc: 0.15625
Global Iter: 1749500 training loss: 1.98587
Global Iter: 1749500 training acc: 0.21875
Global Iter: 1749600 training loss: 2.05278
Global Iter: 1749600 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1749640
Number of Patches: 27598
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1749640
Global Iter: 1749700 training loss: 1.9563
Global Iter: 1749700 training acc: 0.21875
Global Iter: 1749800 training loss: 2.07936
Global Iter: 1749800 training acc: 0.21875
Global Iter: 1749900 training loss: 1.97857
Global Iter: 1749900 training acc: 0.15625
Global Iter: 1750000 training loss: 2.09568
Global Iter: 1750000 training acc: 0.125
Global Iter: 1750100 training loss: 1.99878
Global Iter: 1750100 training acc: 0.1875
Global Iter: 1750200 training loss: 1.98151
Global Iter: 1750200 training acc: 0.1875
Global Iter: 1750300 training loss: 2.07253
Global Iter: 1750300 training acc: 0.09375
Global Iter: 1750400 training loss: 1.92843
Global Iter: 1750400 training acc: 0.09375
Global Iter: 1750500 training loss: 1.95217
Global Iter: 1750500 training acc: 0.1875
Global Iter: 1750600 training loss: 1.9474
Global Iter: 1750600 training acc: 0.3125
Global Iter: 1750700 training loss: 1.94859
Global Iter: 1750700 training acc: 0.25
Global Iter: 1750800 training loss: 2.02241
Global Iter: 1750800 training acc: 0.125
Global Iter: 1750900 training loss: 1.92225
Global Iter: 1750900 training acc: 0.25
Global Iter: 1751000 training loss: 2.00297
Global Iter: 1751000 training acc: 0.21875
Global Iter: 1751100 training loss: 2.08743
Global Iter: 1751100 training acc: 0.125
Global Iter: 1751200 training loss: 1.99075
Global Iter: 1751200 training acc: 0.25
Global Iter: 1751300 training loss: 1.97735
Global Iter: 1751300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1751365
Number of Patches: 27323
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1751365
Global Iter: 1751400 training loss: 1.91004
Global Iter: 1751400 training acc: 0.1875
Global Iter: 1751500 training loss: 1.98674
Global Iter: 1751500 training acc: 0.15625
Global Iter: 1751600 training loss: 2.081
Global Iter: 1751600 training acc: 0.15625
Global Iter: 1751700 training loss: 2.0316
Global Iter: 1751700 training acc: 0.28125
Global Iter: 1751800 training loss: 2.01019
Global Iter: 1751800 training acc: 0.21875
Global Iter: 1751900 training loss: 2.08273
Global Iter: 1751900 training acc: 0.09375
Global Iter: 1752000 training loss: 2.06163
Global Iter: 1752000 training acc: 0.125
Global Iter: 1752100 training loss: 1.93005
Global Iter: 1752100 training acc: 0.21875
Global Iter: 1752200 training loss: 2.00325
Global Iter: 1752200 training acc: 0.15625
Global Iter: 1752300 training loss: 1.97895
Global Iter: 1752300 training acc: 0.1875
Global Iter: 1752400 training loss: 2.05853
Global Iter: 1752400 training acc: 0.09375
Global Iter: 1752500 training loss: 2.00886
Global Iter: 1752500 training acc: 0.21875
Global Iter: 1752600 training loss: 2.03627
Global Iter: 1752600 training acc: 0.1875
Global Iter: 1752700 training loss: 1.97314
Global Iter: 1752700 training acc: 0.21875
Global Iter: 1752800 training loss: 2.02906
Global Iter: 1752800 training acc: 0.09375
Global Iter: 1752900 training loss: 2.07775
Global Iter: 1752900 training acc: 0.09375
Global Iter: 1753000 training loss: 2.01535
Global Iter: 1753000 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1753073
Number of Patches: 27050
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1753073
Global Iter: 1753100 training loss: 2.01587
Global Iter: 1753100 training acc: 0.125
Global Iter: 1753200 training loss: 1.94369
Global Iter: 1753200 traini2017-06-23 06:55:09.363489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1754764
2017-06-23 06:57:59.876396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1756438
ng acc: 0.21875
Global Iter: 1753300 training loss: 1.96764
Global Iter: 1753300 training acc: 0.1875
Global Iter: 1753400 training loss: 1.97857
Global Iter: 1753400 training acc: 0.25
Global Iter: 1753500 training loss: 1.95135
Global Iter: 1753500 training acc: 0.3125
Global Iter: 1753600 training loss: 2.02213
Global Iter: 1753600 training acc: 0.21875
Global Iter: 1753700 training loss: 2.00398
Global Iter: 1753700 training acc: 0.09375
Global Iter: 1753800 training loss: 2.03169
Global Iter: 1753800 training acc: 0.21875
Global Iter: 1753900 training loss: 1.97116
Global Iter: 1753900 training acc: 0.1875
Global Iter: 1754000 training loss: 1.97032
Global Iter: 1754000 training acc: 0.0
Global Iter: 1754100 training loss: 1.99119
Global Iter: 1754100 training acc: 0.125
Global Iter: 1754200 training loss: 1.9482
Global Iter: 1754200 training acc: 0.09375
Global Iter: 1754300 training loss: 2.11481
Global Iter: 1754300 training acc: 0.09375
Global Iter: 1754400 training loss: 2.08807
Global Iter: 1754400 training acc: 0.09375
Global Iter: 1754500 training loss: 2.08058
Global Iter: 1754500 training acc: 0.03125
Global Iter: 1754600 training loss: 1.88213
Global Iter: 1754600 training acc: 0.0625
Global Iter: 1754700 training loss: 2.01856
Global Iter: 1754700 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1754764
Number of Patches: 26780
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1754764
Global Iter: 1754800 training loss: 2.01491
Global Iter: 1754800 training acc: 0.15625
Global Iter: 1754900 training loss: 1.98937
Global Iter: 1754900 training acc: 0.125
Global Iter: 1755000 training loss: 2.00837
Global Iter: 1755000 training acc: 0.0625
Global Iter: 1755100 training loss: 1.92009
Global Iter: 1755100 training acc: 0.15625
Global Iter: 1755200 training loss: 1.85859
Global Iter: 1755200 training acc: 0.21875
Global Iter: 1755300 training loss: 1.94777
Global Iter: 1755300 training acc: 0.1875
Global Iter: 1755400 training loss: 1.98149
Global Iter: 1755400 training acc: 0.25
Global Iter: 1755500 training loss: 2.05717
Global Iter: 1755500 training acc: 0.0625
Global Iter: 1755600 training loss: 2.02147
Global Iter: 1755600 training acc: 0.21875
Global Iter: 1755700 training loss: 1.99431
Global Iter: 1755700 training acc: 0.21875
Global Iter: 1755800 training loss: 2.00965
Global Iter: 1755800 training acc: 0.09375
Global Iter: 1755900 training loss: 2.03391
Global Iter: 1755900 training acc: 0.09375
Global Iter: 1756000 training loss: 2.01305
Global Iter: 1756000 training acc: 0.3125
Global Iter: 1756100 training loss: 1.96851
Global Iter: 1756100 training acc: 0.09375
Global Iter: 1756200 training loss: 2.04023
Global Iter: 1756200 training acc: 0.1875
Global Iter: 1756300 training loss: 2.04356
Global Iter: 1756300 training acc: 0.09375
Global Iter: 1756400 training loss: 1.97343
Global Iter: 1756400 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1756438
Number of Patches: 26513
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1756438
Global Iter: 1756500 training loss: 1.94068
Global Iter: 1756500 training acc: 0.09375
Global Iter: 1756600 training loss: 1.99248
Global Iter: 1756600 training acc: 0.0625
Global Iter: 1756700 training loss: 1.99202
Global Iter: 1756700 training acc: 0.09375
Global Iter: 1756800 training loss: 2.10382
Global Iter: 1756800 training acc: 0.0625
Global Iter: 1756900 training loss: 1.90117
Global Iter: 1756900 training acc: 0.25
Global Iter: 1757000 training loss: 2.04007
Global Iter: 1757000 training acc: 0.125
Global Iter: 1757100 training loss: 2.10217
Global Iter: 1757100 training acc: 0.125
Global Iter: 1757200 training loss: 2.00072
Global Iter: 1757200 training acc: 0.15625
Global Iter: 1757300 training loss: 2.03945
Global Iter: 1757300 training acc: 0.0625
Global Iter: 1757400 training loss: 1.99612
Global Iter: 1757400 training acc: 0.15625
Global Iter: 1752017-06-23 07:00:45.358990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1758096
2017-06-23 07:03:32.192669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1759737
2017-06-23 07:06:15.770628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1761362
7500 training loss: 1.9959
Global Iter: 1757500 training acc: 0.1875
Global Iter: 1757600 training loss: 1.94046
Global Iter: 1757600 training acc: 0.1875
Global Iter: 1757700 training loss: 2.08209
Global Iter: 1757700 training acc: 0.0625
Global Iter: 1757800 training loss: 2.03448
Global Iter: 1757800 training acc: 0.1875
Global Iter: 1757900 training loss: 2.04907
Global Iter: 1757900 training acc: 0.0625
Global Iter: 1758000 training loss: 1.89523
Global Iter: 1758000 training acc: 0.375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1758096
Number of Patches: 26248
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1758096
Global Iter: 1758100 training loss: 2.02406
Global Iter: 1758100 training acc: 0.15625
Global Iter: 1758200 training loss: 1.94367
Global Iter: 1758200 training acc: 0.0625
Global Iter: 1758300 training loss: 1.97723
Global Iter: 1758300 training acc: 0.15625
Global Iter: 1758400 training loss: 1.96357
Global Iter: 1758400 training acc: 0.34375
Global Iter: 1758500 training loss: 2.23139
Global Iter: 1758500 training acc: 0.21875
Global Iter: 1758600 training loss: 1.9875
Global Iter: 1758600 training acc: 0.21875
Global Iter: 1758700 training loss: 1.99572
Global Iter: 1758700 training acc: 0.15625
Global Iter: 1758800 training loss: 2.04863
Global Iter: 1758800 training acc: 0.0625
Global Iter: 1758900 training loss: 2.07643
Global Iter: 1758900 training acc: 0.25
Global Iter: 1759000 training loss: 2.05877
Global Iter: 1759000 training acc: 0.21875
Global Iter: 1759100 training loss: 2.03212
Global Iter: 1759100 training acc: 0.15625
Global Iter: 1759200 training loss: 1.98508
Global Iter: 1759200 training acc: 0.09375
Global Iter: 1759300 training loss: 1.95375
Global Iter: 1759300 training acc: 0.21875
Global Iter: 1759400 training loss: 2.09323
Global Iter: 1759400 training acc: 0.28125
Global Iter: 1759500 training loss: 2.05916
Global Iter: 1759500 training acc: 0.09375
Global Iter: 1759600 training loss: 2.16565
Global Iter: 1759600 training acc: 0.25
Global Iter: 1759700 training loss: 2.05341
Global Iter: 1759700 training acc: 0.28125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1759737
Number of Patches: 25986
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1759737
Global Iter: 1759800 training loss: 1.98319
Global Iter: 1759800 training acc: 0.1875
Global Iter: 1759900 training loss: 2.00297
Global Iter: 1759900 training acc: 0.125
Global Iter: 1760000 training loss: 2.01576
Global Iter: 1760000 training acc: 0.125
Global Iter: 1760100 training loss: 2.05088
Global Iter: 1760100 training acc: 0.09375
Global Iter: 1760200 training loss: 2.00955
Global Iter: 1760200 training acc: 0.15625
Global Iter: 1760300 training loss: 1.96458
Global Iter: 1760300 training acc: 0.15625
Global Iter: 1760400 training loss: 1.98297
Global Iter: 1760400 training acc: 0.15625
Global Iter: 1760500 training loss: 1.95976
Global Iter: 1760500 training acc: 0.09375
Global Iter: 1760600 training loss: 2.02794
Global Iter: 1760600 training acc: 0.125
Global Iter: 1760700 training loss: 1.90414
Global Iter: 1760700 training acc: 0.3125
Global Iter: 1760800 training loss: 2.02195
Global Iter: 1760800 training acc: 0.125
Global Iter: 1760900 training loss: 2.03574
Global Iter: 1760900 training acc: 0.15625
Global Iter: 1761000 training loss: 1.94593
Global Iter: 1761000 training acc: 0.0625
Global Iter: 1761100 training loss: 1.92785
Global Iter: 1761100 training acc: 0.21875
Global Iter: 1761200 training loss: 1.98118
Global Iter: 1761200 training acc: 0.21875
Global Iter: 1761300 training loss: 1.9796
Global Iter: 1761300 training acc: 0.28125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1761362
Number of Patches: 25727
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1761362
Global Iter: 1761400 training loss: 1.93837
Global Iter: 1761400 training ac2017-06-23 07:08:55.750408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1762970
2017-06-23 07:11:34.447811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1764562
c: 0.15625
Global Iter: 1761500 training loss: 2.00063
Global Iter: 1761500 training acc: 0.15625
Global Iter: 1761600 training loss: 1.91581
Global Iter: 1761600 training acc: 0.25
Global Iter: 1761700 training loss: 2.00088
Global Iter: 1761700 training acc: 0.03125
Global Iter: 1761800 training loss: 2.03777
Global Iter: 1761800 training acc: 0.125
Global Iter: 1761900 training loss: 2.07249
Global Iter: 1761900 training acc: 0.125
Global Iter: 1762000 training loss: 2.00539
Global Iter: 1762000 training acc: 0.09375
Global Iter: 1762100 training loss: 2.07589
Global Iter: 1762100 training acc: 0.25
Global Iter: 1762200 training loss: 2.02481
Global Iter: 1762200 training acc: 0.1875
Global Iter: 1762300 training loss: 1.98757
Global Iter: 1762300 training acc: 0.25
Global Iter: 1762400 training loss: 1.97588
Global Iter: 1762400 training acc: 0.15625
Global Iter: 1762500 training loss: 2.05782
Global Iter: 1762500 training acc: 0.15625
Global Iter: 1762600 training loss: 1.98991
Global Iter: 1762600 training acc: 0.0625
Global Iter: 1762700 training loss: 1.98622
Global Iter: 1762700 training acc: 0.0625
Global Iter: 1762800 training loss: 1.90691
Global Iter: 1762800 training acc: 0.25
Global Iter: 1762900 training loss: 2.09544
Global Iter: 1762900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1762970
Number of Patches: 25470
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1762970
Global Iter: 1763000 training loss: 2.06236
Global Iter: 1763000 training acc: 0.125
Global Iter: 1763100 training loss: 2.0853
Global Iter: 1763100 training acc: 0.15625
Global Iter: 1763200 training loss: 2.00614
Global Iter: 1763200 training acc: 0.125
Global Iter: 1763300 training loss: 1.9813
Global Iter: 1763300 training acc: 0.1875
Global Iter: 1763400 training loss: 1.93815
Global Iter: 1763400 training acc: 0.21875
Global Iter: 1763500 training loss: 1.935
Global Iter: 1763500 training acc: 0.25
Global Iter: 1763600 training loss: 2.01386
Global Iter: 1763600 training acc: 0.21875
Global Iter: 1763700 training loss: 1.9291
Global Iter: 1763700 training acc: 0.1875
Global Iter: 1763800 training loss: 2.04967
Global Iter: 1763800 training acc: 0.09375
Global Iter: 1763900 training loss: 2.09576
Global Iter: 1763900 training acc: 0.125
Global Iter: 1764000 training loss: 1.97533
Global Iter: 1764000 training acc: 0.21875
Global Iter: 1764100 training loss: 2.11673
Global Iter: 1764100 training acc: 0.0625
Global Iter: 1764200 training loss: 2.09185
Global Iter: 1764200 training acc: 0.21875
Global Iter: 1764300 training loss: 1.98816
Global Iter: 1764300 training acc: 0.09375
Global Iter: 1764400 training loss: 2.03631
Global Iter: 1764400 training acc: 0.03125
Global Iter: 1764500 training loss: 2.09066
Global Iter: 1764500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1764562
Number of Patches: 25216
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1764562
Global Iter: 1764600 training loss: 2.11006
Global Iter: 1764600 training acc: 0.09375
Global Iter: 1764700 training loss: 1.97414
Global Iter: 1764700 training acc: 0.21875
Global Iter: 1764800 training loss: 1.9781
Global Iter: 1764800 training acc: 0.1875
Global Iter: 1764900 training loss: 2.06798
Global Iter: 1764900 training acc: 0.15625
Global Iter: 1765000 training loss: 1.94282
Global Iter: 1765000 training acc: 0.09375
Global Iter: 1765100 training loss: 2.0506
Global Iter: 1765100 training acc: 0.09375
Global Iter: 1765200 training loss: 1.9763
Global Iter: 1765200 training acc: 0.09375
Global Iter: 1765300 training loss: 2.07973
Global Iter: 1765300 training acc: 0.09375
Global Iter: 1765400 training loss: 1.96322
Global Iter: 1765400 training acc: 0.09375
Global Iter: 1765500 training loss: 2.03618
Global Iter: 1765500 training acc: 0.15625
Global Iter: 1765600 training loss: 2.0855
Global Iter: 1765600 training acc: 0.1875
Global Iter: 1765700 traini2017-06-23 07:14:16.302707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1766138
2017-06-23 07:16:53.675893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1767699
2017-06-23 07:19:28.919411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1769244
ng loss: 1.99404
Global Iter: 1765700 training acc: 0.1875
Global Iter: 1765800 training loss: 1.97168
Global Iter: 1765800 training acc: 0.125
Global Iter: 1765900 training loss: 1.90595
Global Iter: 1765900 training acc: 0.1875
Global Iter: 1766000 training loss: 2.06339
Global Iter: 1766000 training acc: 0.09375
Global Iter: 1766100 training loss: 1.96688
Global Iter: 1766100 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1766138
Number of Patches: 24964
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1766138
Global Iter: 1766200 training loss: 1.94761
Global Iter: 1766200 training acc: 0.21875
Global Iter: 1766300 training loss: 1.92206
Global Iter: 1766300 training acc: 0.09375
Global Iter: 1766400 training loss: 2.05318
Global Iter: 1766400 training acc: 0.15625
Global Iter: 1766500 training loss: 2.09804
Global Iter: 1766500 training acc: 0.25
Global Iter: 1766600 training loss: 1.98284
Global Iter: 1766600 training acc: 0.125
Global Iter: 1766700 training loss: 1.97487
Global Iter: 1766700 training acc: 0.125
Global Iter: 1766800 training loss: 1.97127
Global Iter: 1766800 training acc: 0.125
Global Iter: 1766900 training loss: 2.06138
Global Iter: 1766900 training acc: 0.09375
Global Iter: 1767000 training loss: 2.0169
Global Iter: 1767000 training acc: 0.03125
Global Iter: 1767100 training loss: 2.02973
Global Iter: 1767100 training acc: 0.21875
Global Iter: 1767200 training loss: 2.03
Global Iter: 1767200 training acc: 0.21875
Global Iter: 1767300 training loss: 1.99551
Global Iter: 1767300 training acc: 0.0625
Global Iter: 1767400 training loss: 2.00345
Global Iter: 1767400 training acc: 0.25
Global Iter: 1767500 training loss: 1.99101
Global Iter: 1767500 training acc: 0.21875
Global Iter: 1767600 training loss: 2.06215
Global Iter: 1767600 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1767699
Number of Patches: 24715
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1767699
Global Iter: 1767700 training loss: 1.989
Global Iter: 1767700 training acc: 0.1875
Global Iter: 1767800 training loss: 2.01511
Global Iter: 1767800 training acc: 0.15625
Global Iter: 1767900 training loss: 2.04027
Global Iter: 1767900 training acc: 0.125
Global Iter: 1768000 training loss: 1.99086
Global Iter: 1768000 training acc: 0.21875
Global Iter: 1768100 training loss: 1.99534
Global Iter: 1768100 training acc: 0.09375
Global Iter: 1768200 training loss: 2.03599
Global Iter: 1768200 training acc: 0.125
Global Iter: 1768300 training loss: 1.91097
Global Iter: 1768300 training acc: 0.3125
Global Iter: 1768400 training loss: 1.95474
Global Iter: 1768400 training acc: 0.21875
Global Iter: 1768500 training loss: 1.96408
Global Iter: 1768500 training acc: 0.1875
Global Iter: 1768600 training loss: 1.99025
Global Iter: 1768600 training acc: 0.15625
Global Iter: 1768700 training loss: 2.05303
Global Iter: 1768700 training acc: 0.1875
Global Iter: 1768800 training loss: 2.03275
Global Iter: 1768800 training acc: 0.125
Global Iter: 1768900 training loss: 2.10336
Global Iter: 1768900 training acc: 0.1875
Global Iter: 1769000 training loss: 2.08072
Global Iter: 1769000 training acc: 0.1875
Global Iter: 1769100 training loss: 2.06167
Global Iter: 1769100 training acc: 0.3125
Global Iter: 1769200 training loss: 1.98354
Global Iter: 1769200 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1769244
Number of Patches: 24468
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1769244
Global Iter: 1769300 training loss: 2.01332
Global Iter: 1769300 training acc: 0.25
Global Iter: 1769400 training loss: 1.95524
Global Iter: 1769400 training acc: 0.21875
Global Iter: 1769500 training loss: 2.02258
Global Iter: 1769500 training acc: 0.1875
Global Iter: 1769600 training loss: 2.00664
Global Iter: 1769600 training acc: 0.0625
Global Iter: 12017-06-23 07:22:03.024463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1770774
2017-06-23 07:24:36.496866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1772288
2017-06-23 07:27:08.693685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
769700 training loss: 1.98715
Global Iter: 1769700 training acc: 0.03125
Global Iter: 1769800 training loss: 2.09061
Global Iter: 1769800 training acc: 0.1875
Global Iter: 1769900 training loss: 1.95275
Global Iter: 1769900 training acc: 0.1875
Global Iter: 1770000 training loss: 1.93967
Global Iter: 1770000 training acc: 0.21875
Global Iter: 1770100 training loss: 2.02358
Global Iter: 1770100 training acc: 0.03125
Global Iter: 1770200 training loss: 2.0452
Global Iter: 1770200 training acc: 0.125
Global Iter: 1770300 training loss: 1.96327
Global Iter: 1770300 training acc: 0.15625
Global Iter: 1770400 training loss: 1.99461
Global Iter: 1770400 training acc: 0.125
Global Iter: 1770500 training loss: 1.98253
Global Iter: 1770500 training acc: 0.0625
Global Iter: 1770600 training loss: 2.03553
Global Iter: 1770600 training acc: 0.09375
Global Iter: 1770700 training loss: 2.03956
Global Iter: 1770700 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1770774
Number of Patches: 24224
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1770774
Global Iter: 1770800 training loss: 2.05435
Global Iter: 1770800 training acc: 0.09375
Global Iter: 1770900 training loss: 2.00717
Global Iter: 1770900 training acc: 0.0625
Global Iter: 1771000 training loss: 2.05482
Global Iter: 1771000 training acc: 0.09375
Global Iter: 1771100 training loss: 2.09362
Global Iter: 1771100 training acc: 0.125
Global Iter: 1771200 training loss: 2.02394
Global Iter: 1771200 training acc: 0.25
Global Iter: 1771300 training loss: 1.98668
Global Iter: 1771300 training acc: 0.0625
Global Iter: 1771400 training loss: 2.06301
Global Iter: 1771400 training acc: 0.09375
Global Iter: 1771500 training loss: 2.0052
Global Iter: 1771500 training acc: 0.15625
Global Iter: 1771600 training loss: 1.94729
Global Iter: 1771600 training acc: 0.1875
Global Iter: 1771700 training loss: 1.97252
Global Iter: 1771700 training acc: 0.15625
Global Iter: 1771800 training loss: 1.9879
Global Iter: 1771800 training acc: 0.15625
Global Iter: 1771900 training loss: 2.04181
Global Iter: 1771900 training acc: 0.1875
Global Iter: 1772000 training loss: 2.06559
Global Iter: 1772000 training acc: 0.21875
Global Iter: 1772100 training loss: 2.06554
Global Iter: 1772100 training acc: 0.21875
Global Iter: 1772200 training loss: 1.93178
Global Iter: 1772200 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1772288
Number of Patches: 23982
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1772288
Global Iter: 1772300 training loss: 2.02618
Global Iter: 1772300 training acc: 0.03125
Global Iter: 1772400 training loss: 2.15795
Global Iter: 1772400 training acc: 0.0625
Global Iter: 1772500 training loss: 1.98915
Global Iter: 1772500 training acc: 0.1875
Global Iter: 1772600 training loss: 1.98149
Global Iter: 1772600 training acc: 0.0625
Global Iter: 1772700 training loss: 2.00512
Global Iter: 1772700 training acc: 0.09375
Global Iter: 1772800 training loss: 1.97577
Global Iter: 1772800 training acc: 0.3125
Global Iter: 1772900 training loss: 1.99765
Global Iter: 1772900 training acc: 0.09375
Global Iter: 1773000 training loss: 1.97073
Global Iter: 1773000 training acc: 0.15625
Global Iter: 1773100 training loss: 2.00998
Global Iter: 1773100 training acc: 0.125
Global Iter: 1773200 training loss: 1.95074
Global Iter: 1773200 training acc: 0.09375
Global Iter: 1773300 training loss: 2.02858
Global Iter: 1773300 training acc: 0.15625
Global Iter: 1773400 training loss: 2.00542
Global Iter: 1773400 training acc: 0.125
Global Iter: 1773500 training loss: 1.94089
Global Iter: 1773500 training acc: 0.125
Global Iter: 1773600 training loss: 1.98469
Global Iter: 1773600 training acc: 0.25
Global Iter: 1773700 training loss: 2.01962
Global Iter: 1773700 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1773787
Number of Patches: 23743
cheINFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1773787
2017-06-23 07:29:36.716992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1775271
2017-06-23 07:32:07.505607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1776741
ckpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1773787
Global Iter: 1773800 training loss: 1.97841
Global Iter: 1773800 training acc: 0.28125
Global Iter: 1773900 training loss: 2.04115
Global Iter: 1773900 training acc: 0.125
Global Iter: 1774000 training loss: 2.12568
Global Iter: 1774000 training acc: 0.28125
Global Iter: 1774100 training loss: 1.89709
Global Iter: 1774100 training acc: 0.1875
Global Iter: 1774200 training loss: 2.05923
Global Iter: 1774200 training acc: 0.1875
Global Iter: 1774300 training loss: 2.0175
Global Iter: 1774300 training acc: 0.125
Global Iter: 1774400 training loss: 1.91408
Global Iter: 1774400 training acc: 0.1875
Global Iter: 1774500 training loss: 1.98549
Global Iter: 1774500 training acc: 0.09375
Global Iter: 1774600 training loss: 1.92889
Global Iter: 1774600 training acc: 0.125
Global Iter: 1774700 training loss: 2.1176
Global Iter: 1774700 training acc: 0.15625
Global Iter: 1774800 training loss: 2.01867
Global Iter: 1774800 training acc: 0.09375
Global Iter: 1774900 training loss: 1.89747
Global Iter: 1774900 training acc: 0.15625
Global Iter: 1775000 training loss: 1.97184
Global Iter: 1775000 training acc: 0.15625
Global Iter: 1775100 training loss: 1.87903
Global Iter: 1775100 training acc: 0.25
Global Iter: 1775200 training loss: 2.04227
Global Iter: 1775200 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1775271
Number of Patches: 23506
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1775271
Global Iter: 1775300 training loss: 1.98405
Global Iter: 1775300 training acc: 0.09375
Global Iter: 1775400 training loss: 2.00562
Global Iter: 1775400 training acc: 0.1875
Global Iter: 1775500 training loss: 2.13103
Global Iter: 1775500 training acc: 0.1875
Global Iter: 1775600 training loss: 2.05166
Global Iter: 1775600 training acc: 0.09375
Global Iter: 1775700 training loss: 1.97574
Global Iter: 1775700 training acc: 0.125
Global Iter: 1775800 training loss: 1.98143
Global Iter: 1775800 training acc: 0.1875
Global Iter: 1775900 training loss: 2.09651
Global Iter: 1775900 training acc: 0.1875
Global Iter: 1776000 training loss: 2.12979
Global Iter: 1776000 training acc: 0.15625
Global Iter: 1776100 training loss: 1.95612
Global Iter: 1776100 training acc: 0.15625
Global Iter: 1776200 training loss: 1.8546
Global Iter: 1776200 training acc: 0.34375
Global Iter: 1776300 training loss: 1.9884
Global Iter: 1776300 training acc: 0.0625
Global Iter: 1776400 training loss: 1.95845
Global Iter: 1776400 training acc: 0.15625
Global Iter: 1776500 training loss: 2.01514
Global Iter: 1776500 training acc: 0.09375
Global Iter: 1776600 training loss: 2.09111
Global Iter: 1776600 training acc: 0.09375
Global Iter: 1776700 training loss: 2.01689
Global Iter: 1776700 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1776741
Number of Patches: 23271
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1776741
Global Iter: 1776800 training loss: 1.96321
Global Iter: 1776800 training acc: 0.125
Global Iter: 1776900 training loss: 2.0407
Global Iter: 1776900 training acc: 0.09375
Global Iter: 1777000 training loss: 2.10621
Global Iter: 1777000 training acc: 0.125
Global Iter: 1777100 training loss: 1.94596
Global Iter: 1777100 training acc: 0.0625
Global Iter: 1777200 training loss: 1.97411
Global Iter: 1777200 training acc: 0.125
Global Iter: 1777300 training loss: 1.96715
Global Iter: 1777300 training acc: 0.15625
Global Iter: 1777400 training loss: 1.95059
Global Iter: 1777400 training acc: 0.1875
Global Iter: 1777500 training loss: 2.00062
Global Iter: 1777500 training acc: 0.15625
Global Iter: 1777600 training loss: 1.92648
Global Iter: 1777600 training acc: 0.21875
Global Iter: 1777700 training loss: 2.1163
Global Iter: 1777700 training acc: 0.15625
Global Iter: 1777800 training loss: 1.97152
Global Iter: 1777800 training acc: 0.21875
Global Iter: 1777900 2017-06-23 07:34:36.130663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1778196
2017-06-23 07:37:00.767885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1779636
2017-06-23 07:39:23.884486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1781062
training loss: 1.9383
Global Iter: 1777900 training acc: 0.21875
Global Iter: 1778000 training loss: 2.06183
Global Iter: 1778000 training acc: 0.125
Global Iter: 1778100 training loss: 2.02272
Global Iter: 1778100 training acc: 0.34375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1778196
Number of Patches: 23039
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1778196
Global Iter: 1778200 training loss: 2.077
Global Iter: 1778200 training acc: 0.09375
Global Iter: 1778300 training loss: 1.984
Global Iter: 1778300 training acc: 0.09375
Global Iter: 1778400 training loss: 2.01652
Global Iter: 1778400 training acc: 0.09375
Global Iter: 1778500 training loss: 2.05222
Global Iter: 1778500 training acc: 0.09375
Global Iter: 1778600 training loss: 1.9818
Global Iter: 1778600 training acc: 0.125
Global Iter: 1778700 training loss: 1.98983
Global Iter: 1778700 training acc: 0.15625
Global Iter: 1778800 training loss: 2.09967
Global Iter: 1778800 training acc: 0.0
Global Iter: 1778900 training loss: 1.94123
Global Iter: 1778900 training acc: 0.25
Global Iter: 1779000 training loss: 2.07489
Global Iter: 1779000 training acc: 0.15625
Global Iter: 1779100 training loss: 2.02658
Global Iter: 1779100 training acc: 0.21875
Global Iter: 1779200 training loss: 2.11222
Global Iter: 1779200 training acc: 0.125
Global Iter: 1779300 training loss: 1.93836
Global Iter: 1779300 training acc: 0.0625
Global Iter: 1779400 training loss: 1.96611
Global Iter: 1779400 training acc: 0.125
Global Iter: 1779500 training loss: 1.94185
Global Iter: 1779500 training acc: 0.09375
Global Iter: 1779600 training loss: 2.00599
Global Iter: 1779600 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1779636
Number of Patches: 22809
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1779636
Global Iter: 1779700 training loss: 2.03323
Global Iter: 1779700 training acc: 0.15625
Global Iter: 1779800 training loss: 1.90405
Global Iter: 1779800 training acc: 0.25
Global Iter: 1779900 training loss: 1.94451
Global Iter: 1779900 training acc: 0.125
Global Iter: 1780000 training loss: 1.96065
Global Iter: 1780000 training acc: 0.25
Global Iter: 1780100 training loss: 2.01628
Global Iter: 1780100 training acc: 0.21875
Global Iter: 1780200 training loss: 1.97272
Global Iter: 1780200 training acc: 0.09375
Global Iter: 1780300 training loss: 2.05603
Global Iter: 1780300 training acc: 0.1875
Global Iter: 1780400 training loss: 1.92566
Global Iter: 1780400 training acc: 0.15625
Global Iter: 1780500 training loss: 2.00456
Global Iter: 1780500 training acc: 0.125
Global Iter: 1780600 training loss: 1.91618
Global Iter: 1780600 training acc: 0.21875
Global Iter: 1780700 training loss: 2.0012
Global Iter: 1780700 training acc: 0.25
Global Iter: 1780800 training loss: 2.01265
Global Iter: 1780800 training acc: 0.25
Global Iter: 1780900 training loss: 2.08332
Global Iter: 1780900 training acc: 0.125
Global Iter: 1781000 training loss: 2.02836
Global Iter: 1781000 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1781062
Number of Patches: 22581
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1781062
Global Iter: 1781100 training loss: 2.0572
Global Iter: 1781100 training acc: 0.15625
Global Iter: 1781200 training loss: 2.02818
Global Iter: 1781200 training acc: 0.1875
Global Iter: 1781300 training loss: 2.04658
Global Iter: 1781300 training acc: 0.125
Global Iter: 1781400 training loss: 1.8832
Global Iter: 1781400 training acc: 0.125
Global Iter: 1781500 training loss: 2.15625
Global Iter: 1781500 training acc: 0.21875
Global Iter: 1781600 training loss: 1.99228
Global Iter: 1781600 training acc: 0.15625
Global Iter: 1781700 training loss: 1.9765
Global Iter: 1781700 training acc: 0.09375
Global Iter: 1781800 training loss: 2.12858
Global Iter: 1781800 training acc: 0.09375
Global Iter: 17812017-06-23 07:41:45.740999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1782474
2017-06-23 07:44:07.280906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1783872
2017-06-23 07:46:27.561356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1785256
900 training loss: 1.92713
Global Iter: 1781900 training acc: 0.21875
Global Iter: 1782000 training loss: 1.92646
Global Iter: 1782000 training acc: 0.21875
Global Iter: 1782100 training loss: 1.98191
Global Iter: 1782100 training acc: 0.0625
Global Iter: 1782200 training loss: 2.02562
Global Iter: 1782200 training acc: 0.125
Global Iter: 1782300 training loss: 2.00821
Global Iter: 1782300 training acc: 0.125
Global Iter: 1782400 training loss: 2.0829
Global Iter: 1782400 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1782474
Number of Patches: 22356
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1782474
Global Iter: 1782500 training loss: 1.99986
Global Iter: 1782500 training acc: 0.1875
Global Iter: 1782600 training loss: 1.99384
Global Iter: 1782600 training acc: 0.1875
Global Iter: 1782700 training loss: 1.97461
Global Iter: 1782700 training acc: 0.125
Global Iter: 1782800 training loss: 1.98538
Global Iter: 1782800 training acc: 0.09375
Global Iter: 1782900 training loss: 2.10804
Global Iter: 1782900 training acc: 0.09375
Global Iter: 1783000 training loss: 1.98945
Global Iter: 1783000 training acc: 0.28125
Global Iter: 1783100 training loss: 2.00511
Global Iter: 1783100 training acc: 0.21875
Global Iter: 1783200 training loss: 2.06572
Global Iter: 1783200 training acc: 0.03125
Global Iter: 1783300 training loss: 2.01509
Global Iter: 1783300 training acc: 0.125
Global Iter: 1783400 training loss: 2.02171
Global Iter: 1783400 training acc: 0.125
Global Iter: 1783500 training loss: 1.92256
Global Iter: 1783500 training acc: 0.28125
Global Iter: 1783600 training loss: 1.93514
Global Iter: 1783600 training acc: 0.1875
Global Iter: 1783700 training loss: 2.02926
Global Iter: 1783700 training acc: 0.125
Global Iter: 1783800 training loss: 1.89073
Global Iter: 1783800 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1783872
Number of Patches: 22133
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1783872
Global Iter: 1783900 training loss: 1.99869
Global Iter: 1783900 training acc: 0.0
Global Iter: 1784000 training loss: 2.05832
Global Iter: 1784000 training acc: 0.125
Global Iter: 1784100 training loss: 1.99205
Global Iter: 1784100 training acc: 0.125
Global Iter: 1784200 training loss: 2.02692
Global Iter: 1784200 training acc: 0.15625
Global Iter: 1784300 training loss: 1.94523
Global Iter: 1784300 training acc: 0.15625
Global Iter: 1784400 training loss: 1.97222
Global Iter: 1784400 training acc: 0.15625
Global Iter: 1784500 training loss: 1.8864
Global Iter: 1784500 training acc: 0.09375
Global Iter: 1784600 training loss: 1.98053
Global Iter: 1784600 training acc: 0.09375
Global Iter: 1784700 training loss: 1.93789
Global Iter: 1784700 training acc: 0.15625
Global Iter: 1784800 training loss: 1.92509
Global Iter: 1784800 training acc: 0.28125
Global Iter: 1784900 training loss: 1.91814
Global Iter: 1784900 training acc: 0.21875
Global Iter: 1785000 training loss: 2.10286
Global Iter: 1785000 training acc: 0.09375
Global Iter: 1785100 training loss: 2.02363
Global Iter: 1785100 training acc: 0.15625
Global Iter: 1785200 training loss: 1.9945
Global Iter: 1785200 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1785256
Number of Patches: 21912
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1785256
Global Iter: 1785300 training loss: 1.99484
Global Iter: 1785300 training acc: 0.15625
Global Iter: 1785400 training loss: 1.9736
Global Iter: 1785400 training acc: 0.21875
Global Iter: 1785500 training loss: 2.0043
Global Iter: 1785500 training acc: 0.15625
Global Iter: 1785600 training loss: 2.11075
Global Iter: 1785600 training acc: 0.09375
Global Iter: 1785700 training loss: 2.03163
Global Iter: 1785700 training acc: 0.15625
Global Iter: 1785800 training loss: 1.97232
Global Iter: 1785800 training acc: 02017-06-23 07:48:45.894162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1786626
2017-06-23 07:51:02.256600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1787982
2017-06-23 07:53:17.112324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1789325
.125
Global Iter: 1785900 training loss: 2.04519
Global Iter: 1785900 training acc: 0.125
Global Iter: 1786000 training loss: 2.02385
Global Iter: 1786000 training acc: 0.1875
Global Iter: 1786100 training loss: 1.92492
Global Iter: 1786100 training acc: 0.21875
Global Iter: 1786200 training loss: 1.98329
Global Iter: 1786200 training acc: 0.1875
Global Iter: 1786300 training loss: 2.01282
Global Iter: 1786300 training acc: 0.25
Global Iter: 1786400 training loss: 1.99132
Global Iter: 1786400 training acc: 0.0625
Global Iter: 1786500 training loss: 2.0092
Global Iter: 1786500 training acc: 0.15625
Global Iter: 1786600 training loss: 1.97615
Global Iter: 1786600 training acc: 0.03125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1786626
Number of Patches: 21693
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1786626
Global Iter: 1786700 training loss: 1.98588
Global Iter: 1786700 training acc: 0.125
Global Iter: 1786800 training loss: 2.08556
Global Iter: 1786800 training acc: 0.0625
Global Iter: 1786900 training loss: 2.03234
Global Iter: 1786900 training acc: 0.0625
Global Iter: 1787000 training loss: 2.02702
Global Iter: 1787000 training acc: 0.09375
Global Iter: 1787100 training loss: 2.12534
Global Iter: 1787100 training acc: 0.09375
Global Iter: 1787200 training loss: 2.02017
Global Iter: 1787200 training acc: 0.25
Global Iter: 1787300 training loss: 2.11736
Global Iter: 1787300 training acc: 0.1875
Global Iter: 1787400 training loss: 1.99273
Global Iter: 1787400 training acc: 0.25
Global Iter: 1787500 training loss: 2.0694
Global Iter: 1787500 training acc: 0.15625
Global Iter: 1787600 training loss: 1.97164
Global Iter: 1787600 training acc: 0.21875
Global Iter: 1787700 training loss: 1.92432
Global Iter: 1787700 training acc: 0.25
Global Iter: 1787800 training loss: 2.03253
Global Iter: 1787800 training acc: 0.25
Global Iter: 1787900 training loss: 2.06513
Global Iter: 1787900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1787982
Number of Patches: 21477
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1787982
Global Iter: 1788000 training loss: 2.05103
Global Iter: 1788000 training acc: 0.1875
Global Iter: 1788100 training loss: 2.01924
Global Iter: 1788100 training acc: 0.1875
Global Iter: 1788200 training loss: 2.06329
Global Iter: 1788200 training acc: 0.1875
Global Iter: 1788300 training loss: 2.03415
Global Iter: 1788300 training acc: 0.0625
Global Iter: 1788400 training loss: 1.97075
Global Iter: 1788400 training acc: 0.15625
Global Iter: 1788500 training loss: 1.91591
Global Iter: 1788500 training acc: 0.1875
Global Iter: 1788600 training loss: 2.03098
Global Iter: 1788600 training acc: 0.1875
Global Iter: 1788700 training loss: 2.01064
Global Iter: 1788700 training acc: 0.21875
Global Iter: 1788800 training loss: 2.0144
Global Iter: 1788800 training acc: 0.21875
Global Iter: 1788900 training loss: 1.96361
Global Iter: 1788900 training acc: 0.21875
Global Iter: 1789000 training loss: 2.04465
Global Iter: 1789000 training acc: 0.25
Global Iter: 1789100 training loss: 2.03239
Global Iter: 1789100 training acc: 0.21875
Global Iter: 1789200 training loss: 1.933
Global Iter: 1789200 training acc: 0.1875
Global Iter: 1789300 training loss: 2.02514
Global Iter: 1789300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1789325
Number of Patches: 21263
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1789325
Global Iter: 1789400 training loss: 1.93447
Global Iter: 1789400 training acc: 0.15625
Global Iter: 1789500 training loss: 1.98212
Global Iter: 1789500 training acc: 0.1875
Global Iter: 1789600 training loss: 1.96968
Global Iter: 1789600 training acc: 0.09375
Global Iter: 1789700 training loss: 2.03977
Global Iter: 1789700 training acc: 0.1875
Global Iter: 1789800 training loss: 1.97701
Global Iter: 1789800 train2017-06-23 07:55:30.577360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1790654
2017-06-23 07:57:42.841113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1791970
2017-06-23 07:59:55.193482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1793273
ing acc: 0.21875
Global Iter: 1789900 training loss: 1.92678
Global Iter: 1789900 training acc: 0.15625
Global Iter: 1790000 training loss: 1.92828
Global Iter: 1790000 training acc: 0.21875
Global Iter: 1790100 training loss: 2.08725
Global Iter: 1790100 training acc: 0.09375
Global Iter: 1790200 training loss: 2.05801
Global Iter: 1790200 training acc: 0.0625
Global Iter: 1790300 training loss: 1.94185
Global Iter: 1790300 training acc: 0.03125
Global Iter: 1790400 training loss: 2.02531
Global Iter: 1790400 training acc: 0.125
Global Iter: 1790500 training loss: 1.922
Global Iter: 1790500 training acc: 0.15625
Global Iter: 1790600 training loss: 1.98757
Global Iter: 1790600 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1790654
Number of Patches: 21051
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1790654
Global Iter: 1790700 training loss: 1.95054
Global Iter: 1790700 training acc: 0.28125
Global Iter: 1790800 training loss: 1.93318
Global Iter: 1790800 training acc: 0.28125
Global Iter: 1790900 training loss: 2.01674
Global Iter: 1790900 training acc: 0.21875
Global Iter: 1791000 training loss: 1.99123
Global Iter: 1791000 training acc: 0.1875
Global Iter: 1791100 training loss: 1.91826
Global Iter: 1791100 training acc: 0.15625
Global Iter: 1791200 training loss: 1.99545
Global Iter: 1791200 training acc: 0.28125
Global Iter: 1791300 training loss: 2.0338
Global Iter: 1791300 training acc: 0.1875
Global Iter: 1791400 training loss: 2.12246
Global Iter: 1791400 training acc: 0.125
Global Iter: 1791500 training loss: 1.93368
Global Iter: 1791500 training acc: 0.15625
Global Iter: 1791600 training loss: 2.13638
Global Iter: 1791600 training acc: 0.125
Global Iter: 1791700 training loss: 1.98225
Global Iter: 1791700 training acc: 0.15625
Global Iter: 1791800 training loss: 1.91446
Global Iter: 1791800 training acc: 0.15625
Global Iter: 1791900 training loss: 1.92238
Global Iter: 1791900 training acc: 0.3125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1791970
Number of Patches: 20841
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1791970
Global Iter: 1792000 training loss: 1.94589
Global Iter: 1792000 training acc: 0.1875
Global Iter: 1792100 training loss: 1.97123
Global Iter: 1792100 training acc: 0.28125
Global Iter: 1792200 training loss: 2.03734
Global Iter: 1792200 training acc: 0.125
Global Iter: 1792300 training loss: 2.07759
Global Iter: 1792300 training acc: 0.21875
Global Iter: 1792400 training loss: 1.94452
Global Iter: 1792400 training acc: 0.1875
Global Iter: 1792500 training loss: 1.97476
Global Iter: 1792500 training acc: 0.1875
Global Iter: 1792600 training loss: 2.1219
Global Iter: 1792600 training acc: 0.125
Global Iter: 1792700 training loss: 1.92232
Global Iter: 1792700 training acc: 0.1875
Global Iter: 1792800 training loss: 2.15913
Global Iter: 1792800 training acc: 0.15625
Global Iter: 1792900 training loss: 2.08094
Global Iter: 1792900 training acc: 0.15625
Global Iter: 1793000 training loss: 1.9865
Global Iter: 1793000 training acc: 0.21875
Global Iter: 1793100 training loss: 1.98237
Global Iter: 1793100 training acc: 0.0
Global Iter: 1793200 training loss: 2.0247
Global Iter: 1793200 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1793273
Number of Patches: 20633
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1793273
Global Iter: 1793300 training loss: 2.0533
Global Iter: 1793300 training acc: 0.15625
Global Iter: 1793400 training loss: 2.09077
Global Iter: 1793400 training acc: 0.125
Global Iter: 1793500 training loss: 1.94851
Global Iter: 1793500 training acc: 0.125
Global Iter: 1793600 training loss: 1.9672
Global Iter: 1793600 training acc: 0.15625
Global Iter: 1793700 training loss: 1.95718
Global Iter: 1793700 training acc: 0.15625
Global Iter: 1793800 training loss: 1.8941
Global2017-06-23 08:02:05.410537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1794563
2017-06-23 08:04:15.450477: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1795840
2017-06-23 08:06:23.908796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1797104
 Iter: 1793800 training acc: 0.21875
Global Iter: 1793900 training loss: 2.01663
Global Iter: 1793900 training acc: 0.03125
Global Iter: 1794000 training loss: 1.94901
Global Iter: 1794000 training acc: 0.21875
Global Iter: 1794100 training loss: 1.96451
Global Iter: 1794100 training acc: 0.21875
Global Iter: 1794200 training loss: 2.09416
Global Iter: 1794200 training acc: 0.125
Global Iter: 1794300 training loss: 1.91881
Global Iter: 1794300 training acc: 0.375
Global Iter: 1794400 training loss: 1.947
Global Iter: 1794400 training acc: 0.125
Global Iter: 1794500 training loss: 2.0885
Global Iter: 1794500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1794563
Number of Patches: 20427
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1794563
Global Iter: 1794600 training loss: 2.09798
Global Iter: 1794600 training acc: 0.0625
Global Iter: 1794700 training loss: 1.99886
Global Iter: 1794700 training acc: 0.15625
Global Iter: 1794800 training loss: 2.06003
Global Iter: 1794800 training acc: 0.1875
Global Iter: 1794900 training loss: 2.15202
Global Iter: 1794900 training acc: 0.21875
Global Iter: 1795000 training loss: 1.99911
Global Iter: 1795000 training acc: 0.125
Global Iter: 1795100 training loss: 1.94112
Global Iter: 1795100 training acc: 0.125
Global Iter: 1795200 training loss: 1.93907
Global Iter: 1795200 training acc: 0.21875
Global Iter: 1795300 training loss: 1.93527
Global Iter: 1795300 training acc: 0.34375
Global Iter: 1795400 training loss: 1.93411
Global Iter: 1795400 training acc: 0.25
Global Iter: 1795500 training loss: 2.07587
Global Iter: 1795500 training acc: 0.1875
Global Iter: 1795600 training loss: 1.95149
Global Iter: 1795600 training acc: 0.125
Global Iter: 1795700 training loss: 1.9828
Global Iter: 1795700 training acc: 0.125
Global Iter: 1795800 training loss: 2.06634
Global Iter: 1795800 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1795840
Number of Patches: 20223
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1795840
Global Iter: 1795900 training loss: 1.98547
Global Iter: 1795900 training acc: 0.1875
Global Iter: 1796000 training loss: 1.96965
Global Iter: 1796000 training acc: 0.09375
Global Iter: 1796100 training loss: 1.96641
Global Iter: 1796100 training acc: 0.21875
Global Iter: 1796200 training loss: 1.94901
Global Iter: 1796200 training acc: 0.1875
Global Iter: 1796300 training loss: 1.93602
Global Iter: 1796300 training acc: 0.21875
Global Iter: 1796400 training loss: 1.96094
Global Iter: 1796400 training acc: 0.21875
Global Iter: 1796500 training loss: 2.03796
Global Iter: 1796500 training acc: 0.125
Global Iter: 1796600 training loss: 1.98085
Global Iter: 1796600 training acc: 0.15625
Global Iter: 1796700 training loss: 2.07812
Global Iter: 1796700 training acc: 0.15625
Global Iter: 1796800 training loss: 2.00356
Global Iter: 1796800 training acc: 0.09375
Global Iter: 1796900 training loss: 2.04807
Global Iter: 1796900 training acc: 0.125
Global Iter: 1797000 training loss: 2.0466
Global Iter: 1797000 training acc: 0.09375
Global Iter: 1797100 training loss: 2.0387
Global Iter: 1797100 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1797104
Number of Patches: 20021
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1797104
Global Iter: 1797200 training loss: 2.02546
Global Iter: 1797200 training acc: 0.09375
Global Iter: 1797300 training loss: 1.87953
Global Iter: 1797300 training acc: 0.21875
Global Iter: 1797400 training loss: 1.99465
Global Iter: 1797400 training acc: 0.125
Global Iter: 1797500 training loss: 2.06519
Global Iter: 1797500 training acc: 0.09375
Global Iter: 1797600 training loss: 2.02542
Global Iter: 1797600 training acc: 0.15625
Global Iter: 1797700 training loss: 1.93735
Global Iter: 1797700 training acc: 0.25
Global Iter: 1797800 training los2017-06-23 08:08:30.882883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1798356
2017-06-23 08:10:33.115297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1799595
2017-06-23 08:12:36.323557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1800822
s: 1.89827
Global Iter: 1797800 training acc: 0.25
Global Iter: 1797900 training loss: 2.05262
Global Iter: 1797900 training acc: 0.0625
Global Iter: 1798000 training loss: 2.07831
Global Iter: 1798000 training acc: 0.125
Global Iter: 1798100 training loss: 1.95431
Global Iter: 1798100 training acc: 0.15625
Global Iter: 1798200 training loss: 1.96594
Global Iter: 1798200 training acc: 0.21875
Global Iter: 1798300 training loss: 2.0269
Global Iter: 1798300 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1798356
Number of Patches: 19821
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1798356
Global Iter: 1798400 training loss: 1.93146
Global Iter: 1798400 training acc: 0.125
Global Iter: 1798500 training loss: 2.00458
Global Iter: 1798500 training acc: 0.21875
Global Iter: 1798600 training loss: 2.06225
Global Iter: 1798600 training acc: 0.09375
Global Iter: 1798700 training loss: 2.0552
Global Iter: 1798700 training acc: 0.15625
Global Iter: 1798800 training loss: 1.94871
Global Iter: 1798800 training acc: 0.21875
Global Iter: 1798900 training loss: 1.97823
Global Iter: 1798900 training acc: 0.125
Global Iter: 1799000 training loss: 1.99416
Global Iter: 1799000 training acc: 0.03125
Global Iter: 1799100 training loss: 2.08831
Global Iter: 1799100 training acc: 0.21875
Global Iter: 1799200 training loss: 2.14632
Global Iter: 1799200 training acc: 0.125
Global Iter: 1799300 training loss: 1.98824
Global Iter: 1799300 training acc: 0.09375
Global Iter: 1799400 training loss: 1.97504
Global Iter: 1799400 training acc: 0.15625
Global Iter: 1799500 training loss: 1.96747
Global Iter: 1799500 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1799595
Number of Patches: 19623
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1799595
Global Iter: 1799600 training loss: 1.91845
Global Iter: 1799600 training acc: 0.1875
Global Iter: 1799700 training loss: 1.99179
Global Iter: 1799700 training acc: 0.0625
Global Iter: 1799800 training loss: 2.02272
Global Iter: 1799800 training acc: 0.125
Global Iter: 1799900 training loss: 1.97062
Global Iter: 1799900 training acc: 0.15625
Global Iter: 1800000 training loss: 1.94437
Global Iter: 1800000 training acc: 0.125
Global Iter: 1800100 training loss: 2.02763
Global Iter: 1800100 training acc: 0.125
Global Iter: 1800200 training loss: 1.97723
Global Iter: 1800200 training acc: 0.28125
Global Iter: 1800300 training loss: 2.04182
Global Iter: 1800300 training acc: 0.21875
Global Iter: 1800400 training loss: 1.96876
Global Iter: 1800400 training acc: 0.09375
Global Iter: 1800500 training loss: 2.09055
Global Iter: 1800500 training acc: 0.15625
Global Iter: 1800600 training loss: 1.98928
Global Iter: 1800600 training acc: 0.09375
Global Iter: 1800700 training loss: 1.94011
Global Iter: 1800700 training acc: 0.21875
Global Iter: 1800800 training loss: 1.93317
Global Iter: 1800800 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1800822
Number of Patches: 19427
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1800822
Global Iter: 1800900 training loss: 1.99761
Global Iter: 1800900 training acc: 0.1875
Global Iter: 1801000 training loss: 1.97158
Global Iter: 1801000 training acc: 0.25
Global Iter: 1801100 training loss: 1.9794
Global Iter: 1801100 training acc: 0.1875
Global Iter: 1801200 training loss: 1.99675
Global Iter: 1801200 training acc: 0.09375
Global Iter: 1801300 training loss: 1.92337
Global Iter: 1801300 training acc: 0.1875
Global Iter: 1801400 training loss: 2.13128
Global Iter: 1801400 training acc: 0.28125
Global Iter: 1801500 training loss: 1.95999
Global Iter: 1801500 training acc: 0.1875
Global Iter: 1801600 training loss: 2.06808
Global Iter: 1801600 training acc: 0.25
Global Iter: 1801700 training loss: 1.95724
Global Iter: 1801700 training acc: 0.21875
Global Iter: 2017-06-23 08:14:40.208925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1802037
2017-06-23 08:16:42.227300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1803240
2017-06-23 08:18:44.001924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1804431
1801800 training loss: 2.06066
Global Iter: 1801800 training acc: 0.1875
Global Iter: 1801900 training loss: 1.93502
Global Iter: 1801900 training acc: 0.15625
Global Iter: 1802000 training loss: 2.09266
Global Iter: 1802000 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1802037
Number of Patches: 19233
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1802037
Global Iter: 1802100 training loss: 2.00847
Global Iter: 1802100 training acc: 0.0
Global Iter: 1802200 training loss: 1.98926
Global Iter: 1802200 training acc: 0.15625
Global Iter: 1802300 training loss: 1.96058
Global Iter: 1802300 training acc: 0.125
Global Iter: 1802400 training loss: 2.00668
Global Iter: 1802400 training acc: 0.21875
Global Iter: 1802500 training loss: 2.06116
Global Iter: 1802500 training acc: 0.15625
Global Iter: 1802600 training loss: 1.99088
Global Iter: 1802600 training acc: 0.21875
Global Iter: 1802700 training loss: 1.94828
Global Iter: 1802700 training acc: 0.25
Global Iter: 1802800 training loss: 2.12597
Global Iter: 1802800 training acc: 0.25
Global Iter: 1802900 training loss: 1.99401
Global Iter: 1802900 training acc: 0.1875
Global Iter: 1803000 training loss: 1.90515
Global Iter: 1803000 training acc: 0.125
Global Iter: 1803100 training loss: 2.05083
Global Iter: 1803100 training acc: 0.15625
Global Iter: 1803200 training loss: 2.07655
Global Iter: 1803200 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1803240
Number of Patches: 19041
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1803240
Global Iter: 1803300 training loss: 1.96554
Global Iter: 1803300 training acc: 0.21875
Global Iter: 1803400 training loss: 1.95649
Global Iter: 1803400 training acc: 0.1875
Global Iter: 1803500 training loss: 2.10746
Global Iter: 1803500 training acc: 0.15625
Global Iter: 1803600 training loss: 1.93016
Global Iter: 1803600 training acc: 0.3125
Global Iter: 1803700 training loss: 2.03156
Global Iter: 1803700 training acc: 0.15625
Global Iter: 1803800 training loss: 2.14645
Global Iter: 1803800 training acc: 0.03125
Global Iter: 1803900 training loss: 2.02758
Global Iter: 1803900 training acc: 0.0625
Global Iter: 1804000 training loss: 2.01097
Global Iter: 1804000 training acc: 0.15625
Global Iter: 1804100 training loss: 2.02338
Global Iter: 1804100 training acc: 0.1875
Global Iter: 1804200 training loss: 2.01458
Global Iter: 1804200 training acc: 0.1875
Global Iter: 1804300 training loss: 1.97435
Global Iter: 1804300 training acc: 0.125
Global Iter: 1804400 training loss: 2.00976
Global Iter: 1804400 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1804431
Number of Patches: 18851
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1804431
Global Iter: 1804500 training loss: 1.88379
Global Iter: 1804500 training acc: 0.125
Global Iter: 1804600 training loss: 2.0208
Global Iter: 1804600 training acc: 0.1875
Global Iter: 1804700 training loss: 1.92986
Global Iter: 1804700 training acc: 0.1875
Global Iter: 1804800 training loss: 1.95458
Global Iter: 1804800 training acc: 0.21875
Global Iter: 1804900 training loss: 2.09124
Global Iter: 1804900 training acc: 0.09375
Global Iter: 1805000 training loss: 2.1233
Global Iter: 1805000 training acc: 0.15625
Global Iter: 1805100 training loss: 1.93071
Global Iter: 1805100 training acc: 0.28125
Global Iter: 1805200 training loss: 1.95375
Global Iter: 1805200 training acc: 0.25
Global Iter: 1805300 training loss: 2.01676
Global Iter: 1805300 training acc: 0.125
Global Iter: 1805400 training loss: 2.05308
Global Iter: 1805400 training acc: 0.0625
Global Iter: 1805500 training loss: 2.10374
Global Iter: 1805500 training acc: 0.09375
Global Iter: 1805600 training loss: 1.9542
Global Iter: 1805600 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr00052017-06-23 08:20:40.199855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1805610
2017-06-23 08:22:36.451230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1806777
2017-06-23 08:24:32.207547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1807932
2017-06-23 08:26:26.851801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1809076
/model.ckpt-1805610
Number of Patches: 18663
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1805610
Global Iter: 1805700 training loss: 1.93855
Global Iter: 1805700 training acc: 0.1875
Global Iter: 1805800 training loss: 2.12616
Global Iter: 1805800 training acc: 0.25
Global Iter: 1805900 training loss: 1.99738
Global Iter: 1805900 training acc: 0.25
Global Iter: 1806000 training loss: 2.04878
Global Iter: 1806000 training acc: 0.15625
Global Iter: 1806100 training loss: 1.97655
Global Iter: 1806100 training acc: 0.125
Global Iter: 1806200 training loss: 1.99682
Global Iter: 1806200 training acc: 0.25
Global Iter: 1806300 training loss: 2.01539
Global Iter: 1806300 training acc: 0.09375
Global Iter: 1806400 training loss: 1.97381
Global Iter: 1806400 training acc: 0.34375
Global Iter: 1806500 training loss: 1.99751
Global Iter: 1806500 training acc: 0.25
Global Iter: 1806600 training loss: 1.96639
Global Iter: 1806600 training acc: 0.21875
Global Iter: 1806700 training loss: 1.97732
Global Iter: 1806700 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1806777
Number of Patches: 18477
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1806777
Global Iter: 1806800 training loss: 1.97411
Global Iter: 1806800 training acc: 0.1875
Global Iter: 1806900 training loss: 2.09491
Global Iter: 1806900 training acc: 0.125
Global Iter: 1807000 training loss: 1.98305
Global Iter: 1807000 training acc: 0.21875
Global Iter: 1807100 training loss: 1.989
Global Iter: 1807100 training acc: 0.1875
Global Iter: 1807200 training loss: 1.96725
Global Iter: 1807200 training acc: 0.09375
Global Iter: 1807300 training loss: 1.95757
Global Iter: 1807300 training acc: 0.21875
Global Iter: 1807400 training loss: 2.00428
Global Iter: 1807400 training acc: 0.34375
Global Iter: 1807500 training loss: 2.00453
Global Iter: 1807500 training acc: 0.0625
Global Iter: 1807600 training loss: 1.9305
Global Iter: 1807600 training acc: 0.28125
Global Iter: 1807700 training loss: 2.06912
Global Iter: 1807700 training acc: 0.09375
Global Iter: 1807800 training loss: 1.99327
Global Iter: 1807800 training acc: 0.09375
Global Iter: 1807900 training loss: 2.00265
Global Iter: 1807900 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1807932
Number of Patches: 18293
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1807932
Global Iter: 1808000 training loss: 2.00461
Global Iter: 1808000 training acc: 0.25
Global Iter: 1808100 training loss: 2.04801
Global Iter: 1808100 training acc: 0.0625
Global Iter: 1808200 training loss: 1.98376
Global Iter: 1808200 training acc: 0.15625
Global Iter: 1808300 training loss: 1.96991
Global Iter: 1808300 training acc: 0.15625
Global Iter: 1808400 training loss: 2.04633
Global Iter: 1808400 training acc: 0.25
Global Iter: 1808500 training loss: 2.03355
Global Iter: 1808500 training acc: 0.09375
Global Iter: 1808600 training loss: 2.08246
Global Iter: 1808600 training acc: 0.0625
Global Iter: 1808700 training loss: 1.98527
Global Iter: 1808700 training acc: 0.09375
Global Iter: 1808800 training loss: 2.05738
Global Iter: 1808800 training acc: 0.03125
Global Iter: 1808900 training loss: 2.06052
Global Iter: 1808900 training acc: 0.15625
Global Iter: 1809000 training loss: 1.99292
Global Iter: 1809000 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1809076
Number of Patches: 18111
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1809076
Global Iter: 1809100 training loss: 1.95257
Global Iter: 1809100 training acc: 0.21875
Global Iter: 1809200 training loss: 2.01716
Global Iter: 1809200 training acc: 0.21875
Global Iter: 1809300 training loss: 1.99358
Global Iter: 1809300 training acc: 0.15625
Global Iter: 1809400 training loss: 2.09393
Global Iter: 1809400 training acc: 0.09375
Global Iter2017-06-23 08:28:21.075136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1810208
2017-06-23 08:30:12.868702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1811329
2017-06-23 08:32:00.743574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1812439
: 1809500 training loss: 1.93958
Global Iter: 1809500 training acc: 0.15625
Global Iter: 1809600 training loss: 1.92152
Global Iter: 1809600 training acc: 0.25
Global Iter: 1809700 training loss: 1.97657
Global Iter: 1809700 training acc: 0.125
Global Iter: 1809800 training loss: 2.02693
Global Iter: 1809800 training acc: 0.15625
Global Iter: 1809900 training loss: 1.97155
Global Iter: 1809900 training acc: 0.15625
Global Iter: 1810000 training loss: 1.99111
Global Iter: 1810000 training acc: 0.0625
Global Iter: 1810100 training loss: 1.9854
Global Iter: 1810100 training acc: 0.125
Global Iter: 1810200 training loss: 1.99563
Global Iter: 1810200 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1810208
Number of Patches: 17930
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1810208
Global Iter: 1810300 training loss: 2.00975
Global Iter: 1810300 training acc: 0.15625
Global Iter: 1810400 training loss: 2.00278
Global Iter: 1810400 training acc: 0.15625
Global Iter: 1810500 training loss: 1.96789
Global Iter: 1810500 training acc: 0.1875
Global Iter: 1810600 training loss: 2.19457
Global Iter: 1810600 training acc: 0.09375
Global Iter: 1810700 training loss: 1.95957
Global Iter: 1810700 training acc: 0.15625
Global Iter: 1810800 training loss: 1.95942
Global Iter: 1810800 training acc: 0.09375
Global Iter: 1810900 training loss: 2.09604
Global Iter: 1810900 training acc: 0.125
Global Iter: 1811000 training loss: 1.98951
Global Iter: 1811000 training acc: 0.15625
Global Iter: 1811100 training loss: 2.0148
Global Iter: 1811100 training acc: 0.0
Global Iter: 1811200 training loss: 2.02655
Global Iter: 1811200 training acc: 0.28125
Global Iter: 1811300 training loss: 2.10143
Global Iter: 1811300 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1811329
Number of Patches: 17751
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1811329
Global Iter: 1811400 training loss: 1.9581
Global Iter: 1811400 training acc: 0.15625
Global Iter: 1811500 training loss: 1.97644
Global Iter: 1811500 training acc: 0.1875
Global Iter: 1811600 training loss: 2.10515
Global Iter: 1811600 training acc: 0.1875
Global Iter: 1811700 training loss: 1.99384
Global Iter: 1811700 training acc: 0.1875
Global Iter: 1811800 training loss: 2.04972
Global Iter: 1811800 training acc: 0.15625
Global Iter: 1811900 training loss: 1.95229
Global Iter: 1811900 training acc: 0.21875
Global Iter: 1812000 training loss: 2.09724
Global Iter: 1812000 training acc: 0.25
Global Iter: 1812100 training loss: 2.01054
Global Iter: 1812100 training acc: 0.21875
Global Iter: 1812200 training loss: 2.07966
Global Iter: 1812200 training acc: 0.125
Global Iter: 1812300 training loss: 1.98904
Global Iter: 1812300 training acc: 0.03125
Global Iter: 1812400 training loss: 2.01445
Global Iter: 1812400 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1812439
Number of Patches: 17574
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1812439
Global Iter: 1812500 training loss: 2.05295
Global Iter: 1812500 training acc: 0.125
Global Iter: 1812600 training loss: 1.98802
Global Iter: 1812600 training acc: 0.34375
Global Iter: 1812700 training loss: 2.04519
Global Iter: 1812700 training acc: 0.1875
Global Iter: 1812800 training loss: 1.97402
Global Iter: 1812800 training acc: 0.25
Global Iter: 1812900 training loss: 1.9972
Global Iter: 1812900 training acc: 0.15625
Global Iter: 1813000 training loss: 1.99643
Global Iter: 1813000 training acc: 0.125
Global Iter: 1813100 training loss: 1.96217
Global Iter: 1813100 training acc: 0.09375
Global Iter: 1813200 training loss: 2.0846
Global Iter: 1813200 training acc: 0.15625
Global Iter: 1813300 training loss: 2.03138
Global Iter: 1813300 training acc: 0.1875
Global Iter: 1813400 training loss: 1.96401
Global Iter: 1813400 training acc: 0.092017-06-23 08:33:53.078930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1813538
2017-06-23 08:35:39.577150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1814626
2017-06-23 08:37:27.675349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1815703
2017-06-23 08:39:16.372038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1816769
375
Global Iter: 1813500 training loss: 1.97623
Global Iter: 1813500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1813538
Number of Patches: 17399
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1813538
Global Iter: 1813600 training loss: 1.96142
Global Iter: 1813600 training acc: 0.09375
Global Iter: 1813700 training loss: 2.0001
Global Iter: 1813700 training acc: 0.15625
Global Iter: 1813800 training loss: 1.97381
Global Iter: 1813800 training acc: 0.15625
Global Iter: 1813900 training loss: 1.98262
Global Iter: 1813900 training acc: 0.09375
Global Iter: 1814000 training loss: 1.97338
Global Iter: 1814000 training acc: 0.1875
Global Iter: 1814100 training loss: 1.95404
Global Iter: 1814100 training acc: 0.25
Global Iter: 1814200 training loss: 1.97113
Global Iter: 1814200 training acc: 0.09375
Global Iter: 1814300 training loss: 2.00186
Global Iter: 1814300 training acc: 0.15625
Global Iter: 1814400 training loss: 1.98544
Global Iter: 1814400 training acc: 0.125
Global Iter: 1814500 training loss: 2.02204
Global Iter: 1814500 training acc: 0.0625
Global Iter: 1814600 training loss: 1.96404
Global Iter: 1814600 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1814626
Number of Patches: 17226
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1814626
Global Iter: 1814700 training loss: 1.96935
Global Iter: 1814700 training acc: 0.28125
Global Iter: 1814800 training loss: 1.96109
Global Iter: 1814800 training acc: 0.3125
Global Iter: 1814900 training loss: 1.98337
Global Iter: 1814900 training acc: 0.21875
Global Iter: 1815000 training loss: 1.98156
Global Iter: 1815000 training acc: 0.1875
Global Iter: 1815100 training loss: 2.00948
Global Iter: 1815100 training acc: 0.1875
Global Iter: 1815200 training loss: 1.90787
Global Iter: 1815200 training acc: 0.125
Global Iter: 1815300 training loss: 1.94375
Global Iter: 1815300 training acc: 0.09375
Global Iter: 1815400 training loss: 2.01612
Global Iter: 1815400 training acc: 0.1875
Global Iter: 1815500 training loss: 2.0205
Global Iter: 1815500 training acc: 0.125
Global Iter: 1815600 training loss: 1.88677
Global Iter: 1815600 training acc: 0.28125
Global Iter: 1815700 training loss: 1.98069
Global Iter: 1815700 training acc: 0.28125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1815703
Number of Patches: 17054
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1815703
Global Iter: 1815800 training loss: 2.03488
Global Iter: 1815800 training acc: 0.21875
Global Iter: 1815900 training loss: 2.01321
Global Iter: 1815900 training acc: 0.125
Global Iter: 1816000 training loss: 2.07923
Global Iter: 1816000 training acc: 0.21875
Global Iter: 1816100 training loss: 2.01558
Global Iter: 1816100 training acc: 0.15625
Global Iter: 1816200 training loss: 2.06919
Global Iter: 1816200 training acc: 0.15625
Global Iter: 1816300 training loss: 1.98177
Global Iter: 1816300 training acc: 0.0
Global Iter: 1816400 training loss: 1.99497
Global Iter: 1816400 training acc: 0.125
Global Iter: 1816500 training loss: 1.98583
Global Iter: 1816500 training acc: 0.1875
Global Iter: 1816600 training loss: 2.00022
Global Iter: 1816600 training acc: 0.25
Global Iter: 1816700 training loss: 1.95085
Global Iter: 1816700 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1816769
Number of Patches: 16884
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1816769
Global Iter: 1816800 training loss: 1.88594
Global Iter: 1816800 training acc: 0.21875
Global Iter: 1816900 training loss: 1.97261
Global Iter: 1816900 training acc: 0.125
Global Iter: 1817000 training loss: 1.98283
Global Iter: 1817000 training acc: 0.15625
Global Iter: 1817100 training loss: 1.96875
Global Iter: 1817100 training acc: 0.1875
Global Iter: 12017-06-23 08:41:03.126344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1817825
2017-06-23 08:42:49.228109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1818870
2017-06-23 08:44:33.786331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1819905
2017-06-23 08:46:16.633838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
817200 training loss: 1.95289
Global Iter: 1817200 training acc: 0.03125
Global Iter: 1817300 training loss: 1.98087
Global Iter: 1817300 training acc: 0.09375
Global Iter: 1817400 training loss: 2.03189
Global Iter: 1817400 training acc: 0.1875
Global Iter: 1817500 training loss: 1.96801
Global Iter: 1817500 training acc: 0.21875
Global Iter: 1817600 training loss: 2.04029
Global Iter: 1817600 training acc: 0.0625
Global Iter: 1817700 training loss: 2.02756
Global Iter: 1817700 training acc: 0.21875
Global Iter: 1817800 training loss: 2.07603
Global Iter: 1817800 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1817825
Number of Patches: 16716
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1817825
Global Iter: 1817900 training loss: 2.02524
Global Iter: 1817900 training acc: 0.15625
Global Iter: 1818000 training loss: 2.07651
Global Iter: 1818000 training acc: 0.0625
Global Iter: 1818100 training loss: 2.0215
Global Iter: 1818100 training acc: 0.125
Global Iter: 1818200 training loss: 1.9886
Global Iter: 1818200 training acc: 0.125
Global Iter: 1818300 training loss: 1.97987
Global Iter: 1818300 training acc: 0.1875
Global Iter: 1818400 training loss: 2.0154
Global Iter: 1818400 training acc: 0.125
Global Iter: 1818500 training loss: 2.0497
Global Iter: 1818500 training acc: 0.125
Global Iter: 1818600 training loss: 2.11069
Global Iter: 1818600 training acc: 0.1875
Global Iter: 1818700 training loss: 1.9636
Global Iter: 1818700 training acc: 0.21875
Global Iter: 1818800 training loss: 1.91575
Global Iter: 1818800 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1818870
Number of Patches: 16549
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1818870
Global Iter: 1818900 training loss: 1.93885
Global Iter: 1818900 training acc: 0.15625
Global Iter: 1819000 training loss: 2.09967
Global Iter: 1819000 training acc: 0.09375
Global Iter: 1819100 training loss: 2.19369
Global Iter: 1819100 training acc: 0.21875
Global Iter: 1819200 training loss: 2.09131
Global Iter: 1819200 training acc: 0.0625
Global Iter: 1819300 training loss: 2.0491
Global Iter: 1819300 training acc: 0.1875
Global Iter: 1819400 training loss: 1.94592
Global Iter: 1819400 training acc: 0.15625
Global Iter: 1819500 training loss: 2.07346
Global Iter: 1819500 training acc: 0.1875
Global Iter: 1819600 training loss: 1.93324
Global Iter: 1819600 training acc: 0.0625
Global Iter: 1819700 training loss: 1.95783
Global Iter: 1819700 training acc: 0.1875
Global Iter: 1819800 training loss: 2.05493
Global Iter: 1819800 training acc: 0.25
Global Iter: 1819900 training loss: 1.94201
Global Iter: 1819900 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1819905
Number of Patches: 16384
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1819905
Global Iter: 1820000 training loss: 2.02055
Global Iter: 1820000 training acc: 0.15625
Global Iter: 1820100 training loss: 2.08577
Global Iter: 1820100 training acc: 0.28125
Global Iter: 1820200 training loss: 2.12863
Global Iter: 1820200 training acc: 0.1875
Global Iter: 1820300 training loss: 2.07662
Global Iter: 1820300 training acc: 0.15625
Global Iter: 1820400 training loss: 2.04147
Global Iter: 1820400 training acc: 0.15625
Global Iter: 1820500 training loss: 2.08053
Global Iter: 1820500 training acc: 0.15625
Global Iter: 1820600 training loss: 1.95374
Global Iter: 1820600 training acc: 0.15625
Global Iter: 1820700 training loss: 1.97719
Global Iter: 1820700 training acc: 0.09375
Global Iter: 1820800 training loss: 2.05347
Global Iter: 1820800 training acc: 0.15625
Global Iter: 1820900 training loss: 2.09598
Global Iter: 1820900 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1820929
Number of Patches: 16221
checkpoint found: /home/ahmet/workspINFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1820929
2017-06-23 08:47:56.211023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1821943
2017-06-23 08:49:35.083671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1822947
2017-06-23 08:51:13.584058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1823941
ace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1820929
Global Iter: 1821000 training loss: 2.04395
Global Iter: 1821000 training acc: 0.15625
Global Iter: 1821100 training loss: 2.01134
Global Iter: 1821100 training acc: 0.15625
Global Iter: 1821200 training loss: 1.91707
Global Iter: 1821200 training acc: 0.375
Global Iter: 1821300 training loss: 1.93021
Global Iter: 1821300 training acc: 0.125
Global Iter: 1821400 training loss: 1.96915
Global Iter: 1821400 training acc: 0.15625
Global Iter: 1821500 training loss: 2.1073
Global Iter: 1821500 training acc: 0.09375
Global Iter: 1821600 training loss: 1.91089
Global Iter: 1821600 training acc: 0.21875
Global Iter: 1821700 training loss: 1.97785
Global Iter: 1821700 training acc: 0.1875
Global Iter: 1821800 training loss: 1.93756
Global Iter: 1821800 training acc: 0.25
Global Iter: 1821900 training loss: 1.93201
Global Iter: 1821900 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1821943
Number of Patches: 16059
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1821943
Global Iter: 1822000 training loss: 2.12979
Global Iter: 1822000 training acc: 0.125
Global Iter: 1822100 training loss: 1.93323
Global Iter: 1822100 training acc: 0.125
Global Iter: 1822200 training loss: 1.98646
Global Iter: 1822200 training acc: 0.15625
Global Iter: 1822300 training loss: 1.99784
Global Iter: 1822300 training acc: 0.09375
Global Iter: 1822400 training loss: 2.00986
Global Iter: 1822400 training acc: 0.09375
Global Iter: 1822500 training loss: 1.99043
Global Iter: 1822500 training acc: 0.15625
Global Iter: 1822600 training loss: 2.08781
Global Iter: 1822600 training acc: 0.03125
Global Iter: 1822700 training loss: 2.11351
Global Iter: 1822700 training acc: 0.09375
Global Iter: 1822800 training loss: 2.01975
Global Iter: 1822800 training acc: 0.09375
Global Iter: 1822900 training loss: 2.0095
Global Iter: 1822900 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1822947
Number of Patches: 15899
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1822947
Global Iter: 1823000 training loss: 1.98525
Global Iter: 1823000 training acc: 0.03125
Global Iter: 1823100 training loss: 1.99998
Global Iter: 1823100 training acc: 0.1875
Global Iter: 1823200 training loss: 2.01267
Global Iter: 1823200 training acc: 0.0625
Global Iter: 1823300 training loss: 1.97225
Global Iter: 1823300 training acc: 0.125
Global Iter: 1823400 training loss: 2.04801
Global Iter: 1823400 training acc: 0.15625
Global Iter: 1823500 training loss: 2.0662
Global Iter: 1823500 training acc: 0.125
Global Iter: 1823600 training loss: 2.00579
Global Iter: 1823600 training acc: 0.375
Global Iter: 1823700 training loss: 2.00051
Global Iter: 1823700 training acc: 0.1875
Global Iter: 1823800 training loss: 1.99214
Global Iter: 1823800 training acc: 0.15625
Global Iter: 1823900 training loss: 1.90288
Global Iter: 1823900 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1823941
Number of Patches: 15741
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1823941
Global Iter: 1824000 training loss: 1.99341
Global Iter: 1824000 training acc: 0.21875
Global Iter: 1824100 training loss: 1.97599
Global Iter: 1824100 training acc: 0.1875
Global Iter: 1824200 training loss: 1.97599
Global Iter: 1824200 training acc: 0.1875
Global Iter: 1824300 training loss: 1.97965
Global Iter: 1824300 training acc: 0.15625
Global Iter: 1824400 training loss: 2.06761
Global Iter: 1824400 training acc: 0.125
Global Iter: 1824500 training loss: 1.94201
Global Iter: 1824500 training acc: 0.09375
Global Iter: 1824600 training loss: 1.95735
Global Iter: 1824600 training acc: 0.21875
Global Iter: 1824700 training loss: 1.91323
Global Iter: 1824700 training acc: 0.1875
Global Iter: 1824800 training loss: 1.93603
Global Iter: 1824800 training acc: 0.21875
G2017-06-23 08:52:52.349191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1824925
2017-06-23 08:54:28.741699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1825899
2017-06-23 08:56:03.256050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1826864
2017-06-23 08:57:38.728106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1827819
lobal Iter: 1824900 training loss: 1.94639
Global Iter: 1824900 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1824925
Number of Patches: 15584
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1824925
Global Iter: 1825000 training loss: 1.92227
Global Iter: 1825000 training acc: 0.15625
Global Iter: 1825100 training loss: 1.98472
Global Iter: 1825100 training acc: 0.1875
Global Iter: 1825200 training loss: 1.98389
Global Iter: 1825200 training acc: 0.1875
Global Iter: 1825300 training loss: 1.9976
Global Iter: 1825300 training acc: 0.1875
Global Iter: 1825400 training loss: 1.97071
Global Iter: 1825400 training acc: 0.25
Global Iter: 1825500 training loss: 2.0009
Global Iter: 1825500 training acc: 0.25
Global Iter: 1825600 training loss: 2.01122
Global Iter: 1825600 training acc: 0.09375
Global Iter: 1825700 training loss: 2.06605
Global Iter: 1825700 training acc: 0.15625
Global Iter: 1825800 training loss: 1.99083
Global Iter: 1825800 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1825899
Number of Patches: 15429
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1825899
Global Iter: 1825900 training loss: 2.02846
Global Iter: 1825900 training acc: 0.28125
Global Iter: 1826000 training loss: 1.91269
Global Iter: 1826000 training acc: 0.25
Global Iter: 1826100 training loss: 2.1971
Global Iter: 1826100 training acc: 0.0
Global Iter: 1826200 training loss: 1.95723
Global Iter: 1826200 training acc: 0.21875
Global Iter: 1826300 training loss: 2.02738
Global Iter: 1826300 training acc: 0.03125
Global Iter: 1826400 training loss: 2.00147
Global Iter: 1826400 training acc: 0.15625
Global Iter: 1826500 training loss: 1.98845
Global Iter: 1826500 training acc: 0.21875
Global Iter: 1826600 training loss: 1.98311
Global Iter: 1826600 training acc: 0.21875
Global Iter: 1826700 training loss: 1.93171
Global Iter: 1826700 training acc: 0.125
Global Iter: 1826800 training loss: 2.00913
Global Iter: 1826800 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1826864
Number of Patches: 15275
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1826864
Global Iter: 1826900 training loss: 2.0364
Global Iter: 1826900 training acc: 0.15625
Global Iter: 1827000 training loss: 1.93798
Global Iter: 1827000 training acc: 0.15625
Global Iter: 1827100 training loss: 2.01331
Global Iter: 1827100 training acc: 0.09375
Global Iter: 1827200 training loss: 1.99589
Global Iter: 1827200 training acc: 0.125
Global Iter: 1827300 training loss: 2.04751
Global Iter: 1827300 training acc: 0.15625
Global Iter: 1827400 training loss: 2.09631
Global Iter: 1827400 training acc: 0.0625
Global Iter: 1827500 training loss: 1.99791
Global Iter: 1827500 training acc: 0.15625
Global Iter: 1827600 training loss: 1.95318
Global Iter: 1827600 training acc: 0.15625
Global Iter: 1827700 training loss: 2.01539
Global Iter: 1827700 training acc: 0.125
Global Iter: 1827800 training loss: 2.05344
Global Iter: 1827800 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1827819
Number of Patches: 15123
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1827819
Global Iter: 1827900 training loss: 1.94531
Global Iter: 1827900 training acc: 0.21875
Global Iter: 1828000 training loss: 2.01174
Global Iter: 1828000 training acc: 0.125
Global Iter: 1828100 training loss: 2.00381
Global Iter: 1828100 training acc: 0.34375
Global Iter: 1828200 training loss: 2.05382
Global Iter: 1828200 training acc: 0.3125
Global Iter: 1828300 training loss: 2.12246
Global Iter: 1828300 training acc: 0.125
Global Iter: 1828400 training loss: 2.08188
Global Iter: 1828400 training acc: 0.0
Global Iter: 1828500 training loss: 2.08044
Global Iter: 1828500 training acc: 0.15625
Global Iter: 1828600 tr2017-06-23 08:59:13.492911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1828765
2017-06-23 09:00:49.768738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1829701
2017-06-23 09:02:22.808402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1830628
2017-06-23 09:03:55.071547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1831546
aining loss: 1.93836
Global Iter: 1828600 training acc: 0.21875
Global Iter: 1828700 training loss: 2.01072
Global Iter: 1828700 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1828765
Number of Patches: 14972
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1828765
Global Iter: 1828800 training loss: 1.97456
Global Iter: 1828800 training acc: 0.15625
Global Iter: 1828900 training loss: 1.9753
Global Iter: 1828900 training acc: 0.15625
Global Iter: 1829000 training loss: 2.02714
Global Iter: 1829000 training acc: 0.1875
Global Iter: 1829100 training loss: 1.99563
Global Iter: 1829100 training acc: 0.09375
Global Iter: 1829200 training loss: 2.0299
Global Iter: 1829200 training acc: 0.125
Global Iter: 1829300 training loss: 2.06968
Global Iter: 1829300 training acc: 0.15625
Global Iter: 1829400 training loss: 2.0201
Global Iter: 1829400 training acc: 0.0625
Global Iter: 1829500 training loss: 1.98794
Global Iter: 1829500 training acc: 0.15625
Global Iter: 1829600 training loss: 2.01894
Global Iter: 1829600 training acc: 0.15625
Global Iter: 1829700 training loss: 1.9713
Global Iter: 1829700 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1829701
Number of Patches: 14823
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1829701
Global Iter: 1829800 training loss: 2.03612
Global Iter: 1829800 training acc: 0.21875
Global Iter: 1829900 training loss: 2.00206
Global Iter: 1829900 training acc: 0.15625
Global Iter: 1830000 training loss: 1.90493
Global Iter: 1830000 training acc: 0.15625
Global Iter: 1830100 training loss: 1.99874
Global Iter: 1830100 training acc: 0.15625
Global Iter: 1830200 training loss: 2.00156
Global Iter: 1830200 training acc: 0.125
Global Iter: 1830300 training loss: 1.92518
Global Iter: 1830300 training acc: 0.28125
Global Iter: 1830400 training loss: 2.08551
Global Iter: 1830400 training acc: 0.15625
Global Iter: 1830500 training loss: 2.04546
Global Iter: 1830500 training acc: 0.09375
Global Iter: 1830600 training loss: 1.94165
Global Iter: 1830600 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1830628
Number of Patches: 14675
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1830628
Global Iter: 1830700 training loss: 2.05607
Global Iter: 1830700 training acc: 0.1875
Global Iter: 1830800 training loss: 2.04016
Global Iter: 1830800 training acc: 0.1875
Global Iter: 1830900 training loss: 2.14074
Global Iter: 1830900 training acc: 0.15625
Global Iter: 1831000 training loss: 2.05639
Global Iter: 1831000 training acc: 0.15625
Global Iter: 1831100 training loss: 1.95075
Global Iter: 1831100 training acc: 0.09375
Global Iter: 1831200 training loss: 2.04749
Global Iter: 1831200 training acc: 0.0625
Global Iter: 1831300 training loss: 1.99604
Global Iter: 1831300 training acc: 0.25
Global Iter: 1831400 training loss: 2.02392
Global Iter: 1831400 training acc: 0.25
Global Iter: 1831500 training loss: 1.99672
Global Iter: 1831500 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1831546
Number of Patches: 14529
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1831546
Global Iter: 1831600 training loss: 2.00824
Global Iter: 1831600 training acc: 0.09375
Global Iter: 1831700 training loss: 1.96892
Global Iter: 1831700 training acc: 0.21875
Global Iter: 1831800 training loss: 2.03087
Global Iter: 1831800 training acc: 0.15625
Global Iter: 1831900 training loss: 1.92693
Global Iter: 1831900 training acc: 0.21875
Global Iter: 1832000 training loss: 1.98791
Global Iter: 1832000 training acc: 0.125
Global Iter: 1832100 training loss: 2.03206
Global Iter: 1832100 training acc: 0.09375
Global Iter: 1832200 training loss: 2.02453
Global Iter: 1832200 training acc: 0.1875
Global Iter: 1832300 training 2017-06-23 09:05:26.940683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1832455
2017-06-23 09:06:56.115398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1833354
2017-06-23 09:08:25.916526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1834245
2017-06-23 09:09:53.554836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1835127
loss: 1.97367
Global Iter: 1832300 training acc: 0.09375
Global Iter: 1832400 training loss: 2.03503
Global Iter: 1832400 training acc: 0.3125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1832455
Number of Patches: 14384
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1832455
Global Iter: 1832500 training loss: 2.05396
Global Iter: 1832500 training acc: 0.15625
Global Iter: 1832600 training loss: 2.12473
Global Iter: 1832600 training acc: 0.09375
Global Iter: 1832700 training loss: 2.09609
Global Iter: 1832700 training acc: 0.21875
Global Iter: 1832800 training loss: 1.99923
Global Iter: 1832800 training acc: 0.125
Global Iter: 1832900 training loss: 1.97546
Global Iter: 1832900 training acc: 0.03125
Global Iter: 1833000 training loss: 1.96923
Global Iter: 1833000 training acc: 0.09375
Global Iter: 1833100 training loss: 2.00504
Global Iter: 1833100 training acc: 0.15625
Global Iter: 1833200 training loss: 2.01862
Global Iter: 1833200 training acc: 0.15625
Global Iter: 1833300 training loss: 2.05447
Global Iter: 1833300 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1833354
Number of Patches: 14241
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1833354
Global Iter: 1833400 training loss: 1.97883
Global Iter: 1833400 training acc: 0.15625
Global Iter: 1833500 training loss: 1.93272
Global Iter: 1833500 training acc: 0.25
Global Iter: 1833600 training loss: 2.03585
Global Iter: 1833600 training acc: 0.125
Global Iter: 1833700 training loss: 1.96924
Global Iter: 1833700 training acc: 0.15625
Global Iter: 1833800 training loss: 2.06214
Global Iter: 1833800 training acc: 0.21875
Global Iter: 1833900 training loss: 1.88971
Global Iter: 1833900 training acc: 0.3125
Global Iter: 1834000 training loss: 2.04452
Global Iter: 1834000 training acc: 0.0625
Global Iter: 1834100 training loss: 2.00596
Global Iter: 1834100 training acc: 0.09375
Global Iter: 1834200 training loss: 2.02733
Global Iter: 1834200 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1834245
Number of Patches: 14099
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1834245
Global Iter: 1834300 training loss: 1.95485
Global Iter: 1834300 training acc: 0.125
Global Iter: 1834400 training loss: 1.91286
Global Iter: 1834400 training acc: 0.28125
Global Iter: 1834500 training loss: 2.02036
Global Iter: 1834500 training acc: 0.15625
Global Iter: 1834600 training loss: 2.01405
Global Iter: 1834600 training acc: 0.21875
Global Iter: 1834700 training loss: 2.02218
Global Iter: 1834700 training acc: 0.125
Global Iter: 1834800 training loss: 2.03203
Global Iter: 1834800 training acc: 0.15625
Global Iter: 1834900 training loss: 2.00039
Global Iter: 1834900 training acc: 0.125
Global Iter: 1835000 training loss: 2.0404
Global Iter: 1835000 training acc: 0.1875
Global Iter: 1835100 training loss: 1.97279
Global Iter: 1835100 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1835127
Number of Patches: 13959
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1835127
Global Iter: 1835200 training loss: 2.09164
Global Iter: 1835200 training acc: 0.125
Global Iter: 1835300 training loss: 2.05389
Global Iter: 1835300 training acc: 0.21875
Global Iter: 1835400 training loss: 1.98792
Global Iter: 1835400 training acc: 0.21875
Global Iter: 1835500 training loss: 1.99715
Global Iter: 1835500 training acc: 0.15625
Global Iter: 1835600 training loss: 2.02939
Global Iter: 1835600 training acc: 0.15625
Global Iter: 1835700 training loss: 1.94977
Global Iter: 1835700 training acc: 0.09375
Global Iter: 1835800 training loss: 2.00046
Global Iter: 1835800 training acc: 0.125
Global Iter: 1835900 training loss: 1.9875
Global Iter: 1835900 training acc: 0.1875
Model saved in file: /home/ahmet/work2017-06-23 09:11:21.543721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1836000
2017-06-23 09:12:47.553493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1836864
2017-06-23 09:14:13.525479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1837720
2017-06-23 09:15:37.153517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1838567
2017-06-23 09:16:59.440852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1839406
space/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1836000
Number of Patches: 13820
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1836000
Global Iter: 1836100 training loss: 1.98044
Global Iter: 1836100 training acc: 0.15625
Global Iter: 1836200 training loss: 1.9632
Global Iter: 1836200 training acc: 0.09375
Global Iter: 1836300 training loss: 1.97124
Global Iter: 1836300 training acc: 0.1875
Global Iter: 1836400 training loss: 2.02166
Global Iter: 1836400 training acc: 0.125
Global Iter: 1836500 training loss: 2.04724
Global Iter: 1836500 training acc: 0.1875
Global Iter: 1836600 training loss: 2.04014
Global Iter: 1836600 training acc: 0.1875
Global Iter: 1836700 training loss: 2.04027
Global Iter: 1836700 training acc: 0.125
Global Iter: 1836800 training loss: 2.00605
Global Iter: 1836800 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1836864
Number of Patches: 13682
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1836864
Global Iter: 1836900 training loss: 1.98893
Global Iter: 1836900 training acc: 0.0625
Global Iter: 1837000 training loss: 2.05169
Global Iter: 1837000 training acc: 0.1875
Global Iter: 1837100 training loss: 1.93337
Global Iter: 1837100 training acc: 0.28125
Global Iter: 1837200 training loss: 1.97415
Global Iter: 1837200 training acc: 0.09375
Global Iter: 1837300 training loss: 2.02163
Global Iter: 1837300 training acc: 0.21875
Global Iter: 1837400 training loss: 1.95168
Global Iter: 1837400 training acc: 0.1875
Global Iter: 1837500 training loss: 2.04276
Global Iter: 1837500 training acc: 0.03125
Global Iter: 1837600 training loss: 1.9604
Global Iter: 1837600 training acc: 0.15625
Global Iter: 1837700 training loss: 1.95511
Global Iter: 1837700 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1837720
Number of Patches: 13546
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1837720
Global Iter: 1837800 training loss: 2.0796
Global Iter: 1837800 training acc: 0.09375
Global Iter: 1837900 training loss: 1.94082
Global Iter: 1837900 training acc: 0.28125
Global Iter: 1838000 training loss: 2.01973
Global Iter: 1838000 training acc: 0.0625
Global Iter: 1838100 training loss: 2.01642
Global Iter: 1838100 training acc: 0.125
Global Iter: 1838200 training loss: 2.06058
Global Iter: 1838200 training acc: 0.1875
Global Iter: 1838300 training loss: 2.10492
Global Iter: 1838300 training acc: 0.09375
Global Iter: 1838400 training loss: 1.99948
Global Iter: 1838400 training acc: 0.21875
Global Iter: 1838500 training loss: 1.98071
Global Iter: 1838500 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1838567
Number of Patches: 13411
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1838567
Global Iter: 1838600 training loss: 2.00692
Global Iter: 1838600 training acc: 0.15625
Global Iter: 1838700 training loss: 1.92123
Global Iter: 1838700 training acc: 0.15625
Global Iter: 1838800 training loss: 2.0122
Global Iter: 1838800 training acc: 0.15625
Global Iter: 1838900 training loss: 2.01414
Global Iter: 1838900 training acc: 0.21875
Global Iter: 1839000 training loss: 2.0757
Global Iter: 1839000 training acc: 0.09375
Global Iter: 1839100 training loss: 2.00391
Global Iter: 1839100 training acc: 0.1875
Global Iter: 1839200 training loss: 1.92988
Global Iter: 1839200 training acc: 0.1875
Global Iter: 1839300 training loss: 2.02101
Global Iter: 1839300 training acc: 0.15625
Global Iter: 1839400 training loss: 2.03319
Global Iter: 1839400 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1839406
Number of Patches: 13277
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1839406
Global Iter: 1839500 training loss: 1.91051
Global Iter: 1839500 training acc2017-06-23 09:18:23.469769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1840236
2017-06-23 09:19:44.416123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1841058
2017-06-23 09:21:06.300245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1841872
2017-06-23 09:22:27.069186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1842678
: 0.21875
Global Iter: 1839600 training loss: 1.97255
Global Iter: 1839600 training acc: 0.09375
Global Iter: 1839700 training loss: 2.0767
Global Iter: 1839700 training acc: 0.0625
Global Iter: 1839800 training loss: 2.04214
Global Iter: 1839800 training acc: 0.15625
Global Iter: 1839900 training loss: 2.07738
Global Iter: 1839900 training acc: 0.0625
Global Iter: 1840000 training loss: 1.97506
Global Iter: 1840000 training acc: 0.125
Global Iter: 1840100 training loss: 1.93628
Global Iter: 1840100 training acc: 0.1875
Global Iter: 1840200 training loss: 1.96206
Global Iter: 1840200 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1840236
Number of Patches: 13145
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1840236
Global Iter: 1840300 training loss: 1.94145
Global Iter: 1840300 training acc: 0.21875
Global Iter: 1840400 training loss: 2.1007
Global Iter: 1840400 training acc: 0.21875
Global Iter: 1840500 training loss: 1.94862
Global Iter: 1840500 training acc: 0.1875
Global Iter: 1840600 training loss: 2.00293
Global Iter: 1840600 training acc: 0.125
Global Iter: 1840700 training loss: 1.99765
Global Iter: 1840700 training acc: 0.21875
Global Iter: 1840800 training loss: 1.9338
Global Iter: 1840800 training acc: 0.15625
Global Iter: 1840900 training loss: 1.96222
Global Iter: 1840900 training acc: 0.0625
Global Iter: 1841000 training loss: 2.04222
Global Iter: 1841000 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1841058
Number of Patches: 13014
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1841058
Global Iter: 1841100 training loss: 1.9501
Global Iter: 1841100 training acc: 0.15625
Global Iter: 1841200 training loss: 1.9854
Global Iter: 1841200 training acc: 0.15625
Global Iter: 1841300 training loss: 2.11298
Global Iter: 1841300 training acc: 0.125
Global Iter: 1841400 training loss: 2.00621
Global Iter: 1841400 training acc: 0.09375
Global Iter: 1841500 training loss: 2.02261
Global Iter: 1841500 training acc: 0.1875
Global Iter: 1841600 training loss: 2.00313
Global Iter: 1841600 training acc: 0.09375
Global Iter: 1841700 training loss: 2.01884
Global Iter: 1841700 training acc: 0.125
Global Iter: 1841800 training loss: 2.10743
Global Iter: 1841800 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1841872
Number of Patches: 12884
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1841872
Global Iter: 1841900 training loss: 1.97712
Global Iter: 1841900 training acc: 0.15625
Global Iter: 1842000 training loss: 2.00701
Global Iter: 1842000 training acc: 0.125
Global Iter: 1842100 training loss: 2.08666
Global Iter: 1842100 training acc: 0.09375
Global Iter: 1842200 training loss: 2.01147
Global Iter: 1842200 training acc: 0.125
Global Iter: 1842300 training loss: 2.03013
Global Iter: 1842300 training acc: 0.125
Global Iter: 1842400 training loss: 2.09078
Global Iter: 1842400 training acc: 0.0
Global Iter: 1842500 training loss: 2.0515
Global Iter: 1842500 training acc: 0.21875
Global Iter: 1842600 training loss: 1.93768
Global Iter: 1842600 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1842678
Number of Patches: 12756
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1842678
Global Iter: 1842700 training loss: 2.01293
Global Iter: 1842700 training acc: 0.1875
Global Iter: 1842800 training loss: 1.97746
Global Iter: 1842800 training acc: 0.15625
Global Iter: 1842900 training loss: 1.9889
Global Iter: 1842900 training acc: 0.125
Global Iter: 1843000 training loss: 2.03192
Global Iter: 1843000 training acc: 0.15625
Global Iter: 1843100 training loss: 1.98816
Global Iter: 1843100 training acc: 0.28125
Global Iter: 1843200 training loss: 1.9742
Global Iter: 1843200 training acc: 0.21875
Global Iter: 12017-06-23 09:23:47.004436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1843476
2017-06-23 09:25:05.168553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1844266
2017-06-23 09:26:23.674955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1845048
2017-06-23 09:27:41.101858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1845822
2017-06-23 09:28:57.514056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1846588
843300 training loss: 1.96241
Global Iter: 1843300 training acc: 0.125
Global Iter: 1843400 training loss: 1.97854
Global Iter: 1843400 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1843476
Number of Patches: 12629
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1843476
Global Iter: 1843500 training loss: 2.04888
Global Iter: 1843500 training acc: 0.15625
Global Iter: 1843600 training loss: 1.99711
Global Iter: 1843600 training acc: 0.125
Global Iter: 1843700 training loss: 1.99851
Global Iter: 1843700 training acc: 0.0625
Global Iter: 1843800 training loss: 2.10887
Global Iter: 1843800 training acc: 0.15625
Global Iter: 1843900 training loss: 1.94524
Global Iter: 1843900 training acc: 0.15625
Global Iter: 1844000 training loss: 1.98808
Global Iter: 1844000 training acc: 0.0625
Global Iter: 1844100 training loss: 1.97422
Global Iter: 1844100 training acc: 0.1875
Global Iter: 1844200 training loss: 2.05308
Global Iter: 1844200 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1844266
Number of Patches: 12503
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1844266
Global Iter: 1844300 training loss: 2.17728
Global Iter: 1844300 training acc: 0.15625
Global Iter: 1844400 training loss: 2.06579
Global Iter: 1844400 training acc: 0.125
Global Iter: 1844500 training loss: 2.00179
Global Iter: 1844500 training acc: 0.1875
Global Iter: 1844600 training loss: 2.01315
Global Iter: 1844600 training acc: 0.09375
Global Iter: 1844700 training loss: 2.06365
Global Iter: 1844700 training acc: 0.09375
Global Iter: 1844800 training loss: 2.0885
Global Iter: 1844800 training acc: 0.09375
Global Iter: 1844900 training loss: 2.01536
Global Iter: 1844900 training acc: 0.1875
Global Iter: 1845000 training loss: 2.01766
Global Iter: 1845000 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1845048
Number of Patches: 12378
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1845048
Global Iter: 1845100 training loss: 1.91841
Global Iter: 1845100 training acc: 0.15625
Global Iter: 1845200 training loss: 2.02401
Global Iter: 1845200 training acc: 0.15625
Global Iter: 1845300 training loss: 2.02786
Global Iter: 1845300 training acc: 0.15625
Global Iter: 1845400 training loss: 2.08663
Global Iter: 1845400 training acc: 0.21875
Global Iter: 1845500 training loss: 1.97282
Global Iter: 1845500 training acc: 0.15625
Global Iter: 1845600 training loss: 1.95818
Global Iter: 1845600 training acc: 0.34375
Global Iter: 1845700 training loss: 2.03151
Global Iter: 1845700 training acc: 0.1875
Global Iter: 1845800 training loss: 1.97735
Global Iter: 1845800 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1845822
Number of Patches: 12255
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1845822
Global Iter: 1845900 training loss: 2.03642
Global Iter: 1845900 training acc: 0.0625
Global Iter: 1846000 training loss: 2.02046
Global Iter: 1846000 training acc: 0.21875
Global Iter: 1846100 training loss: 1.93289
Global Iter: 1846100 training acc: 0.1875
Global Iter: 1846200 training loss: 2.01894
Global Iter: 1846200 training acc: 0.3125
Global Iter: 1846300 training loss: 1.9559
Global Iter: 1846300 training acc: 0.15625
Global Iter: 1846400 training loss: 1.95765
Global Iter: 1846400 training acc: 0.15625
Global Iter: 1846500 training loss: 2.04354
Global Iter: 1846500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1846588
Number of Patches: 12133
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1846588
Global Iter: 1846600 training loss: 1.99731
Global Iter: 1846600 training acc: 0.1875
Global Iter: 1846700 training loss: 2.03116
Global Iter2017-06-23 09:30:11.894219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1847347
2017-06-23 09:31:28.107349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1848098
2017-06-23 09:32:42.533726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1848842
2017-06-23 09:33:54.408967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1849578
: 1846700 training acc: 0.21875
Global Iter: 1846800 training loss: 2.06859
Global Iter: 1846800 training acc: 0.125
Global Iter: 1846900 training loss: 2.12357
Global Iter: 1846900 training acc: 0.15625
Global Iter: 1847000 training loss: 1.97552
Global Iter: 1847000 training acc: 0.15625
Global Iter: 1847100 training loss: 2.00341
Global Iter: 1847100 training acc: 0.25
Global Iter: 1847200 training loss: 2.00761
Global Iter: 1847200 training acc: 0.15625
Global Iter: 1847300 training loss: 1.98488
Global Iter: 1847300 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1847347
Number of Patches: 12012
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1847347
Global Iter: 1847400 training loss: 2.05531
Global Iter: 1847400 training acc: 0.21875
Global Iter: 1847500 training loss: 1.91143
Global Iter: 1847500 training acc: 0.21875
Global Iter: 1847600 training loss: 2.01065
Global Iter: 1847600 training acc: 0.0625
Global Iter: 1847700 training loss: 2.19209
Global Iter: 1847700 training acc: 0.0625
Global Iter: 1847800 training loss: 2.03129
Global Iter: 1847800 training acc: 0.28125
Global Iter: 1847900 training loss: 2.04276
Global Iter: 1847900 training acc: 0.09375
Global Iter: 1848000 training loss: 2.0787
Global Iter: 1848000 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1848098
Number of Patches: 11892
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1848098
Global Iter: 1848100 training loss: 1.92949
Global Iter: 1848100 training acc: 0.3125
Global Iter: 1848200 training loss: 2.09221
Global Iter: 1848200 training acc: 0.125
Global Iter: 1848300 training loss: 1.99963
Global Iter: 1848300 training acc: 0.0625
Global Iter: 1848400 training loss: 2.00048
Global Iter: 1848400 training acc: 0.03125
Global Iter: 1848500 training loss: 2.10393
Global Iter: 1848500 training acc: 0.09375
Global Iter: 1848600 training loss: 2.02506
Global Iter: 1848600 training acc: 0.0625
Global Iter: 1848700 training loss: 2.09046
Global Iter: 1848700 training acc: 0.09375
Global Iter: 1848800 training loss: 2.06292
Global Iter: 1848800 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1848842
Number of Patches: 11774
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1848842
Global Iter: 1848900 training loss: 2.01863
Global Iter: 1848900 training acc: 0.1875
Global Iter: 1849000 training loss: 1.98776
Global Iter: 1849000 training acc: 0.15625
Global Iter: 1849100 training loss: 2.08721
Global Iter: 1849100 training acc: 0.28125
Global Iter: 1849200 training loss: 1.96386
Global Iter: 1849200 training acc: 0.125
Global Iter: 1849300 training loss: 1.97016
Global Iter: 1849300 training acc: 0.1875
Global Iter: 1849400 training loss: 1.97857
Global Iter: 1849400 training acc: 0.125
Global Iter: 1849500 training loss: 1.98349
Global Iter: 1849500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1849578
Number of Patches: 11657
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1849578
Global Iter: 1849600 training loss: 2.08425
Global Iter: 1849600 training acc: 0.125
Global Iter: 1849700 training loss: 1.98538
Global Iter: 1849700 training acc: 0.25
Global Iter: 1849800 training loss: 2.00949
Global Iter: 1849800 training acc: 0.3125
Global Iter: 1849900 training loss: 2.08211
Global Iter: 1849900 training acc: 0.125
Global Iter: 1850000 training loss: 2.00832
Global Iter: 1850000 training acc: 0.125
Global Iter: 1850100 training loss: 1.9314
Global Iter: 1850100 training acc: 0.21875
Global Iter: 1850200 training loss: 2.02433
Global Iter: 1850200 training acc: 0.25
Global Iter: 1850300 training loss: 2.00272
Global Iter: 1850300 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b2017-06-23 09:35:10.441274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1850307
2017-06-23 09:36:26.875955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1851029
2017-06-23 09:37:41.786026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1851744
2017-06-23 09:39:00.785234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1852451
2017-06-23 09:40:13.143674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1853151
256_lr0005/model.ckpt-1850307
Number of Patches: 11541
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1850307
Global Iter: 1850400 training loss: 2.06011
Global Iter: 1850400 training acc: 0.1875
Global Iter: 1850500 training loss: 1.9779
Global Iter: 1850500 training acc: 0.28125
Global Iter: 1850600 training loss: 2.00966
Global Iter: 1850600 training acc: 0.15625
Global Iter: 1850700 training loss: 2.06511
Global Iter: 1850700 training acc: 0.0625
Global Iter: 1850800 training loss: 2.01578
Global Iter: 1850800 training acc: 0.1875
Global Iter: 1850900 training loss: 1.96659
Global Iter: 1850900 training acc: 0.1875
Global Iter: 1851000 training loss: 2.20935
Global Iter: 1851000 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1851029
Number of Patches: 11426
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1851029
Global Iter: 1851100 training loss: 2.04334
Global Iter: 1851100 training acc: 0.0625
Global Iter: 1851200 training loss: 1.99756
Global Iter: 1851200 training acc: 0.125
Global Iter: 1851300 training loss: 2.02423
Global Iter: 1851300 training acc: 0.09375
Global Iter: 1851400 training loss: 2.01261
Global Iter: 1851400 training acc: 0.3125
Global Iter: 1851500 training loss: 2.05776
Global Iter: 1851500 training acc: 0.21875
Global Iter: 1851600 training loss: 1.94185
Global Iter: 1851600 training acc: 0.3125
Global Iter: 1851700 training loss: 1.99112
Global Iter: 1851700 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1851744
Number of Patches: 11312
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1851744
Global Iter: 1851800 training loss: 2.05751
Global Iter: 1851800 training acc: 0.125
Global Iter: 1851900 training loss: 1.97795
Global Iter: 1851900 training acc: 0.15625
Global Iter: 1852000 training loss: 1.99571
Global Iter: 1852000 training acc: 0.1875
Global Iter: 1852100 training loss: 1.91906
Global Iter: 1852100 training acc: 0.125
Global Iter: 1852200 training loss: 1.98877
Global Iter: 1852200 training acc: 0.25
Global Iter: 1852300 training loss: 2.10384
Global Iter: 1852300 training acc: 0.15625
Global Iter: 1852400 training loss: 2.03268
Global Iter: 1852400 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1852451
Number of Patches: 11199
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1852451
Global Iter: 1852500 training loss: 2.01269
Global Iter: 1852500 training acc: 0.125
Global Iter: 1852600 training loss: 2.06052
Global Iter: 1852600 training acc: 0.0625
Global Iter: 1852700 training loss: 1.96286
Global Iter: 1852700 training acc: 0.0625
Global Iter: 1852800 training loss: 1.96708
Global Iter: 1852800 training acc: 0.125
Global Iter: 1852900 training loss: 2.09242
Global Iter: 1852900 training acc: 0.125
Global Iter: 1853000 training loss: 2.14824
Global Iter: 1853000 training acc: 0.09375
Global Iter: 1853100 training loss: 1.96963
Global Iter: 1853100 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1853151
Number of Patches: 11088
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1853151
Global Iter: 1853200 training loss: 2.00069
Global Iter: 1853200 training acc: 0.0625
Global Iter: 1853300 training loss: 2.09351
Global Iter: 1853300 training acc: 0.1875
Global Iter: 1853400 training loss: 2.02901
Global Iter: 1853400 training acc: 0.125
Global Iter: 1853500 training loss: 1.99054
Global Iter: 1853500 training acc: 0.28125
Global Iter: 1853600 training loss: 1.93723
Global Iter: 1853600 training acc: 0.125
Global Iter: 1853700 training loss: 1.97687
Global Iter: 1853700 training acc: 0.15625
Global Iter: 1853800 training loss: 1.95656
Global Iter: 1853800 training acc: 0.1875
Model saved in file: /home/ahmet/works2017-06-23 09:41:26.714346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1853844
2017-06-23 09:42:38.684611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1854531
2017-06-23 09:43:54.150923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1855211
2017-06-23 09:45:04.595358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1855884
2017-06-23 09:46:15.292064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1856550
pace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1853844
Number of Patches: 10978
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1853844
Global Iter: 1853900 training loss: 2.00107
Global Iter: 1853900 training acc: 0.15625
Global Iter: 1854000 training loss: 2.11188
Global Iter: 1854000 training acc: 0.09375
Global Iter: 1854100 training loss: 1.92309
Global Iter: 1854100 training acc: 0.125
Global Iter: 1854200 training loss: 1.95067
Global Iter: 1854200 training acc: 0.1875
Global Iter: 1854300 training loss: 2.0628
Global Iter: 1854300 training acc: 0.15625
Global Iter: 1854400 training loss: 2.08279
Global Iter: 1854400 training acc: 0.1875
Global Iter: 1854500 training loss: 2.0121
Global Iter: 1854500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1854531
Number of Patches: 10869
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1854531
Global Iter: 1854600 training loss: 2.07857
Global Iter: 1854600 training acc: 0.125
Global Iter: 1854700 training loss: 1.97092
Global Iter: 1854700 training acc: 0.15625
Global Iter: 1854800 training loss: 1.99623
Global Iter: 1854800 training acc: 0.15625
Global Iter: 1854900 training loss: 2.05047
Global Iter: 1854900 training acc: 0.25
Global Iter: 1855000 training loss: 2.03938
Global Iter: 1855000 training acc: 0.09375
Global Iter: 1855100 training loss: 2.04976
Global Iter: 1855100 training acc: 0.15625
Global Iter: 1855200 training loss: 2.13446
Global Iter: 1855200 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1855211
Number of Patches: 10761
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1855211
Global Iter: 1855300 training loss: 2.02851
Global Iter: 1855300 training acc: 0.125
Global Iter: 1855400 training loss: 1.97494
Global Iter: 1855400 training acc: 0.1875
Global Iter: 1855500 training loss: 1.9563
Global Iter: 1855500 training acc: 0.21875
Global Iter: 1855600 training loss: 2.05649
Global Iter: 1855600 training acc: 0.15625
Global Iter: 1855700 training loss: 2.04424
Global Iter: 1855700 training acc: 0.125
Global Iter: 1855800 training loss: 2.07773
Global Iter: 1855800 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1855884
Number of Patches: 10654
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1855884
Global Iter: 1855900 training loss: 2.07119
Global Iter: 1855900 training acc: 0.21875
Global Iter: 1856000 training loss: 1.99569
Global Iter: 1856000 training acc: 0.09375
Global Iter: 1856100 training loss: 2.05716
Global Iter: 1856100 training acc: 0.21875
Global Iter: 1856200 training loss: 2.04362
Global Iter: 1856200 training acc: 0.1875
Global Iter: 1856300 training loss: 1.99241
Global Iter: 1856300 training acc: 0.21875
Global Iter: 1856400 training loss: 2.08226
Global Iter: 1856400 training acc: 0.21875
Global Iter: 1856500 training loss: 2.01676
Global Iter: 1856500 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1856550
Number of Patches: 10548
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1856550
Global Iter: 1856600 training loss: 1.96075
Global Iter: 1856600 training acc: 0.1875
Global Iter: 1856700 training loss: 1.99502
Global Iter: 1856700 training acc: 0.15625
Global Iter: 1856800 training loss: 1.98205
Global Iter: 1856800 training acc: 0.09375
Global Iter: 1856900 training loss: 2.02456
Global Iter: 1856900 training acc: 0.15625
Global Iter: 1857000 training loss: 1.99324
Global Iter: 1857000 training acc: 0.15625
Global Iter: 1857100 training loss: 2.12381
Global Iter: 1857100 training acc: 0.09375
Global Iter: 1857200 training loss: 2.05032
Global Iter: 1857200 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr002017-06-23 09:47:24.826769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1857210
2017-06-23 09:48:36.202288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1857863
2017-06-23 09:49:42.950076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1858510
2017-06-23 09:50:50.937524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1859150
2017-06-23 09:51:57.780910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1859784
2017-06-23 09:53:06.596939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1860412
05/model.ckpt-1857210
Number of Patches: 10443
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1857210
Global Iter: 1857300 training loss: 1.99164
Global Iter: 1857300 training acc: 0.09375
Global Iter: 1857400 training loss: 2.0378
Global Iter: 1857400 training acc: 0.125
Global Iter: 1857500 training loss: 2.02089
Global Iter: 1857500 training acc: 0.15625
Global Iter: 1857600 training loss: 2.03241
Global Iter: 1857600 training acc: 0.125
Global Iter: 1857700 training loss: 1.96417
Global Iter: 1857700 training acc: 0.15625
Global Iter: 1857800 training loss: 2.00588
Global Iter: 1857800 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1857863
Number of Patches: 10339
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1857863
Global Iter: 1857900 training loss: 2.10289
Global Iter: 1857900 training acc: 0.15625
Global Iter: 1858000 training loss: 1.97879
Global Iter: 1858000 training acc: 0.21875
Global Iter: 1858100 training loss: 2.0799
Global Iter: 1858100 training acc: 0.15625
Global Iter: 1858200 training loss: 1.99616
Global Iter: 1858200 training acc: 0.25
Global Iter: 1858300 training loss: 2.04988
Global Iter: 1858300 training acc: 0.09375
Global Iter: 1858400 training loss: 1.99826
Global Iter: 1858400 training acc: 0.09375
Global Iter: 1858500 training loss: 2.03798
Global Iter: 1858500 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1858510
Number of Patches: 10236
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1858510
Global Iter: 1858600 training loss: 2.01038
Global Iter: 1858600 training acc: 0.1875
Global Iter: 1858700 training loss: 2.05049
Global Iter: 1858700 training acc: 0.15625
Global Iter: 1858800 training loss: 2.14746
Global Iter: 1858800 training acc: 0.09375
Global Iter: 1858900 training loss: 2.0088
Global Iter: 1858900 training acc: 0.15625
Global Iter: 1859000 training loss: 1.99607
Global Iter: 1859000 training acc: 0.0625
Global Iter: 1859100 training loss: 1.95348
Global Iter: 1859100 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1859150
Number of Patches: 10134
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1859150
Global Iter: 1859200 training loss: 2.01266
Global Iter: 1859200 training acc: 0.3125
Global Iter: 1859300 training loss: 1.99551
Global Iter: 1859300 training acc: 0.15625
Global Iter: 1859400 training loss: 2.02875
Global Iter: 1859400 training acc: 0.125
Global Iter: 1859500 training loss: 2.07068
Global Iter: 1859500 training acc: 0.15625
Global Iter: 1859600 training loss: 1.9979
Global Iter: 1859600 training acc: 0.125
Global Iter: 1859700 training loss: 2.07818
Global Iter: 1859700 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1859784
Number of Patches: 10033
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1859784
Global Iter: 1859800 training loss: 2.03894
Global Iter: 1859800 training acc: 0.21875
Global Iter: 1859900 training loss: 2.04402
Global Iter: 1859900 training acc: 0.21875
Global Iter: 1860000 training loss: 2.0088
Global Iter: 1860000 training acc: 0.15625
Global Iter: 1860100 training loss: 2.04007
Global Iter: 1860100 training acc: 0.125
Global Iter: 1860200 training loss: 2.06797
Global Iter: 1860200 training acc: 0.21875
Global Iter: 1860300 training loss: 1.98379
Global Iter: 1860300 training acc: 0.125
Global Iter: 1860400 training loss: 2.04033
Global Iter: 1860400 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1860412
Number of Patches: 9933
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1860412
Global Iter: 1860500 training loss: 2.00336
Global Iter: 1860500 traini2017-06-23 09:54:12.013560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1861033
2017-06-23 09:55:16.411153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1861648
2017-06-23 09:56:24.069599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1862257
2017-06-23 09:57:26.709730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1862860
2017-06-23 09:58:32.080491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1863457
ng acc: 0.125
Global Iter: 1860600 training loss: 2.00644
Global Iter: 1860600 training acc: 0.125
Global Iter: 1860700 training loss: 1.96881
Global Iter: 1860700 training acc: 0.15625
Global Iter: 1860800 training loss: 2.05416
Global Iter: 1860800 training acc: 0.3125
Global Iter: 1860900 training loss: 1.98199
Global Iter: 1860900 training acc: 0.21875
Global Iter: 1861000 training loss: 2.08384
Global Iter: 1861000 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1861033
Number of Patches: 9834
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1861033
Global Iter: 1861100 training loss: 1.90912
Global Iter: 1861100 training acc: 0.15625
Global Iter: 1861200 training loss: 1.96237
Global Iter: 1861200 training acc: 0.21875
Global Iter: 1861300 training loss: 1.97361
Global Iter: 1861300 training acc: 0.15625
Global Iter: 1861400 training loss: 1.9681
Global Iter: 1861400 training acc: 0.15625
Global Iter: 1861500 training loss: 1.99138
Global Iter: 1861500 training acc: 0.125
Global Iter: 1861600 training loss: 2.13915
Global Iter: 1861600 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1861648
Number of Patches: 9736
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1861648
Global Iter: 1861700 training loss: 2.04066
Global Iter: 1861700 training acc: 0.1875
Global Iter: 1861800 training loss: 1.9515
Global Iter: 1861800 training acc: 0.15625
Global Iter: 1861900 training loss: 2.05185
Global Iter: 1861900 training acc: 0.1875
Global Iter: 1862000 training loss: 1.97436
Global Iter: 1862000 training acc: 0.09375
Global Iter: 1862100 training loss: 2.03331
Global Iter: 1862100 training acc: 0.1875
Global Iter: 1862200 training loss: 1.94046
Global Iter: 1862200 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1862257
Number of Patches: 9639
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1862257
Global Iter: 1862300 training loss: 1.96905
Global Iter: 1862300 training acc: 0.09375
Global Iter: 1862400 training loss: 2.02485
Global Iter: 1862400 training acc: 0.125
Global Iter: 1862500 training loss: 2.02891
Global Iter: 1862500 training acc: 0.15625
Global Iter: 1862600 training loss: 1.94141
Global Iter: 1862600 training acc: 0.15625
Global Iter: 1862700 training loss: 1.9607
Global Iter: 1862700 training acc: 0.15625
Global Iter: 1862800 training loss: 2.09665
Global Iter: 1862800 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1862860
Number of Patches: 9543
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1862860
Global Iter: 1862900 training loss: 1.98788
Global Iter: 1862900 training acc: 0.09375
Global Iter: 1863000 training loss: 2.15233
Global Iter: 1863000 training acc: 0.15625
Global Iter: 1863100 training loss: 1.94183
Global Iter: 1863100 training acc: 0.25
Global Iter: 1863200 training loss: 1.97771
Global Iter: 1863200 training acc: 0.21875
Global Iter: 1863300 training loss: 1.95686
Global Iter: 1863300 training acc: 0.21875
Global Iter: 1863400 training loss: 1.95415
Global Iter: 1863400 training acc: 0.25
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1863457
Number of Patches: 9448
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1863457
Global Iter: 1863500 training loss: 1.97683
Global Iter: 1863500 training acc: 0.03125
Global Iter: 1863600 training loss: 1.96766
Global Iter: 1863600 training acc: 0.1875
Global Iter: 1863700 training loss: 2.08487
Global Iter: 1863700 training acc: 0.25
Global Iter: 1863800 training loss: 2.03122
Global Iter: 1863800 training acc: 0.1875
Global Iter: 1863900 training loss: 1.94428
Global Iter: 1863900 training acc: 0.125
Global Iter: 1864000 training loss: 1.2017-06-23 09:59:32.654876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1864048
2017-06-23 10:00:33.782853: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1864633
2017-06-23 10:01:35.892571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1865212
2017-06-23 10:02:37.372677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1865786
2017-06-23 10:03:40.716899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1866354
2017-06-23 10:04:38.603272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1866916
99887
Global Iter: 1864000 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1864048
Number of Patches: 9354
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1864048
Global Iter: 1864100 training loss: 2.01139
Global Iter: 1864100 training acc: 0.1875
Global Iter: 1864200 training loss: 1.97664
Global Iter: 1864200 training acc: 0.09375
Global Iter: 1864300 training loss: 1.99401
Global Iter: 1864300 training acc: 0.15625
Global Iter: 1864400 training loss: 1.98817
Global Iter: 1864400 training acc: 0.15625
Global Iter: 1864500 training loss: 2.07905
Global Iter: 1864500 training acc: 0.1875
Global Iter: 1864600 training loss: 2.03502
Global Iter: 1864600 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1864633
Number of Patches: 9261
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1864633
Global Iter: 1864700 training loss: 1.98863
Global Iter: 1864700 training acc: 0.15625
Global Iter: 1864800 training loss: 2.03332
Global Iter: 1864800 training acc: 0.1875
Global Iter: 1864900 training loss: 1.96324
Global Iter: 1864900 training acc: 0.1875
Global Iter: 1865000 training loss: 2.02916
Global Iter: 1865000 training acc: 0.15625
Global Iter: 1865100 training loss: 1.967
Global Iter: 1865100 training acc: 0.09375
Global Iter: 1865200 training loss: 1.95261
Global Iter: 1865200 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1865212
Number of Patches: 9169
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1865212
Global Iter: 1865300 training loss: 2.06225
Global Iter: 1865300 training acc: 0.125
Global Iter: 1865400 training loss: 2.00717
Global Iter: 1865400 training acc: 0.125
Global Iter: 1865500 training loss: 2.01408
Global Iter: 1865500 training acc: 0.125
Global Iter: 1865600 training loss: 2.03531
Global Iter: 1865600 training acc: 0.0625
Global Iter: 1865700 training loss: 2.06409
Global Iter: 1865700 training acc: 0.3125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1865786
Number of Patches: 9078
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1865786
Global Iter: 1865800 training loss: 1.89416
Global Iter: 1865800 training acc: 0.375
Global Iter: 1865900 training loss: 1.99471
Global Iter: 1865900 training acc: 0.1875
Global Iter: 1866000 training loss: 2.00496
Global Iter: 1866000 training acc: 0.15625
Global Iter: 1866100 training loss: 1.97769
Global Iter: 1866100 training acc: 0.15625
Global Iter: 1866200 training loss: 2.03686
Global Iter: 1866200 training acc: 0.1875
Global Iter: 1866300 training loss: 2.03419
Global Iter: 1866300 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1866354
Number of Patches: 8988
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1866354
Global Iter: 1866400 training loss: 2.05916
Global Iter: 1866400 training acc: 0.15625
Global Iter: 1866500 training loss: 2.0336
Global Iter: 1866500 training acc: 0.0625
Global Iter: 1866600 training loss: 2.01446
Global Iter: 1866600 training acc: 0.09375
Global Iter: 1866700 training loss: 2.00931
Global Iter: 1866700 training acc: 0.125
Global Iter: 1866800 training loss: 1.97264
Global Iter: 1866800 training acc: 0.125
Global Iter: 1866900 training loss: 1.99615
Global Iter: 1866900 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1866916
Number of Patches: 8899
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1866916
Global Iter: 1867000 training loss: 1.96687
Global Iter: 1867000 training acc: 0.25
Global Iter: 1867100 training loss: 1.9807
Global Iter: 1867100 training acc: 0.25
Global Iter: 1867200 training loss: 2.03085
G2017-06-23 10:05:38.103227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1867473
2017-06-23 10:06:37.883496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1868024
2017-06-23 10:07:41.767991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1868570
2017-06-23 10:08:42.932039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1869110
2017-06-23 10:09:41.101186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1869645
2017-06-23 10:10:38.983403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1870175
lobal Iter: 1867200 training acc: 0.25
Global Iter: 1867300 training loss: 2.10046
Global Iter: 1867300 training acc: 0.09375
Global Iter: 1867400 training loss: 1.95278
Global Iter: 1867400 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1867473
Number of Patches: 8811
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1867473
Global Iter: 1867500 training loss: 1.99197
Global Iter: 1867500 training acc: 0.125
Global Iter: 1867600 training loss: 2.0051
Global Iter: 1867600 training acc: 0.0625
Global Iter: 1867700 training loss: 1.95652
Global Iter: 1867700 training acc: 0.125
Global Iter: 1867800 training loss: 2.05861
Global Iter: 1867800 training acc: 0.25
Global Iter: 1867900 training loss: 1.95555
Global Iter: 1867900 training acc: 0.125
Global Iter: 1868000 training loss: 1.92565
Global Iter: 1868000 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1868024
Number of Patches: 8723
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1868024
Global Iter: 1868100 training loss: 2.22746
Global Iter: 1868100 training acc: 0.0625
Global Iter: 1868200 training loss: 2.12328
Global Iter: 1868200 training acc: 0.15625
Global Iter: 1868300 training loss: 1.93598
Global Iter: 1868300 training acc: 0.15625
Global Iter: 1868400 training loss: 2.0557
Global Iter: 1868400 training acc: 0.125
Global Iter: 1868500 training loss: 1.98361
Global Iter: 1868500 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1868570
Number of Patches: 8636
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1868570
Global Iter: 1868600 training loss: 1.92981
Global Iter: 1868600 training acc: 0.125
Global Iter: 1868700 training loss: 2.10516
Global Iter: 1868700 training acc: 0.1875
Global Iter: 1868800 training loss: 1.9987
Global Iter: 1868800 training acc: 0.09375
Global Iter: 1868900 training loss: 1.9402
Global Iter: 1868900 training acc: 0.15625
Global Iter: 1869000 training loss: 2.0037
Global Iter: 1869000 training acc: 0.125
Global Iter: 1869100 training loss: 2.12148
Global Iter: 1869100 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1869110
Number of Patches: 8550
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1869110
Global Iter: 1869200 training loss: 2.0396
Global Iter: 1869200 training acc: 0.25
Global Iter: 1869300 training loss: 1.9458
Global Iter: 1869300 training acc: 0.125
Global Iter: 1869400 training loss: 1.98429
Global Iter: 1869400 training acc: 0.1875
Global Iter: 1869500 training loss: 1.98574
Global Iter: 1869500 training acc: 0.125
Global Iter: 1869600 training loss: 1.99581
Global Iter: 1869600 training acc: 0.28125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1869645
Number of Patches: 8465
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1869645
Global Iter: 1869700 training loss: 1.96345
Global Iter: 1869700 training acc: 0.1875
Global Iter: 1869800 training loss: 1.9647
Global Iter: 1869800 training acc: 0.28125
Global Iter: 1869900 training loss: 2.05677
Global Iter: 1869900 training acc: 0.21875
Global Iter: 1870000 training loss: 2.09753
Global Iter: 1870000 training acc: 0.03125
Global Iter: 1870100 training loss: 2.02328
Global Iter: 1870100 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1870175
Number of Patches: 8381
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1870175
Global Iter: 1870200 training loss: 1.98557
Global Iter: 1870200 training acc: 0.1875
Global Iter: 1870300 training loss: 1.96277
Global Iter: 1870300 training acc: 0.09375
Global Iter: 1870400 training loss: 2.08784
Global Iter: 18702017-06-23 10:11:34.556919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1870699
2017-06-23 10:12:28.773775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1871218
2017-06-23 10:13:25.151265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1871732
2017-06-23 10:14:18.105733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1872241
2017-06-23 10:15:11.950451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1872745
2017-06-23 10:16:05.886481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1873244
400 training acc: 0.125
Global Iter: 1870500 training loss: 2.07218
Global Iter: 1870500 training acc: 0.125
Global Iter: 1870600 training loss: 2.00144
Global Iter: 1870600 training acc: 0.03125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1870699
Number of Patches: 8298
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1870699
Global Iter: 1870700 training loss: 2.12424
Global Iter: 1870700 training acc: 0.125
Global Iter: 1870800 training loss: 2.00814
Global Iter: 1870800 training acc: 0.15625
Global Iter: 1870900 training loss: 1.94932
Global Iter: 1870900 training acc: 0.125
Global Iter: 1871000 training loss: 2.00099
Global Iter: 1871000 training acc: 0.125
Global Iter: 1871100 training loss: 1.96607
Global Iter: 1871100 training acc: 0.09375
Global Iter: 1871200 training loss: 1.95697
Global Iter: 1871200 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1871218
Number of Patches: 8216
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1871218
Global Iter: 1871300 training loss: 1.95563
Global Iter: 1871300 training acc: 0.1875
Global Iter: 1871400 training loss: 2.04153
Global Iter: 1871400 training acc: 0.125
Global Iter: 1871500 training loss: 2.015
Global Iter: 1871500 training acc: 0.125
Global Iter: 1871600 training loss: 1.96154
Global Iter: 1871600 training acc: 0.21875
Global Iter: 1871700 training loss: 1.97817
Global Iter: 1871700 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1871732
Number of Patches: 8134
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1871732
Global Iter: 1871800 training loss: 1.931
Global Iter: 1871800 training acc: 0.0625
Global Iter: 1871900 training loss: 1.92452
Global Iter: 1871900 training acc: 0.25
Global Iter: 1872000 training loss: 1.95811
Global Iter: 1872000 training acc: 0.3125
Global Iter: 1872100 training loss: 1.97485
Global Iter: 1872100 training acc: 0.25
Global Iter: 1872200 training loss: 1.97574
Global Iter: 1872200 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1872241
Number of Patches: 8053
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1872241
Global Iter: 1872300 training loss: 1.99545
Global Iter: 1872300 training acc: 0.21875
Global Iter: 1872400 training loss: 1.9643
Global Iter: 1872400 training acc: 0.21875
Global Iter: 1872500 training loss: 1.93382
Global Iter: 1872500 training acc: 0.15625
Global Iter: 1872600 training loss: 2.0132
Global Iter: 1872600 training acc: 0.125
Global Iter: 1872700 training loss: 2.04806
Global Iter: 1872700 training acc: 0.09375
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1872745
Number of Patches: 7973
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1872745
Global Iter: 1872800 training loss: 2.00394
Global Iter: 1872800 training acc: 0.0625
Global Iter: 1872900 training loss: 2.0499
Global Iter: 1872900 training acc: 0.1875
Global Iter: 1873000 training loss: 1.99316
Global Iter: 1873000 training acc: 0.25
Global Iter: 1873100 training loss: 2.05518
Global Iter: 1873100 training acc: 0.28125
Global Iter: 1873200 training loss: 2.06755
Global Iter: 1873200 training acc: 0.03125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1873244
Number of Patches: 7894
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1873244
Global Iter: 1873300 training loss: 1.95343
Global Iter: 1873300 training acc: 0.0625
Global Iter: 1873400 training loss: 1.99317
Global Iter: 1873400 training acc: 0.21875
Global Iter: 1873500 training loss: 1.98657
Global Iter: 1873500 training acc: 0.25
Global Iter: 1873600 training loss: 1.99856
Global Iter: 1873600 training acc2017-06-23 10:16:58.878519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1873738
2017-06-23 10:17:49.727688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1874227
2017-06-23 10:18:43.122966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1874711
2017-06-23 10:19:31.977314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1875190
2017-06-23 10:20:21.219428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1875665
2017-06-23 10:21:17.683359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1876135
2017-06-23 10:22:05.919841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1876600
: 0.125
Global Iter: 1873700 training loss: 2.02284
Global Iter: 1873700 training acc: 0.15625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1873738
Number of Patches: 7816
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1873738
Global Iter: 1873800 training loss: 2.03588
Global Iter: 1873800 training acc: 0.09375
Global Iter: 1873900 training loss: 2.02132
Global Iter: 1873900 training acc: 0.15625
Global Iter: 1874000 training loss: 1.95218
Global Iter: 1874000 training acc: 0.15625
Global Iter: 1874100 training loss: 2.05649
Global Iter: 1874100 training acc: 0.15625
Global Iter: 1874200 training loss: 1.9313
Global Iter: 1874200 training acc: 0.125
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1874227
Number of Patches: 7738
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1874227
Global Iter: 1874300 training loss: 1.98803
Global Iter: 1874300 training acc: 0.25
Global Iter: 1874400 training loss: 2.01578
Global Iter: 1874400 training acc: 0.125
Global Iter: 1874500 training loss: 1.94154
Global Iter: 1874500 training acc: 0.15625
Global Iter: 1874600 training loss: 1.99808
Global Iter: 1874600 training acc: 0.15625
Global Iter: 1874700 training loss: 1.92238
Global Iter: 1874700 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1874711
Number of Patches: 7661
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1874711
Global Iter: 1874800 training loss: 2.0161
Global Iter: 1874800 training acc: 0.125
Global Iter: 1874900 training loss: 2.01094
Global Iter: 1874900 training acc: 0.09375
Global Iter: 1875000 training loss: 1.98712
Global Iter: 1875000 training acc: 0.1875
Global Iter: 1875100 training loss: 1.94702
Global Iter: 1875100 training acc: 0.21875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1875190
Number of Patches: 7585
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1875190
Global Iter: 1875200 training loss: 2.02569
Global Iter: 1875200 training acc: 0.09375
Global Iter: 1875300 training loss: 2.06086
Global Iter: 1875300 training acc: 0.15625
Global Iter: 1875400 training loss: 2.02807
Global Iter: 1875400 training acc: 0.21875
Global Iter: 1875500 training loss: 1.94391
Global Iter: 1875500 training acc: 0.15625
Global Iter: 1875600 training loss: 2.12808
Global Iter: 1875600 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1875665
Number of Patches: 7510
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1875665
Global Iter: 1875700 training loss: 2.02348
Global Iter: 1875700 training acc: 0.0625
Global Iter: 1875800 training loss: 2.09301
Global Iter: 1875800 training acc: 0.125
Global Iter: 1875900 training loss: 2.03289
Global Iter: 1875900 training acc: 0.09375
Global Iter: 1876000 training loss: 2.09503
Global Iter: 1876000 training acc: 0.15625
Global Iter: 1876100 training loss: 1.99403
Global Iter: 1876100 training acc: 0.1875
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1876135
Number of Patches: 7435
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1876135
Global Iter: 1876200 training loss: 2.12425
Global Iter: 1876200 training acc: 0.03125
Global Iter: 1876300 training loss: 2.00939
Global Iter: 1876300 training acc: 0.09375
Global Iter: 1876400 training loss: 2.02251
Global Iter: 1876400 training acc: 0.09375
Global Iter: 1876500 training loss: 2.03135
Global Iter: 1876500 training acc: 0.0625
Model saved in file: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1876600
Number of Patches: 7361
checkpoint found: /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1876600
Global Iter: 1876700 tr2017-06-23 10:22:55.138587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
INFO:tensorflow:Restoring parameters from /home/ahmet/workspace/tensorboard/tissue_alexnet_b256_lr0005/model.ckpt-1877061
