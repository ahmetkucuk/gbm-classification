TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation2
in WT
TCGA-DU-7012annotation2
in WT
TCGA-DU-7012annotation2
in WT
TCGA-DU-7012annotation2
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in2017-07-11 16:32:56.555765: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 16:32:56.555804: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 16:32:56.555813: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 16:32:56.555819: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 16:32:56.555825: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 16:32:57.054430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.3285
pciBusID 0000:82:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2017-07-11 16:32:57.054492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-07-11 16:32:57.054508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-07-11 16:32:57.054532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:82:00.0)
 WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation3
in WT
TCGA-HT-A4DSannotation3
in WT
TCGA-HT-A4DSannotation3
in WT
Number of Patches: 2038
Global Iter: 100 training loss: 0.71009
Global Iter: 100 training acc: 0.6875
Global Iter: 200 training loss: 0.709344
Global Iter: 200 training acc: 0.34375
Global Iter: 300 training loss: 0.691284
Global Iter: 300 training acc: 0.53125
Global Iter: 400 training loss: 0.708036
Global Iter: 400 training acc: 0.34375
Global Iter: 500 training loss: 0.681331
Global Iter: 500 training acc: 0.59375
Global Iter: 600 training loss: 0.668504
Global Iter: 600 training acc: 0.625
Global Iter: 700 training loss: 0.70221
Global Iter: 700 training acc: 0.46875
Global Iter: 800 training loss: 0.691627
Global Iter: 800 training acc: 0.5
Global Iter: 900 training loss: 0.686304
Global Iter: 900 training acc: 0.5625
Global Iter: 1000 training loss: 0.663232
Global Iter: 1000 training acc: 0.6875
Global Iter: 1100 training loss: 0.691192
Global Iter: 1100 training acc: 0.46875
Global Iter: 1200 training loss: 0.695983
Global Iter: 1200 training acc: 0.5
Global Iter: 1300 training loss: 0.686658
Global Iter: 1300 training acc: 0.5625
Global Iter: 1400 training loss: 0.705007
Global Iter: 1400 training acc: 0.4375
Global Iter: 1500 training loss: 0.708767
Global Iter: 1500 training acc: 0.5
Global Iter: 1600 training loss: 0.697325
Global Iter: 1600 training acc: 0.4375
Global Iter: 1700 training loss: 0.69106
Global Iter: 1700 training acc: 0.5625
Global Iter: 1800 training loss: 0.714796
Global Iter: 1800 training acc: 0.46875
Global Iter: 1900 training loss: 0.697872
Global Iter: 1900 training acc: 0.53125
Global Iter: 2000 training loss: 0.67788
Global Iter: 2000 training acc: 0.625
Global Iter: 2100 training loss: 0.686535
Global Iter: 2100 training acc: 0.65625
Global Iter: 2200 training loss: 0.698451
Global Iter: 2200 training acc: 0.5
Global Iter: 2300 training loss: 0.69987
Global Iter: 2300 training acc: 0.5
Global Iter: 2400 training loss: 0.716614
Global Iter: 2400 training acc: 0.375
Global Iter: 2500 training loss: 0.688904
Global Iter: 2500 training acc: 0.625
Global Iter: 2600 training loss: 0.674165
Global Iter: 2600 training acc: 0.625
Global Iter: 2700 training loss: 0.717081
Global Iter: 2700 training acc: 0.40625
Global Iter: 2800 training loss: 0.685348
Global Iter: 2800 training acc: 0.59375
Global Iter: 2900 training loss: 0.690718
Global Iter: 2900 training acc: 0.53125
Global Iter: 3000 training loss: 0.689615
Global Iter: 3000 training acc: 0.53125
Global Iter: 3100 training loss: 0.708472
Global Iter: 3100 training acc: 0.4375
Global Iter: 3200 training loss: 0.683091
Global Iter: 3200 training acc: 0.5625
Global Iter: 3300 training loss: 0.688522
Global Iter: 3300 training acc: 0.5
Global Iter: 3400 training loss: 0.696807
Global Iter: 3400 training acc: 0.5
Global Iter: 3500 training loss: 0.715771
Global Iter: 3500 training acc: 0.375
Global Iter: 3600 training loss: 0.694448
Global Iter: 3600 training acc: 0.5
Global Iter: 3700 training loss: 0.693975
Global Iter: 3700 training acc: 0.53125
Global Iter: 3800 training loss: 0.698381
Global Iter: 3800 training acc: 0.4375
Global Iter: 3900 training loss: 0.68502
Global Iter: 3900 training acc: 0.59375
Global Iter: 4000 training loss: 0.697389
Global Iter: 4000 training acc: 0.4375
Global Iter: 4100 training loss: 0.709927
Global Iter: 4100 training acc: 0.4375
Global Iter: 4200 training loss: 0.68105
Global Iter: 4200 training acc: 0.59375
Global Iter: 4300 training loss: 0.6887
Global Iter: 4300 training acc: 0.5625
Global Iter: 4400 training loss: 0.714819
Global Iter: 4400 training acc: 0.40625
Global Iter: 4500 training loss: 0.690755
Global Iter: 4500 training acc: 0.53125
Global Iter: 4600 training loss: 0.671234
Global Iter: 4600 training acc: 0.71875
Global Iter: 4700 training loss: 0.692955
Global Iter: 4700 training acc: 0.53125
Global Iter: 4800 training loss: 0.686599
Global Iter: 4800 training acc: 0.53125
Global Iter: 4900 training loss: 0.690062
Global Iter: 4900 training acc: 0.5625
Global Iter: 5000 training loss: 0.707109
Global Iter: 5000 training acc: 0.4375
Global Iter: 5100 training loss: 0.69823
Global Iter: 5100 training acc: 0.5
Global Iter: 5200 training loss: 0.684251
Global Iter: 5200 training acc: 0.59375
Global Iter: 5300 training loss: 0.710907
Global Iter: 5300 training acc: 0.4375
Global Iter: 5400 training loss: 0.67104
Global Iter: 5400 training acc: 0.71875
Global Iter: 5500 training loss: 0.695329
Global Iter: 5500 training acc: 0.53125
Global Iter: 5600 training loss: 0.716815
Global Iter: 5600 training acc: 0.40625
Global Iter: 5700 training loss: 0.683738
Global Iter: 5700 training acc: 0.5625
Global Iter: 5800 training loss: 0.672725
Global Iter: 5800 training acc: 0.625
Global Iter: 5900 training loss: 0.686219
Global Iter: 5900 training acc: 0.59375
Global Iter: 6000 training loss: 0.697636
Global Iter: 6000 training acc: 0.5
Global Iter: 6100 training loss: 0.667423
Global Iter: 6100 training acc: 0.65625
Global Iter: 6200 training loss: 0.675177
Global Iter: 6200 training acc: 0.65625
Global Iter: 6300 training loss: 0.699231
Global Iter: 6300 training acc: 0.46875
Global Iter: 6400 training loss: 0.706702
Global Iter: 6400 training acc: 0.34375
Global Iter: 6500 training loss: 0.695224
Global Iter: 6500 training acc: 0.5
Global Iter: 6600 training loss: 0.676242
Global Iter: 6600 training acc: 0.625
Global Iter: 6700 training loss: 0.703993
Global Iter: 6700 training acc: 0.46875
Global Iter: 6800 training loss: 0.699537
Global Iter: 6800 training acc: 0.5
Global Iter: 6900 training loss: 0.690158
Global Iter: 6900 training acc: 0.5625
Global Iter: 7000 training loss: 0.678421
Global Iter: 7000 training acc: 0.59375
Global Iter: 7100 training loss: 0.701771
Global Iter: 7100 training acc: 0.4375
Global Iter: 7200 training loss: 0.671589
Global Iter: 7200 training acc: 0.65625
Global Iter: 7300 training loss: 0.689356
Global Iter: 7300 training acc: 0.53125
Global Iter: 7400 training loss: 0.69507
Global Iter: 7400 training acc: 0.53125
Global Iter: 7500 training loss: 0.66354
Global Iter: 7500 training acc: 0.71875
Global Iter: 7600 training loss: 0.684429
Global Iter: 7600 training acc: 0.625
Global Iter: 7700 training loss: 0.675484
Global Iter: 7700 training acc: 0.59375
Global Iter: 7800 training loss: 0.687817
Global Iter: 7800 training acc: 0.53125
Global Iter: 7900 training loss: 0.688578
Global Iter: 7900 training acc: 0.5
Global Iter: 8000 training loss: 0.695183
Global Iter: 8000 training acc: 0.5
Global Iter: 8100 training loss: 0.699471
Global Iter: 8100 training acc: 0.4375
Global Iter: 8200 training loss: 0.707113
Global Iter: 8200 training acc: 0.4375
Global Iter: 8300 training loss: 0.677773
Global Iter: 8300 training acc: 0.65625
Global Iter: 8400 training loss: 0.706754
Global Iter: 8400 training acc: 0.46875
Global Iter: 8500 training loss: 0.720533
Global Iter: 8500 training acc: 0.34375
Global Iter: 8600 training loss: 0.674122
Global Iter: 8600 training acc: 0.6875
Global Iter: 8700 training loss: 0.698817
Global Iter: 8700 training acc: 0.46875
Global Iter: 8800 training loss: 0.687171
Global Iter: 8800 training acc: 0.5625
Global Iter: 8900 training loss: 0.689798
Global Iter: 8900 training acc: 0.5625
Global Iter: 9000 training loss: 0.666504
Global Iter: 9000 training acc: 0.6875
Global Iter: 9100 training loss: 0.699177
Global Iter: 9100 training acc: 0.40625
Global Iter: 9200 training loss: 0.685543
Global Iter: 9200 training acc: 0.5625
Global Iter: 9300 training loss: 0.719782
Global Iter: 9300 training acc: 0.4375
Global Iter: 9400 training loss: 0.719714
Global Iter: 9400 training acc: 0.40625
Global Iter: 9500 training loss: 0.693447
Global Iter: 9500 training acc: 0.5
Global Iter: 9600 training loss: 0.689882
Global Iter: 9600 training acc: 0.59375
Global Iter: 9700 training loss: 0.690998
Global Iter: 9700 training acc: 0.53125
Global Iter: 9800 training loss: 0.689842
Global Iter: 9800 training acc: 0.53125
Global Iter: 9900 training loss: 0.679176
Global Iter: 9900 training acc: 0.625
Global Iter: 10000 training loss: 0.676773
Global Iter: 10000 training acc: 0.5625
Global Iter: 10100 training loss: 0.694894
Global Iter: 10100 training acc: 0.53125
Global Iter: 10200 training loss: 0.69717
Global Iter: 10200 training acc: 0.5
Global Iter: 10300 training loss: 0.676719
Global Iter: 10300 training acc: 0.625
Global Iter: 10400 training loss: 0.682801
Global Iter: 10400 training acc: 0.625
Global Iter: 10500 training loss: 0.67573
Global Iter: 10500 training acc: 0.65625
Global Iter: 10600 training loss: 0.712332
Global Iter: 10600 training acc: 0.375
Global Iter: 10700 training loss: 0.692018
Global Iter: 10700 training acc: 0.46875
Global Iter: 10800 training loss: 0.695905
Global Iter: 10800 training acc: 0.46875
Global Iter: 10900 training loss: 0.661952
Global Iter: 10900 training acc: 0.6875
Global Iter: 11000 training loss: 0.699193
Global Iter: 11000 training acc: 0.5
Global Iter: 11100 training loss: 0.695435
Global Iter: 11100 training acc: 0.5
Global Iter: 11200 training loss: 0.687218
Global Iter: 11200 training acc: 0.5625
Global Iter: 11300 training loss: 0.721411
Global Iter: 11300 training acc: 0.3125
Global Iter: 11400 training loss: 0.693516
Global Iter: 11400 training acc: 0.53125
Global Iter: 11500 training loss: 0.672787
Global Iter: 11500 training acc: 0.6875
Global Iter: 11600 training loss: 0.691471
Global Iter: 11600 training acc: 0.5625
Global Iter: 11700 training loss: 0.693459
Global Iter: 11700 training acc: 0.46875
Global Iter: 11800 training loss: 0.694516
Global Iter: 11800 training acc: 0.53125
Global Iter: 11900 training loss: 0.680051
Global Iter: 11900 training acc: 0.65625
Global Iter: 12000 training loss: 0.684416
Global Iter: 12000 training acc: 0.5625
Global Iter: 12100 training loss: 0.683016
Global Iter: 12100 training acc: 0.5625
Global Iter: 12200 training loss: 0.693406
Global Iter: 12200 training acc: 0.53125
Global Iter: 12300 training loss: 0.715595
Global Iter: 12300 training acc: 0.40625
Global Iter: 12400 training loss: 0.692883
Global Iter: 12400 training acc: 0.59375
Global Iter: 12500 training loss: 0.678518
Global Iter: 12500 training acc: 0.625
Global Iter: 12600 training loss: 0.699395
Global Iter: 12600 training acc: 0.46875
Global Iter: 12700 training loss: 0.684228
Global Iter: 12700 training acc: 0.625
Global Iter: 12800 training loss: 0.687099
Global Iter: 12800 training acc: 0.53125
Global Iter: 12900 training loss: 0.690392
Global Iter: 12900 training acc: 0.5
Global Iter: 13000 training loss: 0.701181
Global Iter: 13000 training acc: 0.46875
Global Iter: 13100 training loss: 0.690534
Global Iter: 13100 training acc: 0.53125
Global Iter: 13200 training loss: 0.681718
Global Iter: 13200 training acc: 0.625
Global Iter: 13300 training loss: 0.696303
Global Iter: 13300 training acc: 0.5
Global Iter: 13400 training loss: 0.697992
Global Iter: 13400 training acc: 0.4375
Global Iter: 13500 training loss: 0.705587
Global Iter: 13500 training acc: 0.46875
Global Iter: 13600 training loss: 0.695782
Global Iter: 13600 training acc: 0.5
Global Iter: 13700 training loss: 0.704463
Global Iter: 13700 training acc: 0.4375
Global Iter: 13800 training loss: 0.688915
Global Iter: 13800 training acc: 0.5625
Global Iter: 13900 training loss: 0.703915
Global Iter: 13900 training acc: 0.46875
Global Iter: 14000 training loss: 0.698866
Global Iter: 14000 training acc: 0.4375
Global Iter: 14100 training loss: 0.680174
Global Iter: 14100 training acc: 0.59375
Global Iter: 14200 training loss: 0.691977
Global Iter: 14200 training acc: 0.53125
Global Iter: 14300 training loss: 0.708068
Global Iter: 14300 training acc: 0.40625
Global Iter: 14400 training loss: 0.691616
Global Iter: 14400 training acc: 0.53125
Global Iter: 14500 training loss: 0.669178
Global Iter: 14500 training acc: 0.6875
Global Iter: 14600 training loss: 0.684837
Global Iter: 14600 training acc: 0.53125
Global Iter: 14700 training loss: 0.68888
Global Iter: 14700 training acc: 0.53125
Global Iter: 14800 training loss: 0.694794
Global Iter: 14800 training acc: 0.5
Global Iter: 14900 training loss: 0.702537
Global Iter: 14900 training acc: 0.40625
Global Iter: 15000 training loss: 0.697771
Global Iter: 15000 training acc: 0.5
Global Iter: 15100 training loss: 0.686021
Global Iter: 15100 training acc: 0.5625
Global Iter: 15200 training loss: 0.699867
Global Iter: 15200 training acc: 0.46875
Global Iter: 15300 training loss: 0.670509
Global Iter: 15300 training acc: 0.71875
Global Iter: 15400 training loss: 0.698053
Global Iter: 15400 training acc: 0.5
Global Iter: 15500 training loss: 0.695488
Global Iter: 15500 training acc: 0.46875
Global Iter: 15600 training loss: 0.67883
Global Iter: 15600 training acc: 0.65625
Global Iter: 15700 training loss: 0.670056
Global Iter: 15700 training acc: 0.59375
Global Iter: 15800 training loss: 0.694385
Global Iter: 15800 training acc: 0.53125
Global Iter: 15900 training loss: 0.690717
Global Iter: 15900 training acc: 0.46875
Global Iter: 16000 training loss: 0.66714
Global Iter: 16000 training acc: 0.625
Global Iter: 16100 training loss: 0.685178
Global Iter: 16100 training acc: 0.625
Global Iter: 16200 training loss: 0.695863
Global Iter: 16200 training acc: 0.5
Global Iter: 16300 training loss: 0.710858
Global Iter: 16300 training acc: 0.34375
Global Iter: 16400 training loss: 0.702679
Global Iter: 16400 training acc: 0.5
Global Iter: 16500 training loss: 0.685166
Global Iter: 16500 training acc: 0.625
Global Iter: 16600 training loss: 0.696262
Global Iter: 16600 training acc: 0.46875
Global Iter: 16700 training loss: 0.691429
Global Iter: 16700 training acc: 0.5
Global Iter: 16800 training loss: 0.691424
Global Iter: 16800 training acc: 0.5625
Global Iter: 16900 training loss: 0.678281
Global Iter: 16900 training acc: 0.625
Global Iter: 17000 training loss: 0.702214
Global Iter: 17000 training acc: 0.4375
Global Iter: 17100 training loss: 0.681179
Global Iter: 17100 training acc: 0.625
Global Iter: 17200 training loss: 0.688327
Global Iter: 17200 training acc: 0.5625
Global Iter: 17300 training loss: 0.679802
Global Iter: 17300 training acc: 0.5625
Global Iter: 17400 training loss: 0.666449
Global Iter: 17400 training acc: 0.6875
Global Iter: 17500 training loss: 0.685093
Global Iter: 17500 training acc: 0.59375
Global Iter: 17600 training loss: 0.677083
Global Iter: 17600 training acc: 0.59375
Global Iter: 17700 training loss: 0.689668
Global Iter: 17700 training acc: 0.53125
Global Iter: 17800 training loss: 0.701218
Global Iter: 17800 training acc: 0.4375
Global Iter: 17900 training loss: 0.693267
Global Iter: 17900 training acc: 0.53125
Global Iter: 18000 training loss: 0.699442
Global Iter: 18000 training acc: 0.4375
Global Iter: 18100 training loss: 0.713157
Global Iter: 18100 training acc: 0.375
Global Iter: 18200 training loss: 0.67522
Global Iter: 18200 training acc: 0.65625
Global Iter: 18300 training loss: 0.698347
Global Iter: 18300 training acc: 0.46875
Global Iter: 18400 training loss: 0.717619
Global Iter: 18400 training acc: 0.375
Global Iter: 18500 training loss: 0.679614
Global Iter: 18500 training acc: 0.71875
Global Iter: 18600 training loss: 0.700206
Global Iter: 18600 training acc: 0.4375
Global Iter: 18700 training loss: 0.682717
Global Iter: 18700 training acc: 0.5625
Global Iter: 18800 training loss: 0.688983
Global Iter: 18800 training acc: 0.5625
Global Iter: 18900 training loss: 0.673611
Global Iter: 18900 training acc: 0.6875
Global Iter: 19000 training loss: 0.692897
Global Iter: 19000 training acc: 0.46875
Global Iter: 19100 training loss: 0.690006
Global Iter: 19100 training acc: 0.5
Global Iter: 19200 training loss: 0.704735
Global Iter: 19200 training acc: 0.4375
Global Iter: 19300 training loss: 0.707823
Global Iter: 19300 training acc: 0.40625
Global Iter: 19400 training loss: 0.689619
Global Iter: 19400 training acc: 0.5
Global Iter: 19500 training loss: 0.678694
Global Iter: 19500 training acc: 0.625
Global Iter: 19600 training loss: 0.696247
Global Iter: 19600 training acc: 0.5
Global Iter: 19700 training loss: 0.746775
Global Iter: 19700 training acc: 0.5
Global Iter: 19800 training loss: 0.685039
Global Iter: 19800 training acc: 0.59375
Global Iter: 19900 training loss: 0.691991
Global Iter: 19900 training acc: 0.53125
Global Iter: 20000 training loss: 0.685839
Global Iter: 20000 training acc: 0.5625
Global Iter: 20100 training loss: 0.695931
Global Iter: 20100 training acc: 0.5
Global Iter: 20200 training loss: 0.678131
Global Iter: 20200 training acc: 0.6875
Global Iter: 20300 training loss: 0.669813
Global Iter: 20300 training acc: 0.65625
Global Iter: 20400 training loss: 0.674622
Global Iter: 20400 training acc: 0.65625
Global Iter: 20500 training loss: 0.722515
Global Iter: 20500 training acc: 0.34375
Global Iter: 20600 training loss: 0.697144
Global Iter: 20600 training acc: 0.53125
Global Iter: 20700 training loss: 0.692579
Global Iter: 20700 training acc: 0.46875
Global Iter: 20800 training loss: 0.676643
Global Iter: 20800 training acc: 0.625
Global Iter: 20900 training loss: 0.700018
Global Iter: 20900 training acc: 0.46875
Global Iter: 21000 training loss: 0.697961
Global Iter: 21000 training acc: 0.46875
Global Iter: 21100 training loss: 0.691871
Global Iter: 21100 training acc: 0.53125
Global Iter: 21200 training loss: 0.717262
Global Iter: 21200 training acc: 0.28125
Global Iter: 21300 training loss: 0.693438
Global Iter: 21300 training acc: 0.5
Global Iter: 21400 training loss: 0.691911
Global Iter: 21400 training acc: 0.53125
Global Iter: 21500 training loss: 0.68003
Global Iter: 21500 training acc: 0.625
Global Iter: 21600 training loss: 0.70101
Global Iter: 21600 training acc: 0.4375
Global Iter: 21700 training loss: 0.688887
Global Iter: 21700 training acc: 0.5625
Global Iter: 21800 training loss: 0.679758
Global Iter: 21800 training acc: 0.625
Global Iter: 21900 training loss: 0.699734
Global Iter: 21900 training acc: 0.53125
Global Iter: 22000 training loss: 0.691363
Global Iter: 22000 training acc: 0.53125
Global Iter: 22100 training loss: 0.69472
Global Iter: 22100 training acc: 0.5
Global Iter: 22200 training loss: 0.718076
Global Iter: 22200 training acc: 0.40625
Global Iter: 22300 training loss: 0.685782
Global Iter: 22300 training acc: 0.5625
Global Iter: 22400 training loss: 0.685874
Global Iter: 22400 training acc: 0.59375
Global Iter: 22500 training loss: 0.694214
Global Iter: 22500 training acc: 0.53125
Global Iter: 22600 training loss: 0.679148
Global Iter: 22600 training acc: 0.65625
Global Iter: 22700 training loss: 0.693054
Global Iter: 22700 training acc: 0.5
Global Iter: 22800 training loss: 0.687874
Global Iter: 22800 training acc: 0.53125
Global Iter: 22900 training loss: 0.690165
Global Iter: 22900 training acc: 0.53125
Global Iter: 23000 training loss: 0.698212
Global Iter: 23000 training acc: 0.5
Global Iter: 23100 training loss: 0.683382
Global Iter: 23100 training acc: 0.59375
Global Iter: 23200 training loss: 0.693912
Global Iter: 23200 training acc: 0.5
Global Iter: 23300 training loss: 0.711914
Global Iter: 23300 training acc: 0.375
Global Iter: 23400 training loss: 0.696599
Global Iter: 23400 training acc: 0.46875
Global Iter: 23500 training loss: 0.69864
Global Iter: 23500 training acc: 0.5
Global Iter: 23600 training loss: 0.708177
Global Iter: 23600 training acc: 0.4375
Global Iter: 23700 training loss: 0.682243
Global Iter: 23700 training acc: 0.59375
Global Iter: 23800 training loss: 0.698068
Global Iter: 23800 training acc: 0.46875
Global Iter: 23900 training loss: 0.696564
Global Iter: 23900 training acc: 0.46875
Global Iter: 24000 training loss: 0.680927
Global Iter: 24000 training acc: 0.59375
Global Iter: 24100 training loss: 0.69943
Global Iter: 24100 training acc: 0.5
Global Iter: 24200 training loss: 0.703661
Global Iter: 24200 training acc: 0.4375
Global Iter: 24300 training loss: 0.686567
Global Iter: 24300 training acc: 0.5625
Global Iter: 24400 training loss: 0.671944
Global Iter: 24400 training acc: 0.6875
Global Iter: 24500 training loss: 0.698488
Global Iter: 24500 training acc: 0.46875
Global Iter: 24600 training loss: 0.701582
Global Iter: 24600 training acc: 0.5
Global Iter: 24700 training loss: 0.697387
Global Iter: 24700 training acc: 0.5
Global Iter: 24800 training loss: 0.700309
Global Iter: 24800 training acc: 0.40625
Global Iter: 24900 training loss: 0.69059
Global Iter: 24900 training acc: 0.5
Global Iter: 25000 training loss: 0.682388
Global Iter: 25000 training acc: 0.59375
Global Iter: 25100 training loss: 0.703444
Global Iter: 25100 training acc: 0.46875
Global Iter: 25200 training loss: 0.674715
Global Iter: 25200 training acc: 0.6875
Global Iter: 25300 training loss: 0.693735
Global Iter: 25300 training acc: 0.5
Global Iter: 25400 training loss: 0.705178
Global Iter: 25400 training acc: 0.46875
Global Iter: 25500 training loss: 0.675115
Global Iter: 25500 training acc: 0.65625
Global Iter: 25600 training loss: 0.684155
Global Iter: 25600 training acc: 0.59375
Global Iter: 25700 training loss: 0.69033
Global Iter: 25700 training acc: 0.53125
Global Iter: 25800 training loss: 0.689859
Global Iter: 25800 training acc: 0.53125
Global Iter: 25900 training loss: 0.671775
Global Iter: 25900 training acc: 0.6875
Global Iter: 26000 training loss: 0.689096
Global Iter: 26000 training acc: 0.5625
Global Iter: 26100 training loss: 0.692135
Global Iter: 26100 training acc: 0.5
Global Iter: 26200 training loss: 0.71432
Global Iter: 26200 training acc: 0.34375
Global Iter: 26300 training loss: 0.686395
Global Iter: 26300 training acc: 0.53125
Global Iter: 26400 training loss: 0.678598
Global Iter: 26400 training acc: 0.625
Global Iter: 26500 training loss: 0.698708
Global Iter: 26500 training acc: 0.46875
Global Iter: 26600 training loss: 0.69058
Global Iter: 26600 training acc: 0.5
Global Iter: 26700 training loss: 0.691031
Global Iter: 26700 training acc: 0.53125
Global Iter: 26800 training loss: 0.679314
Global Iter: 26800 training acc: 0.625
Global Iter: 26900 training loss: 0.699486
Global Iter: 26900 training acc: 0.46875
Global Iter: 27000 training loss: 0.682066
Global Iter: 27000 training acc: 0.59375
Global Iter: 27100 training loss: 0.695328
Global Iter: 27100 training acc: 0.53125
Global Iter: 27200 training loss: 0.68121
Global Iter: 27200 training acc: 0.59375
Global Iter: 27300 training loss: 0.672812
Global Iter: 27300 training acc: 0.6875
Global Iter: 27400 training loss: 0.687883
Global Iter: 27400 training acc: 0.53125
Global Iter: 27500 training loss: 0.683369
Global Iter: 27500 training acc: 0.59375
Global Iter: 27600 training loss: 0.696967
Global Iter: 27600 training acc: 0.5
Global Iter: 27700 training loss: 0.703704
Global Iter: 27700 training acc: 0.4375
Global Iter: 27800 training loss: 0.688344
Global Iter: 27800 training acc: 0.53125
Global Iter: 27900 training loss: 0.698746
Global Iter: 27900 training acc: 0.46875
Global Iter: 28000 training loss: 0.709997
Global Iter: 28000 training acc: 0.375
Global Iter: 28100 training loss: 0.670365
Global Iter: 28100 training acc: 0.6875
Global Iter: 28200 training loss: 0.70448
Global Iter: 28200 training acc: 0.46875
Global Iter: 28300 training loss: 0.707506
Global Iter: 28300 training acc: 0.375
Global Iter: 28400 training loss: 0.685804
Global Iter: 28400 training acc: 0.625
Global Iter: 28500 training loss: 0.69828
Global Iter: 28500 training acc: 0.5
Global Iter: 28600 training loss: 0.683456
Global Iter: 28600 training acc: 0.5625
Global Iter: 28700 training loss: 0.68567
Global Iter: 28700 training acc: 0.5625
Global Iter: 28800 training loss: 0.672489
Global Iter: 28800 training acc: 0.6875
Global Iter: 28900 training loss: 0.70071
Global Iter: 28900 training acc: 0.46875
Global Iter: 29000 training loss: 0.693426
Global Iter: 29000 training acc: 0.53125
Global Iter: 29100 training loss: 0.703491
Global Iter: 29100 training acc: 0.4375
Global Iter: 29200 training loss: 0.699025
Global Iter: 29200 training acc: 0.46875
Global Iter: 29300 training loss: 0.694177
Global Iter: 29300 training acc: 0.53125
Global Iter: 29400 training loss: 0.68538
Global Iter: 29400 training acc: 0.5625
Global Iter: 29500 training loss: 0.69326
Global Iter: 29500 training acc: 0.5
Global Iter: 29600 training loss: 0.701052
Global Iter: 29600 training acc: 0.4375
Global Iter: 29700 training loss: 0.676084
Global Iter: 29700 training acc: 0.625
Global Iter: 29800 training loss: 0.690836
Global Iter: 29800 training acc: 0.5625
Global Iter: 29900 training loss: 0.681085
Global Iter: 29900 training acc: 0.5625
Global Iter: 30000 training loss: 0.681604
Global Iter: 30000 training acc: 0.5625
Global Iter: 30100 training loss: 0.667633
Global Iter: 30100 training acc: 0.71875
Global Iter: 30200 training loss: 0.677124
Global Iter: 30200 training acc: 0.65625
Global Iter: 30300 training loss: 0.681507
Global Iter: 30300 training acc: 0.625
Global Iter: 30400 training loss: 0.72166
Global Iter: 30400 training acc: 0.375
Global Iter: 30500 training loss: 0.694729
Global Iter: 30500 training acc: 0.5
Global Iter: 30600 training loss: 0.694308
Global Iter: 30600 training acc: 0.5
Global Iter: 30700 training loss: 0.678511
Global Iter: 30700 training acc: 0.65625
Global Iter: 30800 training loss: 0.703544
Global Iter: 30800 training acc: 0.4375
Global Iter: 30900 training loss: 0.701481
Global Iter: 30900 training acc: 0.4375
Global Iter: 31000 training loss: 0.697357
Global Iter: 31000 training acc: 0.46875
Global Iter: 31100 training loss: 0.715577
Global Iter: 31100 training acc: 0.3125
Global Iter: 31200 training loss: 0.695021
Global Iter: 31200 training acc: 0.5
Global Iter: 31300 training loss: 0.669383
Global Iter: 31300 training acc: 0.6875
Global Iter: 31400 training loss: 0.687661
Global Iter: 31400 training acc: 0.59375
Global Iter: 31500 training loss: 0.699534
Global Iter: 31500 training acc: 0.5
Global Iter: 31600 training loss: 0.684469
Global Iter: 31600 training acc: 0.5625
Global Iter: 31700 training loss: 0.667144
Global Iter: 31700 training acc: 0.65625
Global Iter: 31800 training loss: 0.697362
Global Iter: 31800 training acc: 0.5
Global Iter: 31900 training loss: 0.690968
Global Iter: 31900 training acc: 0.5
Global Iter: 32000 training loss: 0.69492
Global Iter: 32000 training acc: 0.53125
Global Iter: 32100 training loss: 0.709592
Global Iter: 32100 training acc: 0.4375
Global Iter: 32200 training loss: 0.693955
Global Iter: 32200 training acc: 0.5
Global Iter: 32300 training loss: 0.676172
Global Iter: 32300 training acc: 0.625
Global Iter: 32400 training loss: 0.699847
Global Iter: 32400 training acc: 0.5
Global Iter: 32500 training loss: 0.680063
Global Iter: 32500 training acc: 0.625
Global Iter: 32600 training loss: 0.696931
Global Iter: 32600 training acc: 0.5
Global Iter: 32700 training loss: 0.69578
Global Iter: 32700 training acc: 0.5
Global Iter: 32800 training loss: 0.696997
Global Iter: 32800 training acc: 0.5
Global Iter: 32900 training loss: 0.703398
Global Iter: 32900 training acc: 0.4375
Global Iter: 33000 training loss: 0.691976
Global Iter: 33000 training acc: 0.53125
Global Iter: 33100 training loss: 0.688523
Global Iter: 33100 training acc: 0.53125
Global Iter: 33200 training loss: 0.707586
Global Iter: 33200 training acc: 0.40625
Global Iter: 33300 training loss: 0.711084
Global Iter: 33300 training acc: 0.40625
Global Iter: 33400 training loss: 0.704216
Global Iter: 33400 training acc: 0.4375
Global Iter: 33500 training loss: 0.703488
Global Iter: 33500 training acc: 0.4375
Global Iter: 33600 training loss: 0.679185
Global Iter: 33600 training acc: 0.625
Global Iter: 33700 training loss: 0.698738
Global Iter: 33700 training acc: 0.4375
Global Iter: 33800 training loss: 0.706759
Global Iter: 33800 training acc: 0.4375
Global Iter: 33900 training loss: 0.675439
Global Iter: 33900 training acc: 0.625
Global Iter: 34000 training loss: 0.704212
Global Iter: 34000 training acc: 0.4375
Global Iter: 34100 training loss: 0.709865
Global Iter: 34100 training acc: 0.4375
Global Iter: 34200 training loss: 0.695188
Global Iter: 34200 training acc: 0.5
Global Iter: 34300 training loss: 0.676524
Global Iter: 34300 training acc: 0.65625
Global Iter: 34400 training loss: 0.696888
Global Iter: 34400 training acc: 0.5
Global Iter: 34500 training loss: 0.695973
Global Iter: 34500 training acc: 0.5
Global Iter: 34600 training loss: 0.694249
Global Iter: 34600 training acc: 0.5
Global Iter: 34700 training loss: 0.706608
Global Iter: 34700 training acc: 0.4375
Global Iter: 34800 training loss: 0.700102
Global Iter: 34800 training acc: 0.46875
Global Iter: 34900 training loss: 0.683716
Global Iter: 34900 training acc: 0.59375
Global Iter: 35000 training loss: 0.692715
Global Iter: 35000 training acc: 0.5
Global Iter: 35100 training loss: 0.684595
Global Iter: 35100 training acc: 0.65625
Global Iter: 35200 training loss: 0.692288
Global Iter: 35200 training acc: 0.53125
Global Iter: 35300 training loss: 0.700065
Global Iter: 35300 training acc: 0.4375
Global Iter: 35400 training loss: 0.67848
Global Iter: 35400 training acc: 0.65625
Global Iter: 35500 training loss: 0.689755
Global Iter: 35500 training acc: 0.5625
Global Iter: 35600 training loss: 0.687059
Global Iter: 35600 training acc: 0.59375
Global Iter: 35700 training loss: 0.691822
Global Iter: 35700 training acc: 0.53125
Global Iter: 35800 training loss: 0.677646
Global Iter: 35800 training acc: 0.625
Global Iter: 35900 training loss: 0.684343
Global Iter: 35900 training acc: 0.59375
Global Iter: 36000 training loss: 0.695348
Global Iter: 36000 training acc: 0.5
Global Iter: 36100 training loss: 0.715159
Global Iter: 36100 training acc: 0.34375
Global Iter: 36200 training loss: 0.692039
Global Iter: 36200 training acc: 0.53125
Global Iter: 36300 training loss: 0.681633
Global Iter: 36300 training acc: 0.59375
Global Iter: 36400 training loss: 0.698632
Global Iter: 36400 training acc: 0.46875
Global Iter: 36500 training loss: 0.687378
Global Iter: 36500 training acc: 0.5625
Global Iter: 36600 training loss: 0.696674
Global Iter: 36600 training acc: 0.46875
Global Iter: 36700 training loss: 0.673008
Global Iter: 36700 training acc: 0.625
Global Iter: 36800 training loss: 0.695368
Global Iter: 36800 training acc: 0.46875
Global Iter: 36900 training loss: 0.679701
Global Iter: 36900 training acc: 0.625
Global Iter: 37000 training loss: 0.70255
Global Iter: 37000 training acc: 0.46875
Global Iter: 37100 training loss: 0.687663
Global Iter: 37100 training acc: 0.5625
Global Iter: 37200 training loss: 0.665979
Global Iter: 37200 training acc: 0.65625
Global Iter: 37300 training loss: 0.689979
Global Iter: 37300 training acc: 0.53125
Global Iter: 37400 training loss: 0.687186
Global Iter: 37400 training acc: 0.5625
Global Iter: 37500 training loss: 0.70274
Global Iter: 37500 training acc: 0.46875
Global Iter: 37600 training loss: 0.702712
Global Iter: 37600 training acc: 0.4375
Global Iter: 37700 training loss: 0.695925
Global Iter: 37700 training acc: 0.53125
Global Iter: 37800 training loss: 0.692649
Global Iter: 37800 training acc: 0.5
Global Iter: 37900 training loss: 0.703561
Global Iter: 37900 training acc: 0.4375
Global Iter: 38000 training loss: 0.674844
Global Iter: 38000 training acc: 0.6875
Global Iter: 38100 training loss: 0.700175
Global Iter: 38100 training acc: 0.4375
Global Iter: 38200 training loss: 0.712962
Global Iter: 38200 training acc: 0.375
Global Iter: 38300 training loss: 0.672921
Global Iter: 38300 training acc: 0.71875
Global Iter: 38400 training loss: 0.696953
Global Iter: 38400 training acc: 0.5
Global Iter: 38500 training loss: 0.69415
Global Iter: 38500 training acc: 0.5625
Global Iter: 38600 training loss: 0.683033
Global Iter: 38600 training acc: 0.59375
Global Iter: 38700 training loss: 0.674448
Global Iter: 38700 training acc: 0.65625
Global Iter: 38800 training loss: 0.68764
Global Iter: 38800 training acc: 0.625
Global Iter: 38900 training loss: 0.695283
Global Iter: 38900 training acc: 0.53125
Global Iter: 39000 training loss: 0.690332
Global Iter: 39000 training acc: 0.46875
Global Iter: 39100 training loss: 0.700277
Global Iter: 39100 training acc: 0.46875
Global Iter: 39200 training loss: 0.689676
Global Iter: 39200 training acc: 0.5625
Global Iter: 39300 training loss: 0.69073
Global Iter: 39300 training acc: 0.5625
Global Iter: 39400 training loss: 0.684975
Global Iter: 39400 training acc: 0.5625
Global Iter: 39500 training loss: 0.696541
Global Iter: 39500 training acc: 0.46875
Global Iter: 39600 training loss: 0.674557
Global Iter: 39600 training acc: 0.625
Global Iter: 39700 training loss: 0.688011
Global Iter: 39700 training acc: 0.5625
Global Iter: 39800 training loss: 0.684532
Global Iter: 39800 training acc: 0.59375
Global Iter: 39900 training loss: 0.692885
Global Iter: 39900 training acc: 0.53125
Global Iter: 40000 training loss: 0.66984
Global Iter: 40000 training acc: 0.71875
Global Iter: 40100 training loss: 0.680288
Global Iter: 40100 training acc: 0.65625
Global Iter: 40200 training loss: 0.682061
Global Iter: 40200 training acc: 0.625
Global Iter: 40300 training loss: 0.705286
Global Iter: 40300 training acc: 0.40625
Global Iter: 40400 training loss: 0.699074
Global Iter: 40400 training acc: 0.5
Global Iter: 40500 training loss: 0.696189
Global Iter: 40500 training acc: 0.5
Global Iter: 40600 training loss: 0.674088
Global Iter: 40600 training acc: 0.65625
Global Iter: 40700 training loss: 0.69024
Global Iter: 40700 training acc: 0.5
Global Iter: 40800 training loss: 0.698081
Global Iter: 40800 training acc: 0.46875
Global Iter: 40900 training loss: 0.688504
Global Iter: 40900 training acc: 0.5625
Global Iter: 41000 training loss: 0.718429
Global Iter: 41000 training acc: 0.3125
Global Iter: 41100 training loss: 0.704321
Global Iter: 41100 training acc: 0.4375
Global Iter: 41200 training loss: 0.667218
Global Iter: 41200 training acc: 0.75
Global Iter: 41300 training loss: 0.690264
Global Iter: 41300 training acc: 0.53125
Global Iter: 41400 training loss: 0.687492
Global Iter: 41400 training acc: 0.5625
Global Iter: 41500 training loss: 0.690655
Global Iter: 41500 training acc: 0.53125
Global Iter: 41600 training loss: 0.672929
Global Iter: 41600 training acc: 0.65625
Global Iter: 41700 training loss: 0.692798
Global Iter: 41700 training acc: 0.46875
Global Iter: 41800 training loss: 0.694094
Global Iter: 41800 training acc: 0.53125
Global Iter: 41900 training loss: 0.690933
Global Iter: 41900 training acc: 0.5
Global Iter: 42000 training loss: 0.703721
Global Iter: 42000 training acc: 0.4375
Global Iter: 42100 training loss: 0.702171
Global Iter: 42100 training acc: 0.5
Global Iter: 42200 training loss: 0.688254
Global Iter: 42200 training acc: 0.59375
Global Iter: 42300 training loss: 0.694778
Global Iter: 42300 training acc: 0.53125
Global Iter: 42400 training loss: 0.687193
Global Iter: 42400 training acc: 0.5625
Global Iter: 42500 training loss: 0.696161
Global Iter: 42500 training acc: 0.5
Global Iter: 42600 training loss: 0.694413
Global Iter: 42600 training acc: 0.5
Global Iter: 42700 training loss: 0.698523
Global Iter: 42700 training acc: 0.5
Global Iter: 42800 training loss: 0.697222
Global Iter: 42800 training acc: 0.46875
Global Iter: 42900 training loss: 0.687457
Global Iter: 42900 training acc: 0.5625
Global Iter: 43000 training loss: 0.689653
Global Iter: 43000 training acc: 0.53125
Global Iter: 43100 training loss: 0.706448
Global Iter: 43100 training acc: 0.40625
Global Iter: 43200 training loss: 0.713939
Global Iter: 43200 training acc: 0.375
Global Iter: 43300 training loss: 0.704672
Global Iter: 43300 training acc: 0.4375
Global Iter: 43400 training loss: 0.693951
Global Iter: 43400 training acc: 0.46875
Global Iter: 43500 training loss: 0.672934
Global Iter: 43500 training acc: 0.65625
Global Iter: 43600 training loss: 0.699259
Global Iter: 43600 training acc: 0.5
Global Iter: 43700 training loss: 0.701057
Global Iter: 43700 training acc: 0.46875
Global Iter: 43800 training loss: 0.681217
Global Iter: 43800 training acc: 0.5625
Global Iter: 43900 training loss: 0.70612
Global Iter: 43900 training acc: 0.40625
Global Iter: 44000 training loss: 0.697779
Global Iter: 44000 training acc: 0.46875
Global Iter: 44100 training loss: 0.691614
Global Iter: 44100 training acc: 0.53125
Global Iter: 44200 training loss: 0.681353
Global Iter: 44200 training acc: 0.65625
Global Iter: 44300 training loss: 0.694905
Global Iter: 44300 training acc: 0.5
Global Iter: 44400 training loss: 0.687255
Global Iter: 44400 training acc: 0.53125
Global Iter: 44500 training loss: 0.690199
Global Iter: 44500 training acc: 0.53125
Global Iter: 44600 training loss: 0.69882
Global Iter: 44600 training acc: 0.46875
Global Iter: 44700 training loss: 0.695197
Global Iter: 44700 training acc: 0.5
Global Iter: 44800 training loss: 0.688377
Global Iter: 44800 training acc: 0.5625
Global Iter: 44900 training loss: 0.69568
Global Iter: 44900 training acc: 0.5
Global Iter: 45000 training loss: 0.679744
Global Iter: 45000 training acc: 0.625
Global Iter: 45100 training loss: 0.69341
Global Iter: 45100 training acc: 0.5
Global Iter: 45200 training loss: 0.697567
Global Iter: 45200 training acc: 0.4375
Global Iter: 45300 training loss: 0.677762
Global Iter: 45300 training acc: 0.65625
Global Iter: 45400 training loss: 0.695417
Global Iter: 45400 training acc: 0.53125
Global Iter: 45500 training loss: 0.692272
Global Iter: 45500 training acc: 0.53125
Global Iter: 45600 training loss: 0.69532
Global Iter: 45600 training acc: 0.5
Global Iter: 45700 training loss: 0.682508
Global Iter: 45700 training acc: 0.625
Global Iter: 45800 training loss: 0.680329
Global Iter: 45800 training acc: 0.59375
Global Iter: 45900 training loss: 0.696252
Global Iter: 45900 training acc: 0.46875
Global Iter: 46000 training loss: 0.715756
Global Iter: 46000 training acc: 0.3125
Global Iter: 46100 training loss: 0.687985
Global Iter: 46100 training acc: 0.53125
Global Iter: 46200 training loss: 0.69446
Global Iter: 46200 training acc: 0.53125
Global Iter: 46300 training loss: 0.709911
Global Iter: 46300 training acc: 0.40625
Global Iter: 46400 training loss: 0.694424
Global Iter: 46400 training acc: 0.53125
Global Iter: 46500 training loss: 0.695295
Global Iter: 46500 training acc: 0.46875
Global Iter: 46600 training loss: 0.682631
Global Iter: 46600 training acc: 0.59375
Global Iter: 46700 training loss: 0.699488
Global Iter: 46700 training acc: 0.46875
Global Iter: 46800 training loss: 0.681718
Global Iter: 46800 training acc: 0.59375
Global Iter: 46900 training loss: 0.705346
Global Iter: 46900 training acc: 0.4375
Global Iter: 47000 training loss: 0.689829
Global Iter: 47000 training acc: 0.5625
Global Iter: 47100 training loss: 0.672939
Global Iter: 47100 training acc: 0.6875
Global Iter: 47200 training loss: 0.686861
Global Iter: 47200 training acc: 0.53125
Global Iter: 47300 training loss: 0.68865
Global Iter: 47300 training acc: 0.53125
Global Iter: 47400 training loss: 0.693062
Global Iter: 47400 training acc: 0.5
Global Iter: 47500 training loss: 0.703018
Global Iter: 47500 training acc: 0.4375
Global Iter: 47600 training loss: 0.704739
Global Iter: 47600 training acc: 0.46875
Global Iter: 47700 training loss: 0.693191
Global Iter: 47700 training acc: 0.53125
Global Iter: 47800 training loss: 0.690619
Global Iter: 47800 training acc: 0.5
Global Iter: 47900 training loss: 0.677255
Global Iter: 47900 training acc: 0.65625
Global Iter: 48000 training loss: 0.705186
Global Iter: 48000 training acc: 0.40625
Global Iter: 48100 training loss: 0.710555
Global Iter: 48100 training acc: 0.375
Global Iter: 48200 training loss: 0.67395
Global Iter: 48200 training acc: 0.6875
Global Iter: 48300 training loss: 0.693272
Global Iter: 48300 training acc: 0.5
Global Iter: 48400 training loss: 0.690059
Global Iter: 48400 training acc: 0.5625
Global Iter: 48500 training loss: 0.682584
Global Iter: 48500 training acc: 0.59375
Global Iter: 48600 training loss: 0.675676
Global Iter: 48600 training acc: 0.65625
Global Iter: 48700 training loss: 0.700976
Global Iter: 48700 training acc: 0.4375
Global Iter: 48800 training loss: 0.693946
Global Iter: 48800 training acc: 0.5
Global Iter: 48900 training loss: 0.701695
Global Iter: 48900 training acc: 0.46875
Global Iter: 49000 training loss: 0.696456
Global Iter: 49000 training acc: 0.46875
Global Iter: 49100 training loss: 0.692312
Global Iter: 49100 training acc: 0.53125
Global Iter: 49200 training loss: 0.692927
Global Iter: 49200 training acc: 0.5
Global Iter: 49300 training loss: 0.682645
Global Iter: 49300 training acc: 0.59375
Global Iter: 49400 training loss: 0.694962
Global Iter: 49400 training acc: 0.5
Global Iter: 49500 training loss: 0.676256
Global Iter: 49500 training acc: 0.65625
Global Iter: 49600 training loss: 0.692733
Global Iter: 49600 training acc: 0.53125
Global Iter: 49700 training loss: 0.681975
Global Iter: 49700 training acc: 0.625
Global Iter: 49800 training loss: 0.687961
Global Iter: 49800 training acc: 0.53125
Global Iter: 49900 training loss: 0.67491
Global Iter: 49900 training acc: 0.65625
Global Iter: 50000 training loss: 0.685944
Global Iter: 50000 training acc: 0.625
Global Iter: 50100 training loss: 0.675851
Global Iter: 50100 training acc: 0.65625
Global Iter: 50200 training loss: 0.706244
Global Iter: 50200 training acc: 0.4375
Global Iter: 50300 training loss: 0.686192
Global Iter: 50300 training acc: 0.53125
Global Iter: 50400 training loss: 0.687074
Global Iter: 50400 training acc: 0.5625
Global Iter: 50500 training loss: 0.675496
Global Iter: 50500 training acc: 0.65625
Global Iter: 50600 training loss: 0.690091
Global Iter: 50600 training acc: 0.53125
Global Iter: 50700 training loss: 0.69676
Global Iter: 50700 training acc: 0.46875
Global Iter: 50800 training loss: 0.696705
Global Iter: 50800 training acc: 0.5
Global Iter: 50900 training loss: 0.723201
Global Iter: 50900 training acc: 0.3125
Global Iter: 51000 training loss: 0.701353
Global Iter: 51000 training acc: 0.4375
Global Iter: 51100 training loss: 0.668756
Global Iter: 51100 training acc: 0.75
Global Iter: 51200 training loss: 0.697977
Global Iter: 51200 training acc: 0.5
Global Iter: 51300 training loss: 0.693315
Global Iter: 51300 training acc: 0.53125
Global Iter: 51400 training loss: 0.692097
Global Iter: 51400 training acc: 0.53125
Global Iter: 51500 training loss: 0.675426
Global Iter: 51500 training acc: 0.6875
Global Iter: 51600 training loss: 0.704246
Global Iter: 51600 training acc: 0.4375
Global Iter: 51700 training loss: 0.684457
Global Iter: 51700 training acc: 0.5625
Global Iter: 51800 training loss: 0.695785
Global Iter: 51800 training acc: 0.5
Global Iter: 51900 training loss: 0.701123
Global Iter: 51900 training acc: 0.46875
Global Iter: 52000 training loss: 0.69672
Global Iter: 52000 training acc: 0.5
Global Iter: 52100 training loss: 0.687934
Global Iter: 52100 training acc: 0.53125
Global Iter: 52200 training loss: 0.688893
Global Iter: 52200 training acc: 0.53125
Global Iter: 52300 training loss: 0.683425
Global Iter: 52300 training acc: 0.59375
Global Iter: 52400 training loss: 0.691845
Global Iter: 52400 training acc: 0.53125
Global Iter: 52500 training loss: 0.693911
Global Iter: 52500 training acc: 0.5
Global Iter: 52600 training loss: 0.692033
Global Iter: 52600 training acc: 0.5
Global Iter: 52700 training loss: 0.6954
Global Iter: 52700 training acc: 0.5
Global Iter: 52800 training loss: 0.690286
Global Iter: 52800 training acc: 0.5625
Global Iter: 52900 training loss: 0.689224
Global Iter: 52900 training acc: 0.53125
Global Iter: 53000 training loss: 0.701605
Global Iter: 53000 training acc: 0.4375
Global Iter: 53100 training loss: 0.704953
Global Iter: 53100 training acc: 0.40625
Global Iter: 53200 training loss: 0.697485
Global Iter: 53200 training acc: 0.5
Global Iter: 53300 training loss: 0.69923
Global Iter: 53300 training acc: 0.5
Global Iter: 53400 training loss: 0.674389
Global Iter: 53400 training acc: 0.6875
Global Iter: 53500 training loss: 0.691564
Global Iter: 53500 training acc: 0.5
Global Iter: 53600 training loss: 0.701732
Global Iter: 53600 training acc: 0.46875
Global Iter: 53700 training loss: 0.683761
Global Iter: 53700 training acc: 0.59375
Global Iter: 53800 training loss: 0.716097
Global Iter: 53800 training acc: 0.34375
Global Iter: 53900 training loss: 0.697227
Global Iter: 53900 training acc: 0.5
Global Iter: 54000 training loss: 0.683347
Global Iter: 54000 training acc: 0.59375
Global Iter: 54100 training loss: 0.670517
Global Iter: 54100 training acc: 0.6875
Global Iter: 54200 training loss: 0.701271
Global Iter: 54200 training acc: 0.4375
Global Iter: 54300 training loss: 0.69384
Global Iter: 54300 training acc: 0.5
Global Iter: 54400 training loss: 0.689777
Global Iter: 54400 training acc: 0.5625
Global Iter: 54500 training loss: 0.695973
Global Iter: 54500 training acc: 0.46875
Global Iter: 54600 training loss: 0.687639
Global Iter: 54600 training acc: 0.53125
Global Iter: 54700 training loss: 0.682601
Global Iter: 54700 training acc: 0.59375
Global Iter: 54800 training loss: 0.698489
Global Iter: 54800 training acc: 0.5
Global Iter: 54900 training loss: 0.674368
Global Iter: 54900 training acc: 0.65625
Global Iter: 55000 training loss: 0.693007
Global Iter: 55000 training acc: 0.5
Global Iter: 55100 training loss: 0.701267
Global Iter: 55100 training acc: 0.4375
Global Iter: 55200 training loss: 0.677104
Global Iter: 55200 training acc: 0.65625
Global Iter: 55300 training loss: 0.686277
Global Iter: 55300 training acc: 0.53125
Global Iter: 55400 training loss: 0.691273
Global Iter: 55400 training acc: 0.53125
Global Iter: 55500 training loss: 0.692146
Global Iter: 55500 training acc: 0.5
Global Iter: 55600 training loss: 0.680955
Global Iter: 55600 training acc: 0.625
Global Iter: 55700 training loss: 0.681508
Global Iter: 55700 training acc: 0.59375
Global Iter: 55800 training loss: 0.69799
Global Iter: 55800 training acc: 0.46875
Global Iter: 55900 training loss: 0.717611
Global Iter: 55900 training acc: 0.3125
Global Iter: 56000 training loss: 0.683482
Global Iter: 56000 training acc: 0.5625
Global Iter: 56100 training loss: 0.693669
Global Iter: 56100 training acc: 0.53125
Global Iter: 56200 training loss: 0.703984
Global Iter: 56200 training acc: 0.4375
Global Iter: 56300 training loss: 0.681013
Global Iter: 56300 training acc: 0.59375
Global Iter: 56400 training loss: 0.695495
Global Iter: 56400 training acc: 0.5
Global Iter: 56500 training loss: 0.681929
Global Iter: 56500 training acc: 0.5625
Global Iter: 56600 training loss: 0.692584
Global Iter: 56600 training acc: 0.53125
Global Iter: 56700 training loss: 0.682686
Global Iter: 56700 training acc: 0.625
Global Iter: 56800 training loss: 0.700737
Global Iter: 56800 training acc: 0.4375
Global Iter: 56900 training loss: 0.69034
Global Iter: 56900 training acc: 0.53125
Global Iter: 57000 training loss: 0.672317
Global Iter: 57000 training acc: 0.6875
Global Iter: 57100 training loss: 0.693082
Global Iter: 57100 training acc: 0.5
Global Iter: 57200 training loss: 0.69403
Global Iter: 57200 training acc: 0.5
Global Iter: 57300 training loss: 0.693825
Global Iter: 57300 training acc: 0.53125
Global Iter: 57400 training loss: 0.702363
Global Iter: 57400 training acc: 0.40625
Global Iter: 57500 training loss: 0.703052
Global Iter: 57500 training acc: 0.4375
Global Iter: 57600 training loss: 0.693035
Global Iter: 57600 training acc: 0.53125
Global Iter: 57700 training loss: 0.704847
Global Iter: 57700 training acc: 0.4375
Global Iter: 57800 training loss: 0.668385
Global Iter: 57800 training acc: 0.6875
Global Iter: 57900 training loss: 0.702794
Global Iter: 57900 training acc: 0.40625
Global Iter: 58000 training loss: 0.706541
Global Iter: 58000 training acc: 0.34375
Global Iter: 58100 training loss: 0.673463
Global Iter: 58100 training acc: 0.6875
Global Iter: 58200 training loss: 0.692203
Global Iter: 58200 training acc: 0.53125
Global Iter: 58300 training loss: 0.68763
Global Iter: 58300 training acc: 0.59375
Global Iter: 58400 training loss: 0.686034
Global Iter: 58400 training acc: 0.5625
Global Iter: 58500 training loss: 0.669704
Global Iter: 58500 training acc: 0.6875
Global Iter: 58600 training loss: 0.700104
Global Iter: 58600 training acc: 0.4375
Global Iter: 58700 training loss: 0.689229
Global Iter: 58700 training acc: 0.53125
Global Iter: 58800 training loss: 0.706277
Global Iter: 58800 training acc: 0.40625
Global Iter: 58900 training loss: 0.706373
Global Iter: 58900 training acc: 0.46875
Global Iter: 59000 training loss: 0.690246
Global Iter: 59000 training acc: 0.5625
Global Iter: 59100 training loss: 0.696978
Global Iter: 59100 training acc: 0.4375
Global Iter: 59200 training loss: 0.68445
Global Iter: 59200 training acc: 0.625
Global Iter: 59300 training loss: 0.694175
Global Iter: 59300 training acc: 0.5
Global Iter: 59400 training loss: 0.67051
Global Iter: 59400 training acc: 0.65625
Global Iter: 59500 training loss: 0.692707
Global Iter: 59500 training acc: 0.5
Global Iter: 59600 training loss: 0.683384
Global Iter: 59600 training acc: 0.625
Global Iter: 59700 training loss: 0.688842
Global Iter: 59700 training acc: 0.53125
Global Iter: 59800 training loss: 0.675187
Global Iter: 59800 training acc: 0.65625
Global Iter: 59900 training loss: 0.670859
Global Iter: 59900 training acc: 0.6875
Global Iter: 60000 training loss: 0.679378
Global Iter: 60000 training acc: 0.625
Global Iter: 60100 training loss: 0.68819
Global Iter: 60100 training acc: 0.5
Global Iter: 60200 training loss: 0.695777
Global Iter: 60200 training acc: 0.5
Global Iter: 60300 training loss: 0.681556
Global Iter: 60300 training acc: 0.59375
Global Iter: 60400 training loss: 0.685286
Global Iter: 60400 training acc: 0.59375
Global Iter: 60500 training loss: 0.682126
Global Iter: 60500 training acc: 0.53125
Global Iter: 60600 training loss: 0.696089
Global Iter: 60600 training acc: 0.5
Global Iter: 60700 training loss: 0.69381
Global Iter: 60700 training acc: 0.53125
Global Iter: 60800 training loss: 0.717311
Global Iter: 60800 training acc: 0.34375
Global Iter: 60900 training loss: 0.706072
Global Iter: 60900 training acc: 0.40625
Global Iter: 61000 training loss: 0.670606
Global Iter: 61000 training acc: 0.71875
Global Iter: 61100 training loss: 0.698303
Global Iter: 61100 training acc: 0.46875
Global Iter: 61200 training loss: 0.691228
Global Iter: 61200 training acc: 0.53125
Global Iter: 61300 training loss: 0.69255
Global Iter: 61300 training acc: 0.53125
Global Iter: 61400 training loss: 0.671245
Global Iter: 61400 training acc: 0.71875
Global Iter: 61500 training loss: 0.697972
Global Iter: 61500 training acc: 0.46875
Global Iter: 61600 training loss: 0.697914
Global Iter: 61600 training acc: 0.53125
Global Iter: 61700 training loss: 0.695461
Global Iter: 61700 training acc: 0.5
Global Iter: 61800 training loss: 0.701846
Global Iter: 61800 training acc: 0.4375
Global Iter: 61900 training loss: 0.688807
Global Iter: 61900 training acc: 0.5
Global Iter: 62000 training loss: 0.693054
Global Iter: 62000 training acc: 0.53125
Global Iter: 62100 training loss: 0.691583
Global Iter: 62100 training acc: 0.5625
Global Iter: 62200 training loss: 0.693868
Global Iter: 62200 training acc: 0.53125
Global Iter: 62300 training loss: 0.699527
Global Iter: 62300 training acc: 0.46875
Global Iter: 62400 training loss: 0.694159
Global Iter: 62400 training acc: 0.53125
Global Iter: 62500 training loss: 0.688246
Global Iter: 62500 training acc: 0.53125
Global Iter: 62600 training loss: 0.693803
Global Iter: 62600 training acc: 0.5
Global Iter: 62700 training loss: 0.684983
Global Iter: 62700 training acc: 0.5625
Global Iter: 62800 training loss: 0.686104
Global Iter: 62800 training acc: 0.5625
Global Iter: 62900 training loss: 0.702091
Global Iter: 62900 training acc: 0.46875
Global Iter: 63000 training loss: 0.712941
Global Iter: 63000 training acc: 0.375
Global Iter: 63100 training loss: 0.693575
Global Iter: 63100 training acc: 0.46875
Global Iter: 63200 training loss: 0.692079
Global Iter: 63200 training acc: 0.5
Global Iter: 63300 training loss: 0.67324
Global Iter: 63300 training acc: 0.65625
Global Iter: 63400 training loss: 0.696713
Global Iter: 63400 training acc: 0.53125
Global Iter: 63500 training loss: 0.694158
Global Iter: 63500 training acc: 0.5
Global Iter: 63600 training loss: 0.681298
Global Iter: 63600 training acc: 0.625
Global Iter: 63700 training loss: 0.725981
Global Iter: 63700 training acc: 0.3125
Global Iter: 63800 training loss: 0.701075
Global Iter: 63800 training acc: 0.46875
Global Iter: 63900 training loss: 0.679941
Global Iter: 63900 training acc: 0.59375
Global Iter: 64000 training loss: 0.668519
Global Iter: 64000 training acc: 0.6875
Global Iter: 64100 training loss: 0.697302
Global Iter: 64100 training acc: 0.46875
Global Iter: 64200 training loss: 0.699945
Global Iter: 64200 training acc: 0.46875
Global Iter: 64300 training loss: 0.690118
Global Iter: 64300 training acc: 0.5625
Global Iter: 64400 training loss: 0.697749
Global Iter: 64400 training acc: 0.5
Global Iter: 64500 training loss: 0.69004
Global Iter: 64500 training acc: 0.53125
Global Iter: 64600 training loss: 0.680183
Global Iter: 64600 training acc: 0.625
Global Iter: 64700 training loss: 0.69707
Global Iter: 64700 training acc: 0.5
Global Iter: 64800 training loss: 0.675602
Global Iter: 64800 training acc: 0.65625
Global Iter: 64900 training loss: 0.691068
Global Iter: 64900 training acc: 0.53125
Global Iter: 65000 training loss: 0.70648
Global Iter: 65000 training acc: 0.4375
Global Iter: 65100 training loss: 0.678277
Global Iter: 65100 training acc: 0.6875
Global Iter: 65200 training loss: 0.682046
Global Iter: 65200 training acc: 0.5625
Global Iter: 65300 training loss: 0.695909
Global Iter: 65300 training acc: 0.5
Global Iter: 65400 training loss: 0.698807
Global Iter: 65400 training acc: 0.5
Global Iter: 65500 training loss: 0.681162
Global Iter: 65500 training acc: 0.59375
Global Iter: 65600 training loss: 0.695037
Global Iter: 65600 training acc: 0.53125
Global Iter: 65700 training loss: 0.698125
Global Iter: 65700 training acc: 0.46875
Global Iter: 65800 training loss: 0.714753
Global Iter: 65800 training acc: 0.34375
Global Iter: 65900 training loss: 0.688432
Global Iter: 65900 training acc: 0.5625
Global Iter: 66000 training loss: 0.695379
Global Iter: 66000 training acc: 0.5
Global Iter: 66100 training loss: 0.708858
Global Iter: 66100 training acc: 0.40625
Global Iter: 66200 training loss: 0.677077
Global Iter: 66200 training acc: 0.625
Global Iter: 66300 training loss: 0.696234
Global Iter: 66300 training acc: 0.5
Global Iter: 66400 training loss: 0.687763
Global Iter: 66400 training acc: 0.53125
Global Iter: 66500 training loss: 0.695557
Global Iter: 66500 training acc: 0.53125
Global Iter: 66600 training loss: 0.677669
Global Iter: 66600 training acc: 0.65625
Global Iter: 66700 training loss: 0.692492
Global Iter: 66700 training acc: 0.46875
Global Iter: 66800 training loss: 0.690347
Global Iter: 66800 training acc: 0.53125
Global Iter: 66900 training loss: 0.671635
Global Iter: 66900 training acc: 0.65625
Global Iter: 67000 training loss: 0.697007
Global Iter: 67000 training acc: 0.46875
Global Iter: 67100 training loss: 0.697066
Global Iter: 67100 training acc: 0.5
Global Iter: 67200 training loss: 0.678897
Global Iter: 67200 training acc: 0.59375
Global Iter: 67300 training loss: 0.702521
Global Iter: 67300 training acc: 0.46875
Global Iter: 67400 training loss: 0.707892
Global Iter: 67400 training acc: 0.40625
Global Iter: 67500 training loss: 0.682872
Global Iter: 67500 training acc: 0.5625
Global Iter: 67600 training loss: 0.707497
Global Iter: 67600 training acc: 0.40625
Global Iter: 67700 training loss: 0.673894
Global Iter: 67700 training acc: 0.65625
Global Iter: 67800 training loss: 0.701053
Global Iter: 67800 training acc: 0.46875
Global Iter: 67900 training loss: 0.724842
Global Iter: 67900 training acc: 0.28125
Global Iter: 68000 training loss: 0.677571
Global Iter: 68000 training acc: 0.625
Global Iter: 68100 training loss: 0.690179
Global Iter: 68100 training acc: 0.53125
Global Iter: 68200 training loss: 0.684719
Global Iter: 68200 training acc: 0.59375
Global Iter: 68300 training loss: 0.692025
Global Iter: 68300 training acc: 0.53125
Global Iter: 68400 training loss: 0.669778
Global Iter: 68400 training acc: 0.6875
Global Iter: 68500 training loss: 0.697738
Global Iter: 68500 training acc: 0.46875
Global Iter: 68600 training loss: 0.692324
Global Iter: 68600 training acc: 0.53125
Global Iter: 68700 training loss: 0.702897
Global Iter: 68700 training acc: 0.4375
Global Iter: 68800 training loss: 0.699741
Global Iter: 68800 training acc: 0.46875
Global Iter: 68900 training loss: 0.684639
Global Iter: 68900 training acc: 0.5625
Global Iter: 69000 training loss: 0.697703
Global Iter: 69000 training acc: 0.4375
Global Iter: 69100 training loss: 0.684312
Global Iter: 69100 training acc: 0.5625
Global Iter: 69200 training loss: 0.694333
Global Iter: 69200 training acc: 0.53125
Global Iter: 69300 training loss: 0.672251
Global Iter: 69300 training acc: 0.65625
Global Iter: 69400 training loss: 0.697045
Global Iter: 69400 training acc: 0.46875
Global Iter: 69500 training loss: 0.67912
Global Iter: 69500 training acc: 0.625
Global Iter: 69600 training loss: 0.688114
Global Iter: 69600 training acc: 0.5625
Global Iter: 69700 training loss: 0.688831
Global Iter: 69700 training acc: 0.59375
Global Iter: 69800 training loss: 0.66836
Global Iter: 69800 training acc: 0.6875
Global Iter: 69900 training loss: 0.676383
Global Iter: 69900 training acc: 0.625
Global Iter: 70000 training loss: 0.687457
Global Iter: 70000 training acc: 0.5625
Global Iter: 70100 training loss: 0.699151
Global Iter: 70100 training acc: 0.53125
Global Iter: 70200 training loss: 0.678037
Global Iter: 70200 training acc: 0.625
Global Iter: 70300 training loss: 0.68666
Global Iter: 70300 training acc: 0.59375
Global Iter: 70400 training loss: 0.690862
Global Iter: 70400 training acc: 0.53125
Global Iter: 70500 training loss: 0.699735
Global Iter: 70500 training acc: 0.46875
Global Iter: 70600 training loss: 0.67802
Global Iter: 70600 training acc: 0.59375
Global Iter: 70700 training loss: 0.710059
Global Iter: 70700 training acc: 0.375
Global Iter: 70800 training loss: 0.705865
Global Iter: 70800 training acc: 0.40625
Global Iter: 70900 training loss: 0.667943
Global Iter: 70900 training acc: 0.6875
Global Iter: 71000 training loss: 0.69452
Global Iter: 71000 training acc: 0.5
Global Iter: 71100 training loss: 0.693394
Global Iter: 71100 training acc: 0.5
Global Iter: 71200 training loss: 0.685422
Global Iter: 71200 training acc: 0.5625
Global Iter: 71300 training loss: 0.673385
Global Iter: 71300 training acc: 0.71875
Global Iter: 71400 training loss: 0.696917
Global Iter: 71400 training acc: 0.5
Global Iter: 71500 training loss: 0.683943
Global Iter: 71500 training acc: 0.5625
Global Iter: 71600 training loss: 0.694331
Global Iter: 71600 training acc: 0.5
Global Iter: 71700 training loss: 0.711088
Global Iter: 71700 training acc: 0.40625
Global Iter: 71800 training loss: 0.703387
Global Iter: 71800 training acc: 0.4375
Global Iter: 71900 training loss: 0.685507
Global Iter: 71900 training acc: 0.59375
Global Iter: 72000 training loss: 0.691875
Global Iter: 72000 training acc: 0.53125
Global Iter: 72100 training loss: 0.689556
Global Iter: 72100 training acc: 0.5
Global Iter: 72200 training loss: 0.69175
Global Iter: 72200 training acc: 0.5
Global Iter: 72300 training loss: 0.683443
Global Iter: 72300 training acc: 0.5625
Global Iter: 72400 training loss: 0.69017
Global Iter: 72400 training acc: 0.5625
Global Iter: 72500 training loss: 0.693524
Global Iter: 72500 training acc: 0.5
Global Iter: 72600 training loss: 0.69004
Global Iter: 72600 training acc: 0.53125
Global Iter: 72700 training loss: 0.692567
Global Iter: 72700 training acc: 0.5625
Global Iter: 72800 training loss: 0.698478
Global Iter: 72800 training acc: 0.5
Global Iter: 72900 training loss: 0.706435
Global Iter: 72900 training acc: 0.40625
Global Iter: 73000 training loss: 0.696824
Global Iter: 73000 training acc: 0.5
Global Iter: 73100 training loss: 0.692222
Global Iter: 73100 training acc: 0.5
Global Iter: 73200 training loss: 0.67341
Global Iter: 73200 training acc: 0.6875
Global Iter: 73300 training loss: 0.693858
Global Iter: 73300 training acc: 0.53125
Global Iter: 73400 training loss: 0.695771
Global Iter: 73400 training acc: 0.5
Global Iter: 73500 training loss: 0.686842
Global Iter: 73500 training acc: 0.5625
Global Iter: 73600 training loss: 0.722523
Global Iter: 73600 training acc: 0.3125
Global Iter: 73700 training loss: 0.697308
Global Iter: 73700 training acc: 0.46875
Global Iter: 73800 training loss: 0.685662
Global Iter: 73800 training acc: 0.59375
Global Iter: 73900 training loss: 0.673732
Global Iter: 73900 training acc: 0.65625
Global Iter: 74000 training loss: 0.697654
Global Iter: 74000 training acc: 0.46875
Global Iter: 74100 training loss: 0.695986
Global Iter: 74100 training acc: 0.5
Global Iter: 74200 training loss: 0.685484
Global Iter: 74200 training acc: 0.59375
Global Iter: 74300 training loss: 0.695635
Global Iter: 74300 training acc: 0.53125
Global Iter: 74400 training loss: 0.696502
Global Iter: 74400 training acc: 0.5
Global Iter: 74500 training loss: 0.690307
Global Iter: 74500 training acc: 0.5625
Global Iter: 74600 training loss: 0.699063
Global Iter: 74600 training acc: 0.5
Global Iter: 74700 training loss: 0.676196
Global Iter: 74700 training acc: 0.625
Global Iter: 74800 training loss: 0.687678
Global Iter: 74800 training acc: 0.5625
Global Iter: 74900 training loss: 0.699119
Global Iter: 74900 training acc: 0.4375
Global Iter: 75000 training loss: 0.665674
Global Iter: 75000 training acc: 0.71875
Global Iter: 75100 training loss: 0.691574
Global Iter: 75100 training acc: 0.5625
Global Iter: 75200 training loss: 0.694494
Global Iter: 75200 training acc: 0.5
Global Iter: 75300 training loss: 0.69176
Global Iter: 75300 training acc: 0.53125
Global Iter: 75400 training loss: 0.691271
Global Iter: 75400 training acc: 0.53125
Global Iter: 75500 training loss: 0.693566
Global Iter: 75500 training acc: 0.5
Global Iter: 75600 training loss: 0.691066
Global Iter: 75600 training acc: 0.53125
Global Iter: 75700 training loss: 0.712984
Global Iter: 75700 training acc: 0.34375
Global Iter: 75800 training loss: 0.692472
Global Iter: 75800 training acc: 0.53125
Global Iter: 75900 training loss: 0.696719
Global Iter: 75900 training acc: 0.5
Global Iter: 76000 training loss: 0.716109
Global Iter: 76000 training acc: 0.40625
Global Iter: 76100 training loss: 0.679952
Global Iter: 76100 training acc: 0.625
Global Iter: 76200 training loss: 0.693083
Global Iter: 76200 training acc: 0.53125
Global Iter: 76300 training loss: 0.689503
Global Iter: 76300 training acc: 0.53125
Global Iter: 76400 training loss: 0.683457
Global Iter: 76400 training acc: 0.59375
Global Iter: 76500 training loss: 0.680541
Global Iter: 76500 training acc: 0.625
Global Iter: 76600 training loss: 0.699293
Global Iter: 76600 training acc: 0.4375
Global Iter: 76700 training loss: 0.696978
Global Iter: 76700 training acc: 0.53125
Global Iter: 76800 training loss: 0.678612
Global Iter: 76800 training acc: 0.65625
Global Iter: 76900 training loss: 0.704221
Global Iter: 76900 training acc: 0.4375
Global Iter: 77000 training loss: 0.688967
Global Iter: 77000 training acc: 0.5625
Global Iter: 77100 training loss: 0.68622
Global Iter: 77100 training acc: 0.5625
Global Iter: 77200 training loss: 0.695474
Global Iter: 77200 training acc: 0.46875
Global Iter: 77300 training loss: 0.702373
Global Iter: 77300 training acc: 0.46875
Global Iter: 77400 training loss: 0.686956
Global Iter: 77400 training acc: 0.5625
Global Iter: 77500 training loss: 0.699329
Global Iter: 77500 training acc: 0.4375
Global Iter: 77600 training loss: 0.672223
Global Iter: 77600 training acc: 0.6875
Global Iter: 77700 training loss: 0.702437
Global Iter: 77700 training acc: 0.46875
Global Iter: 77800 training loss: 0.713841
Global Iter: 77800 training acc: 0.34375
Global Iter: 77900 training loss: 0.687821
Global Iter: 77900 training acc: 0.59375
Global Iter: 78000 training loss: 0.691715
Global Iter: 78000 training acc: 0.53125
Global Iter: 78100 training loss: 0.678518
Global Iter: 78100 training acc: 0.65625
Global Iter: 78200 training loss: 0.687828
Global Iter: 78200 training acc: 0.5625
Global Iter: 78300 training loss: 0.666284
Global Iter: 78300 training acc: 0.71875
Global Iter: 78400 training loss: 0.693608
Global Iter: 78400 training acc: 0.5
Global Iter: 78500 training loss: 0.689943
Global Iter: 78500 training acc: 0.53125
Global Iter: 78600 training loss: 0.704242
Global Iter: 78600 training acc: 0.40625
Global Iter: 78700 training loss: 0.693595
Global Iter: 78700 training acc: 0.53125
Global Iter: 78800 training loss: 0.680089
Global Iter: 78800 training acc: 0.625
Global Iter: 78900 training loss: 0.700815
Global Iter: 78900 training acc: 0.4375
Global Iter: 79000 training loss: 0.69169
Global Iter: 79000 training acc: 0.53125
Global Iter: 79100 training loss: 0.684682
Global Iter: 79100 training acc: 0.5625
Global Iter: 79200 training loss: 0.667728
Global Iter: 79200 training acc: 0.6875
Global Iter: 79300 training loss: 0.702415
Global Iter: 79300 training acc: 0.4375
Global Iter: 79400 training loss: 0.67588
Global Iter: 79400 training acc: 0.625
Global Iter: 79500 training loss: 0.689454
Global Iter: 79500 training acc: 0.5625
Global Iter: 79600 training loss: 0.682864
Global Iter: 79600 training acc: 0.625
Global Iter: 79700 training loss: 0.670193
Global Iter: 79700 training acc: 0.6875
Global Iter: 79800 training loss: 0.680017
Global Iter: 79800 training acc: 0.625
Global Iter: 79900 training loss: 0.695313
Global Iter: 79900 training acc: 0.53125
Global Iter: 80000 training loss: 0.686455
Global Iter: 80000 training acc: 0.5625
Global Iter: 80100 training loss: 0.680882
Global Iter: 80100 training acc: 0.625
Global Iter: 80200 training loss: 0.687057
Global Iter: 80200 training acc: 0.59375
Global Iter: 80300 training loss: 0.692772
Global Iter: 80300 training acc: 0.5
Global Iter: 80400 training loss: 0.704423
Global Iter: 80400 training acc: 0.4375
Global Iter: 80500 training loss: 0.685045
Global Iter: 80500 training acc: 0.59375
Global Iter: 80600 training loss: 0.70463
Global Iter: 80600 training acc: 0.40625
Global Iter: 80700 training loss: 0.709648
Global Iter: 80700 training acc: 0.375
Global Iter: 80800 training loss: 0.671855
Global Iter: 80800 training acc: 0.6875
Global Iter: 80900 training loss: 0.695566
Global Iter: 80900 training acc: 0.5
Global Iter: 81000 training loss: 0.698245
Global Iter: 81000 training acc: 0.5
Global Iter: 81100 training loss: 0.687309
Global Iter: 81100 training acc: 0.5625
Global Iter: 81200 training loss: 0.663339
Global Iter: 81200 training acc: 0.71875
Global Iter: 81300 training loss: 0.704053
Global Iter: 81300 training acc: 0.46875
Global Iter: 81400 training loss: 0.684785
Global Iter: 81400 training acc: 0.5625
Global Iter: 81500 training loss: 0.695773
Global Iter: 81500 training acc: 0.46875
Global Iter: 81600 training loss: 0.706679
Global Iter: 81600 training acc: 0.4375
Global Iter: 81700 training loss: 0.703574
Global Iter: 81700 training acc: 0.46875
Global Iter: 81800 training loss: 0.683865
Global Iter: 81800 training acc: 0.59375
Global Iter: 81900 training loss: 0.696446
Global Iter: 81900 training acc: 0.5
Global Iter: 82000 training loss: 0.691512
Global Iter: 82000 training acc: 0.53125
Global Iter: 82100 training loss: 0.695508
Global Iter: 82100 training acc: 0.5
Global Iter: 82200 training loss: 0.687015
Global Iter: 82200 training acc: 0.5625
Global Iter: 82300 training loss: 0.690824
Global Iter: 82300 training acc: 0.53125
Global Iter: 82400 training loss: 0.693746
Global Iter: 82400 training acc: 0.5
Global Iter: 82500 training loss: 0.684153
Global Iter: 82500 training acc: 0.59375
Global Iter: 82600 training loss: 0.685021
Global Iter: 82600 training acc: 0.59375
Global Iter: 82700 training loss: 0.688268
Global Iter: 82700 training acc: 0.53125
Global Iter: 82800 training loss: 0.710379
Global Iter: 82800 training acc: 0.40625
Global Iter: 82900 training loss: 0.687768
Global Iter: 82900 training acc: 0.53125
Global Iter: 83000 training loss: 0.698643
Global Iter: 83000 training acc: 0.46875
Global Iter: 83100 training loss: 0.664539
Global Iter: 83100 training acc: 0.6875
Global Iter: 83200 training loss: 0.688591
Global Iter: 83200 training acc: 0.53125
Global Iter: 83300 training loss: 0.703367
Global Iter: 83300 training acc: 0.4375
Global Iter: 83400 training loss: 0.68771
Global Iter: 83400 training acc: 0.5625
Global Iter: 83500 training loss: 0.714315
Global Iter: 83500 training acc: 0.34375
Global Iter: 83600 training loss: 0.701801
Global Iter: 83600 training acc: 0.4375
Global Iter: 83700 training loss: 0.686677
Global Iter: 83700 training acc: 0.59375
Global Iter: 83800 training loss: 0.680595
Global Iter: 83800 training acc: 0.625
Global Iter: 83900 training loss: 0.693396
Global Iter: 83900 training acc: 0.5
Global Iter: 84000 training loss: 0.697885
Global Iter: 84000 training acc: 0.5
Global Iter: 84100 training loss: 0.67926
Global Iter: 84100 training acc: 0.625
Global Iter: 84200 training loss: 0.68722
Global Iter: 84200 training acc: 0.5625
Global Iter: 84300 training loss: 0.693685
Global Iter: 84300 training acc: 0.5
Global Iter: 84400 training loss: 0.692638
Global Iter: 84400 training acc: 0.53125
Global Iter: 84500 training loss: 0.702955
Global Iter: 84500 training acc: 0.46875
Global Iter: 84600 training loss: 0.686353
Global Iter: 84600 training acc: 0.59375
Global Iter: 84700 training loss: 0.683597
Global Iter: 84700 training acc: 0.5625
Global Iter: 84800 training loss: 0.704897
Global Iter: 84800 training acc: 0.40625
Global Iter: 84900 training loss: 0.670672
Global Iter: 84900 training acc: 0.71875
Global Iter: 85000 training loss: 0.689688
Global Iter: 85000 training acc: 0.5625
Global Iter: 85100 training loss: 0.691236
Global Iter: 85100 training acc: 0.53125
Global Iter: 85200 training loss: 0.697901
Global Iter: 85200 training acc: 0.46875
Global Iter: 85300 training loss: 0.686689
Global Iter: 85300 training acc: 0.5625
Global Iter: 85400 training loss: 0.686469
Global Iter: 85400 training acc: 0.53125
Global Iter: 85500 training loss: 0.689995
Global Iter: 85500 training acc: 0.53125
Global Iter: 85600 training loss: 0.712454
Global Iter: 85600 training acc: 0.375
Global Iter: 85700 training loss: 0.694741
Global Iter: 85700 training acc: 0.5
Global Iter: 85800 training loss: 0.695908
Global Iter: 85800 training acc: 0.5
Global Iter: 85900 training loss: 0.700519
Global Iter: 85900 training acc: 0.4375
Global Iter: 86000 training loss: 0.681491
Global Iter: 86000 training acc: 0.59375
Global Iter: 86100 training loss: 0.693238
Global Iter: 86100 training acc: 0.53125
Global Iter: 86200 training loss: 0.696018
Global Iter: 86200 training acc: 0.5
Global Iter: 86300 training loss: 0.6956
Global Iter: 86300 training acc: 0.53125
Global Iter: 86400 training loss: 0.682169
Global Iter: 86400 training acc: 0.625
Global Iter: 86500 training loss: 0.698523
Global Iter: 86500 training acc: 0.4375
Global Iter: 86600 training loss: 0.698086
Global Iter: 86600 training acc: 0.46875
Global Iter: 86700 training loss: 0.672469
Global Iter: 86700 training acc: 0.65625
Global Iter: 86800 training loss: 0.701576
Global Iter: 86800 training acc: 0.4375
Global Iter: 86900 training loss: 0.684281
Global Iter: 86900 training acc: 0.59375
Global Iter: 87000 training loss: 0.684999
Global Iter: 87000 training acc: 0.59375
Global Iter: 87100 training loss: 0.696193
Global Iter: 87100 training acc: 0.46875
Global Iter: 87200 training loss: 0.701746
Global Iter: 87200 training acc: 0.46875
Global Iter: 87300 training loss: 0.680253
Global Iter: 87300 training acc: 0.625
Global Iter: 87400 training loss: 0.698441
Global Iter: 87400 training acc: 0.46875
Global Iter: 87500 training loss: 0.666183
Global Iter: 87500 training acc: 0.75
Global Iter: 87600 training loss: 0.694111
Global Iter: 87600 training acc: 0.5
Global Iter: 87700 training loss: 0.709366
Global Iter: 87700 training acc: 0.375
Global Iter: 87800 training loss: 0.681304
Global Iter: 87800 training acc: 0.625
Global Iter: 87900 training loss: 0.687911
Global Iter: 87900 training acc: 0.5625
Global Iter: 88000 training loss: 0.68063
Global Iter: 88000 training acc: 0.625
Global Iter: 88100 training loss: 0.693719
Global Iter: 88100 training acc: 0.5
Global Iter: 88200 training loss: 0.67298
Global Iter: 88200 training acc: 0.6875
Global Iter: 88300 training loss: 0.692315
Global Iter: 88300 training acc: 0.53125
Global Iter: 88400 training loss: 0.692761
Global Iter: 88400 training acc: 0.53125
Global Iter: 88500 training loss: 0.705186
Global Iter: 88500 training acc: 0.40625
Global Iter: 88600 training loss: 0.693797
Global Iter: 88600 training acc: 0.53125
Global Iter: 88700 training loss: 0.670893
Global Iter: 88700 training acc: 0.6875
Global Iter: 88800 training loss: 0.707475
Global Iter: 88800 training acc: 0.40625
Global Iter: 88900 training loss: 0.690768
Global Iter: 88900 training acc: 0.53125
Global Iter: 89000 training loss: 0.68996
Global Iter: 89000 training acc: 0.5625
Global Iter: 89100 training loss: 0.678629
Global Iter: 89100 training acc: 0.65625
Global Iter: 89200 training loss: 0.69851
Global Iter: 89200 training acc: 0.46875
Global Iter: 89300 training loss: 0.678999
Global Iter: 89300 training acc: 0.625
Global Iter: 89400 training loss: 0.686608
Global Iter: 89400 training acc: 0.5625
Global Iter: 89500 training loss: 0.678873
Global Iter: 89500 training acc: 0.65625
Global Iter: 89600 training loss: 0.669441
Global Iter: 89600 training acc: 0.6875
Global Iter: 89700 training loss: 0.677443
Global Iter: 89700 training acc: 0.65625
Global Iter: 89800 training loss: 0.685851
Global Iter: 89800 training acc: 0.5625
Global Iter: 89900 training loss: 0.687251
Global Iter: 89900 training acc: 0.5625
Global Iter: 90000 training loss: 0.684304
Global Iter: 90000 training acc: 0.59375
Global Iter: 90100 training loss: 0.686539
Global Iter: 90100 training acc: 0.5625
Global Iter: 90200 training loss: 0.690392
Global Iter: 90200 training acc: 0.53125
Global Iter: 90300 training loss: 0.705747
Global Iter: 90300 training acc: 0.4375
Global Iter: 90400 training loss: 0.677618
Global Iter: 90400 training acc: 0.625
Global Iter: 90500 training loss: 0.70531
Global Iter: 90500 training acc: 0.4375
Global Iter: 90600 training loss: 0.710143
Global Iter: 90600 training acc: 0.375
Global Iter: 90700 training loss: 0.665509
Global Iter: 90700 training acc: 0.75
Global Iter: 90800 training loss: 0.693799
Global Iter: 90800 training acc: 0.5
Global Iter: 90900 training loss: 0.693086
Global Iter: 90900 training acc: 0.53125
Global Iter: 91000 training loss: 0.682759
Global Iter: 91000 training acc: 0.59375
Global Iter: 91100 training loss: 0.672387
Global Iter: 91100 training acc: 0.6875
Global Iter: 91200 training loss: 0.706718
Global Iter: 91200 training acc: 0.40625
Global Iter: 91300 training loss: 0.687429
Global Iter: 91300 training acc: 0.59375
Global Iter: 91400 training loss: 0.69741
Global Iter: 91400 training acc: 0.46875
Global Iter: 91500 training loss: 0.708672
Global Iter: 91500 training acc: 0.40625
Global Iter: 91600 training loss: 0.689456
Global Iter: 91600 training acc: 0.53125
Global Iter: 91700 training loss: 0.682591
Global Iter: 91700 training acc: 0.59375
Global Iter: 91800 training loss: 0.689809
Global Iter: 91800 training acc: 0.5625
Global Iter: 91900 training loss: 0.68672
Global Iter: 91900 training acc: 0.5625
Global Iter: 92000 training loss: 0.692645
Global Iter: 92000 training acc: 0.53125
Global Iter: 92100 training loss: 0.692137
Global Iter: 92100 training acc: 0.53125
Global Iter: 92200 training loss: 0.690125
Global Iter: 92200 training acc: 0.53125
Global Iter: 92300 training loss: 0.691567
Global Iter: 92300 training acc: 0.53125
Global Iter: 92400 training loss: 0.685336
Global Iter: 92400 training acc: 0.5625
Global Iter: 92500 training loss: 0.682923
Global Iter: 92500 training acc: 0.59375
Global Iter: 92600 training loss: 0.68817
Global Iter: 92600 training acc: 0.5625
Global Iter: 92700 training loss: 0.703725
Global Iter: 92700 training acc: 0.4375
Global Iter: 92800 training loss: 0.695471
Global Iter: 92800 training acc: 0.5
Global Iter: 92900 training loss: 0.70283
Global Iter: 92900 training acc: 0.4375
Global Iter: 93000 training loss: 0.671691
Global Iter: 93000 training acc: 0.6875
Global Iter: 93100 training loss: 0.684724
Global Iter: 93100 training acc: 0.59375
Global Iter: 93200 training loss: 0.702809
Global Iter: 93200 training acc: 0.4375
Global Iter: 93300 training loss: 0.685345
Global Iter: 93300 training acc: 0.5625
Global Iter: 93400 training loss: 0.71877
Global Iter: 93400 training acc: 0.3125
Global Iter: 93500 training loss: 0.69761
Global Iter: 93500 training acc: 0.46875
Global Iter: 93600 training loss: 0.679273
Global Iter: 93600 training acc: 0.625
Global Iter: 93700 training loss: 0.677676
Global Iter: 93700 training acc: 0.625
Global Iter: 93800 training loss: 0.697461
Global Iter: 93800 training acc: 0.46875
Global Iter: 93900 training loss: 0.690662
Global Iter: 93900 training acc: 0.53125
Global Iter: 94000 training loss: 0.672769
Global Iter: 94000 training acc: 0.65625
Global Iter: 94100 training loss: 0.686617
Global Iter: 94100 training acc: 0.5625
Global Iter: 94200 training loss: 0.692937
Global Iter: 94200 training acc: 0.5
Global Iter: 94300 training loss: 0.694618
Global Iter: 94300 training acc: 0.5
Global Iter: 94400 training loss: 0.713246
Global Iter: 94400 training acc: 0.40625
Global Iter: 94500 training loss: 0.684683
Global Iter: 94500 training acc: 0.59375
Global Iter: 94600 training loss: 0.681187
Global Iter: 94600 training acc: 0.59375
Global Iter: 94700 training loss: 0.707444
Global Iter: 94700 training acc: 0.40625
Global Iter: 94800 training loss: 0.671891
Global Iter: 94800 training acc: 0.6875
Global Iter: 94900 training loss: 0.687831
Global Iter: 94900 training acc: 0.53125
Global Iter: 95000 training loss: 0.689025
Global Iter: 95000 training acc: 0.53125
Global Iter: 95100 training loss: 0.700755
Global Iter: 95100 training acc: 0.4375
Global Iter: 95200 training loss: 0.68854
Global Iter: 95200 training acc: 0.5625
Global Iter: 95300 training loss: 0.688161
Global Iter: 95300 training acc: 0.5625
Global Iter: 95400 training loss: 0.694349
Global Iter: 95400 training acc: 0.5
Global Iter: 95500 training loss: 0.7146
Global Iter: 95500 training acc: 0.34375
Global Iter: 95600 training loss: 0.702523
Global Iter: 95600 training acc: 0.5
Global Iter: 95700 training loss: 0.696982
Global Iter: 95700 training acc: 0.5
Global Iter: 95800 training loss: 0.710469
Global Iter: 95800 training acc: 0.40625
Global Iter: 95900 training loss: 0.686393
Global Iter: 95900 training acc: 0.5625
Global Iter: 96000 training loss: 0.700783
Global Iter: 96000 training acc: 0.46875
Global Iter: 96100 training loss: 0.69909
Global Iter: 96100 training acc: 0.46875
Global Iter: 96200 training loss: 0.686735
Global Iter: 96200 training acc: 0.5625
Global Iter: 96300 training loss: 0.678732
Global Iter: 96300 training acc: 0.625
Global Iter: 96400 training loss: 0.709251
Global Iter: 96400 training acc: 0.40625
Global Iter: 96500 training loss: 0.70258
Global Iter: 96500 training acc: 0.4375
Global Iter: 96600 training loss: 0.670486
Global Iter: 96600 training acc: 0.6875
Global Iter: 96700 training loss: 0.702826
Global Iter: 96700 training acc: 0.46875
Global Iter: 96800 training loss: 0.685321
Global Iter: 96800 training acc: 0.59375
Global Iter: 96900 training loss: 0.689345
Global Iter: 96900 training acc: 0.5625
Global Iter: 97000 training loss: 0.699332
Global Iter: 97000 training acc: 0.46875
Global Iter: 97100 training loss: 0.698916
Global Iter: 97100 training acc: 0.46875
Global Iter: 97200 training loss: 0.682467
Global Iter: 97200 training acc: 0.59375
Global Iter: 97300 training loss: 0.701271
Global Iter: 97300 training acc: 0.4375
Global Iter: 97400 training loss: 0.666957
Global Iter: 97400 training acc: 0.71875
Global Iter: 97500 training loss: 0.69407
Global Iter: 97500 training acc: 0.53125
Global Iter: 97600 training loss: 0.70569
Global Iter: 97600 training acc: 0.40625
Global Iter: 97700 training loss: 0.685677
Global Iter: 97700 training acc: 0.59375
Global Iter: 97800 training loss: 0.679714
Global Iter: 97800 training acc: 0.59375
Global Iter: 97900 training loss: 0.682141
Global Iter: 97900 training acc: 0.59375
Global Iter: 98000 training loss: 0.691492
Global Iter: 98000 training acc: 0.53125
Global Iter: 98100 training loss: 0.670903
Global Iter: 98100 training acc: 0.6875
Global Iter: 98200 training loss: 0.681484
Global Iter: 98200 training acc: 0.59375
Global Iter: 98300 training loss: 0.693962
Global Iter: 98300 training acc: 0.5
Global Iter: 98400 training loss: 0.710631
Global Iter: 98400 training acc: 0.375
Global Iter: 98500 training loss: 0.696188
Global Iter: 98500 training acc: 0.5
Global Iter: 98600 training loss: 0.679774
Global Iter: 98600 training acc: 0.625
Global Iter: 98700 training loss: 0.707938
Global Iter: 98700 training acc: 0.40625
Global Iter: 98800 training loss: 0.696044
Global Iter: 98800 training acc: 0.5
Global Iter: 98900 training loss: 0.683977
Global Iter: 98900 training acc: 0.59375
Global Iter: 99000 training loss: 0.677819
Global Iter: 99000 training acc: 0.625
Global Iter: 99100 training loss: 0.707228
Global Iter: 99100 training acc: 0.4375
Global Iter: 99200 training loss: 0.671531
Global Iter: 99200 training acc: 0.6875
Global Iter: 99300 training loss: 0.692707
Global Iter: 99300 training acc: 0.53125
Global Iter: 99400 training loss: 0.682105
Global Iter: 99400 training acc: 0.59375
Global Iter: 99500 training loss: 0.674071
Global Iter: 99500 training acc: 0.6875
Global Iter: 99600 training loss: 0.675801
Global Iter: 99600 training acc: 0.65625
Global Iter: 99700 training loss: 0.689263
Global Iter: 99700 training acc: 0.59375
Global Iter: 99800 training loss: 0.690812
Global Iter: 99800 training acc: 0.53125
Global Iter: 99900 training loss: 0.686437
Global Iter: 99900 training acc: 0.5625
Global Iter: 100000 training loss: 0.687947
Global Iter: 100000 training acc: 0.53125
Global Iter: 100100 training loss: 0.699012
Global Iter: 100100 training acc: 0.46875
Global Iter: 100200 training loss: 0.713911
Global Iter: 100200 training acc: 0.4375
Global Iter: 100300 training loss: 0.675296
Global Iter: 100300 training acc: 0.65625
Global Iter: 100400 training loss: 0.699924
Global Iter: 100400 training acc: 0.46875
Global Iter: 100500 training loss: 0.717342
Global Iter: 100500 training acc: 0.375
Global Iter: 100600 training loss: 0.668307
Global Iter: 100600 training acc: 0.71875
Global Iter: 100700 training loss: 0.694852
Global Iter: 100700 training acc: 0.5
Global Iter: 100800 training loss: 0.698618
Global Iter: 100800 training acc: 0.5
Global Iter: 100900 training loss: 0.685898
Global Iter: 100900 training acc: 0.5625
Global Iter: 101000 training loss: 0.670081
Global Iter: 101000 training acc: 0.6875
Global Iter: 101100 training loss: 0.708014
Global Iter: 101100 training acc: 0.40625
Global Iter: 101200 training loss: 0.686893
Global Iter: 101200 training acc: 0.5625
Global Iter: 101300 training loss: 0.699522
Global Iter: 101300 training acc: 0.46875
Global Iter: 101400 training loss: 0.704235
Global Iter: 101400 training acc: 0.4375
Global Iter: 101500 training loss: 0.691535
Global Iter: 101500 training acc: 0.53125
Global Iter: 101600 training loss: 0.682902
Global Iter: 101600 training acc: 0.59375
Global Iter: 101700 training loss: 0.694441
Global Iter: 101700 training acc: 0.53125
Global Iter: 101800 training loss: 0.69242
Global Iter: 101800 training acc: 0.53125
Global Iter: 101900 training loss: 0.683848
Global Iter: 101900 training acc: 0.59375
Global Iter: 102000 training loss: 0.689456
Global Iter: 102000 training acc: 0.53125
Global Iter: 102100 training loss: 0.686782
Global Iter: 102100 training acc: 0.5625
Global Iter: 102200 training loss: 0.689771
Global Iter: 102200 training acc: 0.53125
Global Iter: 102300 training loss: 0.686063
Global Iter: 102300 training acc: 0.59375
Global Iter: 102400 training loss: 0.68306
Global Iter: 102400 training acc: 0.625
Global Iter: 102500 training loss: 0.676548
Global Iter: 102500 training acc: 0.625
Global Iter: 102600 training loss: 0.702062
Global Iter: 102600 training acc: 0.4375
Global Iter: 102700 training loss: 0.694184
Global Iter: 102700 training acc: 0.5
Global Iter: 102800 training loss: 0.695625
Global Iter: 102800 training acc: 0.5
Global Iter: 102900 training loss: 0.673711
Global Iter: 102900 training acc: 0.6875
Global Iter: 103000 training loss: 0.689346
Global Iter: 103000 training acc: 0.53125
Global Iter: 103100 training loss: 0.697159
Global Iter: 103100 training acc: 0.5
Global Iter: 103200 training loss: 0.691292
Global Iter: 103200 training acc: 0.5625
Global Iter: 103300 training loss: 0.71197
Global Iter: 103300 training acc: 0.375
Global Iter: 103400 training loss: 0.693672
Global Iter: 103400 training acc: 0.5
Global Iter: 103500 training loss: 0.672431
Global Iter: 103500 training acc: 0.65625
Global Iter: 103600 training loss: 0.68664
Global Iter: 103600 training acc: 0.5625
Global Iter: 103700 training loss: 0.698127
Global Iter: 103700 training acc: 0.46875
Global Iter: 103800 training loss: 0.696157
Global Iter: 103800 training acc: 0.5
Global Iter: 103900 training loss: 0.680558
Global Iter: 103900 training acc: 0.625
Global Iter: 104000 training loss: 0.687918
Global Iter: 104000 training acc: 0.5625
Global Iter: 104100 training loss: 0.692839
Global Iter: 104100 training acc: 0.5
Global Iter: 104200 training loss: 0.697425
Global Iter: 104200 training acc: 0.5
Global Iter: 104300 training loss: 0.711519
Global Iter: 104300 training acc: 0.375
Global Iter: 104400 training loss: 0.678014
Global Iter: 104400 training acc: 0.625
Global Iter: 104500 training loss: 0.67536
Global Iter: 104500 training acc: 0.65625
Global Iter: 104600 training loss: 0.705075
Global Iter: 104600 training acc: 0.40625
Global Iter: 104700 training loss: 0.676885
Global Iter: 104700 training acc: 0.65625
Global Iter: 104800 training loss: 0.690148
Global Iter: 104800 training acc: 0.53125
Global Iter: 104900 training loss: 0.690608
Global Iter: 104900 training acc: 0.53125
Global Iter: 105000 training loss: 0.701692
Global Iter: 105000 training acc: 0.4375
Global Iter: 105100 training loss: 0.685714
Global Iter: 105100 training acc: 0.5625
Global Iter: 105200 training loss: 0.687317
Global Iter: 105200 training acc: 0.5625
Global Iter: 105300 training loss: 0.695889
Global Iter: 105300 training acc: 0.5
Global Iter: 105400 training loss: 0.708965
Global Iter: 105400 training acc: 0.375
Global Iter: 105500 training loss: 0.694323
Global Iter: 105500 training acc: 0.5
Global Iter: 105600 training loss: 0.690682
Global Iter: 105600 training acc: 0.53125
Global Iter: 105700 training loss: 0.703411
Global Iter: 105700 training acc: 0.4375
Global Iter: 105800 training loss: 0.683854
Global Iter: 105800 training acc: 0.59375
Global Iter: 105900 training loss: 0.703087
Global Iter: 105900 training acc: 0.4375
Global Iter: 106000 training loss: 0.704147
Global Iter: 106000 training acc: 0.4375
Global Iter: 106100 training loss: 0.685373
Global Iter: 106100 training acc: 0.59375
Global Iter: 106200 training loss: 0.684326
Global Iter: 106200 training acc: 0.5625
Global Iter: 106300 training loss: 0.708114
Global Iter: 106300 training acc: 0.40625
Global Iter: 106400 training loss: 0.69663
Global Iter: 106400 training acc: 0.5
Global Iter: 106500 training loss: 0.665661
Global Iter: 106500 training acc: 0.71875
Global Iter: 106600 training loss: 0.694483
Global Iter: 106600 training acc: 0.5
Global Iter: 106700 training loss: 0.683594
Global Iter: 106700 training acc: 0.59375
Global Iter: 106800 training loss: 0.690416
Global Iter: 106800 training acc: 0.53125
Global Iter: 106900 training loss: 0.709118
Global Iter: 106900 training acc: 0.40625
Global Iter: 107000 training loss: 0.69546
Global Iter: 107000 training acc: 0.5
Global Iter: 107100 training loss: 0.683618
Global Iter: 107100 training acc: 0.59375
Global Iter: 107200 training loss: 0.704009
Global Iter: 107200 training acc: 0.4375
Global Iter: 107300 training loss: 0.666614
Global Iter: 107300 training acc: 0.71875
Global Iter: 107400 training loss: 0.692223
Global Iter: 107400 training acc: 0.53125
Global Iter: 107500 training loss: 0.708693
Global Iter: 107500 training acc: 0.40625
Global Iter: 107600 training loss: 0.679765
Global Iter: 107600 training acc: 0.625
Global Iter: 107700 training loss: 0.681229
Global Iter: 107700 training acc: 0.625
Global Iter: 107800 training loss: 0.68493
Global Iter: 107800 training acc: 0.59375
Global Iter: 107900 training loss: 0.693375
Global Iter: 107900 training acc: 0.5
Global Iter: 108000 training loss: 0.67872
Global Iter: 108000 training acc: 0.65625
Global Iter: 108100 training loss: 0.675207
Global Iter: 108100 training acc: 0.625
Global Iter: 108200 training loss: 0.696958
Global Iter: 108200 training acc: 0.46875
Global Iter: 108300 training loss: 0.720158
Global Iter: 108300 training acc: 0.34375
Global Iter: 108400 training loss: 0.696608
Global Iter: 108400 training acc: 0.5
Global Iter: 108500 training loss: 0.684097
Global Iter: 108500 training acc: 0.625
Global Iter: 108600 training loss: 0.696242
Global Iter: 108600 training acc: 0.46875
Global Iter: 108700 training loss: 0.694167
Global Iter: 108700 training acc: 0.5
Global Iter: 108800 training loss: 0.686455
Global Iter: 108800 training acc: 0.5625
Global Iter: 108900 training loss: 0.683507
Global Iter: 108900 training acc: 0.59375
Global Iter: 109000 training loss: 0.702184
Global Iter: 109000 training acc: 0.4375
Global Iter: 109100 training loss: 0.676915
Global Iter: 109100 training acc: 0.65625
Global Iter: 109200 training loss: 0.690438
Global Iter: 109200 training acc: 0.53125
Global Iter: 109300 training loss: 0.693823
Global Iter: 109300 training acc: 0.53125
Global Iter: 109400 training loss: 0.666139
Global Iter: 109400 training acc: 0.71875
Global Iter: 109500 training loss: 0.680983
Global Iter: 109500 training acc: 0.625
Global Iter: 109600 training loss: 0.684096
Global Iter: 109600 training acc: 0.59375
Global Iter: 109700 training loss: 0.691722
Global Iter: 109700 training acc: 0.53125
Global Iter: 109800 training loss: 0.692998
Global Iter: 109800 training acc: 0.5
Global Iter: 109900 training loss: 0.699667
Global Iter: 109900 training acc: 0.5
Global Iter: 110000 training loss: 0.702401
Global Iter: 110000 training acc: 0.4375
Global Iter: 110100 training loss: 0.701728
Global Iter: 110100 training acc: 0.4375
Global Iter: 110200 training loss: 0.675185
Global Iter: 110200 training acc: 0.65625
Global Iter: 110300 training loss: 0.701067
Global Iter: 110300 training acc: 0.46875
Global Iter: 110400 training loss: 0.717811
Global Iter: 110400 training acc: 0.34375
Global Iter: 110500 training loss: 0.671286
Global Iter: 110500 training acc: 0.6875
Global Iter: 110600 training loss: 0.700115
Global Iter: 110600 training acc: 0.46875
Global Iter: 110700 training loss: 0.687349
Global Iter: 110700 training acc: 0.5625
Global Iter: 110800 training loss: 0.684747
Global Iter: 110800 training acc: 0.5625
Global Iter: 110900 training loss: 0.669274
Global Iter: 110900 training acc: 0.6875
Global Iter: 111000 training loss: 0.702815
Global Iter: 111000 training acc: 0.4375
Global Iter: 111100 training loss: 0.686834
Global Iter: 111100 training acc: 0.5625
Global Iter: 111200 training loss: 0.701084
Global Iter: 111200 training acc: 0.4375
Global Iter: 111300 training loss: 0.70989
Global Iter: 111300 training acc: 0.40625
Global Iter: 111400 training loss: 0.698387
Global Iter: 111400 training acc: 0.5
Global Iter: 111500 training loss: 0.680745
Global Iter: 111500 training acc: 0.625
Global Iter: 111600 training loss: 0.692778
Global Iter: 111600 training acc: 0.53125
Global Iter: 111700 training loss: 0.68802
Global Iter: 111700 training acc: 0.5625
Global Iter: 111800 training loss: 0.679959
Global Iter: 111800 training acc: 0.625
Global Iter: 111900 training loss: 0.683729
Global Iter: 111900 training acc: 0.5625
Global Iter: 112000 training loss: 0.68526
Global Iter: 112000 training acc: 0.5625
Global Iter: 112100 training loss: 0.694717
Global Iter: 112100 training acc: 0.5
Global Iter: 112200 training loss: 0.680652
Global Iter: 112200 training acc: 0.625
Global Iter: 112300 training loss: 0.68093
Global Iter: 112300 training acc: 0.625
Global Iter: 112400 training loss: 0.67832
Global Iter: 112400 training acc: 0.65625
Global Iter: 112500 training loss: 0.712761
Global Iter: 112500 training acc: 0.375
Global Iter: 112600 training loss: 0.698908
Global Iter: 112600 training acc: 0.46875
Global Iter: 112700 training loss: 0.698735
Global Iter: 112700 training acc: 0.46875
Global Iter: 112800 training loss: 0.669646
Global Iter: 112800 training acc: 0.6875
Global Iter: 112900 training loss: 0.69541
Global Iter: 112900 training acc: 0.5
Global Iter: 113000 training loss: 0.695411
Global Iter: 113000 training acc: 0.5
Global Iter: 113100 training loss: 0.685015
Global Iter: 113100 training acc: 0.5625
Global Iter: 113200 training loss: 0.717014
Global Iter: 113200 training acc: 0.3125
Global Iter: 113300 training loss: 0.694841
Global Iter: 113300 training acc: 0.5
Global Iter: 113400 training loss: 0.665088
Global Iter: 113400 training acc: 0.71875
Global Iter: 113500 training loss: 0.686757
Global Iter: 113500 training acc: 0.5625
Global Iter: 113600 training loss: 0.69832
Global Iter: 113600 training acc: 0.46875
Global Iter: 113700 training loss: 0.68968
Global Iter: 113700 training acc: 0.53125
Global Iter: 113800 training loss: 0.676927
Global Iter: 113800 training acc: 0.65625
Global Iter: 113900 training loss: 0.688383
Global Iter: 113900 training acc: 0.5625
Global Iter: 114000 training loss: 0.688729
Global Iter: 114000 training acc: 0.5625
Global Iter: 114100 training loss: 0.689605
Global Iter: 114100 training acc: 0.53125
Global Iter: 114200 training loss: 0.710295
Global Iter: 114200 training acc: 0.40625
Global Iter: 114300 training loss: 0.681926
Global Iter: 114300 training acc: 0.59375
Global Iter: 114400 training loss: 0.678483
Global Iter: 114400 training acc: 0.625
Global Iter: 114500 training loss: 0.701109
Global Iter: 114500 training acc: 0.46875
Global Iter: 114600 training loss: 0.67996
Global Iter: 114600 training acc: 0.625
Global Iter: 114700 training loss: 0.689185
Global Iter: 114700 training acc: 0.53125
Global Iter: 114800 training loss: 0.697188
Global Iter: 114800 training acc: 0.5
Global Iter: 114900 training loss: 0.698475
Global Iter: 114900 training acc: 0.46875
Global Iter: 115000 training loss: 0.693585
Global Iter: 115000 training acc: 0.53125
Global Iter: 115100 training loss: 0.678425
Global Iter: 115100 training acc: 0.625
Global Iter: 115200 training loss: 0.692476
Global Iter: 115200 training acc: 0.5
Global Iter: 115300 training loss: 0.703045
Global Iter: 115300 training acc: 0.4375
Global Iter: 115400 training loss: 0.704732
Global Iter: 115400 training acc: 0.46875
Global Iter: 115500 training loss: 0.697379
Global Iter: 115500 training acc: 0.5
Global Iter: 115600 training loss: 0.700815
Global Iter: 115600 training acc: 0.4375
Global Iter: 115700 training loss: 0.688671
Global Iter: 115700 training acc: 0.53125
Global Iter: 115800 training loss: 0.700032
Global Iter: 115800 training acc: 0.46875
Global Iter: 115900 training loss: 0.704225
Global Iter: 115900 training acc: 0.4375
Global Iter: 116000 training loss: 0.684911
Global Iter: 116000 training acc: 0.59375
Global Iter: 116100 training loss: 0.689176
Global Iter: 116100 training acc: 0.53125
Global Iter: 116200 training loss: 0.712301
Global Iter: 116200 training acc: 0.40625
Global Iter: 116300 training loss: 0.692981
Global Iter: 116300 training acc: 0.53125
Global Iter: 116400 training loss: 0.67053
Global Iter: 116400 training acc: 0.6875
Global Iter: 116500 training loss: 0.687847
Global Iter: 116500 training acc: 0.53125
Global Iter: 116600 training loss: 0.690152
Global Iter: 116600 training acc: 0.53125
Global Iter: 116700 training loss: 0.683916
Global Iter: 116700 training acc: 0.53125
Global Iter: 116800 training loss: 0.70848
Global Iter: 116800 training acc: 0.40625
Global Iter: 116900 training loss: 0.696587
Global Iter: 116900 training acc: 0.5
Global Iter: 117000 training loss: 0.685151
Global Iter: 117000 training acc: 0.5625
Global Iter: 117100 training loss: 0.694495
Global Iter: 117100 training acc: 0.46875
Global Iter: 117200 training loss: 0.667153
Global Iter: 117200 training acc: 0.71875
Global Iter: 117300 training loss: 0.696239
Global Iter: 117300 training acc: 0.5
Global Iter: 117400 training loss: 0.7022
Global Iter: 117400 training acc: 0.46875
Global Iter: 117500 training loss: 0.671174
Global Iter: 117500 training acc: 0.65625
Global Iter: 117600 training loss: 0.674641
Global Iter: 117600 training acc: 0.65625
Global Iter: 117700 training loss: 0.690323
Global Iter: 117700 training acc: 0.53125
Global Iter: 117800 training loss: 0.695071
Global Iter: 117800 training acc: 0.5
Global Iter: 117900 training loss: 0.679956
Global Iter: 117900 training acc: 0.625
Global Iter: 118000 training loss: 0.680048
Global Iter: 118000 training acc: 0.625
Global Iter: 118100 training loss: 0.696541
Global Iter: 118100 training acc: 0.5
Global Iter: 118200 training loss: 0.713382
Global Iter: 118200 training acc: 0.34375
Global Iter: 118300 training loss: 0.695949
Global Iter: 118300 training acc: 0.5
Global Iter: 118400 training loss: 0.677752
Global Iter: 118400 training acc: 0.625
Global Iter: 118500 training loss: 0.700242
Global Iter: 118500 training acc: 0.46875
Global Iter: 118600 training loss: 0.695165
Global Iter: 118600 training acc: 0.5
Global Iter: 118700 training loss: 0.687072
Global Iter: 118700 training acc: 0.5625
Global Iter: 118800 training loss: 0.678019
Global Iter: 118800 training acc: 0.625
Global Iter: 118900 training loss: 0.701734
Global Iter: 118900 training acc: 0.4375
Global Iter: 119000 training loss: 0.678427
Global Iter: 119000 training acc: 0.625
Global Iter: 119100 training loss: 0.693392
Global Iter: 119100 training acc: 0.53125
Global Iter: 119200 training loss: 0.688794
Global Iter: 119200 training acc: 0.5625
Global Iter: 119300 training loss: 0.672754
Global Iter: 119300 training acc: 0.6875
Global Iter: 119400 training loss: 0.684181
Global Iter: 119400 training acc: 0.59375
Global Iter: 119500 training loss: 0.680061
Global Iter: 119500 training acc: 0.59375
Global Iter: 119600 training loss: 0.688718
Global Iter: 119600 training acc: 0.53125
Global Iter: 119700 training loss: 0.703759
Global Iter: 119700 training acc: 0.4375
Global Iter: 119800 training loss: 0.69032
Global Iter: 119800 training acc: 0.53125
Global Iter: 119900 training loss: 0.703966
Global Iter: 119900 training acc: 0.4375
Global Iter: 120000 training loss: 0.711785
Global Iter: 120000 training acc: 0.375
Global Iter: 120100 training loss: 0.676115
Global Iter: 120100 training acc: 0.65625
Global Iter: 120200 training loss: 0.699848
Global Iter: 120200 training acc: 0.46875
Global Iter: 120300 training loss: 0.711154
Global Iter: 120300 training acc: 0.375
Global Iter: 120400 training loss: 0.667584
Global Iter: 120400 training acc: 0.71875
Global Iter: 120500 training loss: 0.703317
Global Iter: 120500 training acc: 0.4375
Global Iter: 120600 training loss: 0.687913
Global Iter: 120600 training acc: 0.5625
Global Iter: 120700 training loss: 0.69159
Global Iter: 120700 training acc: 0.5625
Global Iter: 120800 training loss: 0.67298
Global Iter: 120800 training acc: 0.6875
Global Iter: 120900 training loss: 0.698599
Global Iter: 120900 training acc: 0.46875
Global Iter: 121000 training loss: 0.692672
Global Iter: 121000 training acc: 0.5
Global Iter: 121100 training loss: 0.704788
Global Iter: 121100 training acc: 0.4375
Global Iter: 121200 training loss: 0.709783
Global Iter: 121200 training acc: 0.40625
Global Iter: 121300 training loss: 0.697094
Global Iter: 121300 training acc: 0.5
Global Iter: 121400 training loss: 0.686243
Global Iter: 121400 training acc: 0.59375
Global Iter: 121500 training loss: 0.701554
Global Iter: 121500 training acc: 0.5
Global Iter: 121600 training loss: 0.695301
Global Iter: 121600 training acc: 0.5
Global Iter: 121700 training loss: 0.685923
Global Iter: 121700 training acc: 0.59375
Global Iter: 121800 training loss: 0.690967
Global Iter: 121800 training acc: 0.53125
Global Iter: 121900 training loss: 0.689085
Global Iter: 121900 training acc: 0.5625
Global Iter: 122000 training loss: 0.695091
Global Iter: 122000 training acc: 0.5
Global Iter: 122100 training loss: 0.671833
Global Iter: 122100 training acc: 0.6875
Global Iter: 122200 training loss: 0.677622
Global Iter: 122200 training acc: 0.65625
Global Iter: 122300 training loss: 0.67683
Global Iter: 122300 training acc: 0.65625
Global Iter: 122400 training loss: 0.714171
Global Iter: 122400 training acc: 0.34375
Global Iter: 122500 training loss: 0.692237
Global Iter: 122500 training acc: 0.53125
Global Iter: 122600 training loss: 0.696827
Global Iter: 122600 training acc: 0.46875
Global Iter: 122700 training loss: 0.678757
Global Iter: 122700 training acc: 0.625
Global Iter: 122800 training loss: 0.699475
Global Iter: 122800 training acc: 0.46875
Global Iter: 122900 training loss: 0.69766
Global Iter: 122900 training acc: 0.46875
Global Iter: 123000 training loss: 0.690736
Global Iter: 123000 training acc: 0.53125
Global Iter: 123100 training loss: 0.722874
Global Iter: 123100 training acc: 0.28125
Global Iter: 123200 training loss: 0.695745
Global Iter: 123200 training acc: 0.5
Global Iter: 123300 training loss: 0.665699
Global Iter: 123300 training acc: 0.71875
Global Iter: 123400 training loss: 0.674722
Global Iter: 123400 training acc: 0.625
Global Iter: 123500 training loss: 0.701153
Global Iter: 123500 training acc: 0.4375
Global Iter: 123600 training loss: 0.690029
Global Iter: 123600 training acc: 0.5625
Global Iter: 123700 training loss: 0.678727
Global Iter: 123700 training acc: 0.625
Global Iter: 123800 training loss: 0.691416
Global Iter: 123800 training acc: 0.53125
Global Iter: 123900 training loss: 0.688877
Global Iter: 123900 training acc: 0.53125
Global Iter: 124000 training loss: 0.692971
Global Iter: 124000 training acc: 0.5
Global Iter: 124100 training loss: 0.70829
Global Iter: 124100 training acc: 0.40625
Global Iter: 124200 training loss: 0.686906
Global Iter: 124200 training acc: 0.5625
Global Iter: 124300 training loss: 0.682073
Global Iter: 124300 training acc: 0.59375
Global Iter: 124400 training loss: 0.689241
Global Iter: 124400 training acc: 0.53125
Global Iter: 124500 training loss: 0.677547
Global Iter: 124500 training acc: 0.65625
Global Iter: 124600 training loss: 0.692158
Global Iter: 124600 training acc: 0.5
Global Iter: 124700 training loss: 0.692508
Global Iter: 124700 training acc: 0.53125
Global Iter: 124800 training loss: 0.691547
Global Iter: 124800 training acc: 0.53125
Global Iter: 124900 training loss: 0.693424
Global Iter: 124900 training acc: 0.5
Global Iter: 125000 training loss: 0.683291
Global Iter: 125000 training acc: 0.59375
Global Iter: 125100 training loss: 0.695488
Global Iter: 125100 training acc: 0.5
Global Iter: 125200 training loss: 0.712345
Global Iter: 125200 training acc: 0.375
Global Iter: 125300 training loss: 0.700148
Global Iter: 125300 training acc: 0.46875
Global Iter: 125400 training loss: 0.697026
Global Iter: 125400 training acc: 0.5
Global Iter: 125500 training loss: 0.704237
Global Iter: 125500 training acc: 0.4375
Global Iter: 125600 training loss: 0.682978
Global Iter: 125600 training acc: 0.59375
Global Iter: 125700 training loss: 0.698034
Global Iter: 125700 training acc: 0.46875
Global Iter: 125800 training loss: 0.701364
Global Iter: 125800 training acc: 0.46875
Global Iter: 125900 training loss: 0.687377
Global Iter: 125900 training acc: 0.59375
Global Iter: 126000 training loss: 0.693067
Global Iter: 126000 training acc: 0.5
Global Iter: 126100 training loss: 0.704322
Global Iter: 126100 training acc: 0.4375
Global Iter: 126200 training loss: 0.687876
Global Iter: 126200 training acc: 0.5625
Global Iter: 126300 training loss: 0.6731
Global Iter: 126300 training acc: 0.6875
Global Iter: 126400 training loss: 0.698549
Global Iter: 126400 training acc: 0.46875
Global Iter: 126500 training loss: 0.692576
Global Iter: 126500 training acc: 0.5
Global Iter: 126600 training loss: 0.69676
Global Iter: 126600 training acc: 0.5
Global Iter: 126700 training loss: 0.70719
Global Iter: 126700 training acc: 0.40625
Global Iter: 126800 training loss: 0.695594
Global Iter: 126800 training acc: 0.5
Global Iter: 126900 training loss: 0.683526
Global Iter: 126900 training acc: 0.59375
Global Iter: 127000 training loss: 0.698827
Global Iter: 127000 training acc: 0.46875
Global Iter: 127100 training loss: 0.672116
Global Iter: 127100 training acc: 0.6875
Global Iter: 127200 training loss: 0.696563
Global Iter: 127200 training acc: 0.5
Global Iter: 127300 training loss: 0.698049
Global Iter: 127300 training acc: 0.46875
Global Iter: 127400 training loss: 0.675477
Global Iter: 127400 training acc: 0.65625
Global Iter: 127500 training loss: 0.681518
Global Iter: 127500 training acc: 0.59375
Global Iter: 127600 training loss: 0.690944
Global Iter: 127600 training acc: 0.53125
Global Iter: 127700 training loss: 0.691868
Global Iter: 127700 training acc: 0.53125
Global Iter: 127800 training loss: 0.671488
Global Iter: 127800 training acc: 0.6875
Global Iter: 127900 training loss: 0.688962
Global Iter: 127900 training acc: 0.5625
Global Iter: 128000 training loss: 0.69661
Global Iter: 128000 training acc: 0.5
Global Iter: 128100 training loss: 0.715678
Global Iter: 128100 training acc: 0.34375
Global Iter: 128200 training loss: 0.690068
Global Iter: 128200 training acc: 0.53125
Global Iter: 128300 training loss: 0.677264
Global Iter: 128300 training acc: 0.625
Global Iter: 128400 training loss: 0.699796
Global Iter: 128400 training acc: 0.46875
Global Iter: 128500 training loss: 0.695169
Global Iter: 128500 training acc: 0.5
Global Iter: 128600 training loss: 0.690232
Global Iter: 128600 training acc: 0.53125
Global Iter: 128700 training loss: 0.678967
Global Iter: 128700 training acc: 0.625
Global Iter: 128800 training loss: 0.701958
Global Iter: 128800 training acc: 0.46875
Global Iter: 128900 training loss: 0.681868
Global Iter: 128900 training acc: 0.59375
Global Iter: 129000 training loss: 0.69103
Global Iter: 129000 training acc: 0.53125
Global Iter: 129100 training loss: 0.68701
Global Iter: 129100 training acc: 0.59375
Global Iter: 129200 training loss: 0.672635
Global Iter: 129200 training acc: 0.6875
Global Iter: 129300 training loss: 0.694076
Global Iter: 129300 training acc: 0.5
Global Iter: 129400 training loss: 0.683119
Global Iter: 129400 training acc: 0.59375
Global Iter: 129500 training loss: 0.692253
Global Iter: 129500 training acc: 0.5
Global Iter: 129600 training loss: 0.704402
Global Iter: 129600 training acc: 0.4375
Global Iter: 129700 training loss: 0.692937
Global Iter: 129700 training acc: 0.53125
Global Iter: 129800 training loss: 0.700001
Global Iter: 129800 training acc: 0.46875
Global Iter: 129900 training loss: 0.712468
Global Iter: 129900 training acc: 0.375
Global Iter: 130000 training loss: 0.674098
Global Iter: 130000 training acc: 0.6875
Global Iter: 130100 training loss: 0.697104
Global Iter: 130100 training acc: 0.46875
Global Iter: 130200 training loss: 0.709068
Global Iter: 130200 training acc: 0.375
Global Iter: 130300 training loss: 0.667966
Global Iter: 130300 training acc: 0.6875
Global Iter: 130400 training loss: 0.697205
Global Iter: 130400 training acc: 0.5
Global Iter: 130500 training loss: 0.687431
Global Iter: 130500 training acc: 0.5625
Global Iter: 130600 training loss: 0.685816
Global Iter: 130600 training acc: 0.5625
Global Iter: 130700 training loss: 0.67292
Global Iter: 130700 training acc: 0.6875
Global Iter: 130800 training loss: 0.69903
Global Iter: 130800 training acc: 0.46875
Global Iter: 130900 training loss: 0.695217
Global Iter: 130900 training acc: 0.53125
Global Iter: 131000 training loss: 0.702658
Global Iter: 131000 training acc: 0.4375
Global Iter: 131100 training loss: 0.696203
Global Iter: 131100 training acc: 0.46875
Global Iter: 131200 training loss: 0.691991
Global Iter: 131200 training acc: 0.53125
Global Iter: 131300 training loss: 0.687356
Global Iter: 131300 training acc: 0.5625
Global Iter: 131400 training loss: 0.695302
Global Iter: 131400 training acc: 0.5
Global Iter: 131500 training loss: 0.70321
Global Iter: 131500 training acc: 0.4375
Global Iter: 131600 training loss: 0.680044
Global Iter: 131600 training acc: 0.625
Global Iter: 131700 training loss: 0.686284
Global Iter: 131700 training acc: 0.5625
Global Iter: 131800 training loss: 0.688757
Global Iter: 131800 training acc: 0.5625
Global Iter: 131900 training loss: 0.688116
Global Iter: 131900 training acc: 0.5625
Global Iter: 132000 training loss: 0.665899
Global Iter: 132000 training acc: 0.71875
Global Iter: 132100 training loss: 0.675448
Global Iter: 132100 training acc: 0.65625
Global Iter: 132200 training loss: 0.67929
Global Iter: 132200 training acc: 0.625
Global Iter: 132300 training loss: 0.7097
Global Iter: 132300 training acc: 0.375
Global Iter: 132400 training loss: 0.694067
Global Iter: 132400 training acc: 0.5
Global Iter: 132500 training loss: 0.69725
Global Iter: 132500 training acc: 0.5
Global Iter: 132600 training loss: 0.674518
Global Iter: 132600 training acc: 0.65625
Global Iter: 132700 training loss: 0.703825
Global Iter: 132700 training acc: 0.4375
Global Iter: 132800 training loss: 0.700282
Global Iter: 132800 training acc: 0.4375
Global Iter: 132900 training loss: 0.701479
Global Iter: 132900 training acc: 0.46875
Global Iter: 133000 training loss: 0.720742
Global Iter: 133000 training acc: 0.3125
Global Iter: 133100 training loss: 0.69702
Global Iter: 133100 training acc: 0.5
Global Iter: 133200 training loss: 0.670714
Global Iter: 133200 training acc: 0.6875
Global Iter: 133300 training loss: 0.681606
Global Iter: 133300 training acc: 0.59375
Global Iter: 133400 training loss: 0.693768
Global Iter: 133400 training acc: 0.5
Global Iter: 133500 training loss: 0.687651
Global Iter: 133500 training acc: 0.5625
Global Iter: 133600 training loss: 0.676634
Global Iter: 133600 training acc: 0.65625
Global Iter: 133700 training loss: 0.69578
Global Iter: 133700 training acc: 0.5
Global Iter: 133800 training loss: 0.695993
Global Iter: 133800 training acc: 0.5
Global Iter: 133900 training loss: 0.688602
Global Iter: 133900 training acc: 0.53125
Global Iter: 134000 training loss: 0.703364
Global Iter: 134000 training acc: 0.4375
Global Iter: 134100 training loss: 0.695003
Global Iter: 134100 training acc: 0.5
Global Iter: 134200 training loss: 0.681073
Global Iter: 134200 training acc: 0.625
Global Iter: 134300 training loss: 0.697882
Global Iter: 134300 training acc: 0.5
Global Iter: 134400 training loss: 0.678159
Global Iter: 134400 training acc: 0.625
Global Iter: 134500 training loss: 0.697047
Global Iter: 134500 training acc: 0.5
Global Iter: 134600 training loss: 0.695591
Global Iter: 134600 training acc: 0.5
Global Iter: 134700 training loss: 0.693324
Global Iter: 134700 training acc: 0.5
Global Iter: 134800 training loss: 0.702639
Global Iter: 134800 training acc: 0.4375
Global Iter: 134900 training loss: 0.69163
Global Iter: 134900 training acc: 0.53125
Global Iter: 135000 training loss: 0.690592
Global Iter: 135000 training acc: 0.53125
Global Iter: 135100 training loss: 0.709905
Global Iter: 135100 training acc: 0.40625
Global Iter: 135200 training loss: 0.705295
Global Iter: 135200 training acc: 0.40625
Global Iter: 135300 training loss: 0.703358
Global Iter: 135300 training acc: 0.4375
Global Iter: 135400 training loss: 0.702148
Global Iter: 135400 training acc: 0.4375
Global Iter: 135500 training loss: 0.680544
Global Iter: 135500 training acc: 0.625
Global Iter: 135600 training loss: 0.701308
Global Iter: 135600 training acc: 0.4375
Global Iter: 135700 training loss: 0.704291
Global Iter: 135700 training acc: 0.4375
Global Iter: 135800 training loss: 0.679292
Global Iter: 135800 training acc: 0.625
Global Iter: 135900 training loss: 0.703708
Global Iter: 135900 training acc: 0.4375
Global Iter: 136000 training loss: 0.704841
Global Iter: 136000 training acc: 0.4375
Global Iter: 136100 training loss: 0.694737
Global Iter: 136100 training acc: 0.5
Global Iter: 136200 training loss: 0.675346
Global Iter: 136200 training acc: 0.65625
Global Iter: 136300 training loss: 0.694619
Global Iter: 136300 training acc: 0.5
Global Iter: 136400 training loss: 0.696567
Global Iter: 136400 training acc: 0.5
Global Iter: 136500 training loss: 0.698478
Global Iter: 136500 training acc: 0.5
Global Iter: 136600 training loss: 0.704009
Global Iter: 136600 training acc: 0.4375
Global Iter: 136700 training loss: 0.697798
Global Iter: 136700 training acc: 0.46875
Global Iter: 136800 training loss: 0.685591
Global Iter: 136800 training acc: 0.59375
Global Iter: 136900 training loss: 0.69708
Global Iter: 136900 training acc: 0.5
Global Iter: 137000 training loss: 0.672997
Global Iter: 137000 training acc: 0.65625
Global Iter: 137100 training loss: 0.691899
Global Iter: 137100 training acc: 0.53125
Global Iter: 137200 training loss: 0.70189
Global Iter: 137200 training acc: 0.4375
Global Iter: 137300 training loss: 0.673977
Global Iter: 137300 training acc: 0.65625
Global Iter: 137400 training loss: 0.686923
Global Iter: 137400 training acc: 0.5625
Global Iter: 137500 training loss: 0.681379
Global Iter: 137500 training acc: 0.59375
Global Iter: 137600 training loss: 0.687705
Global Iter: 137600 training acc: 0.53125
Global Iter: 137700 training loss: 0.676919
Global Iter: 137700 training acc: 0.625
Global Iter: 137800 training loss: 0.682801
Global Iter: 137800 training acc: 0.59375
Global Iter: 137900 training loss: 0.694671
Global Iter: 137900 training acc: 0.5
Global Iter: 138000 training loss: 0.713065
Global Iter: 138000 training acc: 0.34375
Global Iter: 138100 training loss: 0.690716
Global Iter: 138100 training acc: 0.53125
Global Iter: 138200 training loss: 0.683885
Global Iter: 138200 training acc: 0.59375
Global Iter: 138300 training loss: 0.700865
Global Iter: 138300 training acc: 0.46875
Global Iter: 138400 training loss: 0.683902
Global Iter: 138400 training acc: 0.5625
Global Iter: 138500 training loss: 0.698828
Global Iter: 138500 training acc: 0.46875
Global Iter: 138600 training loss: 0.680313
Global Iter: 138600 training acc: 0.625
Global Iter: 138700 training loss: 0.701206
Global Iter: 138700 training acc: 0.46875
Global Iter: 138800 training loss: 0.678914
Global Iter: 138800 training acc: 0.625
Global Iter: 138900 training loss: 0.700645
Global Iter: 138900 training acc: 0.46875
Global Iter: 139000 training loss: 0.688383
Global Iter: 139000 training acc: 0.5625
Global Iter: 139100 training loss: 0.675073
Global Iter: 139100 training acc: 0.65625
Global Iter: 139200 training loss: 0.691298
Global Iter: 139200 training acc: 0.53125
Global Iter: 139300 training loss: 0.687393
Global Iter: 139300 training acc: 0.5625
Global Iter: 139400 training loss: 0.69847
Global Iter: 139400 training acc: 0.46875
Global Iter: 139500 training loss: 0.704003
Global Iter: 139500 training acc: 0.4375
Global Iter: 139600 training loss: 0.690601
Global Iter: 139600 training acc: 0.53125
Global Iter: 139700 training loss: 0.697828
Global Iter: 139700 training acc: 0.5
Global Iter: 139800 training loss: 0.704182
Global Iter: 139800 training acc: 0.4375
Global Iter: 139900 training loss: 0.670945
Global Iter: 139900 training acc: 0.6875
Global Iter: 140000 training loss: 0.704862
Global Iter: 140000 training acc: 0.4375
Global Iter: 140100 training loss: 0.710758
Global Iter: 140100 training acc: 0.375
Global Iter: 140200 training loss: 0.666008
Global Iter: 140200 training acc: 0.71875
Global Iter: 140300 training loss: 0.693671
Global Iter: 140300 training acc: 0.5
Global Iter: 140400 training loss: 0.688933
Global Iter: 140400 training acc: 0.5625
Global Iter: 140500 training loss: 0.683118
Global Iter: 140500 training acc: 0.59375
Global Iter: 140600 training loss: 0.67424
Global Iter: 140600 training acc: 0.65625
Global Iter: 140700 training loss: 0.703497
Global Iter: 140700 training acc: 0.4375
Global Iter: 140800 training loss: 0.690711
Global Iter: 140800 training acc: 0.53125
Global Iter: 140900 training loss: 0.698479
Global Iter: 140900 training acc: 0.46875
Global Iter: 141000 training loss: 0.700885
Global Iter: 141000 training acc: 0.46875
Global Iter: 141100 training loss: 0.688217
Global Iter: 141100 training acc: 0.5625
Global Iter: 141200 training loss: 0.686687
Global Iter: 141200 training acc: 0.5625
Global Iter: 141300 training loss: 0.684575
Global Iter: 141300 training acc: 0.5625
Global Iter: 141400 training loss: 0.698759
Global Iter: 141400 training acc: 0.46875
Global Iter: 141500 training loss: 0.681815
Global Iter: 141500 training acc: 0.625
Global Iter: 141600 training loss: 0.687912
Global Iter: 141600 training acc: 0.5625
Global Iter: 141700 training loss: 0.686835
Global Iter: 141700 training acc: 0.59375
Global Iter: 141800 training loss: 0.690668
Global Iter: 141800 training acc: 0.53125
Global Iter: 141900 training loss: 0.668408
Global Iter: 141900 training acc: 0.71875
Global Iter: 142000 training loss: 0.676246
Global Iter: 142000 training acc: 0.65625
Global Iter: 142100 training loss: 0.680011
Global Iter: 142100 training acc: 0.625
Global Iter: 142200 training loss: 0.706372
Global Iter: 142200 training acc: 0.40625
Global Iter: 142300 training loss: 0.693759
Global Iter: 142300 training acc: 0.5
Global Iter: 142400 training loss: 0.69531
Global Iter: 142400 training acc: 0.5
Global Iter: 142500 training loss: 0.678734
Global Iter: 142500 training acc: 0.65625
Global Iter: 142600 training loss: 0.690977
Global Iter: 142600 training acc: 0.5
Global Iter: 142700 training loss: 0.702982
Global Iter: 142700 training acc: 0.46875
Global Iter: 142800 training loss: 0.69319
Global Iter: 142800 training acc: 0.53125
Global Iter: 142900 training loss: 0.718035
Global Iter: 142900 training acc: 0.3125
Global Iter: 143000 training loss: 0.705353
Global Iter: 143000 training acc: 0.4375
Global Iter: 143100 training loss: 0.660126
Global Iter: 143100 training acc: 0.75
Global Iter: 143200 training loss: 0.687033
Global Iter: 143200 training acc: 0.53125
Global Iter: 143300 training loss: 0.683174
Global Iter: 143300 training acc: 0.5625
Global Iter: 143400 training loss: 0.693211
Global Iter: 143400 training acc: 0.53125
Global Iter: 143500 training loss: 0.675428
Global Iter: 143500 training acc: 0.65625
Global Iter: 143600 training loss: 0.700007
Global Iter: 143600 training acc: 0.46875
Global Iter: 143700 training loss: 0.692254
Global Iter: 143700 training acc: 0.53125
Global Iter: 143800 training loss: 0.693798
Global Iter: 143800 training acc: 0.5
Global Iter: 143900 training loss: 0.702562
Global Iter: 143900 training acc: 0.4375
Global Iter: 144000 training loss: 0.699073
Global Iter: 144000 training acc: 0.5
Global Iter: 144100 training loss: 0.683443
Global Iter: 144100 training acc: 0.59375
Global Iter: 144200 training loss: 0.699126
Global Iter: 144200 training acc: 0.53125
Global Iter: 144300 training loss: 0.690811
Global Iter: 144300 training acc: 0.5625
Global Iter: 144400 training loss: 0.697237
Global Iter: 144400 training acc: 0.5
Global Iter: 144500 training loss: 0.693365
Global Iter: 144500 training acc: 0.5
Global Iter: 144600 training loss: 0.693394
Global Iter: 144600 training acc: 0.5
Global Iter: 144700 training loss: 0.699976
Global Iter: 144700 training acc: 0.46875
Global Iter: 144800 training loss: 0.690604
Global Iter: 144800 training acc: 0.5625
Global Iter: 144900 training loss: 0.691157
Global Iter: 144900 training acc: 0.53125
Global Iter: 145000 training loss: 0.708849
Global Iter: 145000 training acc: 0.40625
Global Iter: 145100 training loss: 0.710539
Global Iter: 145100 training acc: 0.375
Global Iter: 145200 training loss: 0.703275
Global Iter: 145200 training acc: 0.4375
Global Iter: 145300 training loss: 0.703716
Global Iter: 145300 training acc: 0.46875
Global Iter: 145400 training loss: 0.67467
Global Iter: 145400 training acc: 0.65625
Global Iter: 145500 training loss: 0.696013
Global Iter: 145500 training acc: 0.5
Global Iter: 145600 training loss: 0.696765
Global Iter: 145600 training acc: 0.46875
Global Iter: 145700 training loss: 0.686758
Global Iter: 145700 training acc: 0.5625
Global Iter: 145800 training loss: 0.710774
Global Iter: 145800 training acc: 0.40625
Global Iter: 145900 training loss: 0.700296
Global Iter: 145900 training acc: 0.46875
Global Iter: 146000 training loss: 0.694156
Global Iter: 146000 training acc: 0.53125
Global Iter: 146100 training loss: 0.676191
Global Iter: 146100 training acc: 0.65625
Global Iter: 146200 training loss: 0.694587
Global Iter: 146200 training acc: 0.5
Global Iter: 146300 training loss: 0.690105
Global Iter: 146300 training acc: 0.53125
Global Iter: 146400 training loss: 0.68924
Global Iter: 146400 training acc: 0.53125
Global Iter: 146500 training loss: 0.699878
Global Iter: 146500 training acc: 0.46875
Global Iter: 146600 training loss: 0.694527
Global Iter: 146600 training acc: 0.5
Global Iter: 146700 training loss: 0.687778
Global Iter: 146700 training acc: 0.5625
Global Iter: 146800 training loss: 0.69725
Global Iter: 146800 training acc: 0.5
Global Iter: 146900 training loss: 0.678707
Global Iter: 146900 training acc: 0.625
Global Iter: 147000 training loss: 0.696105
Global Iter: 147000 training acc: 0.5
Global Iter: 147100 training loss: 0.702612
Global Iter: 147100 training acc: 0.4375
Global Iter: 147200 training loss: 0.674692
Global Iter: 147200 training acc: 0.65625
Global Iter: 147300 training loss: 0.692497
Global Iter: 147300 training acc: 0.53125
Global Iter: 147400 training loss: 0.690661
Global Iter: 147400 training acc: 0.53125
Global Iter: 147500 training loss: 0.692856
Global Iter: 147500 training acc: 0.5
Global Iter: 147600 training loss: 0.675591
Global Iter: 147600 training acc: 0.625
Global Iter: 147700 training loss: 0.681649
Global Iter: 147700 training acc: 0.59375
Global Iter: 147800 training loss: 0.69764
Global Iter: 147800 training acc: 0.46875
Global Iter: 147900 training loss: 0.71604
Global Iter: 147900 training acc: 0.3125
Global Iter: 148000 training loss: 0.68898
Global Iter: 148000 training acc: 0.53125
Global Iter: 148100 training loss: 0.692067
Global Iter: 148100 training acc: 0.53125
Global Iter: 148200 training loss: 0.70668
Global Iter: 148200 training acc: 0.40625
Global Iter: 148300 training loss: 0.683504
Global Iter: 148300 training acc: 0.53125
Global Iter: 148400 training loss: 0.702054
Global Iter: 148400 training acc: 0.46875
Global Iter: 148500 training loss: 0.68357
Global Iter: 148500 training acc: 0.59375
Global Iter: 148600 training loss: 0.700176
Global Iter: 148600 training acc: 0.46875
Global Iter: 148700 training loss: 0.685113
Global Iter: 148700 training acc: 0.59375
Global Iter: 148800 training loss: 0.704058
Global Iter: 148800 training acc: 0.4375
Global Iter: 148900 training loss: 0.689355
Global Iter: 148900 training acc: 0.5625
Global Iter: 149000 training loss: 0.671395
Global Iter: 149000 training acc: 0.6875
Global Iter: 149100 training loss: 0.6898
Global Iter: 149100 training acc: 0.53125
Global Iter: 149200 training loss: 0.690082
Global Iter: 149200 training acc: 0.53125
Global Iter: 149300 training loss: 0.693896
Global Iter: 149300 training acc: 0.5
Global Iter: 149400 training loss: 0.701245
Global Iter: 149400 training acc: 0.4375
Global Iter: 149500 training loss: 0.699287
Global Iter: 149500 training acc: 0.46875
Global Iter: 149600 training loss: 0.690006
Global Iter: 149600 training acc: 0.53125
Global Iter: 149700 training loss: 0.694705
Global Iter: 149700 training acc: 0.5
Global Iter: 149800 training loss: 0.679518
Global Iter: 149800 training acc: 0.65625
Global Iter: 149900 training loss: 0.705182
Global Iter: 149900 training acc: 0.40625
Global Iter: 150000 training loss: 0.713727
Global Iter: 150000 training acc: 0.375
Global Iter: 150100 training loss: 0.669368
Global Iter: 150100 training acc: 0.6875
Global Iter: 150200 training loss: 0.698479
Global Iter: 150200 training acc: 0.5
Global Iter: 150300 training loss: 0.686957
Global Iter: 150300 training acc: 0.5625
Global Iter: 150400 training loss: 0.689753
Global Iter: 150400 training acc: 0.5625
Global Iter: 150500 training loss: 0.674339
Global Iter: 150500 training acc: 0.65625
Global Iter: 150600 training loss: 0.700994
Global Iter: 150600 training acc: 0.4375
Global Iter: 150700 training loss: 0.691985
Global Iter: 150700 training acc: 0.5
Global Iter: 150800 training loss: 0.69887
Global Iter: 150800 training acc: 0.46875
Global Iter: 150900 training loss: 0.70262
Global Iter: 150900 training acc: 0.46875
Global Iter: 151000 training loss: 0.693566
Global Iter: 151000 training acc: 0.53125
Global Iter: 151100 training loss: 0.697196
Global Iter: 151100 training acc: 0.5
Global Iter: 151200 training loss: 0.684791
Global Iter: 151200 training acc: 0.59375
Global Iter: 151300 training loss: 0.701155
Global Iter: 151300 training acc: 0.5
Global Iter: 151400 training loss: 0.676795
Global Iter: 151400 training acc: 0.65625
Global Iter: 151500 training loss: 0.688729
Global Iter: 151500 training acc: 0.53125
Global Iter: 151600 training loss: 0.680637
Global Iter: 151600 training acc: 0.625
Global Iter: 151700 training loss: 0.691174
Global Iter: 151700 training acc: 0.53125
Global Iter: 151800 training loss: 0.675972
Global Iter: 151800 training acc: 0.65625
Global Iter: 151900 training loss: 0.680512
Global Iter: 151900 training acc: 0.625
Global Iter: 152000 training loss: 0.676391
Global Iter: 152000 training acc: 0.65625
Global Iter: 152100 training loss: 0.701157
Global Iter: 152100 training acc: 0.4375
Global Iter: 152200 training loss: 0.690623
Global Iter: 152200 training acc: 0.53125
Global Iter: 152300 training loss: 0.688787
Global Iter: 152300 training acc: 0.5625
Global Iter: 152400 training loss: 0.679083
Global Iter: 152400 training acc: 0.65625
Global Iter: 152500 training loss: 0.692335
Global Iter: 152500 training acc: 0.53125
Global Iter: 152600 training loss: 0.701839
Global Iter: 152600 training acc: 0.46875
Global Iter: 152700 training loss: 0.694542
Global Iter: 152700 training acc: 0.5
Global Iter: 152800 training loss: 0.720094
Global Iter: 152800 training acc: 0.3125
Global Iter: 152900 training loss: 0.702399
Global Iter: 152900 training acc: 0.4375
Global Iter: 153000 training loss: 0.663634
Global Iter: 153000 training acc: 0.75
Global Iter: 153100 training loss: 0.696548
Global Iter: 153100 training acc: 0.5
Global Iter: 153200 training loss: 0.69309
Global Iter: 153200 training acc: 0.53125
Global Iter: 153300 training loss: 0.690233
Global Iter: 153300 training acc: 0.53125
Global Iter: 153400 training loss: 0.670389
Global Iter: 153400 training acc: 0.6875
Global Iter: 153500 training loss: 0.701653
Global Iter: 153500 training acc: 0.4375
Global Iter: 153600 training loss: 0.687751
Global Iter: 153600 training acc: 0.5625
Global Iter: 153700 training loss: 0.692012
Global Iter: 153700 training acc: 0.5
Global Iter: 153800 training loss: 0.69923
Global Iter: 153800 training acc: 0.46875
Global Iter: 153900 training loss: 0.694192
Global Iter: 153900 training acc: 0.5
Global Iter: 154000 training loss: 0.689581
Global Iter: 154000 training acc: 0.53125
Global Iter: 154100 training loss: 0.690756
Global Iter: 154100 training acc: 0.53125
Global Iter: 154200 training loss: 0.682188
Global Iter: 154200 training acc: 0.59375
Global Iter: 154300 training loss: 0.692017
Global Iter: 154300 training acc: 0.53125
Global Iter: 154400 training loss: 0.697961
Global Iter: 154400 training acc: 0.5
Global Iter: 154500 training loss: 0.69658
Global Iter: 154500 training acc: 0.5
Global Iter: 154600 training loss: 0.694486
Global Iter: 154600 training acc: 0.5
Global Iter: 154700 training loss: 0.688848
Global Iter: 154700 training acc: 0.5625
Global Iter: 154800 training loss: 0.690734
Global Iter: 154800 training acc: 0.53125
Global Iter: 154900 training loss: 0.70394
Global Iter: 154900 training acc: 0.4375
Global Iter: 155000 training loss: 0.708344
Global Iter: 155000 training acc: 0.40625
Global Iter: 155100 training loss: 0.694898
Global Iter: 155100 training acc: 0.5
Global Iter: 155200 training loss: 0.693673
Global Iter: 155200 training acc: 0.5
Global Iter: 155300 training loss: 0.671517
Global Iter: 155300 training acc: 0.6875
Global Iter: 155400 training loss: 0.694332
Global Iter: 155400 training acc: 0.5
Global Iter: 155500 training loss: 0.699328
Global Iter: 155500 training acc: 0.46875
Global Iter: 155600 training loss: 0.680481
Global Iter: 155600 training acc: 0.59375
Global Iter: 155700 training loss: 0.716416
Global Iter: 155700 training acc: 0.34375
Global Iter: 155800 training loss: 0.695522
Global Iter: 155800 training acc: 0.5
Global Iter: 155900 training loss: 0.684334
Global Iter: 155900 training acc: 0.59375
Global Iter: 156000 training loss: 0.668
Global Iter: 156000 training acc: 0.6875
Global Iter: 156100 training loss: 0.702883
Global Iter: 156100 training acc: 0.4375
Global Iter: 156200 training loss: 0.696574
Global Iter: 156200 training acc: 0.5
Global Iter: 156300 training loss: 0.689202
Global Iter: 156300 training acc: 0.5625
Global Iter: 156400 training loss: 0.701497
Global Iter: 156400 training acc: 0.46875
Global Iter: 156500 training loss: 0.695347
Global Iter: 156500 training acc: 0.5
Global Iter: 156600 training loss: 0.684523
Global Iter: 156600 training acc: 0.59375
Global Iter: 156700 training loss: 0.696667
Global Iter: 156700 training acc: 0.5
Global Iter: 156800 training loss: 0.677411
Global Iter: 156800 training acc: 0.65625
Global Iter: 156900 training loss: 0.695471
Global Iter: 156900 training acc: 0.5
Global Iter: 157000 training loss: 0.700027
Global Iter: 157000 training acc: 0.4375
Global Iter: 157100 training loss: 0.674619
Global Iter: 157100 training acc: 0.65625
Global Iter: 157200 training loss: 0.692728
Global Iter: 157200 training acc: 0.53125
Global Iter: 157300 training loss: 0.69226
Global Iter: 157300 training acc: 0.53125
Global Iter: 157400 training loss: 0.693761
Global Iter: 157400 training acc: 0.5
Global Iter: 157500 training loss: 0.681024
Global Iter: 157500 training acc: 0.625
Global Iter: 157600 training loss: 0.684325
Global Iter: 157600 training acc: 0.59375
Global Iter: 157700 training loss: 0.699622
Global Iter: 157700 training acc: 0.46875
Global Iter: 157800 training loss: 0.719493
Global Iter: 157800 training acc: 0.3125
Global Iter: 157900 training loss: 0.686703
Global Iter: 157900 training acc: 0.5625
Global Iter: 158000 training loss: 0.69153
Global Iter: 158000 training acc: 0.53125
Global Iter: 158100 training loss: 0.702454
Global Iter: 158100 training acc: 0.4375
Global Iter: 158200 training loss: 0.684305
Global Iter: 158200 training acc: 0.59375
Global Iter: 158300 training loss: 0.690349
Global Iter: 158300 training acc: 0.5
Global Iter: 158400 training loss: 0.68887
Global Iter: 158400 training acc: 0.5625
Global Iter: 158500 training loss: 0.691806
Global Iter: 158500 training acc: 0.53125
Global Iter: 158600 training loss: 0.677409
Global Iter: 158600 training acc: 0.625
Global Iter: 158700 training loss: 0.703113
Global Iter: 158700 training acc: 0.4375
Global Iter: 158800 training loss: 0.691838
Global Iter: 158800 training acc: 0.53125
Global Iter: 158900 training loss: 0.668742
Global Iter: 158900 training acc: 0.6875
Global Iter: 159000 training loss: 0.694455
Global Iter: 159000 training acc: 0.5
Global Iter: 159100 training loss: 0.693511
Global Iter: 159100 training acc: 0.5
Global Iter: 159200 training loss: 0.696104
Global Iter: 159200 training acc: 0.53125
Global Iter: 159300 training loss: 0.708007
Global Iter: 159300 training acc: 0.40625
Global Iter: 159400 training loss: 0.703823
Global Iter: 159400 training acc: 0.4375
Global Iter: 159500 training loss: 0.691606
Global Iter: 159500 training acc: 0.53125
Global Iter: 159600 training loss: 0.701346
Global Iter: 159600 training acc: 0.4375
Global Iter: 159700 training loss: 0.668961
Global Iter: 159700 training acc: 0.6875
Global Iter: 159800 training loss: 0.707098
Global Iter: 159800 training acc: 0.40625
Global Iter: 159900 training loss: 0.714327
Global Iter: 159900 training acc: 0.34375
Global Iter: 160000 training loss: 0.670217
Global Iter: 160000 training acc: 0.6875
Global Iter: 160100 training loss: 0.692857
Global Iter: 160100 training acc: 0.53125
Global Iter: 160200 training loss: 0.684601
Global Iter: 160200 training acc: 0.59375
Global Iter: 160300 training loss: 0.687508
Global Iter: 160300 training acc: 0.5625
Global Iter: 160400 training loss: 0.669076
Global Iter: 160400 training acc: 0.6875
Global Iter: 160500 training loss: 0.702772
Global Iter: 160500 training acc: 0.4375
Global Iter: 160600 training loss: 0.688452
Global Iter: 160600 training acc: 0.53125
Global Iter: 160700 training loss: 0.706194
Global Iter: 160700 training acc: 0.40625
Global Iter: 160800 training loss: 0.700614
Global Iter: 160800 training acc: 0.46875
Global Iter: 160900 training loss: 0.687888
Global Iter: 160900 training acc: 0.5625
Global Iter: 161000 training loss: 0.705539
Global Iter: 161000 training acc: 0.4375
Global Iter: 161100 training loss: 0.680044
Global Iter: 161100 training acc: 0.625
Global Iter: 161200 training loss: 0.694424
Global Iter: 161200 training acc: 0.5
Global Iter: 161300 training loss: 0.674035
Global Iter: 161300 training acc: 0.65625
Global Iter: 161400 training loss: 0.695648
Global Iter: 161400 training acc: 0.5
Global Iter: 161500 training loss: 0.681905
Global Iter: 161500 training acc: 0.625
Global Iter: 161600 training loss: 0.690843
Global Iter: 161600 training acc: 0.53125
Global Iter: 161700 training loss: 0.675864
Global Iter: 161700 training acc: 0.65625
Global Iter: 161800 training loss: 0.669811
Global Iter: 161800 training acc: 0.6875
Global Iter: 161900 training loss: 0.680076
Global Iter: 161900 training acc: 0.625
Global Iter: 162000 training loss: 0.69597
Global Iter: 162000 training acc: 0.5
Global Iter: 162100 training loss: 0.694057
Global Iter: 162100 training acc: 0.5
Global Iter: 162200 training loss: 0.68865
Global Iter: 162200 training acc: 0.5625
Global Iter: 162300 training loss: 0.682699
Global Iter: 162300 training acc: 0.59375
Global Iter: 162400 training loss: 0.686537
Global Iter: 162400 training acc: 0.5625
Global Iter: 162500 training loss: 0.696456
Global Iter: 162500 training acc: 0.5
Global Iter: 162600 training loss: 0.693766
Global Iter: 162600 training acc: 0.53125
Global Iter: 162700 training loss: 0.720942
Global Iter: 162700 training acc: 0.34375
Global Iter: 162800 training loss: 0.711218
Global Iter: 162800 training acc: 0.40625
Global Iter: 162900 training loss: 0.671625
Global Iter: 162900 training acc: 0.71875
Global Iter: 163000 training loss: 0.701709
Global Iter: 163000 training acc: 0.46875
Global Iter: 163100 training loss: 0.693101
Global Iter: 163100 training acc: 0.53125
Global Iter: 163200 training loss: 0.690691
Global Iter: 163200 training acc: 0.53125
Global Iter: 163300 training loss: 0.669203
Global Iter: 163300 training acc: 0.71875
Global Iter: 163400 training loss: 0.698796
Global Iter: 163400 training acc: 0.46875
Global Iter: 163500 training loss: 0.690721
Global Iter: 163500 training acc: 0.53125
Global Iter: 163600 training loss: 0.693079
Global Iter: 163600 training acc: 0.5
Global Iter: 163700 training loss: 0.702377
Global Iter: 163700 training acc: 0.4375
Global Iter: 163800 training loss: 0.694769
Global Iter: 163800 training acc: 0.5
Global Iter: 163900 training loss: 0.690377
Global Iter: 163900 training acc: 0.53125
Global Iter: 164000 training loss: 0.688138
Global Iter: 164000 training acc: 0.5625
Global Iter: 164100 training loss: 0.690448
Global Iter: 164100 training acc: 0.53125
Global Iter: 164200 training loss: 0.698509
Global Iter: 164200 training acc: 0.46875
Global Iter: 164300 training loss: 0.692252
Global Iter: 164300 training acc: 0.53125
Global Iter: 164400 training loss: 0.688887
Global Iter: 164400 training acc: 0.53125
Global Iter: 164500 training loss: 0.694987
Global Iter: 164500 training acc: 0.5
Global Iter: 164600 training loss: 0.691172
Global Iter: 164600 training acc: 0.5625
Global Iter: 164700 training loss: 0.688372
Global Iter: 164700 training acc: 0.5625
Global Iter: 164800 training loss: 0.698892
Global Iter: 164800 training acc: 0.46875
Global Iter: 164900 training loss: 0.712056
Global Iter: 164900 training acc: 0.375
Global Iter: 165000 training loss: 0.697854
Global Iter: 165000 training acc: 0.46875
Global Iter: 165100 training loss: 0.691613
Global Iter: 165100 training acc: 0.5
Global Iter: 165200 training loss: 0.674458
Global Iter: 165200 training acc: 0.65625
Global Iter: 165300 training loss: 0.69276
Global Iter: 165300 training acc: 0.53125
Global Iter: 165400 training loss: 0.693218
Global Iter: 165400 training acc: 0.5
Global Iter: 165500 training loss: 0.68057
Global Iter: 165500 training acc: 0.625
Global Iter: 165600 training loss: 0.715747
Global Iter: 165600 training acc: 0.3125
Global Iter: 165700 training loss: 0.701872
Global Iter: 165700 training acc: 0.46875
Global Iter: 165800 training loss: 0.681722
Global Iter: 165800 training acc: 0.59375
Global Iter: 165900 training loss: 0.669083
Global Iter: 165900 training acc: 0.6875
Global Iter: 166000 training loss: 0.70058
Global Iter: 166000 training acc: 0.46875
Global Iter: 166100 training loss: 0.698479
Global Iter: 166100 training acc: 0.46875
Global Iter: 166200 training loss: 0.68874
Global Iter: 166200 training acc: 0.5625
Global Iter: 166300 training loss: 0.695783
Global Iter: 166300 training acc: 0.5
Global Iter: 166400 training loss: 0.690645
Global Iter: 166400 training acc: 0.53125
Global Iter: 166500 training loss: 0.681962
Global Iter: 166500 training acc: 0.625
Global Iter: 166600 training loss: 0.693045
Global Iter: 166600 training acc: 0.5
Global Iter: 166700 training loss: 0.675308
Global Iter: 166700 training acc: 0.65625
Global Iter: 166800 training loss: 0.692744
Global Iter: 166800 training acc: 0.53125
Global Iter: 166900 training loss: 0.702468
Global Iter: 166900 training acc: 0.4375
Global Iter: 167000 training loss: 0.672671
Global Iter: 167000 training acc: 0.6875
Global Iter: 167100 training loss: 0.687499
Global Iter: 167100 training acc: 0.5625
Global Iter: 167200 training loss: 0.694836
Global Iter: 167200 training acc: 0.5
Global Iter: 167300 training loss: 0.695652
Global Iter: 167300 training acc: 0.5
Global Iter: 167400 training loss: 0.685219
Global Iter: 167400 training acc: 0.59375
Global Iter: 167500 training loss: 0.689193
Global Iter: 167500 training acc: 0.53125
Global Iter: 167600 training loss: 0.699532
Global Iter: 167600 training acc: 0.46875
Global Iter: 167700 training loss: 0.718068
Global Iter: 167700 training acc: 0.34375
Global Iter: 167800 training loss: 0.686984
Global Iter: 167800 training acc: 0.5625
Global Iter: 167900 training loss: 0.694933
Global Iter: 167900 training acc: 0.5
Global Iter: 168000 training loss: 0.707125
Global Iter: 168000 training acc: 0.40625
Global Iter: 168100 training loss: 0.68093
Global Iter: 168100 training acc: 0.625
Global Iter: 168200 training loss: 0.695473
Global Iter: 168200 training acc: 0.5
Global Iter: 168300 training loss: 0.691645
Global Iter: 168300 training acc: 0.53125
Global Iter: 168400 training loss: 0.690398
Global Iter: 168400 training acc: 0.53125
Global Iter: 168500 training loss: 0.675283
Global Iter: 168500 training acc: 0.65625
Global Iter: 168600 training loss: 0.696675
Global Iter: 168600 training acc: 0.46875
Global Iter: 168700 training loss: 0.690599
Global Iter: 168700 training acc: 0.53125
Global Iter: 168800 training loss: 0.670288
Global Iter: 168800 training acc: 0.65625
Global Iter: 168900 training loss: 0.699967
Global Iter: 168900 training acc: 0.46875
Global Iter: 169000 training loss: 0.695144
Global Iter: 169000 training acc: 0.5
Global Iter: 169100 training loss: 0.682944
Global Iter: 169100 training acc: 0.59375
Global Iter: 169200 training loss: 0.699013
Global Iter: 169200 training acc: 0.46875
Global Iter: 169300 training loss: 0.708008
Global Iter: 169300 training acc: 0.40625
Global Iter: 169400 training loss: 0.689678
Global Iter: 169400 training acc: 0.5625
Global Iter: 169500 training loss: 0.708422
Global Iter: 169500 training acc: 0.40625
Global Iter: 169600 training loss: 0.674207
Global Iter: 169600 training acc: 0.65625
Global Iter: 169700 training loss: 0.705437
Global Iter: 169700 training acc: 0.46875
Global Iter: 169800 training loss: 0.723897
Global Iter: 169800 training acc: 0.28125
Global Iter: 169900 training loss: 0.678757
Global Iter: 169900 training acc: 0.625
Global Iter: 170000 training loss: 0.691483
Global Iter: 170000 training acc: 0.53125
Global Iter: 170100 training loss: 0.683626
Global Iter: 170100 training acc: 0.59375
Global Iter: 170200 training loss: 0.691537
Global Iter: 170200 training acc: 0.53125
Global Iter: 170300 training loss: 0.672349
Global Iter: 170300 training acc: 0.6875
Global Iter: 170400 training loss: 0.698187
Global Iter: 170400 training acc: 0.46875
Global Iter: 170500 training loss: 0.692122
Global Iter: 170500 training acc: 0.53125
Global Iter: 170600 training loss: 0.703587
Global Iter: 170600 training acc: 0.4375
Global Iter: 170700 training loss: 0.698931
Global Iter: 170700 training acc: 0.46875
Global Iter: 170800 training loss: 0.686373
Global Iter: 170800 training acc: 0.5625
Global Iter: 170900 training loss: 0.704527
Global Iter: 170900 training acc: 0.4375
Global Iter: 171000 training loss: 0.686424
Global Iter: 171000 training acc: 0.5625
Global Iter: 171100 training loss: 0.690154
Global Iter: 171100 training acc: 0.53125
Global Iter: 171200 training loss: 0.675184
Global Iter: 171200 training acc: 0.65625
Global Iter: 171300 training loss: 0.697017
Global Iter: 171300 training acc: 0.5
Global Iter: 171400 training loss: 0.678648
Global Iter: 171400 training acc: 0.625
Global Iter: 171500 training loss: 0.68686
Global Iter: 171500 training acc: 0.5625
Global Iter: 171600 training loss: 0.68182
Global Iter: 171600 training acc: 0.59375
Global Iter: 171700 training loss: 0.671151
Global Iter: 171700 training acc: 0.6875
Global Iter: 171800 training loss: 0.678878
Global Iter: 171800 training acc: 0.625
Global Iter: 171900 training loss: 0.68799
Global Iter: 171900 training acc: 0.5625
Global Iter: 172000 training loss: 0.692613
Global Iter: 172000 training acc: 0.53125
Global Iter: 172100 training loss: 0.673813
Global Iter: 172100 training acc: 0.65625
Global Iter: 172200 training loss: 0.685041
Global Iter: 172200 training acc: 0.59375
Global Iter: 172300 training loss: 0.691795
Global Iter: 172300 training acc: 0.53125
Global Iter: 172400 training loss: 0.701221
Global Iter: 172400 training acc: 0.46875
Global Iter: 172500 training loss: 0.678109
Global Iter: 172500 training acc: 0.59375
Global Iter: 172600 training loss: 0.710256
Global Iter: 172600 training acc: 0.40625
Global Iter: 172700 training loss: 0.701592
Global Iter: 172700 training acc: 0.46875
Global Iter: 172800 training loss: 0.66711
Global Iter: 172800 training acc: 0.6875
Global Iter: 172900 training loss: 0.695317
Global Iter: 172900 training acc: 0.5
Global Iter: 173000 training loss: 0.696413
Global Iter: 173000 training acc: 0.5
Global Iter: 173100 training loss: 0.686527
Global Iter: 173100 training acc: 0.5625
Global Iter: 173200 training loss: 0.665032
Global Iter: 173200 training acc: 0.71875
Global Iter: 173300 training loss: 0.696944
Global Iter: 173300 training acc: 0.5
Global Iter: 173400 training loss: 0.686133
Global Iter: 173400 training acc: 0.5625
Global Iter: 173500 training loss: 0.696536
Global Iter: 173500 training acc: 0.5
Global Iter: 173600 training loss: 0.706734
Global Iter: 173600 training acc: 0.40625
Global Iter: 173700 training loss: 0.706334
Global Iter: 173700 training acc: 0.4375
Global Iter: 173800 training loss: 0.683658
Global Iter: 173800 training acc: 0.59375
Global Iter: 173900 training loss: 0.690561
Global Iter: 173900 training acc: 0.53125
Global Iter: 174000 training loss: 0.69605
Global Iter: 174000 training acc: 0.5
Global Iter: 174100 training loss: 0.694902
Global Iter: 174100 training acc: 0.5
Global Iter: 174200 training loss: 0.69064
Global Iter: 174200 training acc: 0.5625
Global Iter: 174300 training loss: 0.688333
Global Iter: 174300 training acc: 0.5625
Global Iter: 174400 training loss: 0.693304
Global Iter: 174400 training acc: 0.5
Global Iter: 174500 training loss: 0.69203
Global Iter: 174500 training acc: 0.53125
Global Iter: 174600 training loss: 0.684805
Global Iter: 174600 training acc: 0.5625
Global Iter: 174700 training loss: 0.693929
Global Iter: 174700 training acc: 0.5
Global Iter: 174800 training loss: 0.708952
Global Iter: 174800 training acc: 0.40625
Global Iter: 174900 training loss: 0.698058
Global Iter: 174900 training acc: 0.5
Global Iter: 175000 training loss: 0.691768
Global Iter: 175000 training acc: 0.5
Global Iter: 175100 training loss: 0.672597
Global Iter: 175100 training acc: 0.6875
Global Iter: 175200 training loss: 0.692228
Global Iter: 175200 training acc: 0.53125
Global Iter: 175300 training loss: 0.696153
Global Iter: 175300 training acc: 0.5
Global Iter: 175400 training loss: 0.689608
Global Iter: 175400 training acc: 0.5625
Global Iter: 175500 training loss: 0.724296
Global Iter: 175500 training acc: 0.3125
Global Iter: 175600 training loss: 0.70019
Global Iter: 175600 training acc: 0.46875
Global Iter: 175700 training loss: 0.680692
Global Iter: 175700 training acc: 0.59375
Global Iter: 175800 training loss: 0.672847
Global Iter: 175800 training acc: 0.65625
Global Iter: 175900 training loss: 0.700244
Global Iter: 175900 training acc: 0.46875
Global Iter: 176000 training loss: 0.695527
Global Iter: 176000 training acc: 0.5
Global Iter: 176100 training loss: 0.685916
Global Iter: 176100 training acc: 0.59375
Global Iter: 176200 training loss: 0.692244
Global Iter: 176200 training acc: 0.53125
Global Iter: 176300 training loss: 0.692463
Global Iter: 176300 training acc: 0.5
Global Iter: 176400 training loss: 0.686754
Global Iter: 176400 training acc: 0.5625
Global Iter: 176500 training loss: 0.69534
Global Iter: 176500 training acc: 0.5
Global Iter: 176600 training loss: 0.679812
Global Iter: 176600 training acc: 0.625
Global Iter: 176700 training loss: 0.686565
Global Iter: 176700 training acc: 0.5625
Global Iter: 176800 training loss: 0.702613
Global Iter: 176800 training acc: 0.4375
Global Iter: 176900 training loss: 0.668962
Global Iter: 176900 training acc: 0.71875
Global Iter: 177000 training loss: 0.6877
Global Iter: 177000 training acc: 0.5625
Global Iter: 177100 training loss: 0.695344
Global Iter: 177100 training acc: 0.5
Global Iter: 177200 training loss: 0.689228
Global Iter: 177200 training acc: 0.53125
Global Iter: 177300 training loss: 0.688256
Global Iter: 177300 training acc: 0.53125
Global Iter: 177400 training loss: 0.696902
Global Iter: 177400 training acc: 0.5
Global Iter: 177500 training loss: 0.688424
Global Iter: 177500 training acc: 0.53125
Global Iter: 177600 training loss: 0.717223
Global Iter: 177600 training acc: 0.34375
Global Iter: 177700 training loss: 0.692225
Global Iter: 177700 training acc: 0.53125
Global Iter: 177800 training loss: 0.695163
Global Iter: 177800 training acc: 0.5
Global Iter: 177900 training loss: 0.704783
Global Iter: 177900 training acc: 0.40625
Global Iter: 178000 training loss: 0.68187
Global Iter: 178000 training acc: 0.625
Global Iter: 178100 training loss: 0.689123
Global Iter: 178100 training acc: 0.53125
Global Iter: 178200 training loss: 0.693015
Global Iter: 178200 training acc: 0.53125
Global Iter: 178300 training loss: 0.679055
Global Iter: 178300 training acc: 0.59375
Global Iter: 178400 training loss: 0.681653
Global Iter: 178400 training acc: 0.625
Global Iter: 178500 training loss: 0.705012
Global Iter: 178500 training acc: 0.40625
Global Iter: 178600 training loss: 0.690948
Global Iter: 178600 training acc: 0.53125
Global Iter: 178700 training loss: 0.676956
Global Iter: 178700 training acc: 0.65625
Global Iter: 178800 training loss: 0.702735
Global Iter: 178800 training acc: 0.4375
Global Iter: 178900 training loss: 0.686862
Global Iter: 178900 training acc: 0.5625
Global Iter: 179000 training loss: 0.685658
Global Iter: 179000 training acc: 0.5625
Global Iter: 179100 training loss: 0.701009
Global Iter: 179100 training acc: 0.46875
Global Iter: 179200 training loss: 0.699216
Global Iter: 179200 training acc: 0.46875
Global Iter: 179300 training loss: 0.686089
Global Iter: 179300 training acc: 0.5625
Global Iter: 179400 training loss: 0.708324
Global Iter: 179400 training acc: 0.4375
Global Iter: 179500 training loss: 0.674698
Global Iter: 179500 training acc: 0.6875
Global Iter: 179600 training loss: 0.699142
Global Iter: 179600 training acc: 0.46875
Global Iter: 179700 training loss: 0.712538
Global Iter: 179700 training acc: 0.34375
Global Iter: 179800 training loss: 0.684299
Global Iter: 179800 training acc: 0.59375
Global Iter: 179900 training loss: 0.691534
Global Iter: 179900 training acc: 0.53125
Global Iter: 180000 training loss: 0.678551
Global Iter: 180000 training acc: 0.65625
Global Iter: 180100 training loss: 0.685915
Global Iter: 180100 training acc: 0.5625
Global Iter: 180200 training loss: 0.66792
Global Iter: 180200 training acc: 0.71875
Global Iter: 180300 training loss: 0.694873
Global Iter: 180300 training acc: 0.5
Global Iter: 180400 training loss: 0.693488
Global Iter: 180400 training acc: 0.53125
Global Iter: 180500 training loss: 0.705328
Global Iter: 180500 training acc: 0.40625
Global Iter: 180600 training loss: 0.6902
Global Iter: 180600 training acc: 0.53125
Global Iter: 180700 training loss: 0.679672
Global Iter: 180700 training acc: 0.625
Global Iter: 180800 training loss: 0.703863
Global Iter: 180800 training acc: 0.4375
Global Iter: 180900 training loss: 0.685987
Global Iter: 180900 training acc: 0.5625
Global Iter: 181000 training loss: 0.685463
Global Iter: 181000 training acc: 0.5625
Global Iter: 181100 training loss: 0.670417
Global Iter: 181100 training acc: 0.6875
Global Iter: 181200 training loss: 0.702895
Global Iter: 181200 training acc: 0.4375
Global Iter: 181300 training loss: 0.678557
Global Iter: 181300 training acc: 0.625
Global Iter: 181400 training loss: 0.690142
Global Iter: 181400 training acc: 0.5625
Global Iter: 181500 training loss: 0.673754
Global Iter: 181500 training acc: 0.65625
Global Iter: 181600 training loss: 0.671171
Global Iter: 181600 training acc: 0.6875
Global Iter: 181700 training loss: 0.677824
Global Iter: 181700 training acc: 0.625
Global Iter: 181800 training loss: 0.689018
Global Iter: 181800 training acc: 0.53125
Global Iter: 181900 training loss: 0.685956
Global Iter: 181900 training acc: 0.5625
Global Iter: 182000 training loss: 0.679311
Global Iter: 182000 training acc: 0.625
Global Iter: 182100 training loss: 0.68419
Global Iter: 182100 training acc: 0.59375
Global Iter: 182200 training loss: 0.694638
Global Iter: 182200 training acc: 0.5
Global Iter: 182300 training loss: 0.700882
Global Iter: 182300 training acc: 0.4375
Global Iter: 182400 training loss: 0.684897
Global Iter: 182400 training acc: 0.59375
Global Iter: 182500 training loss: 0.709867
Global Iter: 182500 training acc: 0.40625
Global Iter: 182600 training loss: 0.709762
Global Iter: 182600 training acc: 0.375
Global Iter: 182700 training loss: 0.672145
Global Iter: 182700 training acc: 0.6875
Global Iter: 182800 training loss: 0.696361
Global Iter: 182800 training acc: 0.5
Global Iter: 182900 training loss: 0.697907
Global Iter: 182900 training acc: 0.5
Global Iter: 183000 training loss: 0.686552
Global Iter: 183000 training acc: 0.5625
Global Iter: 183100 training loss: 0.669814
Global Iter: 183100 training acc: 0.71875
Global Iter: 183200 training loss: 0.699821
Global Iter: 183200 training acc: 0.46875
Global Iter: 183300 training loss: 0.689687
Global Iter: 183300 training acc: 0.5625
Global Iter: 183400 training loss: 0.699853
Global Iter: 183400 training acc: 0.46875
Global Iter: 183500 training loss: 0.705415
Global Iter: 183500 training acc: 0.4375
Global Iter: 183600 training loss: 0.700316
Global Iter: 183600 training acc: 0.46875
Global Iter: 183700 training loss: 0.683372
Global Iter: 183700 training acc: 0.59375
Global Iter: 183800 training loss: 0.697645
Global Iter: 183800 training acc: 0.5
Global Iter: 183900 training loss: 0.692506
Global Iter: 183900 training acc: 0.53125
Global Iter: 184000 training loss: 0.691954
Global Iter: 184000 training acc: 0.5
Global Iter: 184100 training loss: 0.681796
Global Iter: 184100 training acc: 0.5625
Global Iter: 184200 training loss: 0.692216
Global Iter: 184200 training acc: 0.53125
Global Iter: 184300 training loss: 0.695586
Global Iter: 184300 training acc: 0.5
Global Iter: 184400 training loss: 0.682261
Global Iter: 184400 training acc: 0.59375
Global Iter: 184500 training loss: 0.684141
Global Iter: 184500 training acc: 0.59375
Global Iter: 184600 training loss: 0.689677
Global Iter: 184600 training acc: 0.53125
Global Iter: 184700 training loss: 0.707553
Global Iter: 184700 training acc: 0.40625
Global Iter: 184800 training loss: 0.69215
Global Iter: 184800 training acc: 0.53125
Global Iter: 184900 training loss: 0.698588
Global Iter: 184900 training acc: 0.46875
Global Iter: 185000 training loss: 0.671624
Global Iter: 185000 training acc: 0.6875
Global Iter: 185100 training loss: 0.687732
Global Iter: 185100 training acc: 0.5625
Global Iter: 185200 training loss: 0.698744
Global Iter: 185200 training acc: 0.4375
Global Iter: 185300 training loss: 0.670759
Global Iter: 185300 training acc: 0.5625
Global Iter: 185400 training loss: 0.71269
Global Iter: 185400 training acc: 0.34375
Global Iter: 185500 training loss: 0.700471
Global Iter: 185500 training acc: 0.4375
Global Iter: 185600 training loss: 0.685657
Global Iter: 185600 training acc: 0.59375
Global Iter: 185700 training loss: 0.67616
Global Iter: 185700 training acc: 0.625
Global Iter: 185800 training loss: 0.69613
Global Iter: 185800 training acc: 0.5
Global Iter: 185900 training loss: 0.69414
Global Iter: 185900 training acc: 0.5
Global Iter: 186000 training loss: 0.679718
Global Iter: 186000 training acc: 0.625
Global Iter: 186100 training loss: 0.686411
Global Iter: 186100 training acc: 0.5625
Global Iter: 186200 training loss: 0.694941
Global Iter: 186200 training acc: 0.5
Global Iter: 186300 training loss: 0.690749
Global Iter: 186300 training acc: 0.53125
Global Iter: 186400 training loss: 0.699405
Global Iter: 186400 training acc: 0.46875
Global Iter: 186500 training loss: 0.68389
Global Iter: 186500 training acc: 0.59375
Global Iter: 186600 training loss: 0.685356
Global Iter: 186600 training acc: 0.5625
Global Iter: 186700 training loss: 0.708567
Global Iter: 186700 training acc: 0.40625
Global Iter: 186800 training loss: 0.668163
Global Iter: 186800 training acc: 0.71875
Global Iter: 186900 training loss: 0.685591
Global Iter: 186900 training acc: 0.5625
Global Iter: 187000 training loss: 0.690657
Global Iter: 187000 training acc: 0.53125
Global Iter: 187100 training loss: 0.699523
Global Iter: 187100 training acc: 0.46875
Global Iter: 187200 training loss: 0.688211
Global Iter: 187200 training acc: 0.5625
Global Iter: 187300 training loss: 0.692702
Global Iter: 187300 training acc: 0.53125
Global Iter: 187400 training loss: 0.693019
Global Iter: 187400 training acc: 0.53125
Global Iter: 187500 training loss: 0.712749
Global Iter: 187500 training acc: 0.375
Global Iter: 187600 training loss: 0.696297
Global Iter: 187600 training acc: 0.5
Global Iter: 187700 training loss: 0.69502
Global Iter: 187700 training acc: 0.5
Global Iter: 187800 training loss: 0.703724
Global Iter: 187800 training acc: 0.4375
Global Iter: 187900 training loss: 0.683205
Global Iter: 187900 training acc: 0.59375
Global Iter: 188000 training loss: 0.68979
Global Iter: 188000 training acc: 0.53125
Global Iter: 188100 training loss: 0.699828
Global Iter: 188100 training acc: 0.5
Global Iter: 188200 training loss: 0.691331
Global Iter: 188200 training acc: 0.53125
Global Iter: 188300 training loss: 0.677946
Global Iter: 188300 training acc: 0.625
Global Iter: 188400 training loss: 0.694821
Global Iter: 188400 training acc: 0.46875
Global Iter: 188500 training loss: 0.697252
Global Iter: 188500 training acc: 0.46875
Global Iter: 188600 training loss: 0.677167
Global Iter: 188600 training acc: 0.65625
Global Iter: 188700 training loss: 0.702267
Global Iter: 188700 training acc: 0.4375
Global Iter: 188800 training loss: 0.68278
Global Iter: 188800 training acc: 0.59375
Global Iter: 188900 training loss: 0.68594
Global Iter: 188900 training acc: 0.59375
Global Iter: 189000 training loss: 0.696559
Global Iter: 189000 training acc: 0.46875
Global Iter: 189100 training loss: 0.700349
Global Iter: 189100 training acc: 0.46875
Global Iter: 189200 training loss: 0.679281
Global Iter: 189200 training acc: 0.625
Global Iter: 189300 training loss: 0.699036
Global Iter: 189300 training acc: 0.46875
Global Iter: 189400 training loss: 0.663951
Global Iter: 189400 training acc: 0.75
Global Iter: 189500 training loss: 0.694901
Global Iter: 189500 training acc: 0.5
Global Iter: 189600 training loss: 0.709411
Global Iter: 189600 training acc: 0.375
Global Iter: 189700 training loss: 0.681149
Global Iter: 189700 training acc: 0.625
Global Iter: 189800 training loss: 0.684524
Global Iter: 189800 training acc: 0.5625
Global Iter: 189900 training loss: 0.677706
Global Iter: 189900 training acc: 0.625
Global Iter: 190000 training loss: 0.694065
Global Iter: 190000 training acc: 0.5
Global Iter: 190100 training loss: 0.670609
Global Iter: 190100 training acc: 0.6875
Global Iter: 190200 training loss: 0.690052
Global Iter: 190200 training acc: 0.53125
Global Iter: 190300 training loss: 0.694662
Global Iter: 190300 training acc: 0.53125
Global Iter: 190400 training loss: 0.708637
Global Iter: 190400 training acc: 0.40625
Global Iter: 190500 training loss: 0.691269
Global Iter: 190500 training acc: 0.53125
Global Iter: 190600 training loss: 0.672414
Global Iter: 190600 training acc: 0.6875
Global Iter: 190700 training loss: 0.708422
Global Iter: 190700 training acc: 0.40625
Global Iter: 190800 training loss: 0.692423
Global Iter: 190800 training acc: 0.53125
Global Iter: 190900 training loss: 0.687096
Global Iter: 190900 training acc: 0.5625
Global Iter: 191000 training loss: 0.673911
Global Iter: 191000 training acc: 0.65625
Global Iter: 191100 training loss: 0.697397
Global Iter: 191100 training acc: 0.46875
Global Iter: 191200 training loss: 0.680172
Global Iter: 191200 training acc: 0.625
Global Iter: 191300 training loss: 0.688438
Global Iter: 191300 training acc: 0.5625
Global Iter: 191400 training loss: 0.676823
Global Iter: 191400 training acc: 0.65625
Global Iter: 191500 training loss: 0.672412
Global Iter: 191500 training acc: 0.6875
Global Iter: 191600 training loss: 0.677084
Global Iter: 191600 training acc: 0.65625
Global Iter: 191700 training loss: 0.68864
Global Iter: 191700 training acc: 0.5625
Global Iter: 191800 training loss: 0.68821
Global Iter: 191800 training acc: 0.5625
Global Iter: 191900 training loss: 0.685764
Global Iter: 191900 training acc: 0.59375
Global Iter: 192000 training loss: 0.689257
Global Iter: 192000 training acc: 0.5625
Global Iter: 192100 training loss: 0.689644
Global Iter: 192100 training acc: 0.53125
Global Iter: 192200 training loss: 0.70178
Global Iter: 192200 training acc: 0.4375
Global Iter: 192300 training loss: 0.680348
Global Iter: 192300 training acc: 0.625
Global Iter: 192400 training loss: 0.704757
Global Iter: 192400 training acc: 0.4375
Global Iter: 192500 training loss: 0.7112
Global Iter: 192500 training acc: 0.375
Global Iter: 192600 training loss: 0.666192
Global Iter: 192600 training acc: 0.75
Global Iter: 192700 training loss: 0.696614
Global Iter: 192700 training acc: 0.5
Global Iter: 192800 training loss: 0.689795
Global Iter: 192800 training acc: 0.53125
Global Iter: 192900 training loss: 0.682627
Global Iter: 192900 training acc: 0.59375
Global Iter: 193000 training loss: 0.673255
Global Iter: 193000 training acc: 0.6875
Global Iter: 193100 training loss: 0.708392
Global Iter: 193100 training acc: 0.40625
Global Iter: 193200 training loss: 0.683801
Global Iter: 193200 training acc: 0.59375
Global Iter: 193300 training loss: 0.69779
Global Iter: 193300 training acc: 0.46875
Global Iter: 193400 training loss: 0.707923
Global Iter: 193400 training acc: 0.40625
Global Iter: 193500 training loss: 0.689583
Global Iter: 193500 training acc: 0.53125
Global Iter: 193600 training loss: 0.678891
Global Iter: 193600 training acc: 0.59375
Global Iter: 193700 training loss: 0.683458
Global Iter: 193700 training acc: 0.5625
Global Iter: 193800 training loss: 0.68731
Global Iter: 193800 training acc: 0.5625
Global Iter: 193900 training loss: 0.691579
Global Iter: 193900 training acc: 0.53125
Global Iter: 194000 training loss: 0.689571
Global Iter: 194000 training acc: 0.53125
Global Iter: 194100 training loss: 0.691202
Global Iter: 194100 training acc: 0.53125
Global Iter: 194200 training loss: 0.691585
Global Iter: 194200 training acc: 0.53125
Global Iter: 194300 training loss: 0.687571
Global Iter: 194300 training acc: 0.5625
Global Iter: 194400 training loss: 0.683448
Global Iter: 194400 training acc: 0.59375
Global Iter: 194500 training loss: 0.686138
Global Iter: 194500 training acc: 0.5625
Global Iter: 194600 training loss: 0.703595
Global Iter: 194600 training acc: 0.4375
Global Iter: 194700 training loss: 0.693493
Global Iter: 194700 training acc: 0.5
Global Iter: 194800 training loss: 0.704079
Global Iter: 194800 training acc: 0.4375
Global Iter: 194900 training loss: 0.671974
Global Iter: 194900 training acc: 0.6875
Global Iter: 195000 training loss: 0.682602
Global Iter: 195000 training acc: 0.59375
Global Iter: 195100 training loss: 0.702695
Global Iter: 195100 training acc: 0.4375
Global Iter: 195200 training loss: 0.685307
Global Iter: 195200 training acc: 0.5625
Global Iter: 195300 training loss: 0.716524
Global Iter: 195300 training acc: 0.3125
Global Iter: 195400 training loss: 0.69899
Global Iter: 195400 training acc: 0.46875
Global Iter: 195500 training loss: 0.678747
Global Iter: 195500 training acc: 0.625
Global Iter: 195600 training loss: 0.679105
Global Iter: 195600 training acc: 0.625
Global Iter: 195700 training loss: 0.699316
Global Iter: 195700 training acc: 0.46875
Global Iter: 195800 training loss: 0.692006
Global Iter: 195800 training acc: 0.53125
Global Iter: 195900 training loss: 0.67739
Global Iter: 195900 training acc: 0.65625
Global Iter: 196000 training loss: 0.688778
Global Iter: 196000 training acc: 0.5625
Global Iter: 196100 training loss: 0.694419
Global Iter: 196100 training acc: 0.5
Global Iter: 196200 training loss: 0.695942
Global Iter: 196200 training acc: 0.5
Global Iter: 196300 training loss: 0.708399
Global Iter: 196300 training acc: 0.40625
Global Iter: 196400 training loss: 0.682078
Global Iter: 196400 training acc: 0.59375
Global Iter: 196500 training loss: 0.683866
Global Iter: 196500 training acc: 0.59375
Global Iter: 196600 training loss: 0.707359
Global Iter: 196600 training acc: 0.40625
Global Iter: 196700 training loss: 0.670606
Global Iter: 196700 training acc: 0.6875
Global Iter: 196800 training loss: 0.691841
Global Iter: 196800 training acc: 0.53125
Global Iter: 196900 training loss: 0.691927
Global Iter: 196900 training acc: 0.53125
Global Iter: 197000 training loss: 0.705172
Global Iter: 197000 training acc: 0.4375
Global Iter: 197100 training loss: 0.687385
Global Iter: 197100 training acc: 0.5625
Global Iter: 197200 training loss: 0.686759
Global Iter: 197200 training acc: 0.5625
Global Iter: 197300 training loss: 0.6952
Global Iter: 197300 training acc: 0.5
Global Iter: 197400 training loss: 0.714647
Global Iter: 197400 training acc: 0.34375
Global Iter: 197500 training loss: 0.69637
Global Iter: 197500 training acc: 0.5
Global Iter: 197600 training loss: 0.696815
Global Iter: 197600 training acc: 0.5
Global Iter: 197700 training loss: 0.709815
Global Iter: 197700 training acc: 0.40625
Global Iter: 197800 training loss: 0.689186
Global Iter: 197800 training acc: 0.5625
Global Iter: 197900 training loss: 0.703256
Global Iter: 197900 training acc: 0.46875
Global Iter: 198000 training loss: 0.700091
Global Iter: 198000 training acc: 0.46875
Global Iter: 198100 training loss: 0.68699
Global Iter: 198100 training acc: 0.5625
Global Iter: 198200 training loss: 0.679992
Global Iter: 198200 training acc: 0.625
Global Iter: 198300 training loss: 0.7079
Global Iter: 198300 training acc: 0.40625
Global Iter: 198400 training loss: 0.704596
Global Iter: 198400 training acc: 0.4375
Global Iter: 198500 training loss: 0.668375
Global Iter: 198500 training acc: 0.65625
Global Iter: 198600 training loss: 0.698731
Global Iter: 198600 training acc: 0.46875
Global Iter: 198700 training loss: 0.682884
Global Iter: 198700 training acc: 0.59375
Global Iter: 198800 training loss: 0.689491
Global Iter: 198800 training acc: 0.5625
Global Iter: 198900 training loss: 0.700258
Global Iter: 198900 training acc: 0.46875
Global Iter: 199000 training loss: 0.700612
Global Iter: 199000 training acc: 0.46875
Global Iter: 199100 training loss: 0.681089
Global Iter: 199100 training acc: 0.59375
Global Iter: 199200 training loss: 0.702689
Global Iter: 199200 training acc: 0.4375
Global Iter: 199300 training loss: 0.66798
Global Iter: 199300 training acc: 0.71875
Global Iter: 199400 training loss: 0.692175
Global Iter: 199400 training acc: 0.53125
Global Iter: 199500 training loss: 0.707242
Global Iter: 199500 training acc: 0.40625
Global Iter: 199600 training loss: 0.681465
Global Iter: 199600 training acc: 0.59375
Global Iter: 199700 training loss: 0.680858
Global Iter: 199700 training acc: 0.59375
Global Iter: 199800 training loss: 0.683031
Global Iter: 199800 training acc: 0.59375
Global Iter: 199900 training loss: 0.689225
Global Iter: 199900 training acc: 0.5625
Global Iter: 200000 training loss: 0.668319
Global Iter: 200000 training acc: 0.6875
Global Iter: 200100 training loss: 0.684369
Global Iter: 200100 training acc: 0.59375
Global Iter: 200200 training loss: 0.699572
Global Iter: 200200 training acc: 0.46875
Global Iter: 200300 training loss: 0.715049
Global Iter: 200300 training acc: 0.375
Global Iter: 200400 training loss: 0.696984
Global Iter: 200400 training acc: 0.5
Global Iter: 200500 training loss: 0.678507
Global Iter: 200500 training acc: 0.59375
Global Iter: 200600 training loss: 0.708655
Global Iter: 200600 training acc: 0.40625
Global Iter: 200700 training loss: 0.693306
Global Iter: 200700 training acc: 0.5
Global Iter: 200800 training loss: 0.685528
Global Iter: 200800 training acc: 0.59375
Global Iter: 200900 training loss: 0.678687
Global Iter: 200900 training acc: 0.625
Global Iter: 201000 training loss: 0.702001
Global Iter: 201000 training acc: 0.4375
Global Iter: 201100 training loss: 0.670461
Global Iter: 201100 training acc: 0.6875
Global Iter: 201200 training loss: 0.691805
Global Iter: 201200 training acc: 0.53125
Global Iter: 201300 training loss: 0.684296
Global Iter: 201300 training acc: 0.59375
Global Iter: 201400 training loss: 0.67696
Global Iter: 201400 training acc: 0.6875
Global Iter: 201500 training loss: 0.675753
Global Iter: 201500 training acc: 0.65625
Global Iter: 201600 training loss: 0.683064
Global Iter: 201600 training acc: 0.59375
Global Iter: 201700 training loss: 0.69238
Global Iter: 201700 training acc: 0.53125
Global Iter: 201800 training loss: 0.687732
Global Iter: 201800 training acc: 0.5625
Global Iter: 201900 training loss: 0.692919
Global Iter: 201900 training acc: 0.53125
Global Iter: 202000 training loss: 0.70002
Global Iter: 202000 training acc: 0.46875
Global Iter: 202100 training loss: 0.703865
Global Iter: 202100 training acc: 0.4375
Global Iter: 202200 training loss: 0.675095
Global Iter: 202200 training acc: 0.65625
Global Iter: 202300 training loss: 0.69913
Global Iter: 202300 training acc: 0.46875
Global Iter: 202400 training loss: 0.707637
Global Iter: 202400 training acc: 0.375
Global Iter: 202500 training loss: 0.668622
Global Iter: 202500 training acc: 0.71875
Global Iter: 202600 training loss: 0.695168
Global Iter: 202600 training acc: 0.5
Global Iter: 202700 training loss: 0.693857
Global Iter: 202700 training acc: 0.5
Global Iter: 202800 training loss: 0.689297
Global Iter: 202800 training acc: 0.5625
Global Iter: 202900 training loss: 0.672892
Global Iter: 202900 training acc: 0.6875
Global Iter: 203000 training loss: 0.70624
Global Iter: 203000 training acc: 0.40625
Global Iter: 203100 training loss: 0.688097
Global Iter: 203100 training acc: 0.5625
Global Iter: 203200 training loss: 0.697751
Global Iter: 203200 training acc: 0.46875
Global Iter: 203300 training loss: 0.701244
Global Iter: 203300 training acc: 0.4375
Global Iter: 203400 training loss: 0.693052
Global Iter: 203400 training acc: 0.53125
Global Iter: 203500 training loss: 0.683426
Global Iter: 203500 training acc: 0.59375
Global Iter: 203600 training loss: 0.695276
Global Iter: 203600 training acc: 0.53125
Global Iter: 203700 training loss: 0.693258
Global Iter: 203700 training acc: 0.53125
Global Iter: 203800 training loss: 0.682675
Global Iter: 203800 training acc: 0.59375
Global Iter: 203900 training loss: 0.691857
Global Iter: 203900 training acc: 0.53125
Global Iter: 204000 training loss: 0.686566
Global Iter: 204000 training acc: 0.5625
Global Iter: 204100 training loss: 0.691063
Global Iter: 204100 training acc: 0.53125
Global Iter: 204200 training loss: 0.685757
Global Iter: 204200 training acc: 0.59375
Global Iter: 204300 training loss: 0.680532
Global Iter: 204300 training acc: 0.625
Global Iter: 204400 training loss: 0.678605
Global Iter: 204400 training acc: 0.625
Global Iter: 204500 training loss: 0.702071
Global Iter: 204500 training acc: 0.4375
Global Iter: 204600 training loss: 0.693908
Global Iter: 204600 training acc: 0.5
Global Iter: 204700 training loss: 0.694485
Global Iter: 204700 training acc: 0.5
Global Iter: 204800 training loss: 0.671277
Global Iter: 204800 training acc: 0.6875
Global Iter: 204900 training loss: 0.692272
Global Iter: 204900 training acc: 0.53125
Global Iter: 205000 training loss: 0.694255
Global Iter: 205000 training acc: 0.5
Global Iter: 205100 training loss: 0.689775
Global Iter: 205100 training acc: 0.5625
Global Iter: 205200 training loss: 0.709895
Global Iter: 205200 training acc: 0.375
Global Iter: 205300 training loss: 0.694318
Global Iter: 205300 training acc: 0.5
Global Iter: 205400 training loss: 0.674674
Global Iter: 205400 training acc: 0.65625
Global Iter: 205500 training loss: 0.686444
Global Iter: 205500 training acc: 0.5625
Global Iter: 205600 training loss: 0.699082
Global Iter: 205600 training acc: 0.46875
Global Iter: 205700 training loss: 0.696241
Global Iter: 205700 training acc: 0.5
Global Iter: 205800 training loss: 0.679938
Global Iter: 205800 training acc: 0.625
Global Iter: 205900 training loss: 0.687016
Global Iter: 205900 training acc: 0.5625
Global Iter: 206000 training loss: 0.69362
Global Iter: 206000 training acc: 0.5
Global Iter: 206100 training loss: 0.695156
Global Iter: 206100 training acc: 0.5
Global Iter: 206200 training loss: 0.711879
Global Iter: 206200 training acc: 0.375
Global Iter: 206300 training loss: 0.678203
Global Iter: 206300 training acc: 0.625
Global Iter: 206400 training loss: 0.673948
Global Iter: 206400 training acc: 0.65625
Global Iter: 206500 training loss: 0.706724
Global Iter: 206500 training acc: 0.40625
Global Iter: 206600 training loss: 0.677302
Global Iter: 206600 training acc: 0.65625
Global Iter: 206700 training loss: 0.69093
Global Iter: 206700 training acc: 0.53125
Global Iter: 206800 training loss: 0.693
Global Iter: 206800 training acc: 0.53125
Global Iter: 206900 training loss: 0.704203
Global Iter: 206900 training acc: 0.4375
Global Iter: 207000 training loss: 0.687596
Global Iter: 207000 training acc: 0.5625
Global Iter: 207100 training loss: 0.689389
Global Iter: 207100 training acc: 0.5625
Global Iter: 207200 training loss: 0.695505
Global Iter: 207200 training acc: 0.5
Global Iter: 207300 training loss: 0.70873
Global Iter: 207300 training acc: 0.375
Global Iter: 207400 training loss: 0.696059
Global Iter: 207400 training acc: 0.5
Global Iter: 207500 training loss: 0.692105
Global Iter: 207500 training acc: 0.53125
Global Iter: 207600 training loss: 0.702823
Global Iter: 207600 training acc: 0.4375
Global Iter: 207700 training loss: 0.682905
Global Iter: 207700 training acc: 0.59375
Global Iter: 207800 training loss: 0.701771
Global Iter: 207800 training acc: 0.4375
Global Iter: 207900 training loss: 0.703513
Global Iter: 207900 training acc: 0.4375
Global Iter: 208000 training loss: 0.682478
Global Iter: 208000 training acc: 0.59375
Global Iter: 208100 training loss: 0.686918
Global Iter: 208100 training acc: 0.5625
Global Iter: 208200 training loss: 0.706812
Global Iter: 208200 training acc: 0.40625
Global Iter: 208300 training loss: 0.695763
Global Iter: 208300 training acc: 0.5
Global Iter: 208400 training loss: 0.664995
Global Iter: 208400 training acc: 0.71875
Global Iter: 208500 training loss: 0.696139
Global Iter: 208500 training acc: 0.5
Global Iter: 208600 training loss: 0.68126
Global Iter: 208600 training acc: 0.59375
Global Iter: 208700 training loss: 0.694168
Global Iter: 208700 training acc: 0.53125
Global Iter: 208800 training loss: 0.708774
Global Iter: 208800 training acc: 0.40625
Global Iter: 208900 training loss: 0.693659
Global Iter: 208900 training acc: 0.5
Global Iter: 209000 training loss: 0.683861
Global Iter: 209000 training acc: 0.59375
Global Iter: 209100 training loss: 0.703878
Global Iter: 209100 training acc: 0.4375
Global Iter: 209200 training loss: 0.665985
Global Iter: 209200 training acc: 0.71875
Global Iter: 209300 training loss: 0.690974
Global Iter: 209300 training acc: 0.53125
Global Iter: 209400 training loss: 0.707992
Global Iter: 209400 training acc: 0.40625
Global Iter: 209500 training loss: 0.680031
Global Iter: 209500 training acc: 0.625
Global Iter: 209600 training loss: 0.679657
Global Iter: 209600 training acc: 0.625
Global Iter: 209700 training loss: 0.684271
Global Iter: 209700 training acc: 0.59375
Global Iter: 209800 training loss: 0.694986
Global Iter: 209800 training acc: 0.5
Global Iter: 209900 training loss: 0.674984
Global Iter: 209900 training acc: 0.65625
Global Iter: 210000 training loss: 0.679691
Global Iter: 210000 training acc: 0.625
Global Iter: 210100 training loss: 0.69801
Global Iter: 210100 training acc: 0.46875
Global Iter: 210200 training loss: 0.714786
Global Iter: 210200 training acc: 0.34375
Global Iter: 210300 training loss: 0.694667
Global Iter: 210300 training acc: 0.5
Global Iter: 210400 training loss: 0.680243
Global Iter: 210400 training acc: 0.625
Global Iter: 210500 training loss: 0.699133
Global Iter: 210500 training acc: 0.46875
Global Iter: 210600 training loss: 0.694862
Global Iter: 210600 training acc: 0.5
Global Iter: 210700 training loss: 0.687217
Global Iter: 210700 training acc: 0.5625
Global Iter: 210800 training loss: 0.682808
Global Iter: 210800 training acc: 0.59375
Global Iter: 210900 training loss: 0.703156
Global Iter: 210900 training acc: 0.4375
Global Iter: 211000 training loss: 0.674418
Global Iter: 211000 training acc: 0.65625
Global Iter: 211100 training loss: 0.690942
Global Iter: 211100 training acc: 0.53125
Global Iter: 211200 training loss: 0.69029
Global Iter: 211200 training acc: 0.53125
Global Iter: 211300 training loss: 0.651272
Global Iter: 211300 training acc: 0.75
Global Iter: 211400 training loss: 0.679249
Global Iter: 211400 training acc: 0.625
Global Iter: 211500 training loss: 0.683111
Global Iter: 211500 training acc: 0.59375
Global Iter: 211600 training loss: 0.693128
Global Iter: 211600 training acc: 0.53125
Global Iter: 211700 training loss: 0.695327
Global Iter: 211700 training acc: 0.5
Global Iter: 211800 training loss: 0.695017
Global Iter: 211800 training acc: 0.5
Global Iter: 211900 training loss: 0.704453
Global Iter: 211900 training acc: 0.4375
Global Iter: 212000 training loss: 0.702064
Global Iter: 212000 training acc: 0.4375
Global Iter: 212100 training loss: 0.679168
Global Iter: 212100 training acc: 0.65625
Global Iter: 212200 training loss: 0.701086
Global Iter: 212200 training acc: 0.46875
Global Iter: 212300 training loss: 0.711909
Global Iter: 212300 training acc: 0.34375
Global Iter: 212400 training loss: 0.670356
Global Iter: 212400 training acc: 0.6875
Global Iter: 212500 training loss: 0.700392
Global Iter: 212500 training acc: 0.46875
Global Iter: 212600 training loss: 0.685953
Global Iter: 212600 training acc: 0.5625
Global Iter: 212700 training loss: 0.688435
Global Iter: 212700 training acc: 0.5625
Global Iter: 212800 training loss: 0.670923
Global Iter: 212800 training acc: 0.6875
Global Iter: 212900 training loss: 0.701976
Global Iter: 212900 training acc: 0.4375
Global Iter: 213000 training loss: 0.685521
Global Iter: 213000 training acc: 0.5625
Global Iter: 213100 training loss: 0.709294
Global Iter: 213100 training acc: 0.4375
Global Iter: 213200 training loss: 0.709748
Global Iter: 213200 training acc: 0.40625
Global Iter: 213300 training loss: 0.694493
Global Iter: 213300 training acc: 0.5
Global Iter: 213400 training loss: 0.679029
Global Iter: 213400 training acc: 0.625
Global Iter: 213500 training loss: 0.690769
Global Iter: 213500 training acc: 0.53125
Global Iter: 213600 training loss: 0.686866
Global Iter: 213600 training acc: 0.5625
Global Iter: 213700 training loss: 0.678663
Global Iter: 213700 training acc: 0.625
Global Iter: 213800 training loss: 0.68608
Global Iter: 213800 training acc: 0.5625
Global Iter: 213900 training loss: 0.685144
Global Iter: 213900 training acc: 0.5625
Global Iter: 214000 training loss: 0.696587
Global Iter: 214000 training acc: 0.5
Global Iter: 214100 training loss: 0.679416
Global Iter: 214100 training acc: 0.625
Global Iter: 214200 training loss: 0.679776
Global Iter: 214200 training acc: 0.625
Global Iter: 214300 training loss: 0.678734
Global Iter: 214300 training acc: 0.625
Global Iter: 214400 training loss: 0.71132
Global Iter: 214400 training acc: 0.375
Global Iter: 214500 training loss: 0.698011
Global Iter: 214500 training acc: 0.46875
Global Iter: 214600 training loss: 0.697639
Global Iter: 214600 training acc: 0.46875
Global Iter: 214700 training loss: 0.670791
Global Iter: 214700 training acc: 0.6875
Global Iter: 214800 training loss: 0.696346
Global Iter: 214800 training acc: 0.5
Global Iter: 214900 training loss: 0.695595
Global Iter: 214900 training acc: 0.5
Global Iter: 215000 training loss: 0.686102
Global Iter: 215000 training acc: 0.5625
Global Iter: 215100 training loss: 0.717065
Global Iter: 215100 training acc: 0.3125
Global Iter: 215200 training loss: 0.69172
Global Iter: 215200 training acc: 0.53125
Global Iter: 215300 training loss: 0.669337
Global Iter: 215300 training acc: 0.71875
Global Iter: 215400 training loss: 0.682737
Global Iter: 215400 training acc: 0.5625
Global Iter: 215500 training loss: 0.698468
Global Iter: 215500 training acc: 0.46875
Global Iter: 215600 training loss: 0.69043
Global Iter: 215600 training acc: 0.53125
Global Iter: 215700 training loss: 0.67673
Global Iter: 215700 training acc: 0.65625
Global Iter: 215800 training loss: 0.687587
Global Iter: 215800 training acc: 0.5625
Global Iter: 215900 training loss: 0.688556
Global Iter: 215900 training acc: 0.5625
Global Iter: 216000 training loss: 0.688268
Global Iter: 216000 training acc: 0.53125
Global Iter: 216100 training loss: 0.706409
Global Iter: 216100 training acc: 0.40625
Global Iter: 216200 training loss: 0.68163
Global Iter: 216200 training acc: 0.59375
Global Iter: 216300 training loss: 0.681577
Global Iter: 216300 training acc: 0.625
Global Iter: 216400 training loss: 0.697066
Global Iter: 216400 training acc: 0.46875
Global Iter: 216500 training loss: 0.677328
Global Iter: 216500 training acc: 0.625
Global Iter: 216600 training loss: 0.689889
Global Iter: 216600 training acc: 0.53125
Global Iter: 216700 training loss: 0.693837
Global Iter: 216700 training acc: 0.5
Global Iter: 216800 training loss: 0.698246
Global Iter: 216800 training acc: 0.46875
Global Iter: 216900 training loss: 0.692844
Global Iter: 216900 training acc: 0.53125
Global Iter: 217000 training loss: 0.680572
Global Iter: 217000 training acc: 0.625
Global Iter: 217100 training loss: 0.694992
Global Iter: 217100 training acc: 0.5
Global Iter: 217200 training loss: 0.708351
Global Iter: 217200 training acc: 0.4375
Global Iter: 217300 training loss: 0.701212
Global Iter: 217300 training acc: 0.46875
Global Iter: 217400 training loss: 0.694775
Global Iter: 217400 training acc: 0.5
Global Iter: 217500 training loss: 0.701
Global Iter: 217500 training acc: 0.4375
Global Iter: 217600 training loss: 0.688186
Global Iter: 217600 training acc: 0.5625
Global Iter: 217700 training loss: 0.698355
Global Iter: 217700 training acc: 0.46875
Global Iter: 217800 training loss: 0.704646
Global Iter: 217800 training acc: 0.4375
Global Iter: 217900 training loss: 0.682367
Global Iter: 217900 training acc: 0.59375
Global Iter: 218000 training loss: 0.693029
Global Iter: 218000 training acc: 0.53125
Global Iter: 218100 training loss: 0.707126
Global Iter: 218100 training acc: 0.40625
Global Iter: 218200 training loss: 0.69124
Global Iter: 218200 training acc: 0.53125
Global Iter: 218300 training loss: 0.671008
Global Iter: 218300 training acc: 0.6875
Global Iter: 218400 training loss: 0.693624
Global Iter: 218400 training acc: 0.53125
Global Iter: 218500 training loss: 0.691046
Global Iter: 218500 training acc: 0.53125
Global Iter: 218600 training loss: 0.695792
Global Iter: 218600 training acc: 0.5
Global Iter: 218700 training loss: 0.707025
Global Iter: 218700 training acc: 0.40625
Global Iter: 218800 training loss: 0.696688
Global Iter: 218800 training acc: 0.5
Global Iter: 218900 training loss: 0.687628
Global Iter: 218900 training acc: 0.5625
Global Iter: 219000 training loss: 0.696207
Global Iter: 219000 training acc: 0.46875
Global Iter: 219100 training loss: 0.669617
Global Iter: 219100 training acc: 0.71875
Global Iter: 219200 training loss: 0.696328
Global Iter: 219200 training acc: 0.5
Global Iter: 219300 training loss: 0.697735
Global Iter: 219300 training acc: 0.46875
Global Iter: 219400 training loss: 0.673949
Global Iter: 219400 training acc: 0.65625
Global Iter: 219500 training loss: 0.673607
Global Iter: 219500 training acc: 0.65625
Global Iter: 219600 training loss: 0.690334
Global Iter: 219600 training acc: 0.53125
Global Iter: 219700 training loss: 0.695893
Global Iter: 219700 training acc: 0.5
Global Iter: 219800 training loss: 0.679569
Global Iter: 219800 training acc: 0.625
Global Iter: 219900 training loss: 0.680591
Global Iter: 219900 training acc: 0.625
Global Iter: 220000 training loss: 0.695862
Global Iter: 220000 training acc: 0.5
Global Iter: 220100 training loss: 0.717668
Global Iter: 220100 training acc: 0.34375
Global Iter: 220200 training loss: 0.695449
Global Iter: 220200 training acc: 0.5
Global Iter: 220300 training loss: 0.679041
Global Iter: 220300 training acc: 0.625
Global Iter: 220400 training loss: 0.698821
Global Iter: 220400 training acc: 0.46875
Global Iter: 220500 training loss: 0.694003
Global Iter: 220500 training acc: 0.5
Global Iter: 220600 training loss: 0.687814
Global Iter: 220600 training acc: 0.5625
Global Iter: 220700 training loss: 0.679909
Global Iter: 220700 training acc: 0.625
Global Iter: 220800 training loss: 0.701959
Global Iter: 220800 training acc: 0.4375
Global Iter: 220900 training loss: 0.68064
Global Iter: 220900 training acc: 0.625
Global Iter: 221000 training loss: 0.690582
Global Iter: 221000 training acc: 0.53125
Global Iter: 221100 training loss: 0.688293
Global Iter: 221100 training acc: 0.5625
Global Iter: 221200 training loss: 0.669714
Global Iter: 221200 training acc: 0.6875
Global Iter: 221300 training loss: 0.683264
Global Iter: 221300 training acc: 0.59375
Global Iter: 221400 training loss: 0.683795
Global Iter: 221400 training acc: 0.59375
Global Iter: 221500 training loss: 0.689355
Global Iter: 221500 training acc: 0.53125
Global Iter: 221600 training loss: 0.702941
Global Iter: 221600 training acc: 0.4375
Global Iter: 221700 training loss: 0.694768
Global Iter: 221700 training acc: 0.53125
Global Iter: 221800 training loss: 0.700905
Global Iter: 221800 training acc: 0.4375
Global Iter: 221900 training loss: 0.710885
Global Iter: 221900 training acc: 0.375
Global Iter: 222000 training loss: 0.672992
Global Iter: 222000 training acc: 0.6875
Global Iter: 222100 training loss: 0.701394
Global Iter: 222100 training acc: 0.46875
Global Iter: 222200 training loss: 0.710976
Global Iter: 222200 training acc: 0.375
Global Iter: 222300 training loss: 0.667397
Global Iter: 222300 training acc: 0.71875
Global Iter: 222400 training loss: 0.703202
Global Iter: 222400 training acc: 0.4375
Global Iter: 222500 training loss: 0.688338
Global Iter: 222500 training acc: 0.5625
Global Iter: 222600 training loss: 0.686271
Global Iter: 222600 training acc: 0.5625
Global Iter: 222700 training loss: 0.668389
Global Iter: 222700 training acc: 0.6875
Global Iter: 222800 training loss: 0.695927
Global Iter: 222800 training acc: 0.46875
Global Iter: 222900 training loss: 0.695453
Global Iter: 222900 training acc: 0.5
Global Iter: 223000 training loss: 0.70139
Global Iter: 223000 training acc: 0.4375
Global Iter: 223100 training loss: 0.709951
Global Iter: 223100 training acc: 0.40625
Global Iter: 223200 training loss: 0.696318
Global Iter: 223200 training acc: 0.5
Global Iter: 223300 training loss: 0.682223
Global Iter: 223300 training acc: 0.59375
Global Iter: 223400 training loss: 0.695256
Global Iter: 223400 training acc: 0.5
Global Iter: 223500 training loss: 0.696607
Global Iter: 223500 training acc: 0.5
Global Iter: 223600 training loss: 0.684263
Global Iter: 223600 training acc: 0.59375
Global Iter: 223700 training loss: 0.690409
Global Iter: 223700 training acc: 0.53125
Global Iter: 223800 training loss: 0.68622
Global Iter: 223800 training acc: 0.5625
Global Iter: 223900 training loss: 0.694242
Global Iter: 223900 training acc: 0.5
Global Iter: 224000 training loss: 0.669997
Global Iter: 224000 training acc: 0.6875
Global Iter: 224100 training loss: 0.674699
Global Iter: 224100 training acc: 0.65625
Global Iter: 224200 training loss: 0.675153
Global Iter: 224200 training acc: 0.65625
Global Iter: 224300 training loss: 0.714087
Global Iter: 224300 training acc: 0.34375
Global Iter: 224400 training loss: 0.690927
Global Iter: 224400 training acc: 0.53125
Global Iter: 224500 training loss: 0.70027
Global Iter: 224500 training acc: 0.46875
Global Iter: 224600 training loss: 0.682452
Global Iter: 224600 training acc: 0.625
Global Iter: 224700 training loss: 0.69897
Global Iter: 224700 training acc: 0.46875
Global Iter: 224800 training loss: 0.700251
Global Iter: 224800 training acc: 0.46875
Global Iter: 224900 training loss: 0.691437
Global Iter: 224900 training acc: 0.53125
Global Iter: 225000 training loss: 0.726345
Global Iter: 225000 training acc: 0.28125
Global Iter: 225100 training loss: 0.694891
Global Iter: 225100 training acc: 0.5
Global Iter: 225200 training loss: 0.6667
Global Iter: 225200 training acc: 0.71875
Global Iter: 225300 training loss: 0.681373
Global Iter: 225300 training acc: 0.625
Global Iter: 225400 training loss: 0.703173
Global Iter: 225400 training acc: 0.4375
Global Iter: 225500 training loss: 0.687139
Global Iter: 225500 training acc: 0.5625
Global Iter: 225600 training loss: 0.679414
Global Iter: 225600 training acc: 0.625
Global Iter: 225700 training loss: 0.691614
Global Iter: 225700 training acc: 0.53125
Global Iter: 225800 training loss: 0.691137
Global Iter: 225800 training acc: 0.53125
Global Iter: 225900 training loss: 0.694174
Global Iter: 225900 training acc: 0.5
Global Iter: 226000 training loss: 0.706198
Global Iter: 226000 training acc: 0.40625
Global Iter: 226100 training loss: 0.688358
Global Iter: 226100 training acc: 0.5625
Global Iter: 226200 training loss: 0.683716
Global Iter: 226200 training acc: 0.59375
Global Iter: 226300 training loss: 0.69197
Global Iter: 226300 training acc: 0.53125
Global Iter: 226400 training loss: 0.676101
Global Iter: 226400 training acc: 0.65625
Global Iter: 226500 training loss: 0.697071
Global Iter: 226500 training acc: 0.5
Global Iter: 226600 training loss: 0.691696
Global Iter: 226600 training acc: 0.53125
Global Iter: 226700 training loss: 0.691143
Global Iter: 226700 training acc: 0.53125
Global Iter: 226800 training loss: 0.695783
Global Iter: 226800 training acc: 0.5
Global Iter: 226900 training loss: 0.679311
Global Iter: 226900 training acc: 0.59375
Global Iter: 227000 training loss: 0.695138
Global Iter: 227000 training acc: 0.5
Global Iter: 227100 training loss: 0.711746
Global Iter: 227100 training acc: 0.375
Global Iter: 227200 training loss: 0.697856
Global Iter: 227200 training acc: 0.46875
Global Iter: 227300 training loss: 0.696294
Global Iter: 227300 training acc: 0.5
Global Iter: 227400 training loss: 0.703639
Global Iter: 227400 training acc: 0.4375
Global Iter: 227500 training loss: 0.685172
Global Iter: 227500 training acc: 0.59375
Global Iter: 227600 training loss: 0.695071
Global Iter: 227600 training acc: 0.5
Global Iter: 227700 training loss: 0.701564
Global Iter: 227700 training acc: 0.46875
Global Iter: 227800 training loss: 0.682232
Global Iter: 227800 training acc: 0.59375
Global Iter: 227900 training loss: 0.695473
Global Iter: 227900 training acc: 0.5
Global Iter: 228000 training loss: 0.70327
Global Iter: 228000 training acc: 0.4375
Global Iter: 228100 training loss: 0.686174
Global Iter: 228100 training acc: 0.5625
Global Iter: 228200 training loss: 0.670113
Global Iter: 228200 training acc: 0.6875
Global Iter: 228300 training loss: 0.697834
Global Iter: 228300 training acc: 0.46875
Global Iter: 228400 training loss: 0.697378
Global Iter: 228400 training acc: 0.46875
Global Iter: 228500 training loss: 0.694216
Global Iter: 228500 training acc: 0.5
Global Iter: 228600 training loss: 0.70616
Global Iter: 228600 training acc: 0.40625
Global Iter: 228700 training loss: 0.697667
Global Iter: 228700 training acc: 0.5
Global Iter: 228800 training loss: 0.682026
Global Iter: 228800 training acc: 0.59375
Global Iter: 228900 training loss: 0.697239
Global Iter: 228900 training acc: 0.46875
Global Iter: 229000 training loss: 0.670928
Global Iter: 229000 training acc: 0.6875
Global Iter: 229100 training loss: 0.694551
Global Iter: 229100 training acc: 0.5
Global Iter: 229200 training loss: 0.697808
Global Iter: 229200 training acc: 0.46875
Global Iter: 229300 training loss: 0.67673
Global Iter: 229300 training acc: 0.65625
Global Iter: 229400 training loss: 0.683121
Global Iter: 229400 training acc: 0.59375
Global Iter: 229500 training loss: 0.68932
Global Iter: 229500 training acc: 0.53125
Global Iter: 229600 training loss: 0.696974
Global Iter: 229600 training acc: 0.53125
Global Iter: 229700 training loss: 0.672319
Global Iter: 229700 training acc: 0.6875
Global Iter: 229800 training loss: 0.68816
Global Iter: 229800 training acc: 0.5625
Global Iter: 229900 training loss: 0.695081
Global Iter: 229900 training acc: 0.5
Global Iter: 230000 training loss: 0.712549
Global Iter: 230000 training acc: 0.34375
Global Iter: 230100 training loss: 0.690696
Global Iter: 230100 training acc: 0.53125
Global Iter: 230200 training loss: 0.678831
Global Iter: 230200 training acc: 0.625
Global Iter: 230300 training loss: 0.695879
Global Iter: 230300 training acc: 0.5
Global Iter: 230400 training loss: 0.69497
Global Iter: 230400 training acc: 0.5
Global Iter: 230500 training loss: 0.690867
Global Iter: 230500 training acc: 0.53125
Global Iter: 230600 training loss: 0.679151
Global Iter: 230600 training acc: 0.625
Global Iter: 230700 training loss: 0.699627
Global Iter: 230700 training acc: 0.46875
Global Iter: 230800 training loss: 0.68323
Global Iter: 230800 training acc: 0.59375
Global Iter: 230900 training loss: 0.692417
Global Iter: 230900 training acc: 0.53125
Global Iter: 231000 training loss: 0.683831
Global Iter: 231000 training acc: 0.59375
Global Iter: 231100 training loss: 0.67193
Global Iter: 231100 training acc: 0.6875
Global Iter: 231200 training loss: 0.690592
Global Iter: 231200 training acc: 0.53125
Global Iter: 231300 training loss: 0.685721
Global Iter: 231300 training acc: 0.59375
Global Iter: 231400 training loss: 0.694927
Global Iter: 231400 training acc: 0.5
Global Iter: 231500 training loss: 0.679915
Global Iter: 231500 training acc: 0.5
Global Iter: 231600 training loss: 0.687244
Global Iter: 231600 training acc: 0.53125
Global Iter: 231700 training loss: 0.699561
Global Iter: 231700 training acc: 0.46875
Global Iter: 231800 training loss: 0.712
Global Iter: 231800 training acc: 0.375
Global Iter: 231900 training loss: 0.667534
Global Iter: 231900 training acc: 0.6875
Global Iter: 232000 training loss: 0.700141
Global Iter: 232000 training acc: 0.46875
Global Iter: 232100 training loss: 0.711899
Global Iter: 232100 training acc: 0.375
Global Iter: 232200 training loss: 0.669695
Global Iter: 232200 training acc: 0.6875
Global Iter: 232300 training loss: 0.694108
Global Iter: 232300 training acc: 0.5
Global Iter: 232400 training loss: 0.688699
Global Iter: 232400 training acc: 0.5625
Global Iter: 232500 training loss: 0.687153
Global Iter: 232500 training acc: 0.5625
Global Iter: 232600 training loss: 0.672284
Global Iter: 232600 training acc: 0.6875
Global Iter: 232700 training loss: 0.699362
Global Iter: 232700 training acc: 0.46875
Global Iter: 232800 training loss: 0.690505
Global Iter: 232800 training acc: 0.53125
Global Iter: 232900 training loss: 0.705067
Global Iter: 232900 training acc: 0.4375
Global Iter: 233000 training loss: 0.7003
Global Iter: 233000 training acc: 0.46875
Global Iter: 233100 training loss: 0.69214
Global Iter: 233100 training acc: 0.53125
Global Iter: 233200 training loss: 0.68686
Global Iter: 233200 training acc: 0.5625
Global Iter: 233300 training loss: 0.696549
Global Iter: 233300 training acc: 0.5
Global Iter: 233400 training loss: 0.705437
Global Iter: 233400 training acc: 0.4375
Global Iter: 233500 training loss: 0.68005
Global Iter: 233500 training acc: 0.625
Global Iter: 233600 training loss: 0.693377
Global Iter: 233600 training acc: 0.5625
Global Iter: 233700 training loss: 0.686532
Global Iter: 233700 training acc: 0.5625
Global Iter: 233800 training loss: 0.684775
Global Iter: 233800 training acc: 0.5625
Global Iter: 233900 training loss: 0.667608
Global Iter: 233900 training acc: 0.71875
Global Iter: 234000 training loss: 0.673949
Global Iter: 234000 training acc: 0.65625
Global Iter: 234100 training loss: 0.6793
Global Iter: 234100 training acc: 0.625
Global Iter: 234200 training loss: 0.712801
Global Iter: 234200 training acc: 0.375
Global Iter: 234300 training loss: 0.696258
Global Iter: 234300 training acc: 0.5
Global Iter: 234400 training loss: 0.694603
Global Iter: 234400 training acc: 0.5
Global Iter: 234500 training loss: 0.674748
Global Iter: 234500 training acc: 0.65625
Global Iter: 234600 training loss: 0.702887
Global Iter: 234600 training acc: 0.4375
Global Iter: 234700 training loss: 0.698877
Global Iter: 234700 training acc: 0.4375
Global Iter: 234800 training loss: 0.697027
Global Iter: 234800 training acc: 0.46875
Global Iter: 234900 training loss: 0.719939
Global Iter: 234900 training acc: 0.3125
Global Iter: 235000 training loss: 0.692078
Global Iter: 235000 training acc: 0.5
Global Iter: 235100 training loss: 0.673446
Global Iter: 235100 training acc: 0.6875
Global Iter: 235200 training loss: 0.683162
Global Iter: 235200 training acc: 0.59375
Global Iter: 235300 training loss: 0.693484
Global Iter: 235300 training acc: 0.5
Global Iter: 235400 training loss: 0.68595
Global Iter: 235400 training acc: 0.5625
Global Iter: 235500 training loss: 0.674449
Global Iter: 235500 training acc: 0.65625
Global Iter: 235600 training loss: 0.695599
Global Iter: 235600 training acc: 0.5
Global Iter: 235700 training loss: 0.695139
Global Iter: 235700 training acc: 0.5
Global Iter: 235800 training loss: 0.692892
Global Iter: 235800 training acc: 0.53125
Global Iter: 235900 training loss: 0.703361
Global Iter: 235900 training acc: 0.4375
Global Iter: 236000 training loss: 0.695389
Global Iter: 236000 training acc: 0.5
Global Iter: 236100 training loss: 0.679034
Global Iter: 236100 training acc: 0.625
Global Iter: 236200 training loss: 0.693617
Global Iter: 236200 training acc: 0.5
Global Iter: 236300 training loss: 0.679953
Global Iter: 236300 training acc: 0.625
Global Iter: 236400 training loss: 0.694022
Global Iter: 236400 training acc: 0.5
Global Iter: 236500 training loss: 0.69621
Global Iter: 236500 training acc: 0.5
Global Iter: 236600 training loss: 0.693968
Global Iter: 236600 training acc: 0.5
Global Iter: 236700 training loss: 0.701836
Global Iter: 236700 training acc: 0.4375
Global Iter: 236800 training loss: 0.690342
Global Iter: 236800 training acc: 0.53125
Global Iter: 236900 training loss: 0.690346
Global Iter: 236900 training acc: 0.53125
Global Iter: 237000 training loss: 0.708123
Global Iter: 237000 training acc: 0.40625
Global Iter: 237100 training loss: 0.706984
Global Iter: 237100 training acc: 0.40625
Global Iter: 237200 training loss: 0.704633
Global Iter: 237200 training acc: 0.4375
Global Iter: 237300 training loss: 0.702886
Global Iter: 237300 training acc: 0.4375
Global Iter: 237400 training loss: 0.680207
Global Iter: 237400 training acc: 0.625
Global Iter: 237500 training loss: 0.703367
Global Iter: 237500 training acc: 0.4375
Global Iter: 237600 training loss: 0.704425
Global Iter: 237600 training acc: 0.4375
Global Iter: 237700 training loss: 0.679557
Global Iter: 237700 training acc: 0.625
Global Iter: 237800 training loss: 0.704077
Global Iter: 237800 training acc: 0.4375
Global Iter: 237900 training loss: 0.704473
Global Iter: 237900 training acc: 0.4375
Global Iter: 238000 training loss: 0.695518
Global Iter: 238000 training acc: 0.5
Global Iter: 238100 training loss: 0.67522
Global Iter: 238100 training acc: 0.65625
Global Iter: 238200 training loss: 0.695505
Global Iter: 238200 training acc: 0.5
Global Iter: 238300 training loss: 0.694246
Global Iter: 238300 training acc: 0.5
Global Iter: 238400 training loss: 0.693423
Global Iter: 238400 training acc: 0.5
Global Iter: 238500 training loss: 0.70314
Global Iter: 238500 training acc: 0.4375
Global Iter: 238600 training loss: 0.700562
Global Iter: 238600 training acc: 0.46875
Global Iter: 238700 training loss: 0.684075
Global Iter: 238700 training acc: 0.59375
Global Iter: 238800 training loss: 0.695827
Global Iter: 238800 training acc: 0.5
Global Iter: 238900 training loss: 0.675195
Global Iter: 238900 training acc: 0.65625
Global Iter: 239000 training loss: 0.692114
Global Iter: 239000 training acc: 0.53125
Global Iter: 239100 training loss: 0.701468
Global Iter: 239100 training acc: 0.4375
Global Iter: 239200 training loss: 0.676363
Global Iter: 239200 training acc: 0.65625
Global Iter: 239300 training loss: 0.687218
Global Iter: 239300 training acc: 0.5625
Global Iter: 239400 training loss: 0.682704
Global Iter: 239400 training acc: 0.59375
Global Iter: 239500 training loss: 0.691177
Global Iter: 239500 training acc: 0.53125
Global Iter: 239600 training loss: 0.676837
Global Iter: 239600 training acc: 0.625
Global Iter: 239700 training loss: 0.682633
Global Iter: 239700 training acc: 0.59375
Global Iter: 239800 training loss: 0.695857
Global Iter: 239800 training acc: 0.5
Global Iter: 239900 training loss: 0.713852
Global Iter: 239900 training acc: 0.34375
Global Iter: 240000 training loss: 0.690801
Global Iter: 240000 training acc: 0.53125
Global Iter: 240100 training loss: 0.682428
Global Iter: 240100 training acc: 0.59375
Global Iter: 240200 training loss: 0.697462
Global Iter: 240200 training acc: 0.46875
Global Iter: 240300 training loss: 0.687026
Global Iter: 240300 training acc: 0.5625
Global Iter: 240400 training loss: 0.698669
Global Iter: 240400 training acc: 0.46875
Global Iter: 240500 training loss: 0.680102
Global Iter: 240500 training acc: 0.625
Global Iter: 240600 training loss: 0.698971
Global Iter: 240600 training acc: 0.46875
Global Iter: 240700 training loss: 0.680069
Global Iter: 240700 training acc: 0.625
Global Iter: 240800 training loss: 0.701205
Global Iter: 240800 training acc: 0.46875
Global Iter: 240900 training loss: 0.684431
Global Iter: 240900 training acc: 0.5625
Global Iter: 241000 training loss: 0.675919
Global Iter: 241000 training acc: 0.65625
Global Iter: 241100 training loss: 0.690608
Global Iter: 241100 training acc: 0.53125
Global Iter: 241200 training loss: 0.685758
Global Iter: 241200 training acc: 0.5625
Global Iter: 241300 training loss: 0.701286
Global Iter: 241300 training acc: 0.46875
Global Iter: 241400 training loss: 0.704284
Global Iter: 241400 training acc: 0.4375
Global Iter: 241500 training loss: 0.696985
Global Iter: 241500 training acc: 0.53125
Global Iter: 241600 training loss: 0.692126
Global Iter: 241600 training acc: 0.5
Global Iter: 241700 training loss: 0.701794
Global Iter: 241700 training acc: 0.4375
Global Iter: 241800 training loss: 0.671052
Global Iter: 241800 training acc: 0.6875
Global Iter: 241900 training loss: 0.701814
Global Iter: 241900 training acc: 0.4375
Global Iter: 242000 training loss: 0.713882
Global Iter: 242000 training acc: 0.375
Global Iter: 242100 training loss: 0.666738
Global Iter: 242100 training acc: 0.71875
Global Iter: 242200 training loss: 0.696001
Global Iter: 242200 training acc: 0.5
Global Iter: 242300 training loss: 0.687151
Global Iter: 242300 training acc: 0.5625
Global Iter: 242400 training loss: 0.683501
Global Iter: 242400 training acc: 0.59375
Global Iter: 242500 training loss: 0.673643
Global Iter: 242500 training acc: 0.65625
Global Iter: 242600 training loss: 0.702988
Global Iter: 242600 training acc: 0.4375
Global Iter: 242700 training loss: 0.690982
Global Iter: 242700 training acc: 0.53125
Global Iter: 242800 training loss: 0.698632
Global Iter: 242800 training acc: 0.46875
Global Iter: 242900 training loss: 0.6989
Global Iter: 242900 training acc: 0.46875
Global Iter: 243000 training loss: 0.688494
Global Iter: 243000 training acc: 0.5625
Global Iter: 243100 training loss: 0.686197
Global Iter: 243100 training acc: 0.5625
Global Iter: 243200 training loss: 0.686541
Global Iter: 243200 training acc: 0.5625
Global Iter: 243300 training loss: 0.699241
Global Iter: 243300 training acc: 0.46875
Global Iter: 243400 training loss: 0.680716
Global Iter: 243400 training acc: 0.625
Global Iter: 243500 training loss: 0.688242
Global Iter: 243500 training acc: 0.5625
Global Iter: 243600 training loss: 0.68427
Global Iter: 243600 training acc: 0.59375
Global Iter: 243700 training loss: 0.689981
Global Iter: 243700 training acc: 0.53125
Global Iter: 243800 training loss: 0.667043
Global Iter: 243800 training acc: 0.71875
Global Iter: 243900 training loss: 0.676869
Global Iter: 243900 training acc: 0.65625
Global Iter: 244000 training loss: 0.67964
Global Iter: 244000 training acc: 0.625
Global Iter: 244100 training loss: 0.708368
Global Iter: 244100 training acc: 0.40625
Global Iter: 244200 training loss: 0.696439
Global Iter: 244200 training acc: 0.5
Global Iter: 244300 training loss: 0.693576
Global Iter: 244300 training acc: 0.5
Global Iter: 244400 training loss: 0.673307
Global Iter: 244400 training acc: 0.65625
Global Iter: 244500 training loss: 0.695308
Global Iter: 244500 training acc: 0.5
Global Iter: 244600 training loss: 0.699591
Global Iter: 244600 training acc: 0.46875
Global Iter: 244700 training loss: 0.691204
Global Iter: 244700 training acc: 0.53125
Global Iter: 244800 training loss: 0.718198
Global Iter: 244800 training acc: 0.3125
Global Iter: 244900 training loss: 0.704985
Global Iter: 244900 training acc: 0.4375
Global Iter: 245000 training loss: 0.662917
Global Iter: 245000 training acc: 0.75
Global Iter: 245100 training loss: 0.690629
Global Iter: 245100 training acc: 0.53125
Global Iter: 245200 training loss: 0.686603
Global Iter: 245200 training acc: 0.5625
Global Iter: 245300 training loss: 0.692176
Global Iter: 245300 training acc: 0.53125
Global Iter: 245400 training loss: 0.67527
Global Iter: 245400 training acc: 0.65625
Global Iter: 245500 training loss: 0.698932
Global Iter: 245500 training acc: 0.46875
Global Iter: 245600 training loss: 0.68915
Global Iter: 245600 training acc: 0.53125
Global Iter: 245700 training loss: 0.695327
Global Iter: 245700 training acc: 0.5
Global Iter: 245800 training loss: 0.703801
Global Iter: 245800 training acc: 0.4375
Global Iter: 245900 training loss: 0.696334
Global Iter: 245900 training acc: 0.5
Global Iter: 246000 training loss: 0.683479
Global Iter: 246000 training acc: 0.59375
Global Iter: 246100 training loss: 0.691944
Global Iter: 246100 training acc: 0.53125
Global Iter: 246200 training loss: 0.686534
Global Iter: 246200 training acc: 0.5625
Global Iter: 246300 training loss: 0.695182
Global Iter: 246300 training acc: 0.5
Global Iter: 246400 training loss: 0.695083
Global Iter: 246400 training acc: 0.5
Global Iter: 246500 training loss: 0.693586
Global Iter: 246500 training acc: 0.5
Global Iter: 246600 training loss: 0.701361
Global Iter: 246600 training acc: 0.46875
Global Iter: 246700 training loss: 0.687846
Global Iter: 246700 training acc: 0.5625
Global Iter: 246800 training loss: 0.692028
Global Iter: 246800 training acc: 0.53125
Global Iter: 246900 training loss: 0.707244
Global Iter: 246900 training acc: 0.40625
Global Iter: 247000 training loss: 0.712775
Global Iter: 247000 training acc: 0.375
Global Iter: 247100 training loss: 0.703886
Global Iter: 247100 training acc: 0.4375
Global Iter: 247200 training loss: 0.700086
Global Iter: 247200 training acc: 0.46875
Global Iter: 247300 training loss: 0.67521
Global Iter: 247300 training acc: 0.65625
Global Iter: 247400 training loss: 0.695435
Global Iter: 247400 training acc: 0.5
Global Iter: 247500 training loss: 0.699255
Global Iter: 247500 training acc: 0.46875
Global Iter: 247600 training loss: 0.688555
Global Iter: 247600 training acc: 0.5625
Global Iter: 247700 training loss: 0.706949
Global Iter: 247700 training acc: 0.40625
Global Iter: 247800 training loss: 0.700314
Global Iter: 247800 training acc: 0.46875
Global Iter: 247900 training loss: 0.692537
Global Iter: 247900 training acc: 0.53125
Global Iter: 248000 training loss: 0.675883
Global Iter: 248000 training acc: 0.65625
Global Iter: 248100 training loss: 0.694625
Global Iter: 248100 training acc: 0.5
Global Iter: 248200 training loss: 0.690454
Global Iter: 248200 training acc: 0.53125
Global Iter: 248300 training loss: 0.691746
Global Iter: 248300 training acc: 0.53125
Global Iter: 248400 training loss: 0.701752
Global Iter: 248400 training acc: 0.46875
Global Iter: 248500 training loss: 0.694838
Global Iter: 248500 training acc: 0.5
Global Iter: 248600 training loss: 0.686517
Global Iter: 248600 training acc: 0.5625
Global Iter: 248700 training loss: 0.693308
Global Iter: 248700 training acc: 0.5
Global Iter: 248800 training loss: 0.677814
Global Iter: 248800 training acc: 0.625
Global Iter: 248900 training loss: 0.692405
Global Iter: 248900 training acc: 0.5
Global Iter: 249000 training loss: 0.702679
Global Iter: 249000 training acc: 0.4375
Global Iter: 249100 training loss: 0.679513
Global Iter: 249100 training acc: 0.65625
Global Iter: 249200 training loss: 0.69239
Global Iter: 249200 training acc: 0.53125
Global Iter: 249300 training loss: 0.691643
Global Iter: 249300 training acc: 0.53125
Global Iter: 249400 training loss: 0.696056
Global Iter: 249400 training acc: 0.5
Global Iter: 249500 training loss: 0.683018
Global Iter: 249500 training acc: 0.625
Global Iter: 249600 training loss: 0.68254
Global Iter: 249600 training acc: 0.59375
Global Iter: 249700 training loss: 0.700987
Global Iter: 249700 training acc: 0.4375
Global Iter: 249800 training loss: 0.719185
Global Iter: 249800 training acc: 0.3125
Global Iter: 249900 training loss: 0.691884
Global Iter: 249900 training acc: 0.53125
Global Iter: 250000 training loss: 0.688455
Global Iter: 250000 training acc: 0.5625
Global Iter: 250100 training loss: 0.711057
Global Iter: 250100 training acc: 0.375
Global Iter: 250200 training loss: 0.693208
Global Iter: 250200 training acc: 0.53125
Global Iter: 250300 training loss: 0.698664
Global Iter: 250300 training acc: 0.46875
Global Iter: 250400 training loss: 0.686548
Global Iter: 250400 training acc: 0.59375
Global Iter: 250500 training loss: 0.699696
Global Iter: 250500 training acc: 0.46875
Global Iter: 250600 training loss: 0.679296
Global Iter: 250600 training acc: 0.59375
Global Iter: 250700 training loss: 0.702265
Global Iter: 250700 training acc: 0.4375
Global Iter: 250800 training loss: 0.683622
Global Iter: 250800 training acc: 0.5625
Global Iter: 250900 training loss: 0.669726
Global Iter: 250900 training acc: 0.6875
Global Iter: 251000 training loss: 0.692437
Global Iter: 251000 training acc: 0.53125
Global Iter: 251100 training loss: 0.690686
Global Iter: 251100 training acc: 0.53125
Global Iter: 251200 training loss: 0.695544
Global Iter: 251200 training acc: 0.5
Global Iter: 251300 training loss: 0.701519
Global Iter: 251300 training acc: 0.4375
Global Iter: 251400 training loss: 0.698525
Global Iter: 251400 training acc: 0.46875
Global Iter: 251500 training loss: 0.690481
Global Iter: 251500 training acc: 0.53125
Global Iter: 251600 training loss: 0.69498
Global Iter: 251600 training acc: 0.5
Global Iter: 251700 training loss: 0.671272
Global Iter: 251700 training acc: 0.65625
Global Iter: 251800 training loss: 0.709142
Global Iter: 251800 training acc: 0.40625
Global Iter: 251900 training loss: 0.7117
Global Iter: 251900 training acc: 0.375
Global Iter: 252000 training loss: 0.670746
Global Iter: 252000 training acc: 0.6875
Global Iter: 252100 training loss: 0.69619
Global Iter: 252100 training acc: 0.5
Global Iter: 252200 training loss: 0.688472
Global Iter: 252200 training acc: 0.5625
Global Iter: 252300 training loss: 0.686359
Global Iter: 252300 training acc: 0.5625
Global Iter: 252400 training loss: 0.676371
Global Iter: 252400 training acc: 0.65625
Global Iter: 252500 training loss: 0.703523
Global Iter: 252500 training acc: 0.4375
Global Iter: 252600 training loss: 0.695351
Global Iter: 252600 training acc: 0.5
Global Iter: 252700 training loss: 0.700238
Global Iter: 252700 training acc: 0.46875
Global Iter: 252800 training loss: 0.700539
Global Iter: 252800 training acc: 0.46875
Global Iter: 252900 training loss: 0.692124
Global Iter: 252900 training acc: 0.53125
Global Iter: 253000 training loss: 0.696577
Global Iter: 253000 training acc: 0.5
Global Iter: 253100 training loss: 0.681662
Global Iter: 253100 training acc: 0.59375
Global Iter: 253200 training loss: 0.694315
Global Iter: 253200 training acc: 0.5
Global Iter: 253300 training loss: 0.673519
Global Iter: 253300 training acc: 0.65625
Global Iter: 253400 training loss: 0.691463
Global Iter: 253400 training acc: 0.53125
Global Iter: 253500 training loss: 0.678398
Global Iter: 253500 training acc: 0.625
Global Iter: 253600 training loss: 0.69065
Global Iter: 253600 training acc: 0.53125
Global Iter: 253700 training loss: 0.67633
Global Iter: 253700 training acc: 0.65625
Global Iter: 253800 training loss: 0.679126
Global Iter: 253800 training acc: 0.625
Global Iter: 253900 training loss: 0.676109
Global Iter: 253900 training acc: 0.65625
Global Iter: 254000 training loss: 0.698946
Global Iter: 254000 training acc: 0.4375
Global Iter: 254100 training loss: 0.690049
Global Iter: 254100 training acc: 0.53125
Global Iter: 254200 training loss: 0.684769
Global Iter: 254200 training acc: 0.5625
Global Iter: 254300 training loss: 0.671071
Global Iter: 254300 training acc: 0.65625
Global Iter: 254400 training loss: 0.69116
Global Iter: 254400 training acc: 0.53125
Global Iter: 254500 training loss: 0.697881
Global Iter: 254500 training acc: 0.46875
Global Iter: 254600 training loss: 0.694062
Global Iter: 254600 training acc: 0.5
Global Iter: 254700 training loss: 0.718337
Global Iter: 254700 training acc: 0.3125
Global Iter: 254800 training loss: 0.702725
Global Iter: 254800 training acc: 0.4375
Global Iter: 254900 training loss: 0.665368
Global Iter: 254900 training acc: 0.75
Global Iter: 255000 training loss: 0.694845
Global Iter: 255000 training acc: 0.5
Global Iter: 255100 training loss: 0.689576
Global Iter: 255100 training acc: 0.53125
Global Iter: 255200 training loss: 0.693703
Global Iter: 255200 training acc: 0.53125
Global Iter: 255300 training loss: 0.671563
Global Iter: 255300 training acc: 0.6875
Global Iter: 255400 training loss: 0.703607
Global Iter: 255400 training acc: 0.4375
Global Iter: 255500 training loss: 0.685019
Global Iter: 255500 training acc: 0.5625
Global Iter: 255600 training loss: 0.696397
Global Iter: 255600 training acc: 0.5
Global Iter: 255700 training loss: 0.698537
Global Iter: 255700 training acc: 0.46875
Global Iter: 255800 training loss: 0.694237
Global Iter: 255800 training acc: 0.5
Global Iter: 255900 training loss: 0.689969
Global Iter: 255900 training acc: 0.53125
Global Iter: 256000 training loss: 0.691967
Global Iter: 256000 training acc: 0.53125
Global Iter: 256100 training loss: 0.686243
Global Iter: 256100 training acc: 0.59375
Global Iter: 256200 training loss: 0.69135
Global Iter: 256200 training acc: 0.53125
Global Iter: 256300 training loss: 0.693986
Global Iter: 256300 training acc: 0.5
Global Iter: 256400 training loss: 0.694629
Global Iter: 256400 training acc: 0.5
Global Iter: 256500 training loss: 0.69711
Global Iter: 256500 training acc: 0.5
Global Iter: 256600 training loss: 0.687641
Global Iter: 256600 training acc: 0.5625
Global Iter: 256700 training loss: 0.691058
Global Iter: 256700 training acc: 0.53125
Global Iter: 256800 training loss: 0.702978
Global Iter: 256800 training acc: 0.4375
Global Iter: 256900 training loss: 0.7082
Global Iter: 256900 training acc: 0.40625
Global Iter: 257000 training loss: 0.694443
Global Iter: 257000 training acc: 0.5
Global Iter: 257100 training loss: 0.696467
Global Iter: 257100 training acc: 0.5
Global Iter: 257200 training loss: 0.670687
Global Iter: 257200 training acc: 0.6875
Global Iter: 257300 training loss: 0.69555
Global Iter: 257300 training acc: 0.5
Global Iter: 257400 training loss: 0.700699
Global Iter: 257400 training acc: 0.46875
Global Iter: 257500 training loss: 0.683386
Global Iter: 257500 training acc: 0.59375
Global Iter: 257600 training loss: 0.715985
Global Iter: 257600 training acc: 0.34375
Global Iter: 257700 training loss: 0.69684
Global Iter: 257700 training acc: 0.5
Global Iter: 257800 training loss: 0.682152
Global Iter: 257800 training acc: 0.59375
Global Iter: 257900 training loss: 0.671537
Global Iter: 257900 training acc: 0.6875
Global Iter: 258000 training loss: 0.70191
Global Iter: 258000 training acc: 0.4375
Global Iter: 258100 training loss: 0.693359
Global Iter: 258100 training acc: 0.5
Global Iter: 258200 training loss: 0.687533
Global Iter: 258200 training acc: 0.5625
Global Iter: 258300 training loss: 0.699404
Global Iter: 258300 training acc: 0.46875
Global Iter: 258400 training loss: 0.69677
Global Iter: 258400 training acc: 0.5
Global Iter: 258500 training loss: 0.682387
Global Iter: 258500 training acc: 0.59375
Global Iter: 258600 training loss: 0.69502
Global Iter: 258600 training acc: 0.5
Global Iter: 258700 training loss: 0.673254
Global Iter: 258700 training acc: 0.65625
Global Iter: 258800 training loss: 0.696126
Global Iter: 258800 training acc: 0.5
Global Iter: 258900 training loss: 0.704334
Global Iter: 258900 training acc: 0.4375
Global Iter: 259000 training loss: 0.67428
Global Iter: 259000 training acc: 0.65625
Global Iter: 259100 training loss: 0.693416
Global Iter: 259100 training acc: 0.53125
Global Iter: 259200 training loss: 0.690745
Global Iter: 259200 training acc: 0.53125
Global Iter: 259300 training loss: 0.695949
Global Iter: 259300 training acc: 0.5
Global Iter: 259400 training loss: 0.677827
Global Iter: 259400 training acc: 0.625
Global Iter: 259500 training loss: 0.681826
Global Iter: 259500 training acc: 0.59375
Global Iter: 259600 training loss: 0.698075
Global Iter: 259600 training acc: 0.46875
Global Iter: 259700 training loss: 0.717883
Global Iter: 259700 training acc: 0.3125
Global Iter: 259800 training loss: 0.686519
Global Iter: 259800 training acc: 0.5625
Global Iter: 259900 training loss: 0.691714
Global Iter: 259900 training acc: 0.53125
Global Iter: 260000 training loss: 0.703416
Global Iter: 260000 training acc: 0.4375
Global Iter: 260100 training loss: 0.680858
Global Iter: 260100 training acc: 0.59375
Global Iter: 260200 training loss: 0.694149
Global Iter: 260200 training acc: 0.5
Global Iter: 260300 training loss: 0.688014
Global Iter: 260300 training acc: 0.5625
Global Iter: 260400 training loss: 0.691338
Global Iter: 260400 training acc: 0.53125
Global Iter: 260500 training loss: 0.679658
Global Iter: 260500 training acc: 0.625
Global Iter: 260600 training loss: 0.704417
Global Iter: 260600 training acc: 0.4375
Global Iter: 260700 training loss: 0.691994
Global Iter: 260700 training acc: 0.53125
Global Iter: 260800 training loss: 0.67294
Global Iter: 260800 training acc: 0.6875
Global Iter: 260900 training loss: 0.694397
Global Iter: 260900 training acc: 0.5
Global Iter: 261000 training loss: 0.694797
Global Iter: 261000 training acc: 0.5
Global Iter: 261100 training loss: 0.690206
Global Iter: 261100 training acc: 0.53125
Global Iter: 261200 training loss: 0.707089
Global Iter: 261200 training acc: 0.40625
Global Iter: 261300 training loss: 0.701099
Global Iter: 261300 training acc: 0.4375
Global Iter: 261400 training loss: 0.690495
Global Iter: 261400 training acc: 0.53125
Global Iter: 261500 training loss: 0.705941
Global Iter: 261500 training acc: 0.4375
Global Iter: 261600 training loss: 0.671278
Global Iter: 261600 training acc: 0.6875
Global Iter: 261700 training loss: 0.708614
Global Iter: 261700 training acc: 0.40625
Global Iter: 261800 training loss: 0.715504
Global Iter: 261800 training acc: 0.34375
Global Iter: 261900 training loss: 0.668762
Global Iter: 261900 training acc: 0.6875
Global Iter: 262000 training loss: 0.690321
Global Iter: 262000 training acc: 0.53125
Global Iter: 262100 training loss: 0.681639
Global Iter: 262100 training acc: 0.59375
Global Iter: 262200 training loss: 0.687497
Global Iter: 262200 training acc: 0.5625
Global Iter: 262300 training loss: 0.671001
Global Iter: 262300 training acc: 0.6875
Global Iter: 262400 training loss: 0.703574
Global Iter: 262400 training acc: 0.4375
Global Iter: 262500 training loss: 0.691386
Global Iter: 262500 training acc: 0.53125
Global Iter: 262600 training loss: 0.706705
Global Iter: 262600 training acc: 0.40625
Global Iter: 262700 training loss: 0.700616
Global Iter: 262700 training acc: 0.46875
Global Iter: 262800 training loss: 0.68589
Global Iter: 262800 training acc: 0.5625
Global Iter: 262900 training loss: 0.704164
Global Iter: 262900 training acc: 0.4375
Global Iter: 263000 training loss: 0.680637
Global Iter: 263000 training acc: 0.625
Global Iter: 263100 training loss: 0.693464
Global Iter: 263100 training acc: 0.5
Global Iter: 263200 training loss: 0.679635
Global Iter: 263200 training acc: 0.65625
Global Iter: 263300 training loss: 0.695284
Global Iter: 263300 training acc: 0.5
Global Iter: 263400 training loss: 0.679462
Global Iter: 263400 training acc: 0.625
Global Iter: 263500 training loss: 0.690777
Global Iter: 263500 training acc: 0.53125
Global Iter: 263600 training loss: 0.676736
Global Iter: 263600 training acc: 0.65625
Global Iter: 263700 training loss: 0.671266
Global Iter: 263700 training acc: 0.6875
Global Iter: 263800 training loss: 0.678839
Global Iter: 263800 training acc: 0.625
Global Iter: 263900 training loss: 0.69362
Global Iter: 263900 training acc: 0.5
Global Iter: 264000 training loss: 0.694704
Global Iter: 264000 training acc: 0.5
Global Iter: 264100 training loss: 0.68699
Global Iter: 264100 training acc: 0.5625
Global Iter: 264200 training loss: 0.684512
Global Iter: 264200 training acc: 0.59375
Global Iter: 264300 training loss: 0.688346
Global Iter: 264300 training acc: 0.5625
Global Iter: 264400 training loss: 0.695243
Global Iter: 264400 training acc: 0.5
Global Iter: 264500 training loss: 0.691416
Global Iter: 264500 training acc: 0.53125
Global Iter: 264600 training loss: 0.714671
Global Iter: 264600 training acc: 0.34375
Global Iter: 264700 training loss: 0.708289
Global Iter: 264700 training acc: 0.40625
Global Iter: 264800 training loss: 0.667032
Global Iter: 264800 training acc: 0.71875
Global Iter: 264900 training loss: 0.697846
Global Iter: 264900 training acc: 0.46875
Global Iter: 265000 training loss: 0.690072
Global Iter: 265000 training acc: 0.53125
Global Iter: 265100 training loss: 0.682836
Global Iter: 265100 training acc: 0.53125
Global Iter: 265200 training loss: 0.665438
Global Iter: 265200 training acc: 0.71875
Global Iter: 265300 training loss: 0.698046
Global Iter: 265300 training acc: 0.46875
Global Iter: 265400 training loss: 0.690953
Global Iter: 265400 training acc: 0.53125
Global Iter: 265500 training loss: 0.695362
Global Iter: 265500 training acc: 0.5
Global Iter: 265600 training loss: 0.70574
Global Iter: 265600 training acc: 0.40625
Global Iter: 265700 training loss: 0.693181
Global Iter: 265700 training acc: 0.5
Global Iter: 265800 training loss: 0.693
Global Iter: 265800 training acc: 0.53125
Global Iter: 265900 training loss: 0.687335
Global Iter: 265900 training acc: 0.5625
Global Iter: 266000 training loss: 0.692188
Global Iter: 266000 training acc: 0.53125
Global Iter: 266100 training loss: 0.697272
Global Iter: 266100 training acc: 0.46875
Global Iter: 266200 training loss: 0.691766
Global Iter: 266200 training acc: 0.53125
Global Iter: 266300 training loss: 0.690275
Global Iter: 266300 training acc: 0.53125
Global Iter: 266400 training loss: 0.694847
Global Iter: 266400 training acc: 0.5
Global Iter: 266500 training loss: 0.688536
Global Iter: 266500 training acc: 0.5625
Global Iter: 266600 training loss: 0.68566
Global Iter: 266600 training acc: 0.5625
Global Iter: 266700 training loss: 0.698788
Global Iter: 266700 training acc: 0.46875
Global Iter: 266800 training loss: 0.710053
Global Iter: 266800 training acc: 0.375
Global Iter: 266900 training loss: 0.69952
Global Iter: 266900 training acc: 0.46875
Global Iter: 267000 training loss: 0.69588
Global Iter: 267000 training acc: 0.5
Global Iter: 267100 training loss: 0.675655
Global Iter: 267100 training acc: 0.65625
Global Iter: 267200 training loss: 0.693135
Global Iter: 267200 training acc: 0.53125
Global Iter: 267300 training loss: 0.695005
Global Iter: 267300 training acc: 0.5
Global Iter: 267400 training loss: 0.678149
Global Iter: 267400 training acc: 0.625
Global Iter: 267500 training loss: 0.721008
Global Iter: 267500 training acc: 0.3125
Global Iter: 267600 training loss: 0.696556
Global Iter: 267600 training acc: 0.46875
Global Iter: 267700 training loss: 0.683265
Global Iter: 267700 training acc: 0.59375
Global Iter: 267800 training loss: 0.672706
Global Iter: 267800 training acc: 0.6875
Global Iter: 267900 training loss: 0.697396
Global Iter: 267900 training acc: 0.46875
Global Iter: 268000 training loss: 0.699951
Global Iter: 268000 training acc: 0.46875
Global Iter: 268100 training loss: 0.686087
Global Iter: 268100 training acc: 0.5625
Global Iter: 268200 training loss: 0.695338
Global Iter: 268200 training acc: 0.5
Global Iter: 268300 training loss: 0.691651
Global Iter: 268300 training acc: 0.53125
Global Iter: 268400 training loss: 0.678762
Global Iter: 268400 training acc: 0.625
Global Iter: 268500 training loss: 0.693888
Global Iter: 268500 training acc: 0.5
Global Iter: 268600 training loss: 0.675494
Global Iter: 268600 training acc: 0.65625
Global Iter: 268700 training loss: 0.690316
Global Iter: 268700 training acc: 0.53125
Global Iter: 268800 training loss: 0.703413
Global Iter: 268800 training acc: 0.4375
Global Iter: 268900 training loss: 0.671401
Global Iter: 268900 training acc: 0.6875
Global Iter: 269000 training loss: 0.687781
Global Iter: 269000 training acc: 0.5625
Global Iter: 269100 training loss: 0.695261
Global Iter: 269100 training acc: 0.5
Global Iter: 269200 training loss: 0.695551
Global Iter: 269200 training acc: 0.5
Global Iter: 269300 training loss: 0.683242
Global Iter: 269300 training acc: 0.59375
Global Iter: 269400 training loss: 0.691189
Global Iter: 269400 training acc: 0.53125
Global Iter: 269500 training loss: 0.698705
Global Iter: 269500 training acc: 0.46875
Global Iter: 269600 training loss: 0.715645
Global Iter: 269600 training acc: 0.34375
Global Iter: 269700 training loss: 0.687783
Global Iter: 269700 training acc: 0.5625
Global Iter: 269800 training loss: 0.696013
Global Iter: 269800 training acc: 0.5
Global Iter: 269900 training loss: 0.706895
Global Iter: 269900 training acc: 0.40625
Global Iter: 270000 training loss: 0.678756
Global Iter: 270000 training acc: 0.625
Global Iter: 270100 training loss: 0.69412
Global Iter: 270100 training acc: 0.5
Global Iter: 270200 training loss: 0.691787
Global Iter: 270200 training acc: 0.53125
Global Iter: 270300 training loss: 0.693192
Global Iter: 270300 training acc: 0.53125
Global Iter: 270400 training loss: 0.674819
Global Iter: 270400 training acc: 0.65625
Global Iter: 270500 training loss: 0.699196
Global Iter: 270500 training acc: 0.46875
Global Iter: 270600 training loss: 0.692069
Global Iter: 270600 training acc: 0.53125
Global Iter: 270700 training loss: 0.674771
Global Iter: 270700 training acc: 0.65625
Global Iter: 270800 training loss: 0.700058
Global Iter: 270800 training acc: 0.46875
Global Iter: 270900 training loss: 0.695371
Global Iter: 270900 training acc: 0.5
Global Iter: 271000 training loss: 0.683783
Global Iter: 271000 training acc: 0.59375
Global Iter: 271100 training loss: 0.698302
Global Iter: 271100 training acc: 0.46875
Global Iter: 271200 training loss: 0.70677
Global Iter: 271200 training acc: 0.40625
Global Iter: 271300 training loss: 0.687392
Global Iter: 271300 training acc: 0.5625
Global Iter: 271400 training loss: 0.706138
Global Iter: 271400 training acc: 0.40625
Global Iter: 271500 training loss: 0.67377
Global Iter: 271500 training acc: 0.65625
Global Iter: 271600 training loss: 0.698868
Global Iter: 271600 training acc: 0.46875
Global Iter: 271700 training loss: 0.723544
Global Iter: 271700 training acc: 0.28125
Global Iter: 271800 training loss: 0.679158
Global Iter: 271800 training acc: 0.625
Global Iter: 271900 training loss: 0.689543
Global Iter: 271900 training acc: 0.53125
Global Iter: 272000 training loss: 0.68355
Global Iter: 272000 training acc: 0.59375
Global Iter: 272100 training loss: 0.691746
Global Iter: 272100 training acc: 0.53125
Global Iter: 272200 training loss: 0.671951
Global Iter: 272200 training acc: 0.6875
Global Iter: 272300 training loss: 0.698728
Global Iter: 272300 training acc: 0.46875
Global Iter: 272400 training loss: 0.691936
Global Iter: 272400 training acc: 0.53125
Global Iter: 272500 training loss: 0.70377
Global Iter: 272500 training acc: 0.4375
Global Iter: 272600 training loss: 0.699401
Global Iter: 272600 training acc: 0.46875
Global Iter: 272700 training loss: 0.687208
Global Iter: 272700 training acc: 0.5625
Global Iter: 272800 training loss: 0.701447
Global Iter: 272800 training acc: 0.4375
Global Iter: 272900 training loss: 0.687695
Global Iter: 272900 training acc: 0.5625
Global Iter: 273000 training loss: 0.692441
Global Iter: 273000 training acc: 0.53125
Global Iter: 273100 training loss: 0.674138
Global Iter: 273100 training acc: 0.65625
Global Iter: 273200 training loss: 0.692739
Global Iter: 273200 training acc: 0.5
Global Iter: 273300 training loss: 0.680692
Global Iter: 273300 training acc: 0.625
Global Iter: 273400 training loss: 0.688527
Global Iter: 273400 training acc: 0.5625
Global Iter: 273500 training loss: 0.681734
Global Iter: 273500 training acc: 0.59375
Global Iter: 273600 training loss: 0.66992
Global Iter: 273600 training acc: 0.6875
Global Iter: 273700 training loss: 0.679341
Global Iter: 273700 training acc: 0.625
Global Iter: 273800 training loss: 0.686965
Global Iter: 273800 training acc: 0.5625
Global Iter: 273900 training loss: 0.691045
Global Iter: 273900 training acc: 0.53125
Global Iter: 274000 training loss: 0.680722
Global Iter: 274000 training acc: 0.625
Global Iter: 274100 training loss: 0.683844
Global Iter: 274100 training acc: 0.59375
Global Iter: 274200 training loss: 0.690681
Global Iter: 274200 training acc: 0.53125
Global Iter: 274300 training loss: 0.699168
Global Iter: 274300 training acc: 0.46875
Global Iter: 274400 training loss: 0.682755
Global Iter: 274400 training acc: 0.59375
Global Iter: 274500 training loss: 0.710925
Global Iter: 274500 training acc: 0.375
Global Iter: 274600 training loss: 0.706698
Global Iter: 274600 training acc: 0.40625
Global Iter: 274700 training loss: 0.670734
Global Iter: 274700 training acc: 0.6875
Global Iter: 274800 training loss: 0.695068
Global Iter: 274800 training acc: 0.5
Global Iter: 274900 training loss: 0.695365
Global Iter: 274900 training acc: 0.5
Global Iter: 275000 training loss: 0.688055
Global Iter: 275000 training acc: 0.5625
Global Iter: 275100 training loss: 0.667011
Global Iter: 275100 training acc: 0.71875
Global Iter: 275200 training loss: 0.695817
Global Iter: 275200 training acc: 0.5
Global Iter: 275300 training loss: 0.687989
Global Iter: 275300 training acc: 0.5625
Global Iter: 275400 training loss: 0.695898
Global Iter: 275400 training acc: 0.5
Global Iter: 275500 training loss: 0.710014
Global Iter: 275500 training acc: 0.40625
Global Iter: 275600 training loss: 0.702926
Global Iter: 275600 training acc: 0.4375
Global Iter: 275700 training loss: 0.683186
Global Iter: 275700 training acc: 0.59375
Global Iter: 275800 training loss: 0.690148
Global Iter: 275800 training acc: 0.53125
Global Iter: 275900 training loss: 0.69569
Global Iter: 275900 training acc: 0.5
Global Iter: 276000 training loss: 0.694284
Global Iter: 276000 training acc: 0.5
Global Iter: 276100 training loss: 0.687365
Global Iter: 276100 training acc: 0.5625
Global Iter: 276200 training loss: 0.688406
Global Iter: 276200 training acc: 0.5625
Global Iter: 276300 training loss: 0.695455
Global Iter: 276300 training acc: 0.5
Global Iter: 276400 training loss: 0.690686
Global Iter: 276400 training acc: 0.53125
Global Iter: 276500 training loss: 0.685347
Global Iter: 276500 training acc: 0.5625
Global Iter: 276600 training loss: 0.696683
Global Iter: 276600 training acc: 0.5
Global Iter: 276700 training loss: 0.706747
Global Iter: 276700 training acc: 0.40625
Global Iter: 276800 training loss: 0.695906
Global Iter: 276800 training acc: 0.5
Global Iter: 276900 training loss: 0.695954
Global Iter: 276900 training acc: 0.5
Global Iter: 277000 training loss: 0.67165
Global Iter: 277000 training acc: 0.6875
Global Iter: 277100 training loss: 0.691045
Global Iter: 277100 training acc: 0.53125
Global Iter: 277200 training loss: 0.694254
Global Iter: 277200 training acc: 0.5
Global Iter: 277300 training loss: 0.686639
Global Iter: 277300 training acc: 0.5625
Global Iter: 277400 training loss: 0.720345
Global Iter: 277400 training acc: 0.3125
Global Iter: 277500 training loss: 0.698655
Global Iter: 277500 training acc: 0.46875
Global Iter: 277600 training loss: 0.683577
Global Iter: 277600 training acc: 0.59375
Global Iter: 277700 training loss: 0.674629
Global Iter: 277700 training acc: 0.65625
Global Iter: 277800 training loss: 0.698652
Global Iter: 277800 training acc: 0.46875
Global Iter: 277900 training loss: 0.695194
Global Iter: 277900 training acc: 0.5
Global Iter: 278000 training loss: 0.683225
Global Iter: 278000 training acc: 0.59375
Global Iter: 278100 training loss: 0.691383
Global Iter: 278100 training acc: 0.53125
Global Iter: 278200 training loss: 0.699156
Global Iter: 278200 training acc: 0.5
Global Iter: 278300 training loss: 0.684944
Global Iter: 278300 training acc: 0.5625
Global Iter: 278400 training loss: 0.69622
Global Iter: 278400 training acc: 0.5
Global Iter: 278500 training loss: 0.678276
Global Iter: 278500 training acc: 0.625
Global Iter: 278600 training loss: 0.686671
Global Iter: 278600 training acc: 0.5625
Global Iter: 278700 training loss: 0.700174
Global Iter: 278700 training acc: 0.4375
Global Iter: 278800 training loss: 0.667464
Global Iter: 278800 training acc: 0.71875
Global Iter: 278900 training loss: 0.687539
Global Iter: 278900 training acc: 0.5625
Global Iter: 279000 training loss: 0.694221
Global Iter: 279000 training acc: 0.5
Global Iter: 279100 training loss: 0.690655
Global Iter: 279100 training acc: 0.53125
Global Iter: 279200 training loss: 0.690268
Global Iter: 279200 training acc: 0.53125
Global Iter: 279300 training loss: 0.695686
Global Iter: 279300 training acc: 0.5
Global Iter: 279400 training loss: 0.6905
Global Iter: 279400 training acc: 0.53125
Global Iter: 279500 training loss: 0.715509
Global Iter: 279500 training acc: 0.34375
Global Iter: 279600 training loss: 0.690787
Global Iter: 279600 training acc: 0.53125
Global Iter: 279700 training loss: 0.69529
Global Iter: 279700 training acc: 0.5
Global Iter: 279800 training loss: 0.706618
Global Iter: 279800 training acc: 0.40625
Global Iter: 279900 training loss: 0.678231
Global Iter: 279900 training acc: 0.625
Global Iter: 280000 training loss: 0.691452
Global Iter: 280000 training acc: 0.53125
Global Iter: 280100 training loss: 0.691996
Global Iter: 280100 training acc: 0.53125
Global Iter: 280200 training loss: 0.682925
Global Iter: 280200 training acc: 0.59375
Global Iter: 280300 training loss: 0.680099
Global Iter: 280300 training acc: 0.625
Global Iter: 280400 training loss: 0.703406
Global Iter: 280400 training acc: 0.4375
Global Iter: 280500 training loss: 0.691346
Global Iter: 280500 training acc: 0.53125
Global Iter: 280600 training loss: 0.674936
Global Iter: 280600 training acc: 0.65625
Global Iter: 280700 training loss: 0.70386
Global Iter: 280700 training acc: 0.4375
Global Iter: 280800 training loss: 0.686931
Global Iter: 280800 training acc: 0.5625
Global Iter: 280900 training loss: 0.688083
Global Iter: 280900 training acc: 0.5625
Global Iter: 281000 training loss: 0.699038
Global Iter: 281000 training acc: 0.46875
Global Iter: 281100 training loss: 0.69889
Global Iter: 281100 training acc: 0.46875
Global Iter: 281200 training loss: 0.687013
Global Iter: 281200 training acc: 0.5625
Global Iter: 281300 training loss: 0.702778
Global Iter: 281300 training acc: 0.4375
Global Iter: 281400 training loss: 0.671212
Global Iter: 281400 training acc: 0.6875
Global Iter: 281500 training loss: 0.698541
Global Iter: 281500 training acc: 0.46875
Global Iter: 281600 training loss: 0.715747
Global Iter: 281600 training acc: 0.34375
Global Iter: 281700 training loss: 0.682888
Global Iter: 281700 training acc: 0.59375
Global Iter: 281800 training loss: 0.693292
Global Iter: 281800 training acc: 0.53125
Global Iter: 281900 training loss: 0.675079
Global Iter: 281900 training acc: 0.65625
Global Iter: 282000 training loss: 0.685105
Global Iter: 282000 training acc: 0.5625
Global Iter: 282100 training loss: 0.667352
Global Iter: 282100 training acc: 0.71875
Global Iter: 282200 training loss: 0.694163
Global Iter: 282200 training acc: 0.5
Global Iter: 282300 training loss: 0.690638
Global Iter: 282300 training acc: 0.53125
Global Iter: 282400 training loss: 0.706879
Global Iter: 282400 training acc: 0.40625
Global Iter: 282500 training loss: 0.688985
Global Iter: 282500 training acc: 0.53125
Global Iter: 282600 training loss: 0.679323
Global Iter: 282600 training acc: 0.625
Global Iter: 282700 training loss: 0.70476
Global Iter: 282700 training acc: 0.4375
Global Iter: 282800 training loss: 0.691927
Global Iter: 282800 training acc: 0.53125
Global Iter: 282900 training loss: 0.68817
Global Iter: 282900 training acc: 0.5625
Global Iter: 283000 training loss: 0.672135
Global Iter: 283000 training acc: 0.6875
Global Iter: 283100 training loss: 0.702176
Global Iter: 283100 training acc: 0.4375
Global Iter: 283200 training loss: 0.678607
Global Iter: 283200 training acc: 0.625
Global Iter: 283300 training loss: 0.687469
Global Iter: 283300 training acc: 0.5625
Global Iter: 283400 training loss: 0.679539
Global Iter: 283400 training acc: 0.625
Global Iter: 283500 training loss: 0.668705
Global Iter: 283500 training acc: 0.6875
Global Iter: 283600 training loss: 0.680133
Global Iter: 283600 training acc: 0.625
Global Iter: 283700 training loss: 0.693813
Global Iter: 283700 training acc: 0.53125
Global Iter: 283800 training loss: 0.686604
Global Iter: 283800 training acc: 0.5625
Global Iter: 283900 training loss: 0.678562
Global Iter: 283900 training acc: 0.625
Global Iter: 284000 training loss: 0.685581
Global Iter: 284000 training acc: 0.59375
Global Iter: 284100 training loss: 0.696347
Global Iter: 284100 training acc: 0.5
Global Iter: 284200 training loss: 0.70537
Global Iter: 284200 training acc: 0.4375
Global Iter: 284300 training loss: 0.684221
Global Iter: 284300 training acc: 0.59375
Global Iter: 284400 training loss: 0.705908
Global Iter: 284400 training acc: 0.40625
Global Iter: 284500 training loss: 0.711742
Global Iter: 284500 training acc: 0.375
Global Iter: 284600 training loss: 0.670547
Global Iter: 284600 training acc: 0.6875
Global Iter: 284700 training loss: 0.69483
Global Iter: 284700 training acc: 0.5
Global Iter: 284800 training loss: 0.696877
Global Iter: 284800 training acc: 0.5
Global Iter: 284900 training loss: 0.689502
Global Iter: 284900 training acc: 0.5625
Global Iter: 285000 training loss: 0.662995
Global Iter: 285000 training acc: 0.71875
Global Iter: 285100 training loss: 0.698219
Global Iter: 285100 training acc: 0.46875
Global Iter: 285200 training loss: 0.686243
Global Iter: 285200 training acc: 0.5625
Global Iter: 285300 training loss: 0.702311
Global Iter: 285300 training acc: 0.46875
Global Iter: 285400 training loss: 0.703511
Global Iter: 285400 training acc: 0.4375
Global Iter: 285500 training loss: 0.698359
Global Iter: 285500 training acc: 0.46875
Global Iter: 285600 training loss: 0.683903
Global Iter: 285600 training acc: 0.59375
Global Iter: 285700 training loss: 0.696581
Global Iter: 285700 training acc: 0.5
Global Iter: 285800 training loss: 0.689854
Global Iter: 285800 training acc: 0.53125
Global Iter: 285900 training loss: 0.695425
Global Iter: 285900 training acc: 0.5
Global Iter: 286000 training loss: 0.688309
Global Iter: 286000 training acc: 0.5625
Global Iter: 286100 training loss: 0.690296
Global Iter: 286100 training acc: 0.53125
Global Iter: 286200 training loss: 0.695947
Global Iter: 286200 training acc: 0.5
Global Iter: 286300 training loss: 0.682083
Global Iter: 286300 training acc: 0.59375
Global Iter: 286400 training loss: 0.684921
Global Iter: 286400 training acc: 0.59375
Global Iter: 286500 training loss: 0.690518
Global Iter: 286500 training acc: 0.53125
Global Iter: 286600 training loss: 0.708415
Global Iter: 286600 training acc: 0.40625
Global Iter: 286700 training loss: 0.691993
Global Iter: 286700 training acc: 0.53125
Global Iter: 286800 training loss: 0.699005
Global Iter: 286800 training acc: 0.46875
Global Iter: 286900 training loss: 0.668445
Global Iter: 286900 training acc: 0.6875
Global Iter: 287000 training loss: 0.687288
Global Iter: 287000 training acc: 0.5625
Global Iter: 287100 training loss: 0.703806
Global Iter: 287100 training acc: 0.4375
Global Iter: 287200 training loss: 0.687656
Global Iter: 287200 training acc: 0.5625
Global Iter: 287300 training loss: 0.71556
Global Iter: 287300 training acc: 0.34375
Global Iter: 287400 training loss: 0.703153
Global Iter: 287400 training acc: 0.4375
Global Iter: 287500 training loss: 0.68231
Global Iter: 287500 training acc: 0.59375
Global Iter: 287600 training loss: 0.679246
Global Iter: 287600 training acc: 0.625
Global Iter: 287700 training loss: 0.694801
Global Iter: 287700 training acc: 0.5
Global Iter: 287800 training loss: 0.698005
Global Iter: 287800 training acc: 0.5
Global Iter: 287900 training loss: 0.679861
Global Iter: 287900 training acc: 0.625
Global Iter: 288000 training loss: 0.687461
Global Iter: 288000 training acc: 0.5625
Global Iter: 288100 training loss: 0.693715
Global Iter: 288100 training acc: 0.5
Global Iter: 288200 training loss: 0.690318
Global Iter: 288200 training acc: 0.53125
Global Iter: 288300 training loss: 0.699856
Global Iter: 288300 training acc: 0.46875
Global Iter: 288400 training loss: 0.683674
Global Iter: 288400 training acc: 0.59375
Global Iter: 288500 training loss: 0.688772
Global Iter: 288500 training acc: 0.5625
Global Iter: 288600 training loss: 0.706619
Global Iter: 288600 training acc: 0.40625
Global Iter: 288700 training loss: 0.669405
Global Iter: 288700 training acc: 0.71875
Global Iter: 288800 training loss: 0.686512
Global Iter: 288800 training acc: 0.5625
Global Iter: 288900 training loss: 0.689998
Global Iter: 288900 training acc: 0.53125
Global Iter: 289000 training loss: 0.699918
Global Iter: 289000 training acc: 0.46875
Global Iter: 289100 training loss: 0.689245
Global Iter: 289100 training acc: 0.5625
Global Iter: 289200 training loss: 0.689866
Global Iter: 289200 training acc: 0.53125
Global Iter: 289300 training loss: 0.691955
Global Iter: 289300 training acc: 0.53125
Global Iter: 289400 training loss: 0.710212
Global Iter: 289400 training acc: 0.375
Global Iter: 289500 training loss: 0.694757
Global Iter: 289500 training acc: 0.5
Global Iter: 289600 training loss: 0.694646
Global Iter: 289600 training acc: 0.5
Global Iter: 289700 training loss: 0.702816
Global Iter: 289700 training acc: 0.4375
Global Iter: 289800 training loss: 0.684164
Global Iter: 289800 training acc: 0.59375
Global Iter: 289900 training loss: 0.690722
Global Iter: 289900 training acc: 0.53125
Global Iter: 290000 training loss: 0.695147
Global Iter: 290000 training acc: 0.5
Global Iter: 290100 training loss: 0.691649
Global Iter: 290100 training acc: 0.53125
Global Iter: 290200 training loss: 0.677759
Global Iter: 290200 training acc: 0.625
Global Iter: 290300 training loss: 0.702459
Global Iter: 290300 training acc: 0.4375
Global Iter: 290400 training loss: 0.698478
Global Iter: 290400 training acc: 0.46875
Global Iter: 290500 training loss: 0.674588
Global Iter: 290500 training acc: 0.65625
Global Iter: 290600 training loss: 0.704646
Global Iter: 290600 training acc: 0.4375
Global Iter: 290700 training loss: 0.682118
Global Iter: 290700 training acc: 0.59375
Global Iter: 290800 training loss: 0.682755
Global Iter: 290800 training acc: 0.59375
Global Iter: 290900 training loss: 0.702453
Global Iter: 290900 training acc: 0.46875
Global Iter: 291000 training loss: 0.699735
Global Iter: 291000 training acc: 0.46875
Global Iter: 291100 training loss: 0.678846
Global Iter: 291100 training acc: 0.625
Global Iter: 291200 training loss: 0.696733
Global Iter: 291200 training acc: 0.46875
Global Iter: 291300 training loss: 0.665501
Global Iter: 291300 training acc: 0.75
Global Iter: 291400 training loss: 0.696658
Global Iter: 291400 training acc: 0.5
Global Iter: 291500 training loss: 0.711217
Global Iter: 291500 training acc: 0.375
Global Iter: 291600 training loss: 0.679009
Global Iter: 291600 training acc: 0.625
Global Iter: 291700 training loss: 0.687188
Global Iter: 291700 training acc: 0.5625
Global Iter: 291800 training loss: 0.679494
Global Iter: 291800 training acc: 0.625
Global Iter: 291900 training loss: 0.696212
Global Iter: 291900 training acc: 0.5
Global Iter: 292000 training loss: 0.672093
Global Iter: 292000 training acc: 0.6875
Global Iter: 292100 training loss: 0.691684
Global Iter: 292100 training acc: 0.53125
Global Iter: 292200 training loss: 0.690949
Global Iter: 292200 training acc: 0.53125
Global Iter: 292300 training loss: 0.705054
Global Iter: 292300 training acc: 0.40625
Global Iter: 292400 training loss: 0.690595
Global Iter: 292400 training acc: 0.53125
Global Iter: 292500 training loss: 0.669364
Global Iter: 292500 training acc: 0.6875
Global Iter: 292600 training loss: 0.706879
Global Iter: 292600 training acc: 0.40625
Global Iter: 292700 training loss: 0.69213
Global Iter: 292700 training acc: 0.53125
Global Iter: 292800 training loss: 0.685938
Global Iter: 292800 training acc: 0.5625
Global Iter: 292900 training loss: 0.674431
Global Iter: 292900 training acc: 0.65625
Global Iter: 293000 training loss: 0.698278
Global Iter: 293000 training acc: 0.46875
Global Iter: 293100 training loss: 0.675889
Global Iter: 293100 training acc: 0.625
Global Iter: 293200 training loss: 0.68868
Global Iter: 293200 training acc: 0.5625
Global Iter: 293300 training loss: 0.680318
Global Iter: 293300 training acc: 0.65625
Global Iter: 293400 training loss: 0.67533
Global Iter: 293400 training acc: 0.6875
Global Iter: 293500 training loss: 0.673773
Global Iter: 293500 training acc: 0.65625
Global Iter: 293600 training loss: 0.687829
Global Iter: 293600 training acc: 0.5625
Global Iter: 293700 training loss: 0.684898
Global Iter: 293700 training acc: 0.5625
Global Iter: 293800 training loss: 0.682424
Global Iter: 293800 training acc: 0.59375
Global Iter: 293900 training loss: 0.687643
Global Iter: 293900 training acc: 0.5625
Global Iter: 294000 training loss: 0.690413
Global Iter: 294000 training acc: 0.53125
Global Iter: 294100 training loss: 0.702619
Global Iter: 294100 training acc: 0.4375
Global Iter: 294200 training loss: 0.680135
Global Iter: 294200 training acc: 0.625
Global Iter: 294300 training loss: 0.703269
Global Iter: 294300 training acc: 0.4375
Global Iter: 294400 training loss: 0.711459
Global Iter: 294400 training acc: 0.375
Global Iter: 294500 training loss: 0.662219
Global Iter: 294500 training acc: 0.75
Global Iter: 294600 training loss: 0.694548
Global Iter: 294600 training acc: 0.5
Global Iter: 294700 training loss: 0.689689
Global Iter: 294700 training acc: 0.53125
Global Iter: 294800 training loss: 0.683483
Global Iter: 294800 training acc: 0.59375
Global Iter: 294900 training loss: 0.671566
Global Iter: 294900 training acc: 0.6875
Global Iter: 295000 training loss: 0.707701
Global Iter: 295000 training acc: 0.40625
Global Iter: 295100 training loss: 0.683779
Global Iter: 295100 training acc: 0.59375
Global Iter: 295200 training loss: 0.699406
Global Iter: 295200 training acc: 0.46875
Global Iter: 295300 training loss: 0.708538
Global Iter: 295300 training acc: 0.40625
Global Iter: 295400 training loss: 0.691364
Global Iter: 295400 training acc: 0.53125
Global Iter: 295500 training loss: 0.681952
Global Iter: 295500 training acc: 0.59375
Global Iter: 295600 training loss: 0.686166
Global Iter: 295600 training acc: 0.5625
Global Iter: 295700 training loss: 0.6879
Global Iter: 295700 training acc: 0.5625
Global Iter: 295800 training loss: 0.690983
Global Iter: 295800 training acc: 0.53125
Global Iter: 295900 training loss: 0.691651
Global Iter: 295900 training acc: 0.53125
Global Iter: 296000 training loss: 0.690364
Global Iter: 296000 training acc: 0.53125
Global Iter: 296100 training loss: 0.690319
Global Iter: 296100 training acc: 0.53125
Global Iter: 296200 training loss: 0.688005
Global Iter: 296200 training acc: 0.5625
Global Iter: 296300 training loss: 0.684071
Global Iter: 296300 training acc: 0.59375
Global Iter: 296400 training loss: 0.687214
Global Iter: 296400 training acc: 0.5625
Global Iter: 296500 training loss: 0.703877
Global Iter: 296500 training acc: 0.4375
Global Iter: 296600 training loss: 0.693542
Global Iter: 296600 training acc: 0.5
Global Iter: 296700 training loss: 0.704831
Global Iter: 296700 training acc: 0.4375
Global Iter: 296800 training loss: 0.670804
Global Iter: 296800 training acc: 0.6875
Global Iter: 296900 training loss: 0.683836
Global Iter: 296900 training acc: 0.59375
Global Iter: 297000 training loss: 0.703221
Global Iter: 297000 training acc: 0.4375
Global Iter: 297100 training loss: 0.687696
Global Iter: 297100 training acc: 0.5625
Global Iter: 297200 training loss: 0.719217
Global Iter: 297200 training acc: 0.3125
Global Iter: 297300 training loss: 0.698954
Global Iter: 297300 training acc: 0.46875
Global Iter: 297400 training loss: 0.678252
Global Iter: 297400 training acc: 0.625
Global Iter: 297500 training loss: 0.67917
Global Iter: 297500 training acc: 0.625
Global Iter: 297600 training loss: 0.699188
Global Iter: 297600 training acc: 0.46875
Global Iter: 297700 training loss: 0.690689
Global Iter: 297700 training acc: 0.53125
Global Iter: 297800 training loss: 0.67512
Global Iter: 297800 training acc: 0.65625
Global Iter: 297900 training loss: 0.687812
Global Iter: 297900 training acc: 0.5625
Global Iter: 298000 training loss: 0.694227
Global Iter: 298000 training acc: 0.5
Global Iter: 298100 training loss: 0.694829
Global Iter: 298100 training acc: 0.5
Global Iter: 298200 training loss: 0.706859
Global Iter: 298200 training acc: 0.40625
Global Iter: 298300 training loss: 0.682159
Global Iter: 298300 training acc: 0.59375
Global Iter: 298400 training loss: 0.683727
Global Iter: 298400 training acc: 0.59375
Global Iter: 298500 training loss: 0.706048
Global Iter: 298500 training acc: 0.40625
Global Iter: 298600 training loss: 0.670763
Global Iter: 298600 training acc: 0.6875
Global Iter: 298700 training loss: 0.692233
Global Iter: 298700 training acc: 0.53125
Global Iter: 298800 training loss: 0.69092
Global Iter: 298800 training acc: 0.53125
Global Iter: 298900 training loss: 0.704457
Global Iter: 298900 training acc: 0.4375
Global Iter: 299000 training loss: 0.687596
Global Iter: 299000 training acc: 0.5625
Global Iter: 299100 training loss: 0.686195
Global Iter: 299100 training acc: 0.5625
Global Iter: 299200 training loss: 0.693148
Global Iter: 299200 training acc: 0.5
Global Iter: 299300 training loss: 0.713849
Global Iter: 299300 training acc: 0.34375
Global Iter: 299400 training loss: 0.693229
Global Iter: 299400 training acc: 0.5
Global Iter: 299500 training loss: 0.695822
Global Iter: 299500 training acc: 0.5
Global Iter: 299600 training loss: 0.707605
Global Iter: 299600 training acc: 0.40625
Global Iter: 299700 training loss: 0.688056
Global Iter: 299700 training acc: 0.5625
Global Iter: 299800 training loss: 0.697607
Global Iter: 299800 training acc: 0.46875
Global Iter: 299900 training loss: 0.700102
Global Iter: 299900 training acc: 0.46875
Global Iter: 300000 training loss: 0.686625
Global Iter: 300000 training acc: 0.5625
Global Iter: 300100 training loss: 0.680664
Global Iter: 300100 training acc: 0.625
Global Iter: 300200 training loss: 0.70682
Global Iter: 300200 training acc: 0.40625
Global Iter: 300300 training loss: 0.703284
Global Iter: 300300 training acc: 0.4375
Global Iter: 300400 training loss: 0.671786
Global Iter: 300400 training acc: 0.6875
Global Iter: 300500 training loss: 0.699669
Global Iter: 300500 training acc: 0.46875
Global Iter: 300600 training loss: 0.683366
Global Iter: 300600 training acc: 0.59375
Global Iter: 300700 training loss: 0.687222
Global Iter: 300700 training acc: 0.5625
Global Iter: 300800 training loss: 0.699491
Global Iter: 300800 training acc: 0.46875
Global Iter: 300900 training loss: 0.698277
Global Iter: 300900 training acc: 0.46875
Global Iter: 301000 training loss: 0.68266
Global Iter: 301000 training acc: 0.59375
Global Iter: 301100 training loss: 0.704119
Global Iter: 301100 training acc: 0.4375
Global Iter: 301200 training loss: 0.668368
Global Iter: 301200 training acc: 0.71875
Global Iter: 301300 training loss: 0.691171
Global Iter: 301300 training acc: 0.53125
Global Iter: 301400 training loss: 0.70706
Global Iter: 301400 training acc: 0.40625
Global Iter: 301500 training loss: 0.683143
Global Iter: 301500 training acc: 0.59375
Global Iter: 301600 training loss: 0.683131
Global Iter: 301600 training acc: 0.59375
Global Iter: 301700 training loss: 0.684093
Global Iter: 301700 training acc: 0.59375
Global Iter: 301800 training loss: 0.692173
Global Iter: 301800 training acc: 0.53125
Global Iter: 301900 training loss: 0.669313
Global Iter: 301900 training acc: 0.6875
Global Iter: 302000 training loss: 0.683429
Global Iter: 302000 training acc: 0.59375
Global Iter: 302100 training loss: 0.693568
Global Iter: 302100 training acc: 0.5
Global Iter: 302200 training loss: 0.712336
Global Iter: 302200 training acc: 0.375
Global Iter: 302300 training loss: 0.694212
Global Iter: 302300 training acc: 0.5
Global Iter: 302400 training loss: 0.679919
Global Iter: 302400 training acc: 0.625
Global Iter: 302500 training loss: 0.705761
Global Iter: 302500 training acc: 0.40625
Global Iter: 302600 training loss: 0.696195
Global Iter: 302600 training acc: 0.5
Global Iter: 302700 training loss: 0.683345
Global Iter: 302700 training acc: 0.59375
Global Iter: 302800 training loss: 0.678051
Global Iter: 302800 training acc: 0.625
Global Iter: 302900 training loss: 0.702535
Global Iter: 302900 training acc: 0.4375
Global Iter: 303000 training loss: 0.670977
Global Iter: 303000 training acc: 0.6875
Global Iter: 303100 training loss: 0.692009
Global Iter: 303100 training acc: 0.53125
Global Iter: 303200 training loss: 0.682024
Global Iter: 303200 training acc: 0.59375
Global Iter: 303300 training loss: 0.670793
Global Iter: 303300 training acc: 0.6875
Global Iter: 303400 training loss: 0.674903
Global Iter: 303400 training acc: 0.65625
Global Iter: 303500 training loss: 0.682835
Global Iter: 303500 training acc: 0.59375
Global Iter: 303600 training loss: 0.691609
Global Iter: 303600 training acc: 0.53125
Global Iter: 303700 training loss: 0.686537
Global Iter: 303700 training acc: 0.5625
Global Iter: 303800 training loss: 0.691039
Global Iter: 303800 training acc: 0.53125
Global Iter: 303900 training loss: 0.699668
Global Iter: 303900 training acc: 0.46875
Global Iter: 304000 training loss: 0.703413
Global Iter: 304000 training acc: 0.4375
Global Iter: 304100 training loss: 0.67575
Global Iter: 304100 training acc: 0.65625
Global Iter: 304200 training loss: 0.697962
Global Iter: 304200 training acc: 0.46875
Global Iter: 304300 training loss: 0.711288
Global Iter: 304300 training acc: 0.375
Global Iter: 304400 training loss: 0.666903
Global Iter: 304400 training acc: 0.71875
Global Iter: 304500 training loss: 0.695262
Global Iter: 304500 training acc: 0.5
Global Iter: 304600 training loss: 0.695055
Global Iter: 304600 training acc: 0.5
Global Iter: 304700 training loss: 0.687186
Global Iter: 304700 training acc: 0.5625
Global Iter: 304800 training loss: 0.671902
Global Iter: 304800 training acc: 0.6875
Global Iter: 304900 training loss: 0.706314
Global Iter: 304900 training acc: 0.40625
Global Iter: 305000 training loss: 0.68899
Global Iter: 305000 training acc: 0.5625
Global Iter: 305100 training loss: 0.698933
Global Iter: 305100 training acc: 0.46875
Global Iter: 305200 training loss: 0.701968
Global Iter: 305200 training acc: 0.4375
Global Iter: 305300 training loss: 0.691404
Global Iter: 305300 training acc: 0.53125
Global Iter: 305400 training loss: 0.683008
Global Iter: 305400 training acc: 0.59375
Global Iter: 305500 training loss: 0.690964
Global Iter: 305500 training acc: 0.53125
Global Iter: 305600 training loss: 0.691239
Global Iter: 305600 training acc: 0.53125
Global Iter: 305700 training loss: 0.68441
Global Iter: 305700 training acc: 0.59375
Global Iter: 305800 training loss: 0.691719
Global Iter: 305800 training acc: 0.53125
Global Iter: 305900 training loss: 0.685705
Global Iter: 305900 training acc: 0.5625
Global Iter: 306000 training loss: 0.690714
Global Iter: 306000 training acc: 0.53125
Global Iter: 306100 training loss: 0.683422
Global Iter: 306100 training acc: 0.59375
Global Iter: 306200 training loss: 0.663476
Global Iter: 306200 training acc: 0.71875
Global Iter: 306300 training loss: 0.679565
Global Iter: 306300 training acc: 0.625
Global Iter: 306400 training loss: 0.705305
Global Iter: 306400 training acc: 0.4375
Global Iter: 306500 training loss: 0.695832
Global Iter: 306500 training acc: 0.5
Global Iter: 306600 training loss: 0.695148
Global Iter: 306600 training acc: 0.5
Global Iter: 306700 training loss: 0.672545
Global Iter: 306700 training acc: 0.6875
Global Iter: 306800 training loss: 0.689086
Global Iter: 306800 training acc: 0.53125
Global Iter: 306900 training loss: 0.694185
Global Iter: 306900 training acc: 0.53125
Global Iter: 307000 training loss: 0.68859
Global Iter: 307000 training acc: 0.5625
Global Iter: 307100 training loss: 0.711999
Global Iter: 307100 training acc: 0.375
Global Iter: 307200 training loss: 0.695699
Global Iter: 307200 training acc: 0.5
Global Iter: 307300 training loss: 0.674856
Global Iter: 307300 training acc: 0.65625
Global Iter: 307400 training loss: 0.685738
Global Iter: 307400 training acc: 0.5625
Global Iter: 307500 training loss: 0.698891
Global Iter: 307500 training acc: 0.46875
Global Iter: 307600 training loss: 0.694225
Global Iter: 307600 training acc: 0.5
Global Iter: 307700 training loss: 0.67927
Global Iter: 307700 training acc: 0.625
Global Iter: 307800 training loss: 0.686773
Global Iter: 307800 training acc: 0.5625
Global Iter: 307900 training loss: 0.694548
Global Iter: 307900 training acc: 0.5
Global Iter: 308000 training loss: 0.695387
Global Iter: 308000 training acc: 0.5
Global Iter: 308100 training loss: 0.710867
Global Iter: 308100 training acc: 0.375
Global Iter: 308200 training loss: 0.679466
Global Iter: 308200 training acc: 0.625
Global Iter: 308300 training loss: 0.674402
Global Iter: 308300 training acc: 0.65625
Global Iter: 308400 training loss: 0.706997
Global Iter: 308400 training acc: 0.40625
Global Iter: 308500 training loss: 0.676409
Global Iter: 308500 training acc: 0.65625
Global Iter: 308600 training loss: 0.691375
Global Iter: 308600 training acc: 0.53125
Global Iter: 308700 training loss: 0.690951
Global Iter: 308700 training acc: 0.53125
Global Iter: 308800 training loss: 0.704231
Global Iter: 308800 training acc: 0.4375
Global Iter: 308900 training loss: 0.688664
Global Iter: 308900 training acc: 0.5625
Global Iter: 309000 training loss: 0.685705
Global Iter: 309000 training acc: 0.5625
Global Iter: 309100 training loss: 0.694146
Global Iter: 309100 training acc: 0.5
Global Iter: 309200 training loss: 0.711434
Global Iter: 309200 training acc: 0.375
Global Iter: 309300 training loss: 0.695432
Global Iter: 309300 training acc: 0.5
Global Iter: 309400 training loss: 0.691449
Global Iter: 309400 training acc: 0.53125
Global Iter: 309500 training loss: 0.70373
Global Iter: 309500 training acc: 0.4375
Global Iter: 309600 training loss: 0.682913
Global Iter: 309600 training acc: 0.59375
Global Iter: 309700 training loss: 0.702703
Global Iter: 309700 training acc: 0.4375
Global Iter: 309800 training loss: 0.702181
Global Iter: 309800 training acc: 0.4375
Global Iter: 309900 training loss: 0.683785
Global Iter: 309900 training acc: 0.59375
Global Iter: 310000 training loss: 0.687313
Global Iter: 310000 training acc: 0.5625
Global Iter: 310100 training loss: 0.705271
Global Iter: 310100 training acc: 0.40625
Global Iter: 310200 training loss: 0.693906
Global Iter: 310200 training acc: 0.5
Global Iter: 310300 training loss: 0.666812
Global Iter: 310300 training acc: 0.71875
Global Iter: 310400 training loss: 0.693361
Global Iter: 310400 training acc: 0.5
Global Iter: 310500 training loss: 0.682201
Global Iter: 310500 training acc: 0.59375
Global Iter: 310600 training loss: 0.691077
Global Iter: 310600 training acc: 0.53125
Global Iter: 310700 training loss: 0.705985
Global Iter: 310700 training acc: 0.40625
Global Iter: 310800 training loss: 0.695425
Global Iter: 310800 training acc: 0.5
Global Iter: 310900 training loss: 0.683388
Global Iter: 310900 training acc: 0.59375
Global Iter: 311000 training loss: 0.70321
Global Iter: 311000 training acc: 0.4375
Global Iter: 311100 training loss: 0.665863
Global Iter: 311100 training acc: 0.71875
Global Iter: 311200 training loss: 0.691516
Global Iter: 311200 training acc: 0.53125
Global Iter: 311300 training loss: 0.707208
Global Iter: 311300 training acc: 0.40625
Global Iter: 311400 training loss: 0.679519
Global Iter: 311400 training acc: 0.625
Global Iter: 311500 training loss: 0.678246
Global Iter: 311500 training acc: 0.625
Global Iter: 311600 training loss: 0.683191
Global Iter: 311600 training acc: 0.59375
Global Iter: 311700 training loss: 0.696242
Global Iter: 311700 training acc: 0.5
Global Iter: 311800 training loss: 0.674863
Global Iter: 311800 training acc: 0.65625
Global Iter: 311900 training loss: 0.679361
Global Iter: 311900 training acc: 0.625
Global Iter: 312000 training loss: 0.699987
Global Iter: 312000 training acc: 0.46875
Global Iter: 312100 training loss: 0.715872
Global Iter: 312100 training acc: 0.34375
Global Iter: 312200 training loss: 0.695789
Global Iter: 312200 training acc: 0.5
Global Iter: 312300 training loss: 0.678796
Global Iter: 312300 training acc: 0.625
Global Iter: 312400 training loss: 0.698232
Global Iter: 312400 training acc: 0.46875
Global Iter: 312500 training loss: 0.695724
Global Iter: 312500 training acc: 0.5
Global Iter: 312600 training loss: 0.687026
Global Iter: 312600 training acc: 0.5625
Global Iter: 312700 training loss: 0.682899
Global Iter: 312700 training acc: 0.59375
Global Iter: 312800 training loss: 0.70284
Global Iter: 312800 training acc: 0.4375
Global Iter: 312900 training loss: 0.675865
Global Iter: 312900 training acc: 0.65625
Global Iter: 313000 training loss: 0.690878
Global Iter: 313000 training acc: 0.53125
Global Iter: 313100 training loss: 0.69056
Global Iter: 313100 training acc: 0.53125
Global Iter: 313200 training loss: 0.666373
Global Iter: 313200 training acc: 0.71875
Global Iter: 313300 training loss: 0.677869
Global Iter: 313300 training acc: 0.625
Global Iter: 313400 training loss: 0.683562
Global Iter: 313400 training acc: 0.59375
Global Iter: 313500 training loss: 0.690245
Global Iter: 313500 training acc: 0.53125
Global Iter: 313600 training loss: 0.695719
Global Iter: 313600 training acc: 0.5
Global Iter: 313700 training loss: 0.694234
Global Iter: 313700 training acc: 0.5
Global Iter: 313800 training loss: 0.704018
Global Iter: 313800 training acc: 0.4375
Global Iter: 313900 training loss: 0.702056
Global Iter: 313900 training acc: 0.4375
Global Iter: 314000 training loss: 0.678292
Global Iter: 314000 training acc: 0.65625
Global Iter: 314100 training loss: 0.699547
Global Iter: 314100 training acc: 0.46875
Global Iter: 314200 training loss: 0.716392
Global Iter: 314200 training acc: 0.34375
Global Iter: 314300 training loss: 0.668997
Global Iter: 314300 training acc: 0.6875
Global Iter: 314400 training loss: 0.698571
Global Iter: 314400 training acc: 0.46875
Global Iter: 314500 training loss: 0.686771
Global Iter: 314500 training acc: 0.5625
Global Iter: 314600 training loss: 0.686497
Global Iter: 314600 training acc: 0.5625
Global Iter: 314700 training loss: 0.667297
Global Iter: 314700 training acc: 0.6875
Global Iter: 314800 training loss: 0.701962
Global Iter: 314800 training acc: 0.4375
Global Iter: 314900 training loss: 0.68505
Global Iter: 314900 training acc: 0.5625
Global Iter: 315000 training loss: 0.704229
Global Iter: 315000 training acc: 0.4375
Global Iter: 315100 training loss: 0.707014
Global Iter: 315100 training acc: 0.40625
Global Iter: 315200 training loss: 0.705515
Global Iter: 315200 training acc: 0.5
Global Iter: 315300 training loss: 0.677251
Global Iter: 315300 training acc: 0.625
Global Iter: 315400 training loss: 0.690601
Global Iter: 315400 training acc: 0.53125
Global Iter: 315500 training loss: 0.687026
Global Iter: 315500 training acc: 0.5625
Global Iter: 315600 training loss: 0.679814
Global Iter: 315600 training acc: 0.625
Global Iter: 315700 training loss: 0.687762
Global Iter: 315700 training acc: 0.5625
Global Iter: 315800 training loss: 0.686955
Global Iter: 315800 training acc: 0.5625
Global Iter: 315900 training loss: 0.693641
Global Iter: 315900 training acc: 0.5
Global Iter: 316000 training loss: 0.680719
Global Iter: 316000 training acc: 0.625
Global Iter: 316100 training loss: 0.681737
Global Iter: 316100 training acc: 0.625
Global Iter: 316200 training loss: 0.676146
Global Iter: 316200 training acc: 0.65625
Global Iter: 316300 training loss: 0.709729
Global Iter: 316300 training acc: 0.375
Global Iter: 316400 training loss: 0.698828
Global Iter: 316400 training acc: 0.46875
Global Iter: 316500 training loss: 0.699411
Global Iter: 316500 training acc: 0.46875
Global Iter: 316600 training loss: 0.672083
Global Iter: 316600 training acc: 0.6875
Global Iter: 316700 training loss: 0.695471
Global Iter: 316700 training acc: 0.5
Global Iter: 316800 training loss: 0.694573
Global Iter: 316800 training acc: 0.5
Global Iter: 316900 training loss: 0.686856
Global Iter: 316900 training acc: 0.5625
Global Iter: 317000 training loss: 0.718008
Global Iter: 317000 training acc: 0.3125
Global Iter: 317100 training loss: 0.691483
Global Iter: 317100 training acc: 0.53125
Global Iter: 317200 training loss: 0.667685
Global Iter: 317200 training acc: 0.71875
Global Iter: 317300 training loss: 0.686869
Global Iter: 317300 training acc: 0.5625
Global Iter: 317400 training loss: 0.699034
Global Iter: 317400 training acc: 0.46875
Global Iter: 317500 training loss: 0.691319
Global Iter: 317500 training acc: 0.53125
Global Iter: 317600 training loss: 0.673893
Global Iter: 317600 training acc: 0.65625
Global Iter: 317700 training loss: 0.68577
Global Iter: 317700 training acc: 0.5625
Global Iter: 317800 training loss: 0.686897
Global Iter: 317800 training acc: 0.5625
Global Iter: 317900 training loss: 0.6909
Global Iter: 317900 training acc: 0.53125
Global Iter: 318000 training loss: 0.70919
Global Iter: 318000 training acc: 0.40625
Global Iter: 318100 training loss: 0.684236
Global Iter: 318100 training acc: 0.59375
Global Iter: 318200 training loss: 0.678832
Global Iter: 318200 training acc: 0.625
Global Iter: 318300 training loss: 0.698571
Global Iter: 318300 training acc: 0.46875
Global Iter: 318400 training loss: 0.678895
Global Iter: 318400 training acc: 0.625
Global Iter: 318500 training loss: 0.691734
Global Iter: 318500 training acc: 0.53125
Global Iter: 318600 training loss: 0.694409
Global Iter: 318600 training acc: 0.5
Global Iter: 318700 training loss: 0.699604
Global Iter: 318700 training acc: 0.46875
Global Iter: 318800 training loss: 0.691739
Global Iter: 318800 training acc: 0.53125
Global Iter: 318900 training loss: 0.680176
Global Iter: 318900 training acc: 0.625
Global Iter: 319000 training loss: 0.695009
Global Iter: 319000 training acc: 0.5
Global Iter: 319100 training loss: 0.701447
Global Iter: 319100 training acc: 0.4375
Global Iter: 319200 training loss: 0.698711
Global Iter: 319200 training acc: 0.46875
Global Iter: 319300 training loss: 0.695019
Global Iter: 319300 training acc: 0.5
Global Iter: 319400 training loss: 0.703053
Global Iter: 319400 training acc: 0.4375
Global Iter: 319500 training loss: 0.686653
Global Iter: 319500 training acc: 0.5625
Global Iter: 319600 training loss: 0.698874
Global Iter: 319600 training acc: 0.46875
Global Iter: 319700 training loss: 0.704227
Global Iter: 319700 training acc: 0.4375
Global Iter: 319800 training loss: 0.682402
Global Iter: 319800 training acc: 0.59375
Global Iter: 319900 training loss: 0.691367
Global Iter: 319900 training acc: 0.53125
Global Iter: 320000 training loss: 0.705859
Global Iter: 320000 training acc: 0.40625
Global Iter: 320100 training loss: 0.69146
Global Iter: 320100 training acc: 0.53125
Global Iter: 320200 training loss: 0.669493
Global Iter: 320200 training acc: 0.6875
Global Iter: 320300 training loss: 0.692547
Global Iter: 320300 training acc: 0.53125
Global Iter: 320400 training loss: 0.693088
Global Iter: 320400 training acc: 0.53125
Global Iter: 320500 training loss: 0.695406
Global Iter: 320500 training acc: 0.5
Global Iter: 320600 training loss: 0.708407
Global Iter: 320600 training acc: 0.40625
Global Iter: 320700 training loss: 0.696076
Global Iter: 320700 training acc: 0.5
Global Iter: 320800 training loss: 0.687478
Global Iter: 320800 training acc: 0.5625
Global Iter: 320900 training loss: 0.700719
Global Iter: 320900 training acc: 0.46875
Global Iter: 321000 training loss: 0.667035
Global Iter: 321000 training acc: 0.71875
Global Iter: 321100 training loss: 0.696561
Global Iter: 321100 training acc: 0.5
Global Iter: 321200 training loss: 0.699728
Global Iter: 321200 training acc: 0.46875
Global Iter: 321300 training loss: 0.675163
Global Iter: 321300 training acc: 0.65625
Global Iter: 321400 training loss: 0.675742
Global Iter: 321400 training acc: 0.65625
Global Iter: 321500 training loss: 0.692124
Global Iter: 321500 training acc: 0.53125
Global Iter: 321600 training loss: 0.695761
Global Iter: 321600 training acc: 0.5
Global Iter: 321700 training loss: 0.679434
Global Iter: 321700 training acc: 0.625
Global Iter: 321800 training loss: 0.680499
Global Iter: 321800 training acc: 0.625
Global Iter: 321900 training loss: 0.695351
Global Iter: 321900 training acc: 0.5
Global Iter: 322000 training loss: 0.715136
Global Iter: 322000 training acc: 0.34375
Global Iter: 322100 training loss: 0.694679
Global Iter: 322100 training acc: 0.5
Global Iter: 322200 training loss: 0.677218
Global Iter: 322200 training acc: 0.625
Global Iter: 322300 training loss: 0.700246
Global Iter: 322300 training acc: 0.46875
Global Iter: 322400 training loss: 0.693786
Global Iter: 322400 training acc: 0.5
Global Iter: 322500 training loss: 0.688183
Global Iter: 322500 training acc: 0.5625
Global Iter: 322600 training loss: 0.678349
Global Iter: 322600 training acc: 0.625
Global Iter: 322700 training loss: 0.701764
Global Iter: 322700 training acc: 0.4375
Global Iter: 322800 training loss: 0.68066
Global Iter: 322800 training acc: 0.625
Global Iter: 322900 training loss: 0.69225
Global Iter: 322900 training acc: 0.53125
Global Iter: 323000 training loss: 0.68798
Global Iter: 323000 training acc: 0.5625
Global Iter: 323100 training loss: 0.670586
Global Iter: 323100 training acc: 0.6875
Global Iter: 323200 training loss: 0.683693
Global Iter: 323200 training acc: 0.59375
Global Iter: 323300 training loss: 0.683359
Global Iter: 323300 training acc: 0.59375
Global Iter: 323400 training loss: 0.691496
Global Iter: 323400 training acc: 0.53125
Global Iter: 323500 training loss: 0.701991
Global Iter: 323500 training acc: 0.4375
Global Iter: 323600 training loss: 0.690948
Global Iter: 323600 training acc: 0.53125
Global Iter: 323700 training loss: 0.703625
Global Iter: 323700 training acc: 0.4375
Global Iter: 323800 training loss: 0.711435
Global Iter: 323800 training acc: 0.375
Global Iter: 323900 training loss: 0.676432
Global Iter: 323900 training acc: 0.65625
Global Iter: 324000 training loss: 0.699214
Global Iter: 324000 training acc: 0.46875
Global Iter: 324100 training loss: 0.711162
Global Iter: 324100 training acc: 0.375
Global Iter: 324200 training loss: 0.667512
Global Iter: 324200 training acc: 0.71875
Global Iter: 324300 training loss: 0.704916
Global Iter: 324300 training acc: 0.4375
Global Iter: 324400 training loss: 0.68607
Global Iter: 324400 training acc: 0.5625
Global Iter: 324500 training loss: 0.687588
Global Iter: 324500 training acc: 0.5625
Global Iter: 324600 training loss: 0.671237
Global Iter: 324600 training acc: 0.6875
Global Iter: 324700 training loss: 0.69802
Global Iter: 324700 training acc: 0.46875
Global Iter: 324800 training loss: 0.696253
Global Iter: 324800 training acc: 0.5
Global Iter: 324900 training loss: 0.704129
Global Iter: 324900 training acc: 0.4375
Global Iter: 325000 training loss: 0.708781
Global Iter: 325000 training acc: 0.40625
Global Iter: 325100 training loss: 0.695431
Global Iter: 325100 training acc: 0.5
Global Iter: 325200 training loss: 0.68256
Global Iter: 325200 training acc: 0.59375
Global Iter: 325300 training loss: 0.694961
Global Iter: 325300 training acc: 0.5
Global Iter: 325400 training loss: 0.69681
Global Iter: 325400 training acc: 0.5
Global Iter: 325500 training loss: 0.683419
Global Iter: 325500 training acc: 0.59375
Global Iter: 325600 training loss: 0.691998
Global Iter: 325600 training acc: 0.53125
Global Iter: 325700 training loss: 0.68796
Global Iter: 325700 training acc: 0.5625
Global Iter: 325800 training loss: 0.694346
Global Iter: 325800 training acc: 0.5
Global Iter: 325900 training loss: 0.669862
Global Iter: 325900 training acc: 0.6875
Global Iter: 326000 training loss: 0.676078
Global Iter: 326000 training acc: 0.65625
Global Iter: 326100 training loss: 0.676827
Global Iter: 326100 training acc: 0.65625
Global Iter: 326200 training loss: 0.714973
Global Iter: 326200 training acc: 0.34375
Global Iter: 326300 training loss: 0.690767
Global Iter: 326300 training acc: 0.53125
Global Iter: 326400 training loss: 0.699223
Global Iter: 326400 training acc: 0.46875
Global Iter: 326500 training loss: 0.680131
Global Iter: 326500 training acc: 0.625
Global Iter: 326600 training loss: 0.69844
Global Iter: 326600 training acc: 0.46875
Global Iter: 326700 training loss: 0.69905
Global Iter: 326700 training acc: 0.46875
Global Iter: 326800 training loss: 0.690274
Global Iter: 326800 training acc: 0.53125
Global Iter: 326900 training loss: 0.721895
Global Iter: 326900 training acc: 0.28125
Global Iter: 327000 training loss: 0.695666
Global Iter: 327000 training acc: 0.5
Global Iter: 327100 training loss: 0.667722
Global Iter: 327100 training acc: 0.71875
Global Iter: 327200 training loss: 0.680526
Global Iter: 327200 training acc: 0.625
Global Iter: 327300 training loss: 0.703589
Global Iter: 327300 training acc: 0.4375
Global Iter: 327400 training loss: 0.687024
Global Iter: 327400 training acc: 0.5625
Global Iter: 327500 training loss: 0.680573
Global Iter: 327500 training acc: 0.625
Global Iter: 327600 training loss: 0.692772
Global Iter: 327600 training acc: 0.53125
Global Iter: 327700 training loss: 0.692366
Global Iter: 327700 training acc: 0.53125
Global Iter: 327800 training loss: 0.694575
Global Iter: 327800 training acc: 0.5
Global Iter: 327900 training loss: 0.707988
Global Iter: 327900 training acc: 0.40625
Global Iter: 328000 training loss: 0.687758
Global Iter: 328000 training acc: 0.5625
Global Iter: 328100 training loss: 0.682495
Global Iter: 328100 training acc: 0.59375
Global Iter: 328200 training loss: 0.690789
Global Iter: 328200 training acc: 0.53125
Global Iter: 328300 training loss: 0.675551
Global Iter: 328300 training acc: 0.65625
Global Iter: 328400 training loss: 0.69511
Global Iter: 328400 training acc: 0.5
Global Iter: 328500 training loss: 0.691966
Global Iter: 328500 training acc: 0.53125
Global Iter: 328600 training loss: 0.691431
Global Iter: 328600 training acc: 0.53125
Global Iter: 328700 training loss: 0.694999
Global Iter: 328700 training acc: 0.5
Global Iter: 328800 training loss: 0.683301
Global Iter: 328800 training acc: 0.59375
Global Iter: 328900 training loss: 0.694643
Global Iter: 328900 training acc: 0.5
Global Iter: 329000 training loss: 0.710477
Global Iter: 329000 training acc: 0.375
Global Iter: 329100 training loss: 0.701201
Global Iter: 329100 training acc: 0.46875
Global Iter: 329200 training loss: 0.696345
Global Iter: 329200 training acc: 0.5
Global Iter: 329300 training loss: 0.702302
Global Iter: 329300 training acc: 0.4375
Global Iter: 329400 training loss: 0.68275
Global Iter: 329400 training acc: 0.59375
Global Iter: 329500 training loss: 0.697637
Global Iter: 329500 training acc: 0.46875
Global Iter: 329600 training loss: 0.698149
Global Iter: 329600 training acc: 0.46875
Global Iter: 329700 training loss: 0.682783
Global Iter: 329700 training acc: 0.59375
Global Iter: 329800 training loss: 0.693526
Global Iter: 329800 training acc: 0.5
Global Iter: 329900 training loss: 0.70355
Global Iter: 329900 training acc: 0.4375
Global Iter: 330000 training loss: 0.682054
Global Iter: 330000 training acc: 0.5625
Global Iter: 330100 training loss: 0.669919
Global Iter: 330100 training acc: 0.6875
Global Iter: 330200 training loss: 0.697856
Global Iter: 330200 training acc: 0.46875
Global Iter: 330300 training loss: 0.693489
Global Iter: 330300 training acc: 0.5
Global Iter: 330400 training loss: 0.696709
Global Iter: 330400 training acc: 0.5
Global Iter: 330500 training loss: 0.708774
Global Iter: 330500 training acc: 0.40625
Global Iter: 330600 training loss: 0.695108
Global Iter: 330600 training acc: 0.5
Global Iter: 330700 training loss: 0.683465
Global Iter: 330700 training acc: 0.59375
Global Iter: 330800 training loss: 0.703393
Global Iter: 330800 training acc: 0.46875
Global Iter: 330900 training loss: 0.672314
Global Iter: 330900 training acc: 0.6875
Global Iter: 331000 training loss: 0.694649
Global Iter: 331000 training acc: 0.5
Global Iter: 331100 training loss: 0.698447
Global Iter: 331100 training acc: 0.46875
Global Iter: 331200 training loss: 0.67514
Global Iter: 331200 training acc: 0.65625
Global Iter: 331300 training loss: 0.680983
Global Iter: 331300 training acc: 0.59375
Global Iter: 331400 training loss: 0.689633
Global Iter: 331400 training acc: 0.53125
Global Iter: 331500 training loss: 0.691922
Global Iter: 331500 training acc: 0.53125
Global Iter: 331600 training loss: 0.669304
Global Iter: 331600 training acc: 0.6875
Global Iter: 331700 training loss: 0.687331
Global Iter: 331700 training acc: 0.5625
Global Iter: 331800 training loss: 0.694039
Global Iter: 331800 training acc: 0.5
Global Iter: 331900 training loss: 0.717023
Global Iter: 331900 training acc: 0.34375
Global Iter: 332000 training loss: 0.691468
Global Iter: 332000 training acc: 0.53125
Global Iter: 332100 training loss: 0.680122
Global Iter: 332100 training acc: 0.625
Global Iter: 332200 training loss: 0.700255
Global Iter: 332200 training acc: 0.46875
Global Iter: 332300 training loss: 0.695529
Global Iter: 332300 training acc: 0.5
Global Iter: 332400 training loss: 0.691035
Global Iter: 332400 training acc: 0.53125
Global Iter: 332500 training loss: 0.679389
Global Iter: 332500 training acc: 0.625
Global Iter: 332600 training loss: 0.698469
Global Iter: 332600 training acc: 0.46875
Global Iter: 332700 training loss: 0.683278
Global Iter: 332700 training acc: 0.59375
Global Iter: 332800 training loss: 0.691771
Global Iter: 332800 training acc: 0.53125
Global Iter: 332900 training loss: 0.684322
Global Iter: 332900 training acc: 0.59375
Global Iter: 333000 training loss: 0.672055
Global Iter: 333000 training acc: 0.6875
Global Iter: 333100 training loss: 0.690663
Global Iter: 333100 training acc: 0.53125
Global Iter: 333200 training loss: 0.681861
Global Iter: 333200 training acc: 0.59375
Global Iter: 333300 training loss: 0.694795
Global Iter: 333300 training acc: 0.5
Global Iter: 333400 training loss: 0.702779
Global Iter: 333400 training acc: 0.4375
Global Iter: 333500 training loss: 0.690898
Global Iter: 333500 training acc: 0.53125
Global Iter: 333600 training loss: 0.698879
Global Iter: 333600 training acc: 0.46875
Global Iter: 333700 training loss: 0.711221
Global Iter: 333700 training acc: 0.375
Global Iter: 333800 training loss: 0.66946
Global Iter: 333800 training acc: 0.6875
Global Iter: 333900 training loss: 0.698818
Global Iter: 333900 training acc: 0.46875
Global Iter: 334000 training loss: 0.711401
Global Iter: 334000 training acc: 0.375
Global Iter: 334100 training loss: 0.671283
Global Iter: 334100 training acc: 0.6875
Global Iter: 334200 training loss: 0.694632
Global Iter: 334200 training acc: 0.5
Global Iter: 334300 training loss: 0.685838
Global Iter: 334300 training acc: 0.5625
Global Iter: 334400 training loss: 0.689312
Global Iter: 334400 training acc: 0.5625
Global Iter: 334500 training loss: 0.672026
Global Iter: 334500 training acc: 0.6875
Global Iter: 334600 training loss: 0.699575
Global Iter: 334600 training acc: 0.46875
Global Iter: 334700 training loss: 0.689932
Global Iter: 334700 training acc: 0.53125
Global Iter: 334800 training loss: 0.702175
Global Iter: 334800 training acc: 0.4375
Global Iter: 334900 training loss: 0.69915
Global Iter: 334900 training acc: 0.46875
Global Iter: 335000 training loss: 0.69256
Global Iter: 335000 training acc: 0.53125
Global Iter: 335100 training loss: 0.687981
Global Iter: 335100 training acc: 0.5625
Global Iter: 335200 training loss: 0.696087
Global Iter: 335200 training acc: 0.5
Global Iter: 335300 training loss: 0.703911
Global Iter: 335300 training acc: 0.4375
Global Iter: 335400 training loss: 0.67797
Global Iter: 335400 training acc: 0.625
Global Iter: 335500 training loss: 0.685665
Global Iter: 335500 training acc: 0.5625
Global Iter: 335600 training loss: 0.687527
Global Iter: 335600 training acc: 0.5625
Global Iter: 335700 training loss: 0.687543
Global Iter: 335700 training acc: 0.5625
Global Iter: 335800 training loss: 0.667381
Global Iter: 335800 training acc: 0.71875
Global Iter: 335900 training loss: 0.674246
Global Iter: 335900 training acc: 0.65625
Global Iter: 336000 training loss: 0.679645
Global Iter: 336000 training acc: 0.625
Global Iter: 336100 training loss: 0.710952
Global Iter: 336100 training acc: 0.375
Global Iter: 336200 training loss: 0.694613
Global Iter: 336200 training acc: 0.5
Global Iter: 336300 training loss: 0.695363
Global Iter: 336300 training acc: 0.5
Global Iter: 336400 training loss: 0.674612
Global Iter: 336400 training acc: 0.65625
Global Iter: 336500 training loss: 0.702998
Global Iter: 336500 training acc: 0.4375
Global Iter: 336600 training loss: 0.701995
Global Iter: 336600 training acc: 0.4375
Global Iter: 336700 training loss: 0.699954
Global Iter: 336700 training acc: 0.46875
Global Iter: 336800 training loss: 0.718853
Global Iter: 336800 training acc: 0.3125
Global Iter: 336900 training loss: 0.69528
Global Iter: 336900 training acc: 0.5
Global Iter: 337000 training loss: 0.671574
Global Iter: 337000 training acc: 0.6875
Global Iter: 337100 training loss: 0.682508
Global Iter: 337100 training acc: 0.59375
Global Iter: 337200 training loss: 0.695809
Global Iter: 337200 training acc: 0.5
Global Iter: 337300 training loss: 0.686276
Global Iter: 337300 training acc: 0.5625
Global Iter: 337400 training loss: 0.674763
Global Iter: 337400 training acc: 0.65625
Global Iter: 337500 training loss: 0.695897
Global Iter: 337500 training acc: 0.5
Global Iter: 337600 training loss: 0.695622
Global Iter: 337600 training acc: 0.5
Global Iter: 337700 training loss: 0.691503
Global Iter: 337700 training acc: 0.53125
Global Iter: 337800 training loss: 0.703663
Global Iter: 337800 training acc: 0.4375
Global Iter: 337900 training loss: 0.695003
Global Iter: 337900 training acc: 0.5
Global Iter: 338000 training loss: 0.679664
Global Iter: 338000 training acc: 0.625
Global Iter: 338100 training loss: 0.695149
Global Iter: 338100 training acc: 0.5
Global Iter: 338200 training loss: 0.680084
Global Iter: 338200 training acc: 0.625
Global Iter: 338300 training loss: 0.695349
Global Iter: 338300 training acc: 0.5
Global Iter: 338400 training loss: 0.695418
Global Iter: 338400 training acc: 0.5
Global Iter: 338500 training loss: 0.696014
Global Iter: 338500 training acc: 0.5
Global Iter: 338600 training loss: 0.702845
Global Iter: 338600 training acc: 0.4375
Global Iter: 338700 training loss: 0.691195
Global Iter: 338700 training acc: 0.53125
Global Iter: 338800 training loss: 0.691577
Global Iter: 338800 training acc: 0.53125
Global Iter: 338900 training loss: 0.708061
Global Iter: 338900 training acc: 0.40625
Global Iter: 339000 training loss: 0.706858
Global Iter: 339000 training acc: 0.40625
Global Iter: 339100 training loss: 0.703501
Global Iter: 339100 training acc: 0.4375
Global Iter: 339200 training loss: 0.702861
Global Iter: 339200 training acc: 0.4375
Global Iter: 339300 training loss: 0.678832
Global Iter: 339300 training acc: 0.625
Global Iter: 339400 training loss: 0.703056
Global Iter: 339400 training acc: 0.4375
Global Iter: 339500 training loss: 0.704296
Global Iter: 339500 training acc: 0.4375
Global Iter: 339600 training loss: 0.679486
Global Iter: 339600 training acc: 0.625
Global Iter: 339700 training loss: 0.702591
Global Iter: 339700 training acc: 0.4375
Global Iter: 339800 training loss: 0.702195
Global Iter: 339800 training acc: 0.4375
Global Iter: 339900 training loss: 0.694996
Global Iter: 339900 training acc: 0.5
Global Iter: 340000 training loss: 0.675008
Global Iter: 340000 training acc: 0.65625
Global Iter: 340100 training loss: 0.695063
Global Iter: 340100 training acc: 0.5
Global Iter: 340200 training loss: 0.694849
Global Iter: 340200 training acc: 0.5
Global Iter: 340300 training loss: 0.696406
Global Iter: 340300 training acc: 0.5
Global Iter: 340400 training loss: 0.701389
Global Iter: 340400 training acc: 0.4375
Global Iter: 340500 training loss: 0.699155
Global Iter: 340500 training acc: 0.46875
Global Iter: 340600 training loss: 0.682008
Global Iter: 340600 training acc: 0.59375
Global Iter: 340700 training loss: 0.694816
Global Iter: 340700 training acc: 0.5
Global Iter: 340800 training loss: 0.675769
Global Iter: 340800 training acc: 0.65625
Global Iter: 340900 training loss: 0.691347
Global Iter: 340900 training acc: 0.53125
Global Iter: 341000 training loss: 0.702774
Global Iter: 341000 training acc: 0.4375
Global Iter: 341100 training loss: 0.67231
Global Iter: 341100 training acc: 0.65625
Global Iter: 341200 training loss: 0.685259
Global Iter: 341200 training acc: 0.5625
Global Iter: 341300 training loss: 0.684836
Global Iter: 341300 training acc: 0.59375
Global Iter: 341400 training loss: 0.691743
Global Iter: 341400 training acc: 0.53125
Global Iter: 341500 training loss: 0.679026
Global Iter: 341500 training acc: 0.625
Global Iter: 341600 training loss: 0.68355
Global Iter: 341600 training acc: 0.59375
Global Iter: 341700 training loss: 0.695312
Global Iter: 341700 training acc: 0.5
Global Iter: 341800 training loss: 0.713958
Global Iter: 341800 training acc: 0.34375
Global Iter: 341900 training loss: 0.693642
Global Iter: 341900 training acc: 0.53125
Global Iter: 342000 training loss: 0.68403
Global Iter: 342000 training acc: 0.59375
Global Iter: 342100 training loss: 0.698096
Global Iter: 342100 training acc: 0.46875
Global Iter: 342200 training loss: 0.687977
Global Iter: 342200 training acc: 0.5625
Global Iter: 342300 training loss: 0.69844
Global Iter: 342300 training acc: 0.46875
Global Iter: 342400 training loss: 0.678797
Global Iter: 342400 training acc: 0.625
Global Iter: 342500 training loss: 0.69875
Global Iter: 342500 training acc: 0.46875
Global Iter: 342600 training loss: 0.679479
Global Iter: 342600 training acc: 0.625
Global Iter: 342700 training loss: 0.698334
Global Iter: 342700 training acc: 0.46875
Global Iter: 342800 training loss: 0.687332
Global Iter: 342800 training acc: 0.5625
Global Iter: 342900 training loss: 0.673207
Global Iter: 342900 training acc: 0.65625
Global Iter: 343000 training loss: 0.691647
Global Iter: 343000 training acc: 0.53125
Global Iter: 343100 training loss: 0.685786
Global Iter: 343100 training acc: 0.5625
Global Iter: 343200 training loss: 0.699989
Global Iter: 343200 training acc: 0.46875
Global Iter: 343300 training loss: 0.702749
Global Iter: 343300 training acc: 0.4375
Global Iter: 343400 training loss: 0.692478
Global Iter: 343400 training acc: 0.53125
Global Iter: 343500 training loss: 0.693623
Global Iter: 343500 training acc: 0.5
Global Iter: 343600 training loss: 0.703256
Global Iter: 343600 training acc: 0.4375
Global Iter: 343700 training loss: 0.668472
Global Iter: 343700 training acc: 0.6875
Global Iter: 343800 training loss: 0.703831
Global Iter: 343800 training acc: 0.4375
Global Iter: 343900 training loss: 0.714584
Global Iter: 343900 training acc: 0.375
Global Iter: 344000 training loss: 0.665953
Global Iter: 344000 training acc: 0.71875
Global Iter: 344100 training loss: 0.693712
Global Iter: 344100 training acc: 0.5
Global Iter: 344200 training loss: 0.686308
Global Iter: 344200 training acc: 0.5625
Global Iter: 344300 training loss: 0.6827
Global Iter: 344300 training acc: 0.59375
Global Iter: 344400 training loss: 0.676497
Global Iter: 344400 training acc: 0.65625
Global Iter: 344500 training loss: 0.703745
Global Iter: 344500 training acc: 0.4375
Global Iter: 344600 training loss: 0.690773
Global Iter: 344600 training acc: 0.53125
Global Iter: 344700 training loss: 0.699784
Global Iter: 344700 training acc: 0.46875
Global Iter: 344800 training loss: 0.697424
Global Iter: 344800 training acc: 0.46875
Global Iter: 344900 training loss: 0.687683
Global Iter: 344900 training acc: 0.5625
Global Iter: 345000 training loss: 0.687982
Global Iter: 345000 training acc: 0.5625
Global Iter: 345100 training loss: 0.685916
Global Iter: 345100 training acc: 0.5625
Global Iter: 345200 training loss: 0.699318
Global Iter: 345200 training acc: 0.46875
Global Iter: 345300 training loss: 0.679452
Global Iter: 345300 training acc: 0.625
Global Iter: 345400 training loss: 0.688088
Global Iter: 345400 training acc: 0.5625
Global Iter: 345500 training loss: 0.681907
Global Iter: 345500 training acc: 0.59375
Global Iter: 345600 training loss: 0.691353
Global Iter: 345600 training acc: 0.53125
Global Iter: 345700 training loss: 0.668974
Global Iter: 345700 training acc: 0.71875
Global Iter: 345800 training loss: 0.676067
Global Iter: 345800 training acc: 0.65625
Global Iter: 345900 training loss: 0.679481
Global Iter: 345900 training acc: 0.625
Global Iter: 346000 training loss: 0.705547
Global Iter: 346000 training acc: 0.40625
Global Iter: 346100 training loss: 0.69578
Global Iter: 346100 training acc: 0.5
Global Iter: 346200 training loss: 0.694929
Global Iter: 346200 training acc: 0.5
Global Iter: 346300 training loss: 0.675637
Global Iter: 346300 training acc: 0.65625
Global Iter: 346400 training loss: 0.695664
Global Iter: 346400 training acc: 0.5
Global Iter: 346500 training loss: 0.699241
Global Iter: 346500 training acc: 0.46875
Global Iter: 346600 training loss: 0.691153
Global Iter: 346600 training acc: 0.53125
Global Iter: 346700 training loss: 0.718385
Global Iter: 346700 training acc: 0.3125
Global Iter: 346800 training loss: 0.703033
Global Iter: 346800 training acc: 0.4375
Global Iter: 346900 training loss: 0.663134
Global Iter: 346900 training acc: 0.75
Global Iter: 347000 training loss: 0.69081
Global Iter: 347000 training acc: 0.53125
Global Iter: 347100 training loss: 0.686794
Global Iter: 347100 training acc: 0.5625
Global Iter: 347200 training loss: 0.691132
Global Iter: 347200 training acc: 0.53125
Global Iter: 347300 training loss: 0.676106
Global Iter: 347300 training acc: 0.65625
Global Iter: 347400 training loss: 0.698496
Global Iter: 347400 training acc: 0.46875
Global Iter: 347500 training loss: 0.691184
Global Iter: 347500 training acc: 0.53125
Global Iter: 347600 training loss: 0.694955
Global Iter: 347600 training acc: 0.5
Global Iter: 347700 training loss: 0.702644
Global Iter: 347700 training acc: 0.4375
Global Iter: 347800 training loss: 0.695082
Global Iter: 347800 training acc: 0.5
Global Iter: 347900 training loss: 0.683469
Global Iter: 347900 training acc: 0.59375
Global Iter: 348000 training loss: 0.691226
Global Iter: 348000 training acc: 0.53125
Global Iter: 348100 training loss: 0.688191
Global Iter: 348100 training acc: 0.5625
Global Iter: 348200 training loss: 0.69481
Global Iter: 348200 training acc: 0.5
Global Iter: 348300 training loss: 0.696494
Global Iter: 348300 training acc: 0.5
Global Iter: 348400 training loss: 0.695667
Global Iter: 348400 training acc: 0.5
Global Iter: 348500 training loss: 0.698931
Global Iter: 348500 training acc: 0.46875
Global Iter: 348600 training loss: 0.685935
Global Iter: 348600 training acc: 0.5625
Global Iter: 348700 training loss: 0.692413
Global Iter: 348700 training acc: 0.53125
Global Iter: 348800 training loss: 0.707876
Global Iter: 348800 training acc: 0.40625
Global Iter: 348900 training loss: 0.711189
Global Iter: 348900 training acc: 0.375
Global Iter: 349000 training loss: 0.703022
Global Iter: 349000 training acc: 0.4375
Global Iter: 349100 training loss: 0.700282
Global Iter: 349100 training acc: 0.46875
Global Iter: 349200 training loss: 0.674133
Global Iter: 349200 training acc: 0.65625
Global Iter: 349300 training loss: 0.695925
Global Iter: 349300 training acc: 0.5
Global Iter: 349400 training loss: 0.699476
Global Iter: 349400 training acc: 0.46875
Global Iter: 349500 training loss: 0.686667
Global Iter: 349500 training acc: 0.5625
Global Iter: 349600 training loss: 0.707322
Global Iter: 349600 training acc: 0.40625
Global Iter: 349700 training loss: 0.699336
Global Iter: 349700 training acc: 0.46875
Global Iter: 349800 training loss: 0.68876
Global Iter: 349800 training acc: 0.53125
Global Iter: 349900 training loss: 0.675488
Global Iter: 349900 training acc: 0.65625
Global Iter: 350000 training loss: 0.695255
Global Iter: 350000 training acc: 0.5
Global Iter: 350100 training loss: 0.69106
Global Iter: 350100 training acc: 0.53125
Global Iter: 350200 training loss: 0.69083
Global Iter: 350200 training acc: 0.53125
Global Iter: 350300 training loss: 0.698411
Global Iter: 350300 training acc: 0.46875
Global Iter: 350400 training loss: 0.696471
Global Iter: 350400 training acc: 0.5
Global Iter: 350500 training loss: 0.68799
Global Iter: 350500 training acc: 0.5625
Global Iter: 350600 training loss: 0.695684
Global Iter: 350600 training acc: 0.5
Global Iter: 350700 training loss: 0.679458
Global Iter: 350700 training acc: 0.625
Global Iter: 350800 training loss: 0.694978
Global Iter: 350800 training acc: 0.5
Global Iter: 350900 training loss: 0.703791
Global Iter: 350900 training acc: 0.4375
Global Iter: 351000 training loss: 0.676115
Global Iter: 351000 training acc: 0.65625
Global Iter: 351100 training loss: 0.691752
Global Iter: 351100 training acc: 0.53125
Global Iter: 351200 training loss: 0.692333
Global Iter: 351200 training acc: 0.53125
Global Iter: 351300 training loss: 0.694259
Global Iter: 351300 training acc: 0.5
Global Iter: 351400 training loss: 0.679192
Global Iter: 351400 training acc: 0.625
Global Iter: 351500 training loss: 0.683707
Global Iter: 351500 training acc: 0.59375
Global Iter: 351600 training loss: 0.698564
Global Iter: 351600 training acc: 0.46875
Global Iter: 351700 training loss: 0.718728
Global Iter: 351700 training acc: 0.3125
Global Iter: 351800 training loss: 0.691268
Global Iter: 351800 training acc: 0.53125
Global Iter: 351900 training loss: 0.691301
Global Iter: 351900 training acc: 0.53125
Global Iter: 352000 training loss: 0.707157
Global Iter: 352000 training acc: 0.40625
Global Iter: 352100 training loss: 0.692124
Global Iter: 352100 training acc: 0.53125
Global Iter: 352200 training loss: 0.698158
Global Iter: 352200 training acc: 0.46875
Global Iter: 352300 training loss: 0.683226
Global Iter: 352300 training acc: 0.59375
Global Iter: 352400 training loss: 0.699438
Global Iter: 352400 training acc: 0.46875
Global Iter: 352500 training loss: 0.683134
Global Iter: 352500 training acc: 0.59375
Global Iter: 352600 training loss: 0.70287
Global Iter: 352600 training acc: 0.4375
Global Iter: 352700 training loss: 0.688301
Global Iter: 352700 training acc: 0.5625
Global Iter: 352800 training loss: 0.670362
Global Iter: 352800 training acc: 0.6875
Global Iter: 352900 training loss: 0.691757
Global Iter: 352900 training acc: 0.53125
Global Iter: 353000 training loss: 0.690422
Global Iter: 353000 training acc: 0.53125
Global Iter: 353100 training loss: 0.695745
Global Iter: 353100 training acc: 0.5
Global Iter: 353200 training loss: 0.702308
Global Iter: 353200 training acc: 0.4375
Global Iter: 353300 training loss: 0.701553
Global Iter: 353300 training acc: 0.46875
Global Iter: 353400 training loss: 0.690982
Global Iter: 353400 training acc: 0.53125
Global Iter: 353500 training loss: 0.695569
Global Iter: 353500 training acc: 0.5
Global Iter: 353600 training loss: 0.676284
Global Iter: 353600 training acc: 0.65625
Global Iter: 353700 training loss: 0.706484
Global Iter: 353700 training acc: 0.40625
Global Iter: 353800 training loss: 0.710797
Global Iter: 353800 training acc: 0.375
Global Iter: 353900 training loss: 0.670078
Global Iter: 353900 training acc: 0.6875
Global Iter: 354000 training loss: 0.69464
Global Iter: 354000 training acc: 0.5
Global Iter: 354100 training loss: 0.68673
Global Iter: 354100 training acc: 0.5625
Global Iter: 354200 training loss: 0.68827
Global Iter: 354200 training acc: 0.5625
Global Iter: 354300 training loss: 0.674201
Global Iter: 354300 training acc: 0.65625
Global Iter: 354400 training loss: 0.701952
Global Iter: 354400 training acc: 0.4375
Global Iter: 354500 training loss: 0.694714
Global Iter: 354500 training acc: 0.5
Global Iter: 354600 training loss: 0.699552
Global Iter: 354600 training acc: 0.46875
Global Iter: 354700 training loss: 0.699511
Global Iter: 354700 training acc: 0.46875
Global Iter: 354800 training loss: 0.691432
Global Iter: 354800 training acc: 0.53125
Global Iter: 354900 training loss: 0.695573
Global Iter: 354900 training acc: 0.5
Global Iter: 355000 training loss: 0.683264
Global Iter: 355000 training acc: 0.59375
Global Iter: 355100 training loss: 0.695852
Global Iter: 355100 training acc: 0.5
Global Iter: 355200 training loss: 0.674832
Global Iter: 355200 training acc: 0.65625
Global Iter: 355300 training loss: 0.69066
Global Iter: 355300 training acc: 0.53125
Global Iter: 355400 training loss: 0.679551
Global Iter: 355400 training acc: 0.625
Global Iter: 355500 training loss: 0.69148
Global Iter: 355500 training acc: 0.53125
Global Iter: 355600 training loss: 0.675309
Global Iter: 355600 training acc: 0.65625
Global Iter: 355700 training loss: 0.679787
Global Iter: 355700 training acc: 0.625
Global Iter: 355800 training loss: 0.674827
Global Iter: 355800 training acc: 0.65625
Global Iter: 355900 training loss: 0.702182
Global Iter: 355900 training acc: 0.4375
Global Iter: 356000 training loss: 0.692073
Global Iter: 356000 training acc: 0.53125
Global Iter: 356100 training loss: 0.688025
Global Iter: 356100 training acc: 0.5625
Global Iter: 356200 training loss: 0.674919
Global Iter: 356200 training acc: 0.65625
Global Iter: 356300 training loss: 0.694922
Global Iter: 356300 training acc: 0.53125
Global Iter: 356400 training loss: 0.697713
Global Iter: 356400 training acc: 0.46875
Global Iter: 356500 training loss: 0.691964
Global Iter: 356500 training acc: 0.5
Global Iter: 356600 training loss: 0.718936
Global Iter: 356600 training acc: 0.3125
Global Iter: 356700 training loss: 0.702259
Global Iter: 356700 training acc: 0.4375
Global Iter: 356800 training loss: 0.663388
Global Iter: 356800 training acc: 0.75
Global Iter: 356900 training loss: 0.694777
Global Iter: 356900 training acc: 0.5
Global Iter: 357000 training loss: 0.691203
Global Iter: 357000 training acc: 0.53125
Global Iter: 357100 training loss: 0.691437
Global Iter: 357100 training acc: 0.53125
Global Iter: 357200 training loss: 0.671447
Global Iter: 357200 training acc: 0.6875
Global Iter: 357300 training loss: 0.703068
Global Iter: 357300 training acc: 0.4375
Global Iter: 357400 training loss: 0.68725
Global Iter: 357400 training acc: 0.5625
Global Iter: 357500 training loss: 0.69669
Global Iter: 357500 training acc: 0.5
Global Iter: 357600 training loss: 0.698879
Global Iter: 357600 training acc: 0.46875
Global Iter: 357700 training loss: 0.694049
Global Iter: 357700 training acc: 0.5
Global Iter: 357800 training loss: 0.689181
Global Iter: 357800 training acc: 0.53125
Global Iter: 357900 training loss: 0.691178
Global Iter: 357900 training acc: 0.53125
Global Iter: 358000 training loss: 0.683506
Global Iter: 358000 training acc: 0.59375
Global Iter: 358100 training loss: 0.691381
Global Iter: 358100 training acc: 0.53125
Global Iter: 358200 training loss: 0.694466
Global Iter: 358200 training acc: 0.5
Global Iter: 358300 training loss: 0.694645
Global Iter: 358300 training acc: 0.5
Global Iter: 358400 training loss: 0.695181
Global Iter: 358400 training acc: 0.5
Global Iter: 358500 training loss: 0.686883
Global Iter: 358500 training acc: 0.5625
Global Iter: 358600 training loss: 0.690379
Global Iter: 358600 training acc: 0.53125
Global Iter: 358700 training loss: 0.703685
Global Iter: 358700 training acc: 0.4375
Global Iter: 358800 training loss: 0.707562
Global Iter: 358800 training acc: 0.40625
Global Iter: 358900 training loss: 0.695593
Global Iter: 358900 training acc: 0.5
Global Iter: 359000 training loss: 0.693803
Global Iter: 359000 training acc: 0.5
Global Iter: 359100 training loss: 0.66456
Global Iter: 359100 training acc: 0.71875
Global Iter: 359200 training loss: 0.695471
Global Iter: 359200 training acc: 0.5
Global Iter: 359300 training loss: 0.69964
Global Iter: 359300 training acc: 0.46875
Global Iter: 359400 training loss: 0.68331
Global Iter: 359400 training acc: 0.59375
Global Iter: 359500 training loss: 0.713772
Global Iter: 359500 training acc: 0.34375
Global Iter: 359600 training loss: 0.694569
Global Iter: 359600 training acc: 0.5
Global Iter: 359700 training loss: 0.683395
Global Iter: 359700 training acc: 0.59375
Global Iter: 359800 training loss: 0.671351
Global Iter: 359800 training acc: 0.6875
Global Iter: 359900 training loss: 0.702653
Global Iter: 359900 training acc: 0.4375
Global Iter: 360000 training loss: 0.694783
Global Iter: 360000 training acc: 0.5
Global Iter: 360100 training loss: 0.686616
Global Iter: 360100 training acc: 0.5625
Global Iter: 360200 training loss: 0.698114
Global Iter: 360200 training acc: 0.46875
Global Iter: 360300 training loss: 0.695303
Global Iter: 360300 training acc: 0.5
Global Iter: 360400 training loss: 0.683266
Global Iter: 360400 training acc: 0.59375
Global Iter: 360500 training loss: 0.693712
Global Iter: 360500 training acc: 0.5
Global Iter: 360600 training loss: 0.674769
Global Iter: 360600 training acc: 0.65625
Global Iter: 360700 training loss: 0.696702
Global Iter: 360700 training acc: 0.5
Global Iter: 360800 training loss: 0.703536
Global Iter: 360800 training acc: 0.4375
Global Iter: 360900 training loss: 0.674697
Global Iter: 360900 training acc: 0.65625
Global Iter: 361000 training loss: 0.691465
Global Iter: 361000 training acc: 0.53125
Global Iter: 361100 training loss: 0.690304
Global Iter: 361100 training acc: 0.53125
Global Iter: 361200 training loss: 0.69621
Global Iter: 361200 training acc: 0.5
Global Iter: 361300 training loss: 0.677628
Global Iter: 361300 training acc: 0.625
Global Iter: 361400 training loss: 0.682974
Global Iter: 361400 training acc: 0.59375
Global Iter: 361500 training loss: 0.699528
Global Iter: 361500 training acc: 0.46875
Global Iter: 361600 training loss: 0.720202
Global Iter: 361600 training acc: 0.3125
Global Iter: 361700 training loss: 0.689459
Global Iter: 361700 training acc: 0.5625
Global Iter: 361800 training loss: 0.690086
Global Iter: 361800 training acc: 0.53125
Global Iter: 361900 training loss: 0.703732
Global Iter: 361900 training acc: 0.4375
Global Iter: 362000 training loss: 0.684153
Global Iter: 362000 training acc: 0.59375
Global Iter: 362100 training loss: 0.697082
Global Iter: 362100 training acc: 0.5
Global Iter: 362200 training loss: 0.687405
Global Iter: 362200 training acc: 0.5625
Global Iter: 362300 training loss: 0.69109
Global Iter: 362300 training acc: 0.53125
Global Iter: 362400 training loss: 0.678456
Global Iter: 362400 training acc: 0.625
Global Iter: 362500 training loss: 0.704016
Global Iter: 362500 training acc: 0.4375
Global Iter: 362600 training loss: 0.690899
Global Iter: 362600 training acc: 0.53125
Global Iter: 362700 training loss: 0.670797
Global Iter: 362700 training acc: 0.6875
Global Iter: 362800 training loss: 0.693824
Global Iter: 362800 training acc: 0.5
Global Iter: 362900 training loss: 0.695692
Global Iter: 362900 training acc: 0.5
Global Iter: 363000 training loss: 0.690545
Global Iter: 363000 training acc: 0.53125
Global Iter: 363100 training loss: 0.708153
Global Iter: 363100 training acc: 0.40625
Global Iter: 363200 training loss: 0.703421
Global Iter: 363200 training acc: 0.4375
Global Iter: 363300 training loss: 0.69146
Global Iter: 363300 training acc: 0.53125
Global Iter: 363400 training loss: 0.703428
Global Iter: 363400 training acc: 0.4375
Global Iter: 363500 training loss: 0.672597
Global Iter: 363500 training acc: 0.6875
Global Iter: 363600 training loss: 0.707025
Global Iter: 363600 training acc: 0.40625
Global Iter: 363700 training loss: 0.715966
Global Iter: 363700 training acc: 0.34375
Global Iter: 363800 training loss: 0.670741
Global Iter: 363800 training acc: 0.6875
Global Iter: 363900 training loss: 0.691561
Global Iter: 363900 training acc: 0.53125
Global Iter: 364000 training loss: 0.681339
Global Iter: 364000 training acc: 0.59375
Global Iter: 364100 training loss: 0.686888
Global Iter: 364100 training acc: 0.5625
Global Iter: 364200 training loss: 0.670123
Global Iter: 364200 training acc: 0.6875
Global Iter: 364300 training loss: 0.6976
Global Iter: 364300 training acc: 0.46875
Global Iter: 364400 training loss: 0.691095
Global Iter: 364400 training acc: 0.53125
Global Iter: 364500 training loss: 0.707321
Global Iter: 364500 training acc: 0.40625
Global Iter: 364600 training loss: 0.699347
Global Iter: 364600 training acc: 0.46875
Global Iter: 364700 training loss: 0.6865
Global Iter: 364700 training acc: 0.5625
Global Iter: 364800 training loss: 0.702952
Global Iter: 364800 training acc: 0.4375
Global Iter: 364900 training loss: 0.679676
Global Iter: 364900 training acc: 0.625
Global Iter: 365000 training loss: 0.695213
Global Iter: 365000 training acc: 0.5
Global Iter: 365100 training loss: 0.675352
Global Iter: 365100 training acc: 0.65625
Global Iter: 365200 training loss: 0.695137
Global Iter: 365200 training acc: 0.5
Global Iter: 365300 training loss: 0.679517
Global Iter: 365300 training acc: 0.625
Global Iter: 365400 training loss: 0.692246
Global Iter: 365400 training acc: 0.53125
Global Iter: 365500 training loss: 0.675112
Global Iter: 365500 training acc: 0.65625
Global Iter: 365600 training loss: 0.672832
Global Iter: 365600 training acc: 0.6875
Global Iter: 365700 training loss: 0.68093
Global Iter: 365700 training acc: 0.625
Global Iter: 365800 training loss: 0.694542
Global Iter: 365800 training acc: 0.5
Global Iter: 365900 training loss: 0.69443
Global Iter: 365900 training acc: 0.5
Global Iter: 366000 training loss: 0.687951
Global Iter: 366000 training acc: 0.5625
Global Iter: 366100 training loss: 0.68451
Global Iter: 366100 training acc: 0.59375
Global Iter: 366200 training loss: 0.687242
Global Iter: 366200 training acc: 0.5625
Global Iter: 366300 training loss: 0.694593
Global Iter: 366300 training acc: 0.5
Global Iter: 366400 training loss: 0.691058
Global Iter: 366400 training acc: 0.53125
Global Iter: 366500 training loss: 0.713956
Global Iter: 366500 training acc: 0.34375
Global Iter: 366600 training loss: 0.706895
Global Iter: 366600 training acc: 0.40625
Global Iter: 366700 training loss: 0.669013
Global Iter: 366700 training acc: 0.71875
Global Iter: 366800 training loss: 0.698064
Global Iter: 366800 training acc: 0.46875
Global Iter: 366900 training loss: 0.690786
Global Iter: 366900 training acc: 0.53125
Global Iter: 367000 training loss: 0.693281
Global Iter: 367000 training acc: 0.53125
Global Iter: 367100 training loss: 0.668661
Global Iter: 367100 training acc: 0.71875
Global Iter: 367200 training loss: 0.698356
Global Iter: 367200 training acc: 0.46875
Global Iter: 367300 training loss: 0.693236
Global Iter: 367300 training acc: 0.53125
Global Iter: 367400 training loss: 0.694276
Global Iter: 367400 training acc: 0.5
Global Iter: 367500 training loss: 0.702533
Global Iter: 367500 training acc: 0.4375
Global Iter: 367600 training loss: 0.697115
Global Iter: 367600 training acc: 0.5
Global Iter: 367700 training loss: 0.691853
Global Iter: 367700 training acc: 0.53125
Global Iter: 367800 training loss: 0.690665
Global Iter: 367800 training acc: 0.5625
Global Iter: 367900 training loss: 0.690433
Global Iter: 367900 training acc: 0.53125
Global Iter: 368000 training loss: 0.698887
Global Iter: 368000 training acc: 0.46875
Global Iter: 368100 training loss: 0.688928
Global Iter: 368100 training acc: 0.53125
Global Iter: 368200 training loss: 0.690186
Global Iter: 368200 training acc: 0.53125
Global Iter: 368300 training loss: 0.695377
Global Iter: 368300 training acc: 0.5
Global Iter: 368400 training loss: 0.685123
Global Iter: 368400 training acc: 0.5625
Global Iter: 368500 training loss: 0.687444
Global Iter: 368500 training acc: 0.5625
Global Iter: 368600 training loss: 0.701145
Global Iter: 368600 training acc: 0.46875
Global Iter: 368700 training loss: 0.709643
Global Iter: 368700 training acc: 0.375
Global Iter: 368800 training loss: 0.699302
Global Iter: 368800 training acc: 0.46875
Global Iter: 368900 training loss: 0.69644
Global Iter: 368900 training acc: 0.5
Global Iter: 369000 training loss: 0.673688
Global Iter: 369000 training acc: 0.65625
Global Iter: 369100 training loss: 0.691835
Global Iter: 369100 training acc: 0.53125
Global Iter: 369200 training loss: 0.694556
Global Iter: 369200 training acc: 0.5
Global Iter: 369300 training loss: 0.67856
Global Iter: 369300 training acc: 0.625
Global Iter: 369400 training loss: 0.720587
Global Iter: 369400 training acc: 0.3125
Global Iter: 369500 training loss: 0.698191
Global Iter: 369500 training acc: 0.46875
Global Iter: 369600 training loss: 0.68256
Global Iter: 369600 training acc: 0.59375
Global Iter: 369700 training loss: 0.670745
Global Iter: 369700 training acc: 0.6875
Global Iter: 369800 training loss: 0.699563
Global Iter: 369800 training acc: 0.46875
Global Iter: 369900 training loss: 0.699732
Global Iter: 369900 training acc: 0.46875
Global Iter: 370000 training loss: 0.686477
Global Iter: 370000 training acc: 0.5625
Global Iter: 370100 training loss: 0.695882
Global Iter: 370100 training acc: 0.5
Global Iter: 370200 training loss: 0.690909
Global Iter: 370200 training acc: 0.53125
Global Iter: 370300 training loss: 0.678937
Global Iter: 370300 training acc: 0.625
Global Iter: 370400 training loss: 0.694592
Global Iter: 370400 training acc: 0.5
Global Iter: 370500 training loss: 0.674732
Global Iter: 370500 training acc: 0.65625
Global Iter: 370600 training loss: 0.689346
Global Iter: 370600 training acc: 0.53125
Global Iter: 370700 training loss: 0.702861
Global Iter: 370700 training acc: 0.4375
Global Iter: 370800 training loss: 0.668121
Global Iter: 370800 training acc: 0.6875
Global Iter: 370900 training loss: 0.686297
Global Iter: 370900 training acc: 0.5625
Global Iter: 371000 training loss: 0.694823
Global Iter: 371000 training acc: 0.5
Global Iter: 371100 training loss: 0.695598
Global Iter: 371100 training acc: 0.5
Global Iter: 371200 training loss: 0.683657
Global Iter: 371200 training acc: 0.59375
Global Iter: 371300 training loss: 0.693334
Global Iter: 371300 training acc: 0.53125
Global Iter: 371400 training loss: 0.699652
Global Iter: 371400 training acc: 0.46875
Global Iter: 371500 training loss: 0.715312
Global Iter: 371500 training acc: 0.34375
Global Iter: 371600 training loss: 0.686481
Global Iter: 371600 training acc: 0.5625
Global Iter: 371700 training loss: 0.693782
Global Iter: 371700 training acc: 0.5
Global Iter: 371800 training loss: 0.707268
Global Iter: 371800 training acc: 0.40625
Global Iter: 371900 training loss: 0.678334
Global Iter: 371900 training acc: 0.625
Global Iter: 372000 training loss: 0.695847
Global Iter: 372000 training acc: 0.5
Global Iter: 372100 training loss: 0.691745
Global Iter: 372100 training acc: 0.53125
Global Iter: 372200 training loss: 0.692473
Global Iter: 372200 training acc: 0.53125
Global Iter: 372300 training loss: 0.675901
Global Iter: 372300 training acc: 0.65625
Global Iter: 372400 training loss: 0.698361
Global Iter: 372400 training acc: 0.46875
Global Iter: 372500 training loss: 0.691213
Global Iter: 372500 training acc: 0.53125
Global Iter: 372600 training loss: 0.676767
Global Iter: 372600 training acc: 0.65625
Global Iter: 372700 training loss: 0.697864
Global Iter: 372700 training acc: 0.46875
Global Iter: 372800 training loss: 0.691231
Global Iter: 372800 training acc: 0.46875
Global Iter: 372900 training loss: 0.684553
Global Iter: 372900 training acc: 0.59375
Global Iter: 373000 training loss: 0.699468
Global Iter: 373000 training acc: 0.46875
Global Iter: 373100 training loss: 0.707547
Global Iter: 373100 training acc: 0.40625
Global Iter: 373200 training loss: 0.68796
Global Iter: 373200 training acc: 0.5625
Global Iter: 373300 training loss: 0.708544
Global Iter: 373300 training acc: 0.40625
Global Iter: 373400 training loss: 0.67381
Global Iter: 373400 training acc: 0.65625
Global Iter: 373500 training loss: 0.697773
Global Iter: 373500 training acc: 0.46875
Global Iter: 373600 training loss: 0.724874
Global Iter: 373600 training acc: 0.28125
Global Iter: 373700 training loss: 0.679764
Global Iter: 373700 training acc: 0.625
Global Iter: 373800 training loss: 0.693396
Global Iter: 373800 training acc: 0.53125
Global Iter: 373900 training loss: 0.683458
Global Iter: 373900 training acc: 0.59375
Global Iter: 374000 training loss: 0.692548
Global Iter: 374000 training acc: 0.53125
Global Iter: 374100 training loss: 0.671492
Global Iter: 374100 training acc: 0.6875
Global Iter: 374200 training loss: 0.699459
Global Iter: 374200 training acc: 0.46875
Global Iter: 374300 training loss: 0.690046
Global Iter: 374300 training acc: 0.53125
Global Iter: 374400 training loss: 0.703053
Global Iter: 374400 training acc: 0.4375
Global Iter: 374500 training loss: 0.700014
Global Iter: 374500 training acc: 0.46875
Global Iter: 374600 training loss: 0.686509
Global Iter: 374600 training acc: 0.5625
Global Iter: 374700 training loss: 0.704289
Global Iter: 374700 training acc: 0.4375
Global Iter: 374800 training loss: 0.688774
Global Iter: 374800 training acc: 0.5625
Global Iter: 374900 training loss: 0.689253
Global Iter: 374900 training acc: 0.53125
Global Iter: 375000 training loss: 0.675075
Global Iter: 375000 training acc: 0.65625
Global Iter: 375100 training loss: 0.693815
Global Iter: 375100 training acc: 0.5
Global Iter: 375200 training loss: 0.679356
Global Iter: 375200 training acc: 0.625
Global Iter: 375300 training loss: 0.687231
Global Iter: 375300 training acc: 0.5625
Global Iter: 375400 training loss: 0.682463
Global Iter: 375400 training acc: 0.59375
Global Iter: 375500 training loss: 0.671228
Global Iter: 375500 training acc: 0.6875
Global Iter: 375600 training loss: 0.678749
Global Iter: 375600 training acc: 0.625
Global Iter: 375700 training loss: 0.688366
Global Iter: 375700 training acc: 0.5625
Global Iter: 375800 training loss: 0.689664
Global Iter: 375800 training acc: 0.53125
Global Iter: 375900 training loss: 0.680712
Global Iter: 375900 training acc: 0.625
Global Iter: 376000 training loss: 0.68186
Global Iter: 376000 training acc: 0.59375
Global Iter: 376100 training loss: 0.691599
Global Iter: 376100 training acc: 0.53125
Global Iter: 376200 training loss: 0.700433
Global Iter: 376200 training acc: 0.46875
Global Iter: 376300 training loss: 0.685384
Global Iter: 376300 training acc: 0.59375
Global Iter: 376400 training loss: 0.708974
Global Iter: 376400 training acc: 0.375
Global Iter: 376500 training loss: 0.707964
Global Iter: 376500 training acc: 0.40625
Global Iter: 376600 training loss: 0.669471
Global Iter: 376600 training acc: 0.6875
Global Iter: 376700 training loss: 0.695657
Global Iter: 376700 training acc: 0.5
Global Iter: 376800 training loss: 0.694388
Global Iter: 376800 training acc: 0.5
Global Iter: 376900 training loss: 0.687656
Global Iter: 376900 training acc: 0.5625
Global Iter: 377000 training loss: 0.665809
Global Iter: 377000 training acc: 0.71875
Global Iter: 377100 training loss: 0.694112
Global Iter: 377100 training acc: 0.5
Global Iter: 377200 training loss: 0.68606
Global Iter: 377200 training acc: 0.5625
Global Iter: 377300 training loss: 0.695636
Global Iter: 377300 training acc: 0.5
Global Iter: 377400 training loss: 0.707804
Global Iter: 377400 training acc: 0.40625
Global Iter: 377500 training loss: 0.703069
Global Iter: 377500 training acc: 0.4375
Global Iter: 377600 training loss: 0.683756
Global Iter: 377600 training acc: 0.59375
Global Iter: 377700 training loss: 0.691108
Global Iter: 377700 training acc: 0.53125
Global Iter: 377800 training loss: 0.695096
Global Iter: 377800 training acc: 0.5
Global Iter: 377900 training loss: 0.695022
Global Iter: 377900 training acc: 0.5
Global Iter: 378000 training loss: 0.687926
Global Iter: 378000 training acc: 0.5625
Global Iter: 378100 training loss: 0.688044
Global Iter: 378100 training acc: 0.5625
Global Iter: 378200 training loss: 0.695956
Global Iter: 378200 training acc: 0.5
Global Iter: 378300 training loss: 0.688752
Global Iter: 378300 training acc: 0.53125
Global Iter: 378400 training loss: 0.687784
Global Iter: 378400 training acc: 0.5625
Global Iter: 378500 training loss: 0.695673
Global Iter: 378500 training acc: 0.5
Global Iter: 378600 training loss: 0.707515
Global Iter: 378600 training acc: 0.40625
Global Iter: 378700 training loss: 0.693969
Global Iter: 378700 training acc: 0.5
Global Iter: 378800 training loss: 0.694759
Global Iter: 378800 training acc: 0.5
Global Iter: 378900 training loss: 0.6714
Global Iter: 378900 training acc: 0.6875
Global Iter: 379000 training loss: 0.689992
Global Iter: 379000 training acc: 0.53125
Global Iter: 379100 training loss: 0.695825
Global Iter: 379100 training acc: 0.5
Global Iter: 379200 training loss: 0.689816
Global Iter: 379200 training acc: 0.5625
Global Iter: 379300 training loss: 0.719774
Global Iter: 379300 training acc: 0.3125
Global Iter: 379400 training loss: 0.698007
Global Iter: 379400 training acc: 0.46875
Global Iter: 379500 training loss: 0.681487
Global Iter: 379500 training acc: 0.59375
Global Iter: 379600 training loss: 0.675322
Global Iter: 379600 training acc: 0.65625
Global Iter: 379700 training loss: 0.701075
Global Iter: 379700 training acc: 0.46875
Global Iter: 379800 training loss: 0.695642
Global Iter: 379800 training acc: 0.5
Global Iter: 379900 training loss: 0.682973
Global Iter: 379900 training acc: 0.59375
Global Iter: 380000 training loss: 0.690712
Global Iter: 380000 training acc: 0.53125
Global Iter: 380100 training loss: 0.695645
Global Iter: 380100 training acc: 0.5
Global Iter: 380200 training loss: 0.687095
Global Iter: 380200 training acc: 0.5625
Global Iter: 380300 training loss: 0.69386
Global Iter: 380300 training acc: 0.5
Global Iter: 380400 training loss: 0.679843
Global Iter: 380400 training acc: 0.625
Global Iter: 380500 training loss: 0.687369
Global Iter: 380500 training acc: 0.5625
Global Iter: 380600 training loss: 0.702586
Global Iter: 380600 training acc: 0.4375
Global Iter: 380700 training loss: 0.667605
Global Iter: 380700 training acc: 0.71875
Global Iter: 380800 training loss: 0.688633
Global Iter: 380800 training acc: 0.5625
Global Iter: 380900 training loss: 0.696113
Global Iter: 380900 training acc: 0.5
Global Iter: 381000 training loss: 0.690558
Global Iter: 381000 training acc: 0.53125
Global Iter: 381100 training loss: 0.69277
Global Iter: 381100 training acc: 0.53125
Global Iter: 381200 training loss: 0.695184
Global Iter: 381200 training acc: 0.5
Global Iter: 381300 training loss: 0.691214
Global Iter: 381300 training acc: 0.53125
Global Iter: 381400 training loss: 0.715872
Global Iter: 381400 training acc: 0.34375
Global Iter: 381500 training loss: 0.690582
Global Iter: 381500 training acc: 0.53125
Global Iter: 381600 training loss: 0.694931
Global Iter: 381600 training acc: 0.5
Global Iter: 381700 training loss: 0.707477
Global Iter: 381700 training acc: 0.40625
Global Iter: 381800 training loss: 0.679404
Global Iter: 381800 training acc: 0.625
Global Iter: 381900 training loss: 0.691647
Global Iter: 381900 training acc: 0.53125
Global Iter: 382000 training loss: 0.691239
Global Iter: 382000 training acc: 0.53125
Global Iter: 382100 training loss: 0.684114
Global Iter: 382100 training acc: 0.59375
Global Iter: 382200 training loss: 0.679481
Global Iter: 382200 training acc: 0.625
Global Iter: 382300 training loss: 0.703828
Global Iter: 382300 training acc: 0.4375
Global Iter: 382400 training loss: 0.689872
Global Iter: 382400 training acc: 0.53125
Global Iter: 382500 training loss: 0.676161
Global Iter: 382500 training acc: 0.65625
Global Iter: 382600 training loss: 0.702632
Global Iter: 382600 training acc: 0.4375
Global Iter: 382700 training loss: 0.686253
Global Iter: 382700 training acc: 0.5625
Global Iter: 382800 training loss: 0.686354
Global Iter: 382800 training acc: 0.5625
Global Iter: 382900 training loss: 0.700189
Global Iter: 382900 training acc: 0.46875
Global Iter: 383000 training loss: 0.699452
Global Iter: 383000 training acc: 0.46875
Global Iter: 383100 training loss: 0.687168
Global Iter: 383100 training acc: 0.5625
Global Iter: 383200 training loss: 0.702273
Global Iter: 383200 training acc: 0.4375
Global Iter: 383300 training loss: 0.671021
Global Iter: 383300 training acc: 0.6875
Global Iter: 383400 training loss: 0.699609
Global Iter: 383400 training acc: 0.46875
Global Iter: 383500 training loss: 0.715437
Global Iter: 383500 training acc: 0.34375
Global Iter: 383600 training loss: 0.682351
Global Iter: 383600 training acc: 0.59375
Global Iter: 383700 training loss: 0.69083
Global Iter: 383700 training acc: 0.53125
Global Iter: 383800 training loss: 0.674891
Global Iter: 383800 training acc: 0.65625
Global Iter: 383900 training loss: 0.686957
Global Iter: 383900 training acc: 0.5625
Global Iter: 384000 training loss: 0.667999
Global Iter: 384000 training acc: 0.71875
Global Iter: 384100 training loss: 0.69485
Global Iter: 384100 training acc: 0.5
Global Iter: 384200 training loss: 0.691052
Global Iter: 384200 training acc: 0.53125
Global Iter: 384300 training loss: 0.707408
Global Iter: 384300 training acc: 0.40625
Global Iter: 384400 training loss: 0.691686
Global Iter: 384400 training acc: 0.53125
Global Iter: 384500 training loss: 0.678615
Global Iter: 384500 training acc: 0.625
Global Iter: 384600 training loss: 0.701602
Global Iter: 384600 training acc: 0.4375
Global Iter: 384700 training loss: 0.691374
Global Iter: 384700 training acc: 0.53125
Global Iter: 384800 training loss: 0.687669
Global Iter: 384800 training acc: 0.5625
Global Iter: 384900 training loss: 0.672861
Global Iter: 384900 training acc: 0.6875
Global Iter: 385000 training loss: 0.702569
Global Iter: 385000 training acc: 0.4375
Global Iter: 385100 training loss: 0.678118
Global Iter: 385100 training acc: 0.625
Global Iter: 385200 training loss: 0.688197
Global Iter: 385200 training acc: 0.5625
Global Iter: 385300 training loss: 0.679135
Global Iter: 385300 training acc: 0.625
Global Iter: 385400 training loss: 0.672324
Global Iter: 385400 training acc: 0.6875
Global Iter: 385500 training loss: 0.679992
Global Iter: 385500 training acc: 0.625
Global Iter: 385600 training loss: 0.692297
Global Iter: 385600 training acc: 0.53125
Global Iter: 385700 training loss: 0.687168
Global Iter: 385700 training acc: 0.5625
Global Iter: 385800 training loss: 0.676713
Global Iter: 385800 training acc: 0.625
Global Iter: 385900 training loss: 0.684104
Global Iter: 385900 training acc: 0.59375
Global Iter: 386000 training loss: 0.695809
Global Iter: 386000 training acc: 0.5
Global Iter: 386100 training loss: 0.703377
Global Iter: 386100 training acc: 0.4375
Global Iter: 386200 training loss: 0.682588
Global Iter: 386200 training acc: 0.59375
Global Iter: 386300 training loss: 0.707633
Global Iter: 386300 training acc: 0.40625
Global Iter: 386400 training loss: 0.712363
Global Iter: 386400 training acc: 0.375
Global Iter: 386500 training loss: 0.669691
Global Iter: 386500 training acc: 0.6875
Global Iter: 386600 training loss: 0.696383
Global Iter: 386600 training acc: 0.5
Global Iter: 386700 training loss: 0.693579
Global Iter: 386700 training acc: 0.5
Global Iter: 386800 training loss: 0.687479
Global Iter: 386800 training acc: 0.5625
Global Iter: 386900 training loss: 0.6685
Global Iter: 386900 training acc: 0.71875
Global Iter: 387000 training loss: 0.697597
Global Iter: 387000 training acc: 0.46875
Global Iter: 387100 training loss: 0.689094
Global Iter: 387100 training acc: 0.5625
Global Iter: 387200 training loss: 0.699046
Global Iter: 387200 training acc: 0.46875
Global Iter: 387300 training loss: 0.702278
Global Iter: 387300 training acc: 0.4375
Global Iter: 387400 training loss: 0.70018
Global Iter: 387400 training acc: 0.46875
Global Iter: 387500 training loss: 0.682107
Global Iter: 387500 training acc: 0.59375
Global Iter: 387600 training loss: 0.694811
Global Iter: 387600 training acc: 0.5
Global Iter: 387700 training loss: 0.691567
Global Iter: 387700 training acc: 0.53125
Global Iter: 387800 training loss: 0.695465
Global Iter: 387800 training acc: 0.5
Global Iter: 387900 training loss: 0.687411
Global Iter: 387900 training acc: 0.5625
Global Iter: 388000 training loss: 0.691159
Global Iter: 388000 training acc: 0.53125
Global Iter: 388100 training loss: 0.695318
Global Iter: 388100 training acc: 0.5
Global Iter: 388200 training loss: 0.682833
Global Iter: 388200 training acc: 0.59375
Global Iter: 388300 training loss: 0.68387
Global Iter: 388300 training acc: 0.59375
Global Iter: 388400 training loss: 0.691585
Global Iter: 388400 training acc: 0.53125
Global Iter: 388500 training loss: 0.706966
Global Iter: 388500 training acc: 0.40625
Global Iter: 388600 training loss: 0.691478
Global Iter: 388600 training acc: 0.53125
Global Iter: 388700 training loss: 0.699569
Global Iter: 388700 training acc: 0.46875
Global Iter: 388800 training loss: 0.670953
Global Iter: 388800 training acc: 0.6875
Global Iter: 388900 training loss: 0.68772
Global Iter: 388900 training acc: 0.5625
Global Iter: 389000 training loss: 0.703859
Global Iter: 389000 training acc: 0.4375
Global Iter: 389100 training loss: 0.687449
Global Iter: 389100 training acc: 0.5625
Global Iter: 389200 training loss: 0.715667
Global Iter: 389200 training acc: 0.34375
Global Iter: 389300 training loss: 0.703239
Global Iter: 389300 training acc: 0.4375
Global Iter: 389400 training loss: 0.683938
Global Iter: 389400 training acc: 0.59375
Global Iter: 389500 training loss: 0.679157
Global Iter: 389500 training acc: 0.625
Global Iter: 389600 training loss: 0.695216
Global Iter: 389600 training acc: 0.5
Global Iter: 389700 training loss: 0.694883
Global Iter: 389700 training acc: 0.5
Global Iter: 389800 training loss: 0.679091
Global Iter: 389800 training acc: 0.625
Global Iter: 389900 training loss: 0.686234
Global Iter: 389900 training acc: 0.5625
Global Iter: 390000 training loss: 0.696229
Global Iter: 390000 training acc: 0.5
Global Iter: 390100 training loss: 0.690794
Global Iter: 390100 training acc: 0.53125
Global Iter: 390200 training loss: 0.698191
Global Iter: 390200 training acc: 0.46875
Global Iter: 390300 training loss: 0.683586
Global Iter: 390300 training acc: 0.59375
Global Iter: 390400 training loss: 0.686115
Global Iter: 390400 training acc: 0.5625
Global Iter: 390500 training loss: 0.707721
Global Iter: 390500 training acc: 0.40625
Global Iter: 390600 training loss: 0.66582
Global Iter: 390600 training acc: 0.71875
Global Iter: 390700 training loss: 0.687281
Global Iter: 390700 training acc: 0.5625
Global Iter: 390800 training loss: 0.69152
Global Iter: 390800 training acc: 0.53125
Global Iter: 390900 training loss: 0.697862
Global Iter: 390900 training acc: 0.46875
Global Iter: 391000 training loss: 0.688026
Global Iter: 391000 training acc: 0.5625
Global Iter: 391100 training loss: 0.690708
Global Iter: 391100 training acc: 0.53125
Global Iter: 391200 training loss: 0.690854
Global Iter: 391200 training acc: 0.53125
Global Iter: 391300 training loss: 0.711582
Global Iter: 391300 training acc: 0.375
Global Iter: 391400 training loss: 0.695561
Global Iter: 391400 training acc: 0.5
Global Iter: 391500 training loss: 0.695518
Global Iter: 391500 training acc: 0.5
Global Iter: 391600 training loss: 0.703555
Global Iter: 391600 training acc: 0.4375
Global Iter: 391700 training loss: 0.682861
Global Iter: 391700 training acc: 0.59375
Global Iter: 391800 training loss: 0.69151
Global Iter: 391800 training acc: 0.53125
Global Iter: 391900 training loss: 0.693893
Global Iter: 391900 training acc: 0.5
Global Iter: 392000 training loss: 0.692222
Global Iter: 392000 training acc: 0.53125
Global Iter: 392100 training loss: 0.677708
Global Iter: 392100 training acc: 0.625
Global Iter: 392200 training loss: 0.703429
Global Iter: 392200 training acc: 0.4375
Global Iter: 392300 training loss: 0.699056
Global Iter: 392300 training acc: 0.46875
Global Iter: 392400 training loss: 0.675449
Global Iter: 392400 training acc: 0.65625
Global Iter: 392500 training loss: 0.701916
Global Iter: 392500 training acc: 0.4375
Global Iter: 392600 training loss: 0.682548
Global Iter: 392600 training acc: 0.59375
Global Iter: 392700 training loss: 0.682704
Global Iter: 392700 training acc: 0.59375
Global Iter: 392800 training loss: 0.699736
Global Iter: 392800 training acc: 0.46875
Global Iter: 392900 training loss: 0.700155
Global Iter: 392900 training acc: 0.46875
Global Iter: 393000 training loss: 0.678326
Global Iter: 393000 training acc: 0.625
Global Iter: 393100 training loss: 0.698507
Global Iter: 393100 training acc: 0.46875
Global Iter: 393200 training loss: 0.661067
Global Iter: 393200 training acc: 0.75
Global Iter: 393300 training loss: 0.698032
Global Iter: 393300 training acc: 0.5
Global Iter: 393400 training loss: 0.711523
Global Iter: 393400 training acc: 0.375
Global Iter: 393500 training loss: 0.67915
Global Iter: 393500 training acc: 0.625
Global Iter: 393600 training loss: 0.686317
Global Iter: 393600 training acc: 0.5625
Global Iter: 393700 training loss: 0.680629
Global Iter: 393700 training acc: 0.625
Global Iter: 393800 training loss: 0.694446
Global Iter: 393800 training acc: 0.5
Global Iter: 393900 training loss: 0.671674
Global Iter: 393900 training acc: 0.6875
Global Iter: 394000 training loss: 0.690802
Global Iter: 394000 training acc: 0.53125
Global Iter: 394100 training loss: 0.690223
Global Iter: 394100 training acc: 0.53125
Global Iter: 394200 training loss: 0.706998
Global Iter: 394200 training acc: 0.40625
Global Iter: 394300 training loss: 0.690533
Global Iter: 394300 training acc: 0.53125
Global Iter: 394400 training loss: 0.670506
Global Iter: 394400 training acc: 0.6875
Global Iter: 394500 training loss: 0.707975
Global Iter: 394500 training acc: 0.40625
Global Iter: 394600 training loss: 0.691568
Global Iter: 394600 training acc: 0.53125
Global Iter: 394700 training loss: 0.686331
Global Iter: 394700 training acc: 0.5625
Global Iter: 394800 training loss: 0.674512
Global Iter: 394800 training acc: 0.65625
Global Iter: 394900 training loss: 0.69948
Global Iter: 394900 training acc: 0.46875
Global Iter: 395000 training loss: 0.678685
Global Iter: 395000 training acc: 0.625
Global Iter: 395100 training loss: 0.687556
Global Iter: 395100 training acc: 0.5625
Global Iter: 395200 training loss: 0.675561
Global Iter: 395200 training acc: 0.65625
Global Iter: 395300 training loss: 0.672183
Global Iter: 395300 training acc: 0.6875
Global Iter: 395400 training loss: 0.675076
Global Iter: 395400 training acc: 0.65625
Global Iter: 395500 training loss: 0.687561
Global Iter: 395500 training acc: 0.5625
Global Iter: 395600 training loss: 0.687512
Global Iter: 395600 training acc: 0.5625
Global Iter: 395700 training loss: 0.683084
Global Iter: 395700 training acc: 0.59375
Global Iter: 395800 training loss: 0.686885
Global Iter: 395800 training acc: 0.5625
Global Iter: 395900 training loss: 0.69161
Global Iter: 395900 training acc: 0.53125
Global Iter: 396000 training loss: 0.703474
Global Iter: 396000 training acc: 0.4375
Global Iter: 396100 training loss: 0.678882
Global Iter: 396100 training acc: 0.625
Global Iter: 396200 training loss: 0.703344
Global Iter: 396200 training acc: 0.4375
Global Iter: 396300 training loss: 0.711359
Global Iter: 396300 training acc: 0.375
Global Iter: 396400 training loss: 0.661463
Global Iter: 396400 training acc: 0.75
Global Iter: 396500 training loss: 0.694378
Global Iter: 396500 training acc: 0.5
Global Iter: 396600 training loss: 0.691191
Global Iter: 396600 training acc: 0.53125
Global Iter: 396700 training loss: 0.679579
Global Iter: 396700 training acc: 0.59375
Global Iter: 396800 training loss: 0.670878
Global Iter: 396800 training acc: 0.6875
Global Iter: 396900 training loss: 0.707014
Global Iter: 396900 training acc: 0.40625
Global Iter: 397000 training loss: 0.683738
Global Iter: 397000 training acc: 0.59375
Global Iter: 397100 training loss: 0.698722
Global Iter: 397100 training acc: 0.46875
Global Iter: 397200 training loss: 0.707274
Global Iter: 397200 training acc: 0.40625
Global Iter: 397300 training loss: 0.690699
Global Iter: 397300 training acc: 0.53125
Global Iter: 397400 training loss: 0.683337
Global Iter: 397400 training acc: 0.59375
Global Iter: 397500 training loss: 0.687829
Global Iter: 397500 training acc: 0.5625
Global Iter: 397600 training loss: 0.687109
Global Iter: 397600 training acc: 0.5625
Global Iter: 397700 training loss: 0.690806
Global Iter: 397700 training acc: 0.53125
Global Iter: 397800 training loss: 0.690224
Global Iter: 397800 training acc: 0.53125
Global Iter: 397900 training loss: 0.690898
Global Iter: 397900 training acc: 0.53125
Global Iter: 398000 training loss: 0.691188
Global Iter: 398000 training acc: 0.53125
Global Iter: 398100 training loss: 0.686984
Global Iter: 398100 training acc: 0.5625
Global Iter: 398200 training loss: 0.683719
Global Iter: 398200 training acc: 0.59375
Global Iter: 398300 training loss: 0.687034
Global Iter: 398300 training acc: 0.5625
Global Iter: 398400 training loss: 0.703379
Global Iter: 398400 training acc: 0.4375
Global Iter: 398500 training loss: 0.694685
Global Iter: 398500 training acc: 0.5
Global Iter: 398600 training loss: 0.703127
Global Iter: 398600 training acc: 0.4375
Global Iter: 398700 training loss: 0.671871
Global Iter: 398700 training acc: 0.6875
Global Iter: 398800 training loss: 0.682971
Global Iter: 398800 training acc: 0.59375
Global Iter: 398900 training loss: 0.709084
Global Iter: 398900 training acc: 0.4375
Global Iter: 399000 training loss: 0.68555
Global Iter: 399000 training acc: 0.5625
Global Iter: 399100 training loss: 0.718644
Global Iter: 399100 training acc: 0.3125
Global Iter: 399200 training loss: 0.698759
Global Iter: 399200 training acc: 0.46875
Global Iter: 399300 training loss: 0.679404
Global Iter: 399300 training acc: 0.625
Global Iter: 399400 training loss: 0.679291
Global Iter: 399400 training acc: 0.625
Global Iter: 399500 training loss: 0.69999
Global Iter: 399500 training acc: 0.46875
Global Iter: 399600 training loss: 0.691816
Global Iter: 399600 training acc: 0.53125
Global Iter: 399700 training loss: 0.675259
Global Iter: 399700 training acc: 0.65625
Global Iter: 399800 training loss: 0.687504
Global Iter: 399800 training acc: 0.5625
Global Iter: 399900 training loss: 0.695173
Global Iter: 399900 training acc: 0.5
Global Iter: 400000 training loss: 0.695407
Global Iter: 400000 training acc: 0.5
Global Iter: 400100 training loss: 0.706446
Global Iter: 400100 training acc: 0.40625
Global Iter: 400200 training loss: 0.683528
Global Iter: 400200 training acc: 0.59375
Global Iter: 400300 training loss: 0.683391
Global Iter: 400300 training acc: 0.59375
Global Iter: 400400 training loss: 0.707161
Global Iter: 400400 training acc: 0.40625
Global Iter: 400500 training loss: 0.672
Global Iter: 400500 training acc: 0.6875
Global Iter: 400600 training loss: 0.691063
Global Iter: 400600 training acc: 0.53125
Global Iter: 400700 training loss: 0.690965
Global Iter: 400700 training acc: 0.53125
Global Iter: 400800 training loss: 0.702915
Global Iter: 400800 training acc: 0.4375
Global Iter: 400900 training loss: 0.687364
Global Iter: 400900 training acc: 0.5625
Global Iter: 401000 training loss: 0.686976
Global Iter: 401000 training acc: 0.5625
Global Iter: 401100 training loss: 0.694923
Global Iter: 401100 training acc: 0.5
Global Iter: 401200 training loss: 0.715099
Global Iter: 401200 training acc: 0.34375
Global Iter: 401300 training loss: 0.69568
Global Iter: 401300 training acc: 0.5
Global Iter: 401400 training loss: 0.695045
Global Iter: 401400 training acc: 0.5
Global Iter: 401500 training loss: 0.706824
Global Iter: 401500 training acc: 0.40625
Global Iter: 401600 training loss: 0.68679
Global Iter: 401600 training acc: 0.5625
Global Iter: 401700 training loss: 0.698763
Global Iter: 401700 training acc: 0.46875
Global Iter: 401800 training loss: 0.698756
Global Iter: 401800 training acc: 0.46875
Global Iter: 401900 training loss: 0.686778
Global Iter: 401900 training acc: 0.5625
Global Iter: 402000 training loss: 0.679081
Global Iter: 402000 training acc: 0.625
Global Iter: 402100 training loss: 0.706707
Global Iter: 402100 training acc: 0.40625
Global Iter: 402200 training loss: 0.70269
Global Iter: 402200 training acc: 0.4375
Global Iter: 402300 training loss: 0.670872
Global Iter: 402300 training acc: 0.6875
Global Iter: 402400 training loss: 0.69873
Global Iter: 402400 training acc: 0.46875
Global Iter: 402500 training loss: 0.683121
Global Iter: 402500 training acc: 0.59375
Global Iter: 402600 training loss: 0.687535
Global Iter: 402600 training acc: 0.5625
Global Iter: 402700 training loss: 0.698673
Global Iter: 402700 training acc: 0.46875
Global Iter: 402800 training loss: 0.698944
Global Iter: 402800 training acc: 0.46875
Global Iter: 402900 training loss: 0.682401
Global Iter: 402900 training acc: 0.59375
Global Iter: 403000 training loss: 0.702182
Global Iter: 403000 training acc: 0.4375
Global Iter: 403100 training loss: 0.668018
Global Iter: 403100 training acc: 0.71875
Global Iter: 403200 training loss: 0.690208
Global Iter: 403200 training acc: 0.53125
Global Iter: 403300 training loss: 0.706237
Global Iter: 403300 training acc: 0.40625
Global Iter: 403400 training loss: 0.68402
Global Iter: 403400 training acc: 0.59375
Global Iter: 403500 training loss: 0.683298
Global Iter: 403500 training acc: 0.59375
Global Iter: 403600 training loss: 0.684213
Global Iter: 403600 training acc: 0.59375
Global Iter: 403700 training loss: 0.690817
Global Iter: 403700 training acc: 0.53125
Global Iter: 403800 training loss: 0.67131
Global Iter: 403800 training acc: 0.6875
Global Iter: 403900 training loss: 0.683516
Global Iter: 403900 training acc: 0.59375
Global Iter: 404000 training loss: 0.694518
Global Iter: 404000 training acc: 0.5
Global Iter: 404100 training loss: 0.710893
Global Iter: 404100 training acc: 0.375
Global Iter: 404200 training loss: 0.694705
Global Iter: 404200 training acc: 0.5
Global Iter: 404300 training loss: 0.679253
Global Iter: 404300 training acc: 0.625
Global Iter: 404400 training loss: 0.706425
Global Iter: 404400 training acc: 0.40625
Global Iter: 404500 training loss: 0.695203
Global Iter: 404500 training acc: 0.5
Global Iter: 404600 training loss: 0.683429
Global Iter: 404600 training acc: 0.59375
Global Iter: 404700 training loss: 0.677746
Global Iter: 404700 training acc: 0.625
Global Iter: 404800 training loss: 0.70246
Global Iter: 404800 training acc: 0.4375
Global Iter: 404900 training loss: 0.671821
Global Iter: 404900 training acc: 0.6875
Global Iter: 405000 training loss: 0.690335
Global Iter: 405000 training acc: 0.53125
Global Iter: 405100 training loss: 0.683207
Global Iter: 405100 training acc: 0.59375
Global Iter: 405200 training loss: 0.671547
Global Iter: 405200 training acc: 0.6875
Global Iter: 405300 training loss: 0.674958
Global Iter: 405300 training acc: 0.65625
Global Iter: 405400 training loss: 0.682329
Global Iter: 405400 training acc: 0.59375
Global Iter: 405500 training loss: 0.690688
Global Iter: 405500 training acc: 0.53125
Global Iter: 405600 training loss: 0.686129
Global Iter: 405600 training acc: 0.5625
Global Iter: 405700 training loss: 0.692077
Global Iter: 405700 training acc: 0.53125
Global Iter: 405800 training loss: 0.699565
Global Iter: 405800 training acc: 0.46875
Global Iter: 405900 training loss: 0.70348
Global Iter: 405900 training acc: 0.4375
Global Iter: 406000 training loss: 0.674414
Global Iter: 406000 training acc: 0.65625
Global Iter: 406100 training loss: 0.698954
Global Iter: 406100 training acc: 0.46875
Global Iter: 406200 training loss: 0.711824
Global Iter: 406200 training acc: 0.375
Global Iter: 406300 training loss: 0.666543
Global Iter: 406300 training acc: 0.71875
Global Iter: 406400 training loss: 0.695267
Global Iter: 406400 training acc: 0.5
Global Iter: 406500 training loss: 0.694263
Global Iter: 406500 training acc: 0.5
Global Iter: 406600 training loss: 0.688364
Global Iter: 406600 training acc: 0.5625
Global Iter: 406700 training loss: 0.67201
Global Iter: 406700 training acc: 0.6875
Global Iter: 406800 training loss: 0.707804
Global Iter: 406800 training acc: 0.40625
Global Iter: 406900 training loss: 0.68645
Global Iter: 406900 training acc: 0.5625
Global Iter: 407000 training loss: 0.699139
Global Iter: 407000 training acc: 0.46875
Global Iter: 407100 training loss: 0.703032
Global Iter: 407100 training acc: 0.4375
Global Iter: 407200 training loss: 0.69113
Global Iter: 407200 training acc: 0.53125
Global Iter: 407300 training loss: 0.683991
Global Iter: 407300 training acc: 0.59375
Global Iter: 407400 training loss: 0.69058
Global Iter: 407400 training acc: 0.53125
Global Iter: 407500 training loss: 0.691137
Global Iter: 407500 training acc: 0.53125
Global Iter: 407600 training loss: 0.683493
Global Iter: 407600 training acc: 0.59375
Global Iter: 407700 training loss: 0.691469
Global Iter: 407700 training acc: 0.53125
Global Iter: 407800 training loss: 0.687084
Global Iter: 407800 training acc: 0.5625
Global Iter: 407900 training loss: 0.691235
Global Iter: 407900 training acc: 0.53125
Global Iter: 408000 training loss: 0.683494
Global Iter: 408000 training acc: 0.59375
Global Iter: 408100 training loss: 0.680434
Global Iter: 408100 training acc: 0.625
Global Iter: 408200 training loss: 0.678409
Global Iter: 408200 training acc: 0.625
Global Iter: 408300 training loss: 0.703212
Global Iter: 408300 training acc: 0.4375
Global Iter: 408400 training loss: 0.695585
Global Iter: 408400 training acc: 0.5
Global Iter: 408500 training loss: 0.696161
Global Iter: 408500 training acc: 0.5
Global Iter: 408600 training loss: 0.673444
Global Iter: 408600 training acc: 0.6875
Global Iter: 408700 training loss: 0.689952
Global Iter: 408700 training acc: 0.53125
Global Iter: 408800 training loss: 0.696663
Global Iter: 408800 training acc: 0.5
Global Iter: 408900 training loss: 0.686929
Global Iter: 408900 training acc: 0.5625
Global Iter: 409000 training loss: 0.710992
Global Iter: 409000 training acc: 0.375
Global Iter: 409100 training loss: 0.695888
Global Iter: 409100 training acc: 0.5
Global Iter: 409200 training loss: 0.675212
Global Iter: 409200 training acc: 0.65625
Global Iter: 409300 training loss: 0.686984
Global Iter: 409300 training acc: 0.5625
Global Iter: 409400 training loss: 0.699103
Global Iter: 409400 training acc: 0.46875
Global Iter: 409500 training loss: 0.695443
Global Iter: 409500 training acc: 0.5
Global Iter: 409600 training loss: 0.678581
Global Iter: 409600 training acc: 0.625
Global Iter: 409700 training loss: 0.687408
Global Iter: 409700 training acc: 0.5625
Global Iter: 409800 training loss: 0.695379
Global Iter: 409800 training acc: 0.5
Global Iter: 409900 training loss: 0.693967
Global Iter: 409900 training acc: 0.5
Global Iter: 410000 training loss: 0.712722
Global Iter: 410000 training acc: 0.375
Global Iter: 410100 training loss: 0.679272
Global Iter: 410100 training acc: 0.625
Global Iter: 410200 training loss: 0.669694
Global Iter: 410200 training acc: 0.65625
Global Iter: 410300 training loss: 0.707611
Global Iter: 410300 training acc: 0.40625
Global Iter: 410400 training loss: 0.672642
Global Iter: 410400 training acc: 0.65625
Global Iter: 410500 training loss: 0.690432
Global Iter: 410500 training acc: 0.53125
Global Iter: 410600 training loss: 0.692671
Global Iter: 410600 training acc: 0.53125
Global Iter: 410700 training loss: 0.703406
Global Iter: 410700 training acc: 0.4375
Global Iter: 410800 training loss: 0.686218
Global Iter: 410800 training acc: 0.5625
Global Iter: 410900 training loss: 0.688451
Global Iter: 410900 training acc: 0.5625
Global Iter: 411000 training loss: 0.691682
Global Iter: 411000 training acc: 0.5
Global Iter: 411100 training loss: 0.707701
Global Iter: 411100 training acc: 0.375
Global Iter: 411200 training loss: 0.696312
Global Iter: 411200 training acc: 0.5
Global Iter: 411300 training loss: 0.690323
Global Iter: 411300 training acc: 0.53125
Global Iter: 411400 training loss: 0.70319
Global Iter: 411400 training acc: 0.4375
Global Iter: 411500 training loss: 0.683449
Global Iter: 411500 training acc: 0.59375
Global Iter: 411600 training loss: 0.702249
Global Iter: 411600 training acc: 0.4375
Global Iter: 411700 training loss: 0.703708
Global Iter: 411700 training acc: 0.4375
Global Iter: 411800 training loss: 0.68234
Global Iter: 411800 training acc: 0.59375
Global Iter: 411900 training loss: 0.685562
Global Iter: 411900 training acc: 0.5625
Global Iter: 412000 training loss: 0.707989
Global Iter: 412000 training acc: 0.40625
Global Iter: 412100 training loss: 0.694955
Global Iter: 412100 training acc: 0.5
Global Iter: 412200 training loss: 0.668795
Global Iter: 412200 training acc: 0.71875
Global Iter: 412300 training loss: 0.695733
Global Iter: 412300 training acc: 0.5
Global Iter: 412400 training loss: 0.682331
Global Iter: 412400 training acc: 0.59375
Global Iter: 412500 training loss: 0.690264
Global Iter: 412500 training acc: 0.53125
Global Iter: 412600 training loss: 0.704955
Global Iter: 412600 training acc: 0.40625
Global Iter: 412700 training loss: 0.695593
Global Iter: 412700 training acc: 0.5
Global Iter: 412800 training loss: 0.683042
Global Iter: 412800 training acc: 0.59375
Global Iter: 412900 training loss: 0.702101
Global Iter: 412900 training acc: 0.4375
Global Iter: 413000 training loss: 0.667203
Global Iter: 413000 training acc: 0.71875
Global Iter: 413100 training loss: 0.691614
Global Iter: 413100 training acc: 0.53125
Global Iter: 413200 training loss: 0.707434
Global Iter: 413200 training acc: 0.40625
Global Iter: 413300 training loss: 0.680787
Global Iter: 413300 training acc: 0.625
Global Iter: 413400 training loss: 0.679549
Global Iter: 413400 training acc: 0.625
Global Iter: 413500 training loss: 0.684366
Global Iter: 413500 training acc: 0.59375
Global Iter: 413600 training loss: 0.694226
Global Iter: 413600 training acc: 0.5
Global Iter: 413700 training loss: 0.675608
Global Iter: 413700 training acc: 0.65625
Global Iter: 413800 training loss: 0.678684
Global Iter: 413800 training acc: 0.625
Global Iter: 413900 training loss: 0.698651
Global Iter: 413900 training acc: 0.46875
Global Iter: 414000 training loss: 0.715582
Global Iter: 414000 training acc: 0.34375
Global Iter: 414100 training loss: 0.695454
Global Iter: 414100 training acc: 0.5
Global Iter: 414200 training loss: 0.679387
Global Iter: 414200 training acc: 0.625
Global Iter: 414300 training loss: 0.698297
Global Iter: 414300 training acc: 0.46875
Global Iter: 414400 training loss: 0.695211
Global Iter: 414400 training acc: 0.5
Global Iter: 414500 training loss: 0.687116
Global Iter: 414500 training acc: 0.5625
Global Iter: 414600 training loss: 0.682022
Global Iter: 414600 training acc: 0.59375
Global Iter: 414700 training loss: 0.703285
Global Iter: 414700 training acc: 0.4375
Global Iter: 414800 training loss: 0.67515
Global Iter: 414800 training acc: 0.65625
Global Iter: 414900 training loss: 0.691301
Global Iter: 414900 training acc: 0.53125
Global Iter: 415000 training loss: 0.689377
Global Iter: 415000 training acc: 0.53125
Global Iter: 415100 training loss: 0.667773
Global Iter: 415100 training acc: 0.71875
Global Iter: 415200 training loss: 0.678886
Global Iter: 415200 training acc: 0.625
Global Iter: 415300 training loss: 0.68286
Global Iter: 415300 training acc: 0.59375
Global Iter: 415400 training loss: 0.691545
Global Iter: 415400 training acc: 0.53125
Global Iter: 415500 training loss: 0.693804
Global Iter: 415500 training acc: 0.5
Global Iter: 415600 training loss: 0.692837
Global Iter: 415600 training acc: 0.5
Global Iter: 415700 training loss: 0.703775
Global Iter: 415700 training acc: 0.4375
Global Iter: 415800 training loss: 0.703164
Global Iter: 415800 training acc: 0.4375
Global Iter: 415900 training loss: 0.67426
Global Iter: 415900 training acc: 0.65625
Global Iter: 416000 training loss: 0.700161
Global Iter: 416000 training acc: 0.46875
Global Iter: 416100 training loss: 0.714941
Global Iter: 416100 training acc: 0.34375
Global Iter: 416200 training loss: 0.6714
Global Iter: 416200 training acc: 0.6875
Global Iter: 416300 training loss: 0.699969
Global Iter: 416300 training acc: 0.46875
Global Iter: 416400 training loss: 0.686229
Global Iter: 416400 training acc: 0.5625
Global Iter: 416500 training loss: 0.687383
Global Iter: 416500 training acc: 0.5625
Global Iter: 416600 training loss: 0.671076
Global Iter: 416600 training acc: 0.6875
Global Iter: 416700 training loss: 0.70321
Global Iter: 416700 training acc: 0.4375
Global Iter: 416800 training loss: 0.687413
Global Iter: 416800 training acc: 0.5625
Global Iter: 416900 training loss: 0.701791
Global Iter: 416900 training acc: 0.4375
Global Iter: 417000 training loss: 0.707736
Global Iter: 417000 training acc: 0.40625
Global Iter: 417100 training loss: 0.694519
Global Iter: 417100 training acc: 0.5
Global Iter: 417200 training loss: 0.678921
Global Iter: 417200 training acc: 0.625
Global Iter: 417300 training loss: 0.692542
Global Iter: 417300 training acc: 0.53125
Global Iter: 417400 training loss: 0.686822
Global Iter: 417400 training acc: 0.5625
Global Iter: 417500 training loss: 0.679618
Global Iter: 417500 training acc: 0.625
Global Iter: 417600 training loss: 0.687524
Global Iter: 417600 training acc: 0.5625
Global Iter: 417700 training loss: 0.686852
Global Iter: 417700 training acc: 0.5625
Global Iter: 417800 training loss: 0.695358
Global Iter: 417800 training acc: 0.5
Global Iter: 417900 training loss: 0.679503
Global Iter: 417900 training acc: 0.625
Global Iter: 418000 training loss: 0.678823
Global Iter: 418000 training acc: 0.625
Global Iter: 418100 training loss: 0.675276
Global Iter: 418100 training acc: 0.65625
Global Iter: 418200 training loss: 0.711702
Global Iter: 418200 training acc: 0.375
Global Iter: 418300 training loss: 0.699044
Global Iter: 418300 training acc: 0.46875
Global Iter: 418400 training loss: 0.699129
Global Iter: 418400 training acc: 0.46875
Global Iter: 418500 training loss: 0.671287
Global Iter: 418500 training acc: 0.6875
Global Iter: 418600 training loss: 0.694675
Global Iter: 418600 training acc: 0.5
Global Iter: 418700 training loss: 0.695057
Global Iter: 418700 training acc: 0.5
Global Iter: 418800 training loss: 0.68765
Global Iter: 418800 training acc: 0.5625
Global Iter: 418900 training loss: 0.718469
Global Iter: 418900 training acc: 0.3125
Global Iter: 419000 training loss: 0.690866
Global Iter: 419000 training acc: 0.53125
Global Iter: 419100 training loss: 0.666963
Global Iter: 419100 training acc: 0.71875
Global Iter: 419200 training loss: 0.687957
Global Iter: 419200 training acc: 0.5625
Global Iter: 419300 training loss: 0.698263
Global Iter: 419300 training acc: 0.46875
Global Iter: 419400 training loss: 0.691432
Global Iter: 419400 training acc: 0.53125
Global Iter: 419500 training loss: 0.675546
Global Iter: 419500 training acc: 0.65625
Global Iter: 419600 training loss: 0.686648
Global Iter: 419600 training acc: 0.5625
Global Iter: 419700 training loss: 0.687868
Global Iter: 419700 training acc: 0.5625
Global Iter: 419800 training loss: 0.691808
Global Iter: 419800 training acc: 0.53125
Global Iter: 419900 training loss: 0.707473
Global Iter: 419900 training acc: 0.40625
Global Iter: 420000 training loss: 0.683325
Global Iter: 420000 training acc: 0.59375
Global Iter: 420100 training loss: 0.679231
Global Iter: 420100 training acc: 0.625
Global Iter: 420200 training loss: 0.699217
Global Iter: 420200 training acc: 0.46875
Global Iter: 420300 training loss: 0.680099
Global Iter: 420300 training acc: 0.625
Global Iter: 420400 training loss: 0.692385
Global Iter: 420400 training acc: 0.53125
Global Iter: 420500 training loss: 0.696227
Global Iter: 420500 training acc: 0.5
Global Iter: 420600 training loss: 0.69912
Global Iter: 420600 training acc: 0.46875
Global Iter: 420700 training loss: 0.690819
Global Iter: 420700 training acc: 0.53125
Global Iter: 420800 training loss: 0.679847
Global Iter: 420800 training acc: 0.625
Global Iter: 420900 training loss: 0.694725
Global Iter: 420900 training acc: 0.5
Global Iter: 421000 training loss: 0.702854
Global Iter: 421000 training acc: 0.4375
Global Iter: 421100 training loss: 0.699029
Global Iter: 421100 training acc: 0.46875
Global Iter: 421200 training loss: 0.694854
Global Iter: 421200 training acc: 0.5
Global Iter: 421300 training loss: 0.703768
Global Iter: 421300 training acc: 0.4375
Global Iter: 421400 training loss: 0.686962
Global Iter: 421400 training acc: 0.5625
Global Iter: 421500 training loss: 0.699613
Global Iter: 421500 training acc: 0.46875
Global Iter: 421600 training loss: 0.702678
Global Iter: 421600 training acc: 0.4375
Global Iter: 421700 training loss: 0.683425
Global Iter: 421700 training acc: 0.59375
Global Iter: 421800 training loss: 0.690946
Global Iter: 421800 training acc: 0.53125
Global Iter: 421900 training loss: 0.706279
Global Iter: 421900 training acc: 0.40625
Global Iter: 422000 training loss: 0.691593
Global Iter: 422000 training acc: 0.53125
Global Iter: 422100 training loss: 0.670776
Global Iter: 422100 training acc: 0.6875
Global Iter: 422200 training loss: 0.690952
Global Iter: 422200 training acc: 0.53125
Global Iter: 422300 training loss: 0.692755
Global Iter: 422300 training acc: 0.53125
Global Iter: 422400 training loss: 0.695478
Global Iter: 422400 training acc: 0.5
Global Iter: 422500 training loss: 0.707485
Global Iter: 422500 training acc: 0.40625
Global Iter: 422600 training loss: 0.695575
Global Iter: 422600 training acc: 0.5
Global Iter: 422700 training loss: 0.6861
Global Iter: 422700 training acc: 0.5625
Global Iter: 422800 training loss: 0.699001
Global Iter: 422800 training acc: 0.46875
Global Iter: 422900 training loss: 0.668589
Global Iter: 422900 training acc: 0.71875
Global Iter: 423000 training loss: 0.694516
Global Iter: 423000 training acc: 0.5
Global Iter: 423100 training loss: 0.698545
Global Iter: 423100 training acc: 0.46875
Global Iter: 423200 training loss: 0.675044
Global Iter: 423200 training acc: 0.65625
Global Iter: 423300 training loss: 0.672837
Global Iter: 423300 training acc: 0.65625
Global Iter: 423400 training loss: 0.691981
Global Iter: 423400 training acc: 0.53125
Global Iter: 423500 training loss: 0.693988
Global Iter: 423500 training acc: 0.5
Global Iter: 423600 training loss: 0.679786
Global Iter: 423600 training acc: 0.625
Global Iter: 423700 training loss: 0.680939
Global Iter: 423700 training acc: 0.625
Global Iter: 423800 training loss: 0.696528
Global Iter: 423800 training acc: 0.5
Global Iter: 423900 training loss: 0.714607
Global Iter: 423900 training acc: 0.34375
Global Iter: 424000 training loss: 0.696886
Global Iter: 424000 training acc: 0.5
Global Iter: 424100 training loss: 0.679107
Global Iter: 424100 training acc: 0.625
Global Iter: 424200 training loss: 0.699667
Global Iter: 424200 training acc: 0.46875
Global Iter: 424300 training loss: 0.693841
Global Iter: 424300 training acc: 0.5
Global Iter: 424400 training loss: 0.68641
Global Iter: 424400 training acc: 0.5625
Global Iter: 424500 training loss: 0.676832
Global Iter: 424500 training acc: 0.625
Global Iter: 424600 training loss: 0.704092
Global Iter: 424600 training acc: 0.4375
Global Iter: 424700 training loss: 0.678413
Global Iter: 424700 training acc: 0.625
Global Iter: 424800 training loss: 0.691923
Global Iter: 424800 training acc: 0.53125
Global Iter: 424900 training loss: 0.687723
Global Iter: 424900 training acc: 0.5625
Global Iter: 425000 training loss: 0.670452
Global Iter: 425000 training acc: 0.6875
Global Iter: 425100 training loss: 0.683038
Global Iter: 425100 training acc: 0.59375
Global Iter: 425200 training loss: 0.682716
Global Iter: 425200 training acc: 0.59375
Global Iter: 425300 training loss: 0.690139
Global Iter: 425300 training acc: 0.53125
Global Iter: 425400 training loss: 0.703515
Global Iter: 425400 training acc: 0.4375
Global Iter: 425500 training loss: 0.692559
Global Iter: 425500 training acc: 0.53125
Global Iter: 425600 training loss: 0.704036
Global Iter: 425600 training acc: 0.4375
Global Iter: 425700 training loss: 0.711321
Global Iter: 425700 training acc: 0.375
Global Iter: 425800 training loss: 0.675448
Global Iter: 425800 training acc: 0.65625
Global Iter: 425900 training loss: 0.699586
Global Iter: 425900 training acc: 0.46875
Global Iter: 426000 training loss: 0.712149
Global Iter: 426000 training acc: 0.375
Global Iter: 426100 training loss: 0.667072
Global Iter: 426100 training acc: 0.71875
Global Iter: 426200 training loss: 0.704691
Global Iter: 426200 training acc: 0.4375
Global Iter: 426300 training loss: 0.686884
Global Iter: 426300 training acc: 0.5625
Global Iter: 426400 training loss: 0.689092
Global Iter: 426400 training acc: 0.5625
Global Iter: 426500 training loss: 0.671967
Global Iter: 426500 training acc: 0.6875
Global Iter: 426600 training loss: 0.697317
Global Iter: 426600 training acc: 0.46875
Global Iter: 426700 training loss: 0.694462
Global Iter: 426700 training acc: 0.5
Global Iter: 426800 training loss: 0.702334
Global Iter: 426800 training acc: 0.4375
Global Iter: 426900 training loss: 0.705413
Global Iter: 426900 training acc: 0.40625
Global Iter: 427000 training loss: 0.695136
Global Iter: 427000 training acc: 0.5
Global Iter: 427100 training loss: 0.683695
Global Iter: 427100 training acc: 0.59375
Global Iter: 427200 training loss: 0.695359
Global Iter: 427200 training acc: 0.5
Global Iter: 427300 training loss: 0.695485
Global Iter: 427300 training acc: 0.5
Global Iter: 427400 training loss: 0.683486
Global Iter: 427400 training acc: 0.59375
Global Iter: 427500 training loss: 0.691531
Global Iter: 427500 training acc: 0.53125
Global Iter: 427600 training loss: 0.687887
Global Iter: 427600 training acc: 0.5625
Global Iter: 427700 training loss: 0.694373
Global Iter: 427700 training acc: 0.5
Global Iter: 427800 training loss: 0.671705
Global Iter: 427800 training acc: 0.6875
Global Iter: 427900 training loss: 0.674804
Global Iter: 427900 training acc: 0.65625
Global Iter: 428000 training loss: 0.675309
Global Iter: 428000 training acc: 0.65625
Global Iter: 428100 training loss: 0.714857
Global Iter: 428100 training acc: 0.34375
Global Iter: 428200 training loss: 0.691194
Global Iter: 428200 training acc: 0.53125
Global Iter: 428300 training loss: 0.699781
Global Iter: 428300 training acc: 0.46875
Global Iter: 428400 training loss: 0.679378
Global Iter: 428400 training acc: 0.625
Global Iter: 428500 training loss: 0.699616
Global Iter: 428500 training acc: 0.46875
Global Iter: 428600 training loss: 0.698342
Global Iter: 428600 training acc: 0.46875
Global Iter: 428700 training loss: 0.690669
Global Iter: 428700 training acc: 0.53125
Global Iter: 428800 training loss: 0.722558
Global Iter: 428800 training acc: 0.28125
Global Iter: 428900 training loss: 0.694993
Global Iter: 428900 training acc: 0.5
Global Iter: 429000 training loss: 0.667816
Global Iter: 429000 training acc: 0.71875
Global Iter: 429100 training loss: 0.679107
Global Iter: 429100 training acc: 0.625
Global Iter: 429200 training loss: 0.702416
Global Iter: 429200 training acc: 0.4375
Global Iter: 429300 training loss: 0.687326
Global Iter: 429300 training acc: 0.5625
Global Iter: 429400 training loss: 0.679476
Global Iter: 429400 training acc: 0.625
Global Iter: 429500 training loss: 0.690528
Global Iter: 429500 training acc: 0.53125
Global Iter: 429600 training loss: 0.691146
Global Iter: 429600 training acc: 0.53125
Global Iter: 429700 training loss: 0.695063
Global Iter: 429700 training acc: 0.5
Global Iter: 429800 training loss: 0.707178
Global Iter: 429800 training acc: 0.40625
Global Iter: 429900 training loss: 0.687586
Global Iter: 429900 training acc: 0.5625
Global Iter: 430000 training loss: 0.684029
Global Iter: 430000 training acc: 0.59375
Global Iter: 430100 training loss: 0.690828
Global Iter: 430100 training acc: 0.53125
Global Iter: 430200 training loss: 0.676166
Global Iter: 430200 training acc: 0.65625
Global Iter: 430300 training loss: 0.69588
Global Iter: 430300 training acc: 0.5
Global Iter: 430400 training loss: 0.691401
Global Iter: 430400 training acc: 0.53125
Global Iter: 430500 training loss: 0.690386
Global Iter: 430500 training acc: 0.53125
Global Iter: 430600 training loss: 0.694811
Global Iter: 430600 training acc: 0.5
Global Iter: 430700 training loss: 0.682839
Global Iter: 430700 training acc: 0.59375
Global Iter: 430800 training loss: 0.695093
Global Iter: 430800 training acc: 0.5
Global Iter: 430900 training loss: 0.710928
Global Iter: 430900 training acc: 0.375
Global Iter: 431000 training loss: 0.699381
Global Iter: 431000 training acc: 0.46875
Global Iter: 431100 training loss: 0.69542
Global Iter: 431100 training acc: 0.5
Global Iter: 431200 training loss: 0.703337
Global Iter: 431200 training acc: 0.4375
Global Iter: 431300 training loss: 0.682967
Global Iter: 431300 training acc: 0.59375
Global Iter: 431400 training loss: 0.699134
Global Iter: 431400 training acc: 0.46875
Global Iter: 431500 training loss: 0.699333
Global Iter: 431500 training acc: 0.46875
Global Iter: 431600 training loss: 0.683399
Global Iter: 431600 training acc: 0.59375
Global Iter: 431700 training loss: 0.695366
Global Iter: 431700 training acc: 0.5
Global Iter: 431800 training loss: 0.703348
Global Iter: 431800 training acc: 0.4375
Global Iter: 431900 training loss: 0.687647
Global Iter: 431900 training acc: 0.5625
Global Iter: 432000 training loss: 0.67206
Global Iter: 432000 training acc: 0.6875
Global Iter: 432100 training loss: 0.696984
Global Iter: 432100 training acc: 0.46875
Global Iter: 432200 training loss: 0.696305
Global Iter: 432200 training acc: 0.5
Global Iter: 432300 training loss: 0.694846
Global Iter: 432300 training acc: 0.5
Global Iter: 432400 training loss: 0.707246
Global Iter: 432400 training acc: 0.40625
Global Iter: 432500 training loss: 0.694116
Global Iter: 432500 training acc: 0.5
Global Iter: 432600 training loss: 0.683515
Global Iter: 432600 training acc: 0.59375
Global Iter: 432700 training loss: 0.70073
Global Iter: 432700 training acc: 0.46875
Global Iter: 432800 training loss: 0.670671
Global Iter: 432800 training acc: 0.6875
Global Iter: 432900 training loss: 0.694338
Global Iter: 432900 training acc: 0.5
Global Iter: 433000 training loss: 0.698845
Global Iter: 433000 training acc: 0.46875
Global Iter: 433100 training loss: 0.675963
Global Iter: 433100 training acc: 0.65625
Global Iter: 433200 training loss: 0.682899
Global Iter: 433200 training acc: 0.59375
Global Iter: 433300 training loss: 0.69127
Global Iter: 433300 training acc: 0.53125
Global Iter: 433400 training loss: 0.691151
Global Iter: 433400 training acc: 0.53125
Global Iter: 433500 training loss: 0.6712
Global Iter: 433500 training acc: 0.6875
Global Iter: 433600 training loss: 0.687417
Global Iter: 433600 training acc: 0.5625
Global Iter: 433700 training loss: 0.695508
Global Iter: 433700 training acc: 0.5
Global Iter: 433800 training loss: 0.715364
Global Iter: 433800 training acc: 0.34375
Global Iter: 433900 training loss: 0.691402
Global Iter: 433900 training acc: 0.53125
Global Iter: 434000 training loss: 0.679966
Global Iter: 434000 training acc: 0.625
Global Iter: 434100 training loss: 0.698797
Global Iter: 434100 training acc: 0.46875
Global Iter: 434200 training loss: 0.695089
Global Iter: 434200 training acc: 0.5
Global Iter: 434300 training loss: 0.69126
Global Iter: 434300 training acc: 0.53125
Global Iter: 434400 training loss: 0.678964
Global Iter: 434400 training acc: 0.625
Global Iter: 434500 training loss: 0.699312
Global Iter: 434500 training acc: 0.46875
Global Iter: 434600 training loss: 0.682847
Global Iter: 434600 training acc: 0.59375
Global Iter: 434700 training loss: 0.691207
Global Iter: 434700 training acc: 0.53125
Global Iter: 434800 training loss: 0.682988
Global Iter: 434800 training acc: 0.59375
Global Iter: 434900 training loss: 0.67035
Global Iter: 434900 training acc: 0.6875
Global Iter: 435000 training loss: 0.691561
Global Iter: 435000 training acc: 0.53125
Global Iter: 435100 training loss: 0.683982
Global Iter: 435100 training acc: 0.59375
Global Iter: 435200 training loss: 0.69528
Global Iter: 435200 training acc: 0.5
Global Iter: 435300 training loss: 0.703854
Global Iter: 435300 training acc: 0.4375
Global Iter: 435400 training loss: 0.69183
Global Iter: 435400 training acc: 0.53125
Global Iter: 435500 training loss: 0.698248
Global Iter: 435500 training acc: 0.46875
Global Iter: 435600 training loss: 0.711849
Global Iter: 435600 training acc: 0.375
Global Iter: 435700 training loss: 0.671124
Global Iter: 435700 training acc: 0.6875
Global Iter: 435800 training loss: 0.699379
Global Iter: 435800 training acc: 0.46875
Global Iter: 435900 training loss: 0.711527
Global Iter: 435900 training acc: 0.375
Global Iter: 436000 training loss: 0.671353
Global Iter: 436000 training acc: 0.6875
Global Iter: 436100 training loss: 0.695675
Global Iter: 436100 training acc: 0.5
Global Iter: 436200 training loss: 0.687611
Global Iter: 436200 training acc: 0.5625
Global Iter: 436300 training loss: 0.687088
Global Iter: 436300 training acc: 0.5625
Global Iter: 436400 training loss: 0.67091
Global Iter: 436400 training acc: 0.6875
Global Iter: 436500 training loss: 0.699414
Global Iter: 436500 training acc: 0.46875
Global Iter: 436600 training loss: 0.690988
Global Iter: 436600 training acc: 0.53125
Global Iter: 436700 training loss: 0.703049
Global Iter: 436700 training acc: 0.4375
Global Iter: 436800 training loss: 0.699212
Global Iter: 436800 training acc: 0.46875
Global Iter: 436900 training loss: 0.69128
Global Iter: 436900 training acc: 0.53125
Global Iter: 437000 training loss: 0.687222
Global Iter: 437000 training acc: 0.5625
Global Iter: 437100 training loss: 0.6954
Global Iter: 437100 training acc: 0.5
Global Iter: 437200 training loss: 0.7032
Global Iter: 437200 training acc: 0.4375
Global Iter: 437300 training loss: 0.678921
Global Iter: 437300 training acc: 0.625
Global Iter: 437400 training loss: 0.686146
Global Iter: 437400 training acc: 0.5625
Global Iter: 437500 training loss: 0.686132
Global Iter: 437500 training acc: 0.5625
Global Iter: 437600 training loss: 0.686686
Global Iter: 437600 training acc: 0.5625
Global Iter: 437700 training loss: 0.665146
Global Iter: 437700 training acc: 0.71875
Global Iter: 437800 training loss: 0.675159
Global Iter: 437800 training acc: 0.65625
Global Iter: 437900 training loss: 0.678904
Global Iter: 437900 training acc: 0.625
Global Iter: 438000 training loss: 0.710888
Global Iter: 438000 training acc: 0.375
Global Iter: 438100 training loss: 0.694901
Global Iter: 438100 training acc: 0.5
Global Iter: 438200 training loss: 0.69523
Global Iter: 438200 training acc: 0.5
Global Iter: 438300 training loss: 0.675727
Global Iter: 438300 training acc: 0.65625
Global Iter: 438400 training loss: 0.702937
Global Iter: 438400 training acc: 0.4375
Global Iter: 438500 training loss: 0.704447
Global Iter: 438500 training acc: 0.4375
Global Iter: 438600 training loss: 0.699386
Global Iter: 438600 training acc: 0.46875
Global Iter: 438700 training loss: 0.720224
Global Iter: 438700 training acc: 0.3125
Global Iter: 438800 training loss: 0.695249
Global Iter: 438800 training acc: 0.5
Global Iter: 438900 training loss: 0.670128
Global Iter: 438900 training acc: 0.6875
Global Iter: 439000 training loss: 0.68221
Global Iter: 439000 training acc: 0.59375
Global Iter: 439100 training loss: 0.695961
Global Iter: 439100 training acc: 0.5
Global Iter: 439200 training loss: 0.687595
Global Iter: 439200 training acc: 0.5625
Global Iter: 439300 training loss: 0.675561
Global Iter: 439300 training acc: 0.65625
Global Iter: 439400 training loss: 0.694805
Global Iter: 439400 training acc: 0.5
Global Iter: 439500 training loss: 0.696249
Global Iter: 439500 training acc: 0.5
Global Iter: 439600 training loss: 0.690946
Global Iter: 439600 training acc: 0.53125
Global Iter: 439700 training loss: 0.702587
Global Iter: 439700 training acc: 0.4375
Global Iter: 439800 training loss: 0.695854
Global Iter: 439800 training acc: 0.5
Global Iter: 439900 training loss: 0.678565
Global Iter: 439900 training acc: 0.625
Global Iter: 440000 training loss: 0.694898
Global Iter: 440000 training acc: 0.5
Global Iter: 440100 training loss: 0.67929
Global Iter: 440100 training acc: 0.625
Global Iter: 440200 training loss: 0.69516
Global Iter: 440200 training acc: 0.5
Global Iter: 440300 training loss: 0.695102
Global Iter: 440300 training acc: 0.5
Global Iter: 440400 training loss: 0.695256
Global Iter: 440400 training acc: 0.5
Global Iter: 440500 training loss: 0.705062
Global Iter: 440500 training acc: 0.4375
Global Iter: 440600 training loss: 0.690285
Global Iter: 440600 training acc: 0.53125
Global Iter: 440700 training loss: 0.689731
Global Iter: 440700 training acc: 0.53125
Global Iter: 440800 training loss: 0.706892
Global Iter: 440800 training acc: 0.40625
Global Iter: 440900 training loss: 0.707002
Global Iter: 440900 training acc: 0.40625
Global Iter: 441000 training loss: 0.702683
Global Iter: 441000 training acc: 0.4375
Global Iter: 441100 training loss: 0.703131
Global Iter: 441100 training acc: 0.4375
Global Iter: 441200 training loss: 0.680308
Global Iter: 441200 training acc: 0.625
Global Iter: 441300 training loss: 0.704062
Global Iter: 441300 training acc: 0.4375
Global Iter: 441400 training loss: 0.702624
Global Iter: 441400 training acc: 0.4375
Global Iter: 441500 training loss: 0.678901
Global Iter: 441500 training acc: 0.625
Global Iter: 441600 training loss: 0.703292
Global Iter: 441600 training acc: 0.4375
Global Iter: 441700 training loss: 0.703032
Global Iter: 441700 training acc: 0.4375
Global Iter: 441800 training loss: 0.697156
Global Iter: 441800 training acc: 0.5
Global Iter: 441900 training loss: 0.675427
Global Iter: 441900 training acc: 0.65625
Global Iter: 442000 training loss: 0.69441
Global Iter: 442000 training acc: 0.5
Global Iter: 442100 training loss: 0.695116
Global Iter: 442100 training acc: 0.5
Global Iter: 442200 training loss: 0.694956
Global Iter: 442200 training acc: 0.5
Global Iter: 442300 training loss: 0.703647
Global Iter: 442300 training acc: 0.4375
Global Iter: 442400 training loss: 0.698719
Global Iter: 442400 training acc: 0.46875
Global Iter: 442500 training loss: 0.683458
Global Iter: 442500 training acc: 0.59375
Global Iter: 442600 training loss: 0.69562
Global Iter: 442600 training acc: 0.5
Global Iter: 442700 training loss: 0.675076
Global Iter: 442700 training acc: 0.65625
Global Iter: 442800 training loss: 0.690529
Global Iter: 442800 training acc: 0.53125
Global Iter: 442900 training loss: 0.703222
Global Iter: 442900 training acc: 0.4375
Global Iter: 443000 training loss: 0.675454
Global Iter: 443000 training acc: 0.65625
Global Iter: 443100 training loss: 0.686923
Global Iter: 443100 training acc: 0.5625
Global Iter: 443200 training loss: 0.683202
Global Iter: 443200 training acc: 0.59375
Global Iter: 443300 training loss: 0.689046
Global Iter: 443300 training acc: 0.53125
Global Iter: 443400 training loss: 0.68092
Global Iter: 443400 training acc: 0.625
Global Iter: 443500 training loss: 0.684272
Global Iter: 443500 training acc: 0.59375
Global Iter: 443600 training loss: 0.695797
Global Iter: 443600 training acc: 0.5
Global Iter: 443700 training loss: 0.714849
Global Iter: 443700 training acc: 0.34375
Global Iter: 443800 training loss: 0.690718
Global Iter: 443800 training acc: 0.53125
Global Iter: 443900 training loss: 0.683199
Global Iter: 443900 training acc: 0.59375
Global Iter: 444000 training loss: 0.698739
Global Iter: 444000 training acc: 0.46875
Global Iter: 444100 training loss: 0.686765
Global Iter: 444100 training acc: 0.5625
Global Iter: 444200 training loss: 0.699371
Global Iter: 444200 training acc: 0.46875
Global Iter: 444300 training loss: 0.679323
Global Iter: 444300 training acc: 0.625
Global Iter: 444400 training loss: 0.699302
Global Iter: 444400 training acc: 0.46875
Global Iter: 444500 training loss: 0.678967
Global Iter: 444500 training acc: 0.625
Global Iter: 444600 training loss: 0.699079
Global Iter: 444600 training acc: 0.46875
Global Iter: 444700 training loss: 0.687014
Global Iter: 444700 training acc: 0.5625
Global Iter: 444800 training loss: 0.675767
Global Iter: 444800 training acc: 0.65625
Global Iter: 444900 training loss: 0.691023
Global Iter: 444900 training acc: 0.53125
Global Iter: 445000 training loss: 0.687317
Global Iter: 445000 training acc: 0.5625
Global Iter: 445100 training loss: 0.699107
Global Iter: 445100 training acc: 0.46875
Global Iter: 445200 training loss: 0.702789
Global Iter: 445200 training acc: 0.4375
Global Iter: 445300 training loss: 0.691163
Global Iter: 445300 training acc: 0.53125
Global Iter: 445400 training loss: 0.69559
Global Iter: 445400 training acc: 0.5
Global Iter: 445500 training loss: 0.702648
Global Iter: 445500 training acc: 0.4375
Global Iter: 445600 training loss: 0.671592
Global Iter: 445600 training acc: 0.6875
Global Iter: 445700 training loss: 0.703273
Global Iter: 445700 training acc: 0.4375
Global Iter: 445800 training loss: 0.711671
Global Iter: 445800 training acc: 0.375
Global Iter: 445900 training loss: 0.666951
Global Iter: 445900 training acc: 0.71875
Global Iter: 446000 training loss: 0.695524
Global Iter: 446000 training acc: 0.5
Global Iter: 446100 training loss: 0.686916
Global Iter: 446100 training acc: 0.5625
Global Iter: 446200 training loss: 0.684165
Global Iter: 446200 training acc: 0.59375
Global Iter: 446300 training loss: 0.674707
Global Iter: 446300 training acc: 0.65625
Global Iter: 446400 training loss: 0.702804
Global Iter: 446400 training acc: 0.4375
Global Iter: 446500 training loss: 0.691003
Global Iter: 446500 training acc: 0.53125
Global Iter: 446600 training loss: 0.701075
Global Iter: 446600 training acc: 0.46875
Global Iter: 446700 training loss: 0.701599
Global Iter: 446700 training acc: 0.46875
Global Iter: 446800 training loss: 0.687166
Global Iter: 446800 training acc: 0.5625
Global Iter: 446900 training loss: 0.686226
Global Iter: 446900 training acc: 0.5625
Global Iter: 447000 training loss: 0.688218
Global Iter: 447000 training acc: 0.5625
Global Iter: 447100 training loss: 0.697942
Global Iter: 447100 training acc: 0.46875
Global Iter: 447200 training loss: 0.679357
Global Iter: 447200 training acc: 0.625
Global Iter: 447300 training loss: 0.687031
Global Iter: 447300 training acc: 0.5625
Global Iter: 447400 training loss: 0.68361
Global Iter: 447400 training acc: 0.59375
Global Iter: 447500 training loss: 0.691448
Global Iter: 447500 training acc: 0.53125
Global Iter: 447600 training loss: 0.666323
Global Iter: 447600 training acc: 0.71875
Global Iter: 447700 training loss: 0.674088
Global Iter: 447700 training acc: 0.65625
Global Iter: 447800 training loss: 0.679193
Global Iter: 447800 training acc: 0.625
Global Iter: 447900 training loss: 0.708153
Global Iter: 447900 training acc: 0.40625
Global Iter: 448000 training loss: 0.695173
Global Iter: 448000 training acc: 0.5
Global Iter: 448100 training loss: 0.694275
Global Iter: 448100 training acc: 0.5
Global Iter: 448200 training loss: 0.674172
Global Iter: 448200 training acc: 0.65625
Global Iter: 448300 training loss: 0.696296
Global Iter: 448300 training acc: 0.5
Global Iter: 448400 training loss: 0.701297
Global Iter: 448400 training acc: 0.46875
Global Iter: 448500 training loss: 0.691608
Global Iter: 448500 training acc: 0.53125
Global Iter: 448600 training loss: 0.718331
Global Iter: 448600 training acc: 0.3125
Global Iter: 448700 training loss: 0.702772
Global Iter: 448700 training acc: 0.4375
Global Iter: 448800 training loss: 0.660875
Global Iter: 448800 training acc: 0.75
Global Iter: 448900 training loss: 0.687684
Global Iter: 448900 training acc: 0.53125
Global Iter: 449000 training loss: 0.685444
Global Iter: 449000 training acc: 0.5625
Global Iter: 449100 training loss: 0.691924
Global Iter: 449100 training acc: 0.53125
Global Iter: 449200 training loss: 0.675775
Global Iter: 449200 training acc: 0.65625
Global Iter: 449300 training loss: 0.698788
Global Iter: 449300 training acc: 0.46875
Global Iter: 449400 training loss: 0.690205
Global Iter: 449400 training acc: 0.53125
Global Iter: 449500 training loss: 0.695196
Global Iter: 449500 training acc: 0.5
Global Iter: 449600 training loss: 0.702257
Global Iter: 449600 training acc: 0.4375
Global Iter: 449700 training loss: 0.697601
Global Iter: 449700 training acc: 0.5
Global Iter: 449800 training loss: 0.683274
Global Iter: 449800 training acc: 0.59375
Global Iter: 449900 training loss: 0.690052
Global Iter: 449900 training acc: 0.53125
Global Iter: 450000 training loss: 0.688589
Global Iter: 450000 training acc: 0.5625
Global Iter: 450100 training loss: 0.696439
Global Iter: 450100 training acc: 0.5
Global Iter: 450200 training loss: 0.695292
Global Iter: 450200 training acc: 0.5
Global Iter: 450300 training loss: 0.696196
Global Iter: 450300 training acc: 0.5
Global Iter: 450400 training loss: 0.698736
Global Iter: 450400 training acc: 0.46875
Global Iter: 450500 training loss: 0.687831
Global Iter: 450500 training acc: 0.5625
Global Iter: 450600 training loss: 0.691196
Global Iter: 450600 training acc: 0.53125
Global Iter: 450700 training loss: 0.710019
Global Iter: 450700 training acc: 0.40625
Global Iter: 450800 training loss: 0.709997
Global Iter: 450800 training acc: 0.375
Global Iter: 450900 training loss: 0.701462
Global Iter: 450900 training acc: 0.4375
Global Iter: 451000 training loss: 0.701818
Global Iter: 451000 training acc: 0.46875
Global Iter: 451100 training loss: 0.677476
Global Iter: 451100 training acc: 0.65625
Global Iter: 451200 training loss: 0.693843
Global Iter: 451200 training acc: 0.5
Global Iter: 451300 training loss: 0.702152
Global Iter: 451300 training acc: 0.46875
Global Iter: 451400 training loss: 0.687991
Global Iter: 451400 training acc: 0.5625
Global Iter: 451500 training loss: 0.707313
Global Iter: 451500 training acc: 0.40625
Global Iter: 451600 training loss: 0.698846
Global Iter: 451600 training acc: 0.46875
Global Iter: 451700 training loss: 0.69076
Global Iter: 451700 training acc: 0.53125
Global Iter: 451800 training loss: 0.675427
Global Iter: 451800 training acc: 0.65625
Global Iter: 451900 training loss: 0.696946
Global Iter: 451900 training acc: 0.5
Global Iter: 452000 training loss: 0.690581
Global Iter: 452000 training acc: 0.53125
Global Iter: 452100 training loss: 0.69177
Global Iter: 452100 training acc: 0.53125
Global Iter: 452200 training loss: 0.699659
Global Iter: 452200 training acc: 0.46875
Global Iter: 452300 training loss: 0.694404
Global Iter: 452300 training acc: 0.5
Global Iter: 452400 training loss: 0.688676
Global Iter: 452400 training acc: 0.5625
Global Iter: 452500 training loss: 0.694597
Global Iter: 452500 training acc: 0.5
Global Iter: 452600 training loss: 0.678975
Global Iter: 452600 training acc: 0.625
Global Iter: 452700 training loss: 0.69574
Global Iter: 452700 training acc: 0.5
Global Iter: 452800 training loss: 0.701903
Global Iter: 452800 training acc: 0.4375
Global Iter: 452900 training loss: 0.675974
Global Iter: 452900 training acc: 0.65625
Global Iter: 453000 training loss: 0.690517
Global Iter: 453000 training acc: 0.53125
Global Iter: 453100 training loss: 0.691609
Global Iter: 453100 training acc: 0.53125
Global Iter: 453200 training loss: 0.695494
Global Iter: 453200 training acc: 0.5
Global Iter: 453300 training loss: 0.681209
Global Iter: 453300 training acc: 0.625
Global Iter: 453400 training loss: 0.684563
Global Iter: 453400 training acc: 0.59375
Global Iter: 453500 training loss: 0.697608
Global Iter: 453500 training acc: 0.46875
Global Iter: 453600 training loss: 0.71854
Global Iter: 453600 training acc: 0.3125
Global Iter: 453700 training loss: 0.689826
Global Iter: 453700 training acc: 0.53125
Global Iter: 453800 training loss: 0.689919
Global Iter: 453800 training acc: 0.53125
Global Iter: 453900 training loss: 0.708493
Global Iter: 453900 training acc: 0.40625
Global Iter: 454000 training loss: 0.691541
Global Iter: 454000 training acc: 0.53125
Global Iter: 454100 training loss: 0.700889
Global Iter: 454100 training acc: 0.46875
Global Iter: 454200 training loss: 0.682879
Global Iter: 454200 training acc: 0.59375
Global Iter: 454300 training loss: 0.70041
Global Iter: 454300 training acc: 0.46875
Global Iter: 454400 training loss: 0.683196
Global Iter: 454400 training acc: 0.59375
Global Iter: 454500 training loss: 0.706101
Global Iter: 454500 training acc: 0.4375
Global Iter: 454600 training loss: 0.686747
Global Iter: 454600 training acc: 0.5625
Global Iter: 454700 training loss: 0.671001
Global Iter: 454700 training acc: 0.6875
Global Iter: 454800 training loss: 0.691309
Global Iter: 454800 training acc: 0.53125
Global Iter: 454900 training loss: 0.690628
Global Iter: 454900 training acc: 0.53125
Global Iter: 455000 training loss: 0.693016
Global Iter: 455000 training acc: 0.5
Global Iter: 455100 training loss: 0.702743
Global Iter: 455100 training acc: 0.4375
Global Iter: 455200 training loss: 0.699687
Global Iter: 455200 training acc: 0.46875
Global Iter: 455300 training loss: 0.69307
Global Iter: 455300 training acc: 0.53125
Global Iter: 455400 training loss: 0.695276
Global Iter: 455400 training acc: 0.5
Global Iter: 455500 training loss: 0.67468
Global Iter: 455500 training acc: 0.65625
Global Iter: 455600 training loss: 0.706444
Global Iter: 455600 training acc: 0.40625
Global Iter: 455700 training loss: 0.711077
Global Iter: 455700 training acc: 0.375
Global Iter: 455800 training loss: 0.672865
Global Iter: 455800 training acc: 0.6875
Global Iter: 455900 training loss: 0.696985
Global Iter: 455900 training acc: 0.5
Global Iter: 456000 training loss: 0.687765
Global Iter: 456000 training acc: 0.5625
Global Iter: 456100 training loss: 0.687906
Global Iter: 456100 training acc: 0.5625
Global Iter: 456200 training loss: 0.67461
Global Iter: 456200 training acc: 0.65625
Global Iter: 456300 training loss: 0.703344
Global Iter: 456300 training acc: 0.4375
Global Iter: 456400 training loss: 0.69475
Global Iter: 456400 training acc: 0.5
Global Iter: 456500 training loss: 0.699143
Global Iter: 456500 training acc: 0.46875
Global Iter: 456600 training loss: 0.700681
Global Iter: 456600 training acc: 0.46875
Global Iter: 456700 training loss: 0.691108
Global Iter: 456700 training acc: 0.53125
Global Iter: 456800 training loss: 0.695902
Global Iter: 456800 training acc: 0.5
Global Iter: 456900 training loss: 0.683775
Global Iter: 456900 training acc: 0.59375
Global Iter: 457000 training loss: 0.694508
Global Iter: 457000 training acc: 0.5
Global Iter: 457100 training loss: 0.675335
Global Iter: 457100 training acc: 0.65625
Global Iter: 457200 training loss: 0.689699
Global Iter: 457200 training acc: 0.53125
Global Iter: 457300 training loss: 0.678905
Global Iter: 457300 training acc: 0.625
Global Iter: 457400 training loss: 0.692092
Global Iter: 457400 training acc: 0.53125
Global Iter: 457500 training loss: 0.674374
Global Iter: 457500 training acc: 0.65625
Global Iter: 457600 training loss: 0.678471
Global Iter: 457600 training acc: 0.625
Global Iter: 457700 training loss: 0.676472
Global Iter: 457700 training acc: 0.65625
Global Iter: 457800 training loss: 0.702267
Global Iter: 457800 training acc: 0.4375
Global Iter: 457900 training loss: 0.690273
Global Iter: 457900 training acc: 0.53125
Global Iter: 458000 training loss: 0.68702
Global Iter: 458000 training acc: 0.5625
Global Iter: 458100 training loss: 0.674254
Global Iter: 458100 training acc: 0.65625
Global Iter: 458200 training loss: 0.692466
Global Iter: 458200 training acc: 0.53125
Global Iter: 458300 training loss: 0.699582
Global Iter: 458300 training acc: 0.46875
Global Iter: 458400 training loss: 0.69623
Global Iter: 458400 training acc: 0.5
Global Iter: 458500 training loss: 0.720892
Global Iter: 458500 training acc: 0.3125
Global Iter: 458600 training loss: 0.702503
Global Iter: 458600 training acc: 0.4375
Global Iter: 458700 training loss: 0.663818
Global Iter: 458700 training acc: 0.75
Global Iter: 458800 training loss: 0.693146
Global Iter: 458800 training acc: 0.5
Global Iter: 458900 training loss: 0.691545
Global Iter: 458900 training acc: 0.53125
Global Iter: 459000 training loss: 0.692963
Global Iter: 459000 training acc: 0.53125
Global Iter: 459100 training loss: 0.669
Global Iter: 459100 training acc: 0.6875
Global Iter: 459200 training loss: 0.705092
Global Iter: 459200 training acc: 0.4375
Global Iter: 459300 training loss: 0.686111
Global Iter: 459300 training acc: 0.5625
Global Iter: 459400 training loss: 0.699333
Global Iter: 459400 training acc: 0.5
Global Iter: 459500 training loss: 0.69967
Global Iter: 459500 training acc: 0.46875
Global Iter: 459600 training loss: 0.697507
Global Iter: 459600 training acc: 0.5
Global Iter: 459700 training loss: 0.691545
Global Iter: 459700 training acc: 0.53125
Global Iter: 459800 training loss: 0.692757
Global Iter: 459800 training acc: 0.53125
Global Iter: 459900 training loss: 0.683317
Global Iter: 459900 training acc: 0.59375
Global Iter: 460000 training loss: 0.691463
Global Iter: 460000 training acc: 0.53125
Global Iter: 460100 training loss: 0.695058
Global Iter: 460100 training acc: 0.5
Global Iter: 460200 training loss: 0.695294
Global Iter: 460200 training acc: 0.5
Global Iter: 460300 training loss: 0.695656
Global Iter: 460300 training acc: 0.5
Global Iter: 460400 training loss: 0.687178
Global Iter: 460400 training acc: 0.5625
Global Iter: 460500 training loss: 0.691123
Global Iter: 460500 training acc: 0.53125
Global Iter: 460600 training loss: 0.703757
Global Iter: 460600 training acc: 0.4375
Global Iter: 460700 training loss: 0.706182
Global Iter: 460700 training acc: 0.40625
Global Iter: 460800 training loss: 0.696524
Global Iter: 460800 training acc: 0.5
Global Iter: 460900 training loss: 0.696183
Global Iter: 460900 training acc: 0.5
Global Iter: 461000 training loss: 0.66867
Global Iter: 461000 training acc: 0.6875
Global Iter: 461100 training loss: 0.697122
Global Iter: 461100 training acc: 0.5
Global Iter: 461200 training loss: 0.699926
Global Iter: 461200 training acc: 0.46875
Global Iter: 461300 training loss: 0.684662
Global Iter: 461300 training acc: 0.59375
Global Iter: 461400 training loss: 0.71699
Global Iter: 461400 training acc: 0.34375
Global Iter: 461500 training loss: 0.692778
Global Iter: 461500 training acc: 0.5
Global Iter: 461600 training loss: 0.682179
Global Iter: 461600 training acc: 0.59375
Global Iter: 461700 training loss: 0.670527
Global Iter: 461700 training acc: 0.6875
Global Iter: 461800 training loss: 0.703654
Global Iter: 461800 training acc: 0.4375
Global Iter: 461900 training loss: 0.69263
Global Iter: 461900 training acc: 0.5
Global Iter: 462000 training loss: 0.68653
Global Iter: 462000 training acc: 0.5625
Global Iter: 462100 training loss: 0.69944
Global Iter: 462100 training acc: 0.46875
Global Iter: 462200 training loss: 0.696517
Global Iter: 462200 training acc: 0.5
Global Iter: 462300 training loss: 0.682483
Global Iter: 462300 training acc: 0.59375
Global Iter: 462400 training loss: 0.692848
Global Iter: 462400 training acc: 0.5
Global Iter: 462500 training loss: 0.675513
Global Iter: 462500 training acc: 0.65625
Global Iter: 462600 training loss: 0.694062
Global Iter: 462600 training acc: 0.5
Global Iter: 462700 training loss: 0.703725
Global Iter: 462700 training acc: 0.4375
Global Iter: 462800 training loss: 0.674641
Global Iter: 462800 training acc: 0.65625
Global Iter: 462900 training loss: 0.689239
Global Iter: 462900 training acc: 0.53125
Global Iter: 463000 training loss: 0.692862
Global Iter: 463000 training acc: 0.53125
Global Iter: 463100 training loss: 0.694071
Global Iter: 463100 training acc: 0.5
Global Iter: 463200 training loss: 0.680645
Global Iter: 463200 training acc: 0.625
Global Iter: 463300 training loss: 0.686912
Global Iter: 463300 training acc: 0.59375
Global Iter: 463400 training loss: 0.69854
Global Iter: 463400 training acc: 0.46875
Global Iter: 463500 training loss: 0.717913
Global Iter: 463500 training acc: 0.3125
Global Iter: 463600 training loss: 0.687108
Global Iter: 463600 training acc: 0.5625
Global Iter: 463700 training loss: 0.691229
Global Iter: 463700 training acc: 0.53125
Global Iter: 463800 training loss: 0.702501
Global Iter: 463800 training acc: 0.4375
Global Iter: 463900 training loss: 0.681547
Global Iter: 463900 training acc: 0.59375
Global Iter: 464000 training loss: 0.696423
Global Iter: 464000 training acc: 0.5
Global Iter: 464100 training loss: 0.686756
Global Iter: 464100 training acc: 0.5625
Global Iter: 464200 training loss: 0.690432
Global Iter: 464200 training acc: 0.53125
Global Iter: 464300 training loss: 0.67949
Global Iter: 464300 training acc: 0.625
Global Iter: 464400 training loss: 0.703694
Global Iter: 464400 training acc: 0.4375
Global Iter: 464500 training loss: 0.691415
Global Iter: 464500 training acc: 0.53125
Global Iter: 464600 training loss: 0.671733
Global Iter: 464600 training acc: 0.6875
Global Iter: 464700 training loss: 0.693689
Global Iter: 464700 training acc: 0.5
Global Iter: 464800 training loss: 0.696566
Global Iter: 464800 training acc: 0.5
Global Iter: 464900 training loss: 0.691146
Global Iter: 464900 training acc: 0.53125
Global Iter: 465000 training loss: 0.70843
Global Iter: 465000 training acc: 0.40625
Global Iter: 465100 training loss: 0.704129
Global Iter: 465100 training acc: 0.4375
Global Iter: 465200 training loss: 0.691905
Global Iter: 465200 training acc: 0.53125
Global Iter: 465300 training loss: 0.704045
Global Iter: 465300 training acc: 0.4375
Global Iter: 465400 training loss: 0.670066
Global Iter: 465400 training acc: 0.6875
Global Iter: 465500 training loss: 0.706656
Global Iter: 465500 training acc: 0.40625
Global Iter: 465600 training loss: 0.715271
Global Iter: 465600 training acc: 0.34375
Global Iter: 465700 training loss: 0.671821
Global Iter: 465700 training acc: 0.6875
Global Iter: 465800 training loss: 0.690804
Global Iter: 465800 training acc: 0.53125
Global Iter: 465900 training loss: 0.683399
Global Iter: 465900 training acc: 0.59375
Global Iter: 466000 training loss: 0.684259
Global Iter: 466000 training acc: 0.5625
Global Iter: 466100 training loss: 0.670439
Global Iter: 466100 training acc: 0.6875
Global Iter: 466200 training loss: 0.704128
Global Iter: 466200 training acc: 0.4375
Global Iter: 466300 training loss: 0.692133
Global Iter: 466300 training acc: 0.53125
Global Iter: 466400 training loss: 0.706251
Global Iter: 466400 training acc: 0.40625
Global Iter: 466500 training loss: 0.698327
Global Iter: 466500 training acc: 0.46875
Global Iter: 466600 training loss: 0.687258
Global Iter: 466600 training acc: 0.5625
Global Iter: 466700 training loss: 0.702736
Global Iter: 466700 training acc: 0.4375
Global Iter: 466800 training loss: 0.681357
Global Iter: 466800 training acc: 0.625
Global Iter: 466900 training loss: 0.695412
Global Iter: 466900 training acc: 0.5
Global Iter: 467000 training loss: 0.675527
Global Iter: 467000 training acc: 0.65625
Global Iter: 467100 training loss: 0.695444
Global Iter: 467100 training acc: 0.5
Global Iter: 467200 training loss: 0.68058
Global Iter: 467200 training acc: 0.625
Global Iter: 467300 training loss: 0.692471
Global Iter: 467300 training acc: 0.53125
Global Iter: 467400 training loss: 0.673643
Global Iter: 467400 training acc: 0.65625
Global Iter: 467500 training loss: 0.670947
Global Iter: 467500 training acc: 0.6875
Global Iter: 467600 training loss: 0.683225
Global Iter: 467600 training acc: 0.625
Global Iter: 467700 training loss: 0.694953
Global Iter: 467700 training acc: 0.5
Global Iter: 467800 training loss: 0.695658
Global Iter: 467800 training acc: 0.5
Global Iter: 467900 training loss: 0.688283
Global Iter: 467900 training acc: 0.5625
Global Iter: 468000 training loss: 0.681587
Global Iter: 468000 training acc: 0.59375
Global Iter: 468100 training loss: 0.689592
Global Iter: 468100 training acc: 0.5625
Global Iter: 468200 training loss: 0.694691
Global Iter: 468200 training acc: 0.5
Global Iter: 468300 training loss: 0.691302
Global Iter: 468300 training acc: 0.53125
Global Iter: 468400 training loss: 0.713514
Global Iter: 468400 training acc: 0.34375
Global Iter: 468500 training loss: 0.70706
Global Iter: 468500 training acc: 0.40625
Global Iter: 468600 training loss: 0.669023
Global Iter: 468600 training acc: 0.71875
Global Iter: 468700 training loss: 0.698314
Global Iter: 468700 training acc: 0.46875
Global Iter: 468800 training loss: 0.690751
Global Iter: 468800 training acc: 0.53125
Global Iter: 468900 training loss: 0.690071
Global Iter: 468900 training acc: 0.53125
Global Iter: 469000 training loss: 0.668689
Global Iter: 469000 training acc: 0.71875
Global Iter: 469100 training loss: 0.699655
Global Iter: 469100 training acc: 0.46875
Global Iter: 469200 training loss: 0.690351
Global Iter: 469200 training acc: 0.53125
Global Iter: 469300 training loss: 0.694231
Global Iter: 469300 training acc: 0.5
Global Iter: 469400 training loss: 0.702787
Global Iter: 469400 training acc: 0.4375
Global Iter: 469500 training loss: 0.693011
Global Iter: 469500 training acc: 0.5
Global Iter: 469600 training loss: 0.690997
Global Iter: 469600 training acc: 0.53125
Global Iter: 469700 training loss: 0.687477
Global Iter: 469700 training acc: 0.5625
Global Iter: 469800 training loss: 0.693402
Global Iter: 469800 training acc: 0.53125
Global Iter: 469900 training loss: 0.69964
Global Iter: 469900 training acc: 0.46875
Global Iter: 470000 training loss: 0.693652
Global Iter: 470000 training acc: 0.53125
Global Iter: 470100 training loss: 0.689804
Global Iter: 470100 training acc: 0.53125
Global Iter: 470200 training loss: 0.695672
Global Iter: 470200 training acc: 0.5
Global Iter: 470300 training loss: 0.686665
Global Iter: 470300 training acc: 0.5625
Global Iter: 470400 training loss: 0.687266
Global Iter: 470400 training acc: 0.5625
Global Iter: 470500 training loss: 0.699765
Global Iter: 470500 training acc: 0.46875
Global Iter: 470600 training loss: 0.709224
Global Iter: 470600 training acc: 0.375
Global Iter: 470700 training loss: 0.698893
Global Iter: 470700 training acc: 0.46875
Global Iter: 470800 training loss: 0.695083
Global Iter: 470800 training acc: 0.5
Global Iter: 470900 training loss: 0.676317
Global Iter: 470900 training acc: 0.65625
Global Iter: 471000 training loss: 0.691946
Global Iter: 471000 training acc: 0.53125
Global Iter: 471100 training loss: 0.695304
Global Iter: 471100 training acc: 0.5
Global Iter: 471200 training loss: 0.679829
Global Iter: 471200 training acc: 0.625
Global Iter: 471300 training loss: 0.718191
Global Iter: 471300 training acc: 0.3125
Global Iter: 471400 training loss: 0.697829
Global Iter: 471400 training acc: 0.46875
Global Iter: 471500 training loss: 0.683588
Global Iter: 471500 training acc: 0.59375
Global Iter: 471600 training loss: 0.669595
Global Iter: 471600 training acc: 0.6875
Global Iter: 471700 training loss: 0.697574
Global Iter: 471700 training acc: 0.46875
Global Iter: 471800 training loss: 0.698594
Global Iter: 471800 training acc: 0.46875
Global Iter: 471900 training loss: 0.685347
Global Iter: 471900 training acc: 0.5625
Global Iter: 472000 training loss: 0.69431
Global Iter: 472000 training acc: 0.5
Global Iter: 472100 training loss: 0.691572
Global Iter: 472100 training acc: 0.53125
Global Iter: 472200 training loss: 0.682417
Global Iter: 472200 training acc: 0.625
Global Iter: 472300 training loss: 0.693708
Global Iter: 472300 training acc: 0.5
Global Iter: 472400 training loss: 0.675879
Global Iter: 472400 training acc: 0.65625
Global Iter: 472500 training loss: 0.69204
Global Iter: 472500 training acc: 0.53125
Global Iter: 472600 training loss: 0.703407
Global Iter: 472600 training acc: 0.4375
Global Iter: 472700 training loss: 0.673421
Global Iter: 472700 training acc: 0.6875
Global Iter: 472800 training loss: 0.691112
Global Iter: 472800 training acc: 0.5625
Global Iter: 472900 training loss: 0.696818
Global Iter: 472900 training acc: 0.5
Global Iter: 473000 training loss: 0.698175
Global Iter: 473000 training acc: 0.5
Global Iter: 473100 training loss: 0.687029
Global Iter: 473100 training acc: 0.59375
Global Iter: 473200 training loss: 0.692659
Global Iter: 473200 training acc: 0.53125
Global Iter: 473300 training loss: 0.697855
Global Iter: 473300 training acc: 0.46875
Global Iter: 473400 training loss: 0.713375
Global Iter: 473400 training acc: 0.34375
Global Iter: 473500 training loss: 0.689205
Global Iter: 473500 training acc: 0.5625
Global Iter: 473600 training loss: 0.697332
Global Iter: 473600 training acc: 0.5
Global Iter: 473700 training loss: 0.704266
Global Iter: 473700 training acc: 0.40625
Global Iter: 473800 training loss: 0.677647
Global Iter: 473800 training acc: 0.625
Global Iter: 473900 training loss: 0.694508
Global Iter: 473900 training acc: 0.5
Global Iter: 474000 training loss: 0.694655
Global Iter: 474000 training acc: 0.53125
Global Iter: 474100 training loss: 0.689813
Global Iter: 474100 training acc: 0.53125
Global Iter: 474200 training loss: 0.674411
Global Iter: 474200 training acc: 0.65625
Global Iter: 474300 training loss: 0.700006
Global Iter: 474300 training acc: 0.46875
Global Iter: 474400 training loss: 0.689881
Global Iter: 474400 training acc: 0.53125
Global Iter: 474500 training loss: 0.676149
Global Iter: 474500 training acc: 0.65625
Global Iter: 474600 training loss: 0.69673
Global Iter: 474600 training acc: 0.46875
Global Iter: 474700 training loss: 0.694456
Global Iter: 474700 training acc: 0.5
Global Iter: 474800 training loss: 0.686318
Global Iter: 474800 training acc: 0.59375
Global Iter: 474900 training loss: 0.698485
Global Iter: 474900 training acc: 0.46875
Global Iter: 475000 training loss: 0.706502
Global Iter: 475000 training acc: 0.40625
Global Iter: 475100 training loss: 0.687608
Global Iter: 475100 training acc: 0.5625
Global Iter: 475200 training loss: 0.706695
Global Iter: 475200 training acc: 0.40625
Global Iter: 475300 training loss: 0.675859
Global Iter: 475300 training acc: 0.65625
Global Iter: 475400 training loss: 0.701146
Global Iter: 475400 training acc: 0.46875
Global Iter: 475500 training loss: 0.722492
Global Iter: 475500 training acc: 0.28125
Global Iter: 475600 training loss: 0.677226
Global Iter: 475600 training acc: 0.625
Global Iter: 475700 training loss: 0.689789
Global Iter: 475700 training acc: 0.53125
Global Iter: 475800 training loss: 0.683198
Global Iter: 475800 training acc: 0.59375
Global Iter: 475900 training loss: 0.691704
Global Iter: 475900 training acc: 0.53125
Global Iter: 476000 training loss: 0.670206
Global Iter: 476000 training acc: 0.6875
Global Iter: 476100 training loss: 0.700198
Global Iter: 476100 training acc: 0.46875
Global Iter: 476200 training loss: 0.692295
Global Iter: 476200 training acc: 0.53125
Global Iter: 476300 training loss: 0.706221
Global Iter: 476300 training acc: 0.4375
Global Iter: 476400 training loss: 0.697549
Global Iter: 476400 training acc: 0.46875
Global Iter: 476500 training loss: 0.685844
Global Iter: 476500 training acc: 0.5625
Global Iter: 476600 training loss: 0.702383
Global Iter: 476600 training acc: 0.4375
Global Iter: 476700 training loss: 0.685766
Global Iter: 476700 training acc: 0.5625
Global Iter: 476800 training loss: 0.689333
Global Iter: 476800 training acc: 0.53125
Global Iter: 476900 training loss: 0.675088
Global Iter: 476900 training acc: 0.65625
Global Iter: 477000 training loss: 0.697417
Global Iter: 477000 training acc: 0.5
Global Iter: 477100 training loss: 0.67757
Global Iter: 477100 training acc: 0.625
Global Iter: 477200 training loss: 0.687783
Global Iter: 477200 training acc: 0.5625
Global Iter: 477300 training loss: 0.680394
Global Iter: 477300 training acc: 0.59375
Global Iter: 477400 training loss: 0.671197
Global Iter: 477400 training acc: 0.6875
Global Iter: 477500 training loss: 0.677435
Global Iter: 477500 training acc: 0.625
Global Iter: 477600 training loss: 0.687675
Global Iter: 477600 training acc: 0.5625
Global Iter: 477700 training loss: 0.692046
Global Iter: 477700 training acc: 0.53125
Global Iter: 477800 training loss: 0.680484
Global Iter: 477800 training acc: 0.625
Global Iter: 477900 training loss: 0.683477
Global Iter: 477900 training acc: 0.59375
Global Iter: 478000 training loss: 0.6913
Global Iter: 478000 training acc: 0.53125
Global Iter: 478100 training loss: 0.699178
Global Iter: 478100 training acc: 0.46875
Global Iter: 478200 training loss: 0.682285
Global Iter: 478200 training acc: 0.59375
Global Iter: 478300 training loss: 0.714845
Global Iter: 478300 training acc: 0.375
Global Iter: 478400 training loss: 0.708826
Global Iter: 478400 training acc: 0.40625
Global Iter: 478500 training loss: 0.671671
Global Iter: 478500 training acc: 0.6875
Global Iter: 478600 training loss: 0.696158
Global Iter: 478600 training acc: 0.5
Global Iter: 478700 training loss: 0.698855
Global Iter: 478700 training acc: 0.5
Global Iter: 478800 training loss: 0.686572
Global Iter: 478800 training acc: 0.5625
Global Iter: 478900 training loss: 0.668441
Global Iter: 478900 training acc: 0.71875
Global Iter: 479000 training loss: 0.699013
Global Iter: 479000 training acc: 0.5
Global Iter: 479100 training loss: 0.686787
Global Iter: 479100 training acc: 0.5625
Global Iter: 479200 training loss: 0.693342
Global Iter: 479200 training acc: 0.5
Global Iter: 479300 training loss: 0.707614
Global Iter: 479300 training acc: 0.40625
Global Iter: 479400 training loss: 0.703477
Global Iter: 479400 training acc: 0.4375
Global Iter: 479500 training loss: 0.684687
Global Iter: 479500 training acc: 0.59375
Global Iter: 479600 training loss: 0.687546
Global Iter: 479600 training acc: 0.53125
Global Iter: 479700 training loss: 0.69754
Global Iter: 479700 training acc: 0.5
Global Iter: 479800 training loss: 0.695064
Global Iter: 479800 training acc: 0.5
Global Iter: 479900 training loss: 0.689319
Global Iter: 479900 training acc: 0.5625
Global Iter: 480000 training loss: 0.686647
Global Iter: 480000 training acc: 0.5625
Global Iter: 480100 training loss: 0.694276
Global Iter: 480100 training acc: 0.5
Global Iter: 4