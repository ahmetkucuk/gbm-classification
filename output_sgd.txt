TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation0
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation1
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-CS-5393annotation3
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation0
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation1
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation2
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5270annotation3
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation0
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation1
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation2
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5273annotation3
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation0
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation1
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation2
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5275annotation3
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation0
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation1
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation2
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5276annotation3
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation0
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation1
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation2
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-5277annotation3
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation0
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation1
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation2
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A4XBannotation3
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation0
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation1
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation2
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DB-A64Xannotation3
in MUT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation0
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation1
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation2
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5140annotation3
in WT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation0
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation1
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation2
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5142annotation3
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation0
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation1
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-5143annotation2
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation0
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation1
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Bannotation2
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation0
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation1
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation2
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DH-A66Dannotation3
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation0
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation1
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation2
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-5855annotation3
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation0
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation1
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation2
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6396annotation3
in MUT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation0
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation1
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation2
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6402annotation3
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation0
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation1
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation2
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6403annotation3
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation0
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation1
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation2
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-6405annotation3
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation0
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation1
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation2
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7006annotation3
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation0
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation1
in WT
TCGA-DU-7012annotation2
in WT
TCGA-DU-7012annotation2
in WT
TCGA-DU-7012annotation2
in WT
TCGA-DU-7012annotation2
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7012annotation3
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation0
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation1
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation2
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7013annotation3
in WT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation0
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-7019annotation1
in MUT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation0
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation1
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation2
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8158annotation3
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation0
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation2
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8161annotation3
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation0
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation1
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation2
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8162annotation3
in WT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation0
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation1
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8163annotation2
in MUT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation0
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation1
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation2
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-DU-8165annotation3
in WT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation0
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation1
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation2
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-8185annotation3
in MUT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation0
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation1
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation2
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A4MWannotation3
in WT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation0
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation1
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation2
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A6J3annotation3
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation0
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation1
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation2
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-FG-A87Nannotation3
in MUT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation0
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation1
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation2
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7469annotation3
in WT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation0
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation1
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7475annotation2
in MUT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation0
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation1
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation2
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7477annotation3
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation0
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation1
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation2
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-7860annotation3
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation0
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation1
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation2
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8011annotation3
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation0
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation1
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation2
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation3
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8019annotation4
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation0
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation1
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation2
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8110annotation3
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation0
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation1
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation2
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-8564annotation3
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation0
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in2017-07-11 14:24:24.183430: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 14:24:24.183460: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 14:24:24.183468: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 14:24:24.183474: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 14:24:24.183480: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-07-11 14:24:24.642838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.3285
pciBusID 0000:02:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2017-07-11 14:24:24.642991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-07-11 14:24:24.642999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-07-11 14:24:24.643012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:02:00.0)
 WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation1
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation2
in WT
TCGA-HT-A4DSannotation3
in WT
TCGA-HT-A4DSannotation3
in WT
TCGA-HT-A4DSannotation3
in WT
Number of Patches: 2038
Global Iter: 100 training loss: 0.68735
Global Iter: 100 training acc: 0.5625
Global Iter: 200 training loss: 0.675304
Global Iter: 200 training acc: 0.71875
Global Iter: 300 training loss: 0.666733
Global Iter: 300 training acc: 0.65625
Global Iter: 400 training loss: 0.704255
Global Iter: 400 training acc: 0.40625
Global Iter: 500 training loss: 0.674407
Global Iter: 500 training acc: 0.59375
Global Iter: 600 training loss: 0.687057
Global Iter: 600 training acc: 0.59375
Global Iter: 700 training loss: 0.728687
Global Iter: 700 training acc: 0.25
Global Iter: 800 training loss: 0.697136
Global Iter: 800 training acc: 0.46875
Global Iter: 900 training loss: 0.68979
Global Iter: 900 training acc: 0.5625
Global Iter: 1000 training loss: 0.677087
Global Iter: 1000 training acc: 0.625
Global Iter: 1100 training loss: 0.676218
Global Iter: 1100 training acc: 0.71875
Global Iter: 1200 training loss: 0.699549
Global Iter: 1200 training acc: 0.46875
Global Iter: 1300 training loss: 0.668132
Global Iter: 1300 training acc: 0.71875
Global Iter: 1400 training loss: 0.694783
Global Iter: 1400 training acc: 0.5
Global Iter: 1500 training loss: 0.687607
Global Iter: 1500 training acc: 0.53125
Global Iter: 1600 training loss: 0.680211
Global Iter: 1600 training acc: 0.625
Global Iter: 1700 training loss: 0.703339
Global Iter: 1700 training acc: 0.46875
Global Iter: 1800 training loss: 0.699897
Global Iter: 1800 training acc: 0.4375
Global Iter: 1900 training loss: 0.675241
Global Iter: 1900 training acc: 0.65625
Global Iter: 2000 training loss: 0.687267
Global Iter: 2000 training acc: 0.53125
Global Iter: 2100 training loss: 0.71641
Global Iter: 2100 training acc: 0.34375
Global Iter: 2200 training loss: 0.69032
Global Iter: 2200 training acc: 0.53125
Global Iter: 2300 training loss: 0.690366
Global Iter: 2300 training acc: 0.5625
Global Iter: 2400 training loss: 0.702157
Global Iter: 2400 training acc: 0.4375
Global Iter: 2500 training loss: 0.671592
Global Iter: 2500 training acc: 0.71875
Global Iter: 2600 training loss: 0.705512
Global Iter: 2600 training acc: 0.40625
Global Iter: 2700 training loss: 0.692199
Global Iter: 2700 training acc: 0.5625
Global Iter: 2800 training loss: 0.682504
Global Iter: 2800 training acc: 0.625
Global Iter: 2900 training loss: 0.667298
Global Iter: 2900 training acc: 0.6875
Global Iter: 3000 training loss: 0.693239
Global Iter: 3000 training acc: 0.5
Global Iter: 3100 training loss: 0.679317
Global Iter: 3100 training acc: 0.65625
Global Iter: 3200 training loss: 0.667508
Global Iter: 3200 training acc: 0.75
Global Iter: 3300 training loss: 0.697326
Global Iter: 3300 training acc: 0.4375
Global Iter: 3400 training loss: 0.697983
Global Iter: 3400 training acc: 0.5
Global Iter: 3500 training loss: 0.696601
Global Iter: 3500 training acc: 0.5
Global Iter: 3600 training loss: 0.732201
Global Iter: 3600 training acc: 0.25
Global Iter: 3700 training loss: 0.698703
Global Iter: 3700 training acc: 0.5
Global Iter: 3800 training loss: 0.703802
Global Iter: 3800 training acc: 0.46875
Global Iter: 3900 training loss: 0.704473
Global Iter: 3900 training acc: 0.46875
Global Iter: 4000 training loss: 0.697895
Global Iter: 4000 training acc: 0.46875
Global Iter: 4100 training loss: 0.670777
Global Iter: 4100 training acc: 0.65625
Global Iter: 4200 training loss: 0.686929
Global Iter: 4200 training acc: 0.59375
Global Iter: 4300 training loss: 0.703547
Global Iter: 4300 training acc: 0.4375
Global Iter: 4400 training loss: 0.680711
Global Iter: 4400 training acc: 0.59375
Global Iter: 4500 training loss: 0.696836
Global Iter: 4500 training acc: 0.5
Global Iter: 4600 training loss: 0.688518
Global Iter: 4600 training acc: 0.5625
Global Iter: 4700 training loss: 0.699563
Global Iter: 4700 training acc: 0.46875
Global Iter: 4800 training loss: 0.694334
Global Iter: 4800 training acc: 0.5
Global Iter: 4900 training loss: 0.699034
Global Iter: 4900 training acc: 0.5
Global Iter: 5000 training loss: 0.70465
Global Iter: 5000 training acc: 0.4375
Global Iter: 5100 training loss: 0.71369
Global Iter: 5100 training acc: 0.375
Global Iter: 5200 training loss: 0.69502
Global Iter: 5200 training acc: 0.53125
Global Iter: 5300 training loss: 0.680449
Global Iter: 5300 training acc: 0.65625
Global Iter: 5400 training loss: 0.679437
Global Iter: 5400 training acc: 0.59375
Global Iter: 5500 training loss: 0.696735
Global Iter: 5500 training acc: 0.5
Global Iter: 5600 training loss: 0.688851
Global Iter: 5600 training acc: 0.5625
Global Iter: 5700 training loss: 0.697552
Global Iter: 5700 training acc: 0.46875
Global Iter: 5800 training loss: 0.680534
Global Iter: 5800 training acc: 0.5625
Global Iter: 5900 training loss: 0.697661
Global Iter: 5900 training acc: 0.5
Global Iter: 6000 training loss: 0.690619
Global Iter: 6000 training acc: 0.53125
Global Iter: 6100 training loss: 0.667271
Global Iter: 6100 training acc: 0.75
Global Iter: 6200 training loss: 0.692314
Global Iter: 6200 training acc: 0.46875
Global Iter: 6300 training loss: 0.675799
Global Iter: 6300 training acc: 0.625
Global Iter: 6400 training loss: 0.679995
Global Iter: 6400 training acc: 0.59375
Global Iter: 6500 training loss: 0.723716
Global Iter: 6500 training acc: 0.34375
Global Iter: 6600 training loss: 0.700403
Global Iter: 6600 training acc: 0.46875
Global Iter: 6700 training loss: 0.688776
Global Iter: 6700 training acc: 0.5625
Global Iter: 6800 training loss: 0.691841
Global Iter: 6800 training acc: 0.53125
Global Iter: 6900 training loss: 0.691916
Global Iter: 6900 training acc: 0.5
Global Iter: 7000 training loss: 0.684057
Global Iter: 7000 training acc: 0.59375
Global Iter: 7100 training loss: 0.699638
Global Iter: 7100 training acc: 0.5
Global Iter: 7200 training loss: 0.675051
Global Iter: 7200 training acc: 0.65625
Global Iter: 7300 training loss: 0.692045
Global Iter: 7300 training acc: 0.5625
Global Iter: 7400 training loss: 0.686358
Global Iter: 7400 training acc: 0.625
Global Iter: 7500 training loss: 0.680987
Global Iter: 7500 training acc: 0.59375
Global Iter: 7600 training loss: 0.702806
Global Iter: 7600 training acc: 0.40625
Global Iter: 7700 training loss: 0.712832
Global Iter: 7700 training acc: 0.34375
Global Iter: 7800 training loss: 0.70263
Global Iter: 7800 training acc: 0.40625
Global Iter: 7900 training loss: 0.689324
Global Iter: 7900 training acc: 0.5625
Global Iter: 8000 training loss: 0.703599
Global Iter: 8000 training acc: 0.40625
Global Iter: 8100 training loss: 0.680574
Global Iter: 8100 training acc: 0.59375
Global Iter: 8200 training loss: 0.70079
Global Iter: 8200 training acc: 0.5
Global Iter: 8300 training loss: 0.699579
Global Iter: 8300 training acc: 0.46875
Global Iter: 8400 training loss: 0.6791
Global Iter: 8400 training acc: 0.625
Global Iter: 8500 training loss: 0.684339
Global Iter: 8500 training acc: 0.59375
Global Iter: 8600 training loss: 0.682727
Global Iter: 8600 training acc: 0.53125
Global Iter: 8700 training loss: 0.70695
Global Iter: 8700 training acc: 0.5
Global Iter: 8800 training loss: 0.69104
Global Iter: 8800 training acc: 0.53125
Global Iter: 8900 training loss: 0.690159
Global Iter: 8900 training acc: 0.5
Global Iter: 9000 training loss: 0.680037
Global Iter: 9000 training acc: 0.59375
Global Iter: 9100 training loss: 0.711174
Global Iter: 9100 training acc: 0.375
Global Iter: 9200 training loss: 0.671926
Global Iter: 9200 training acc: 0.71875
Global Iter: 9300 training loss: 0.674108
Global Iter: 9300 training acc: 0.5625
Global Iter: 9400 training loss: 0.700962
Global Iter: 9400 training acc: 0.5
Global Iter: 9500 training loss: 0.687815
Global Iter: 9500 training acc: 0.5625
Global Iter: 9600 training loss: 0.698749
Global Iter: 9600 training acc: 0.5625
Global Iter: 9700 training loss: 0.704366
Global Iter: 9700 training acc: 0.46875
Global Iter: 9800 training loss: 0.683171
Global Iter: 9800 training acc: 0.625
Global Iter: 9900 training loss: 0.702169
Global Iter: 9900 training acc: 0.46875
Global Iter: 10000 training loss: 0.690317
Global Iter: 10000 training acc: 0.46875
Global Iter: 10100 training loss: 0.674447
Global Iter: 10100 training acc: 0.75
Global Iter: 10200 training loss: 0.677439
Global Iter: 10200 training acc: 0.625
Global Iter: 10300 training loss: 0.695171
Global Iter: 10300 training acc: 0.4375
Global Iter: 10400 training loss: 0.685768
Global Iter: 10400 training acc: 0.59375
Global Iter: 10500 training loss: 0.685584
Global Iter: 10500 training acc: 0.625
Global Iter: 10600 training loss: 0.728861
Global Iter: 10600 training acc: 0.21875
Global Iter: 10700 training loss: 0.707136
Global Iter: 10700 training acc: 0.40625
Global Iter: 10800 training loss: 0.682173
Global Iter: 10800 training acc: 0.5625
Global Iter: 10900 training loss: 0.685598
Global Iter: 10900 training acc: 0.59375
Global Iter: 11000 training loss: 0.671979
Global Iter: 11000 training acc: 0.6875
Global Iter: 11100 training loss: 0.702222
Global Iter: 11100 training acc: 0.46875
Global Iter: 11200 training loss: 0.673483
Global Iter: 11200 training acc: 0.71875
Global Iter: 11300 training loss: 0.687239
Global Iter: 11300 training acc: 0.53125
Global Iter: 11400 training loss: 0.706076
Global Iter: 11400 training acc: 0.46875
Global Iter: 11500 training loss: 0.685677
Global Iter: 11500 training acc: 0.59375
Global Iter: 11600 training loss: 0.710018
Global Iter: 11600 training acc: 0.4375
Global Iter: 11700 training loss: 0.701485
Global Iter: 11700 training acc: 0.40625
Global Iter: 11800 training loss: 0.678049
Global Iter: 11800 training acc: 0.65625
Global Iter: 11900 training loss: 0.687818
Global Iter: 11900 training acc: 0.5
Global Iter: 12000 training loss: 0.719895
Global Iter: 12000 training acc: 0.3125
Global Iter: 12100 training loss: 0.689097
Global Iter: 12100 training acc: 0.53125
Global Iter: 12200 training loss: 0.686703
Global Iter: 12200 training acc: 0.5625
Global Iter: 12300 training loss: 0.711412
Global Iter: 12300 training acc: 0.4375
Global Iter: 12400 training loss: 0.678065
Global Iter: 12400 training acc: 0.6875
Global Iter: 12500 training loss: 0.706306
Global Iter: 12500 training acc: 0.4375
Global Iter: 12600 training loss: 0.683265
Global Iter: 12600 training acc: 0.53125
Global Iter: 12700 training loss: 0.682637
Global Iter: 12700 training acc: 0.65625
Global Iter: 12800 training loss: 0.681112
Global Iter: 12800 training acc: 0.625
Global Iter: 12900 training loss: 0.689293
Global Iter: 12900 training acc: 0.5625
Global Iter: 13000 training loss: 0.673273
Global Iter: 13000 training acc: 0.65625
Global Iter: 13100 training loss: 0.663606
Global Iter: 13100 training acc: 0.75
Global Iter: 13200 training loss: 0.702181
Global Iter: 13200 training acc: 0.4375
Global Iter: 13300 training loss: 0.696762
Global Iter: 13300 training acc: 0.46875
Global Iter: 13400 training loss: 0.692967
Global Iter: 13400 training acc: 0.5
Global Iter: 13500 training loss: 0.736988
Global Iter: 13500 training acc: 0.25
Global Iter: 13600 training loss: 0.702039
Global Iter: 13600 training acc: 0.5
Global Iter: 13700 training loss: 0.698469
Global Iter: 13700 training acc: 0.46875
Global Iter: 13800 training loss: 0.692932
Global Iter: 13800 training acc: 0.53125
Global Iter: 13900 training loss: 0.693787
Global Iter: 13900 training acc: 0.5
Global Iter: 14000 training loss: 0.676902
Global Iter: 14000 training acc: 0.625
Global Iter: 14100 training loss: 0.678041
Global Iter: 14100 training acc: 0.625
Global Iter: 14200 training loss: 0.696168
Global Iter: 14200 training acc: 0.4375
Global Iter: 14300 training loss: 0.681414
Global Iter: 14300 training acc: 0.59375
Global Iter: 14400 training loss: 0.699721
Global Iter: 14400 training acc: 0.4375
Global Iter: 14500 training loss: 0.690094
Global Iter: 14500 training acc: 0.5625
Global Iter: 14600 training loss: 0.700211
Global Iter: 14600 training acc: 0.40625
Global Iter: 14700 training loss: 0.691017
Global Iter: 14700 training acc: 0.625
Global Iter: 14800 training loss: 0.697302
Global Iter: 14800 training acc: 0.5
Global Iter: 14900 training loss: 0.701702
Global Iter: 14900 training acc: 0.4375
Global Iter: 15000 training loss: 0.710221
Global Iter: 15000 training acc: 0.375
Global Iter: 15100 training loss: 0.698593
Global Iter: 15100 training acc: 0.46875
Global Iter: 15200 training loss: 0.677768
Global Iter: 15200 training acc: 0.65625
Global Iter: 15300 training loss: 0.688169
Global Iter: 15300 training acc: 0.5625
Global Iter: 15400 training loss: 0.693606
Global Iter: 15400 training acc: 0.46875
Global Iter: 15500 training loss: 0.691201
Global Iter: 15500 training acc: 0.5625
Global Iter: 15600 training loss: 0.699262
Global Iter: 15600 training acc: 0.4375
Global Iter: 15700 training loss: 0.682445
Global Iter: 15700 training acc: 0.59375
Global Iter: 15800 training loss: 0.695759
Global Iter: 15800 training acc: 0.5
Global Iter: 15900 training loss: 0.68129
Global Iter: 15900 training acc: 0.59375
Global Iter: 16000 training loss: 0.659208
Global Iter: 16000 training acc: 0.78125
Global Iter: 16100 training loss: 0.692504
Global Iter: 16100 training acc: 0.5
Global Iter: 16200 training loss: 0.686247
Global Iter: 16200 training acc: 0.59375
Global Iter: 16300 training loss: 0.682949
Global Iter: 16300 training acc: 0.59375
Global Iter: 16400 training loss: 0.721632
Global Iter: 16400 training acc: 0.34375
Global Iter: 16500 training loss: 0.699446
Global Iter: 16500 training acc: 0.5
Global Iter: 16600 training loss: 0.695163
Global Iter: 16600 training acc: 0.5
Global Iter: 16700 training loss: 0.692878
Global Iter: 16700 training acc: 0.5
Global Iter: 16800 training loss: 0.694257
Global Iter: 16800 training acc: 0.5
Global Iter: 16900 training loss: 0.681554
Global Iter: 16900 training acc: 0.59375
Global Iter: 17000 training loss: 0.690429
Global Iter: 17000 training acc: 0.5
Global Iter: 17100 training loss: 0.686515
Global Iter: 17100 training acc: 0.625
Global Iter: 17200 training loss: 0.684845
Global Iter: 17200 training acc: 0.59375
Global Iter: 17300 training loss: 0.692769
Global Iter: 17300 training acc: 0.5625
Global Iter: 17400 training loss: 0.674014
Global Iter: 17400 training acc: 0.625
Global Iter: 17500 training loss: 0.708034
Global Iter: 17500 training acc: 0.4375
Global Iter: 17600 training loss: 0.712295
Global Iter: 17600 training acc: 0.34375
Global Iter: 17700 training loss: 0.702679
Global Iter: 17700 training acc: 0.4375
Global Iter: 17800 training loss: 0.687388
Global Iter: 17800 training acc: 0.53125
Global Iter: 17900 training loss: 0.709295
Global Iter: 17900 training acc: 0.40625
Global Iter: 18000 training loss: 0.677705
Global Iter: 18000 training acc: 0.625
Global Iter: 18100 training loss: 0.691843
Global Iter: 18100 training acc: 0.53125
Global Iter: 18200 training loss: 0.704464
Global Iter: 18200 training acc: 0.46875
Global Iter: 18300 training loss: 0.68558
Global Iter: 18300 training acc: 0.59375
Global Iter: 18400 training loss: 0.688314
Global Iter: 18400 training acc: 0.5625
Global Iter: 18500 training loss: 0.697541
Global Iter: 18500 training acc: 0.53125
Global Iter: 18600 training loss: 0.690203
Global Iter: 18600 training acc: 0.53125
Global Iter: 18700 training loss: 0.687039
Global Iter: 18700 training acc: 0.59375
Global Iter: 18800 training loss: 0.698222
Global Iter: 18800 training acc: 0.46875
Global Iter: 18900 training loss: 0.682305
Global Iter: 18900 training acc: 0.65625
Global Iter: 19000 training loss: 0.7121
Global Iter: 19000 training acc: 0.375
Global Iter: 19100 training loss: 0.670217
Global Iter: 19100 training acc: 0.6875
Global Iter: 19200 training loss: 0.675466
Global Iter: 19200 training acc: 0.625
Global Iter: 19300 training loss: 0.701357
Global Iter: 19300 training acc: 0.5
Global Iter: 19400 training loss: 0.693272
Global Iter: 19400 training acc: 0.5625
Global Iter: 19500 training loss: 0.687566
Global Iter: 19500 training acc: 0.5625
Global Iter: 19600 training loss: 0.698175
Global Iter: 19600 training acc: 0.46875
Global Iter: 19700 training loss: 0.682764
Global Iter: 19700 training acc: 0.59375
Global Iter: 19800 training loss: 0.7089
Global Iter: 19800 training acc: 0.40625
Global Iter: 19900 training loss: 0.705743
Global Iter: 19900 training acc: 0.40625
Global Iter: 20000 training loss: 0.666409
Global Iter: 20000 training acc: 0.71875
Global Iter: 20100 training loss: 0.690065
Global Iter: 20100 training acc: 0.59375
Global Iter: 20200 training loss: 0.692845
Global Iter: 20200 training acc: 0.46875
Global Iter: 20300 training loss: 0.685779
Global Iter: 20300 training acc: 0.5625
Global Iter: 20400 training loss: 0.67941
Global Iter: 20400 training acc: 0.59375
Global Iter: 20500 training loss: 0.726025
Global Iter: 20500 training acc: 0.25
Global Iter: 20600 training loss: 0.699701
Global Iter: 20600 training acc: 0.4375
Global Iter: 20700 training loss: 0.681699
Global Iter: 20700 training acc: 0.59375
Global Iter: 20800 training loss: 0.687358
Global Iter: 20800 training acc: 0.53125
Global Iter: 20900 training loss: 0.676085
Global Iter: 20900 training acc: 0.65625
Global Iter: 21000 training loss: 0.70655
Global Iter: 21000 training acc: 0.40625
Global Iter: 21100 training loss: 0.669679
Global Iter: 21100 training acc: 0.6875
Global Iter: 21200 training loss: 0.699143
Global Iter: 21200 training acc: 0.53125
Global Iter: 21300 training loss: 0.696617
Global Iter: 21300 training acc: 0.46875
Global Iter: 21400 training loss: 0.689225
Global Iter: 21400 training acc: 0.59375
Global Iter: 21500 training loss: 0.699504
Global Iter: 21500 training acc: 0.4375
Global Iter: 21600 training loss: 0.705919
Global Iter: 21600 training acc: 0.4375
Global Iter: 21700 training loss: 0.679391
Global Iter: 21700 training acc: 0.65625
Global Iter: 21800 training loss: 0.686111
Global Iter: 21800 training acc: 0.5625
Global Iter: 21900 training loss: 0.72083
Global Iter: 21900 training acc: 0.3125
Global Iter: 22000 training loss: 0.686485
Global Iter: 22000 training acc: 0.5625
Global Iter: 22100 training loss: 0.685732
Global Iter: 22100 training acc: 0.5625
Global Iter: 22200 training loss: 0.698899
Global Iter: 22200 training acc: 0.5
Global Iter: 22300 training loss: 0.678635
Global Iter: 22300 training acc: 0.625
Global Iter: 22400 training loss: 0.704286
Global Iter: 22400 training acc: 0.4375
Global Iter: 22500 training loss: 0.683262
Global Iter: 22500 training acc: 0.53125
Global Iter: 22600 training loss: 0.693998
Global Iter: 22600 training acc: 0.5625
Global Iter: 22700 training loss: 0.679349
Global Iter: 22700 training acc: 0.625
Global Iter: 22800 training loss: 0.689449
Global Iter: 22800 training acc: 0.5625
Global Iter: 22900 training loss: 0.678539
Global Iter: 22900 training acc: 0.65625
Global Iter: 23000 training loss: 0.66349
Global Iter: 23000 training acc: 0.78125
Global Iter: 23100 training loss: 0.699918
Global Iter: 23100 training acc: 0.40625
Global Iter: 23200 training loss: 0.695902
Global Iter: 23200 training acc: 0.46875
Global Iter: 23300 training loss: 0.700327
Global Iter: 23300 training acc: 0.46875
Global Iter: 23400 training loss: 0.723654
Global Iter: 23400 training acc: 0.25
Global Iter: 23500 training loss: 0.701275
Global Iter: 23500 training acc: 0.5
Global Iter: 23600 training loss: 0.698311
Global Iter: 23600 training acc: 0.53125
Global Iter: 23700 training loss: 0.70102
Global Iter: 23700 training acc: 0.53125
Global Iter: 23800 training loss: 0.682167
Global Iter: 23800 training acc: 0.5625
Global Iter: 23900 training loss: 0.681378
Global Iter: 23900 training acc: 0.625
Global Iter: 24000 training loss: 0.687419
Global Iter: 24000 training acc: 0.5625
Global Iter: 24100 training loss: 0.694574
Global Iter: 24100 training acc: 0.5
Global Iter: 24200 training loss: 0.681818
Global Iter: 24200 training acc: 0.59375
Global Iter: 24300 training loss: 0.688593
Global Iter: 24300 training acc: 0.5
Global Iter: 24400 training loss: 0.695965
Global Iter: 24400 training acc: 0.5
Global Iter: 24500 training loss: 0.706553
Global Iter: 24500 training acc: 0.4375
Global Iter: 24600 training loss: 0.684033
Global Iter: 24600 training acc: 0.59375
Global Iter: 24700 training loss: 0.699024
Global Iter: 24700 training acc: 0.5
Global Iter: 24800 training loss: 0.712388
Global Iter: 24800 training acc: 0.375
Global Iter: 24900 training loss: 0.712779
Global Iter: 24900 training acc: 0.375
Global Iter: 25000 training loss: 0.697285
Global Iter: 25000 training acc: 0.5
Global Iter: 25100 training loss: 0.674757
Global Iter: 25100 training acc: 0.625
Global Iter: 25200 training loss: 0.688101
Global Iter: 25200 training acc: 0.5625
Global Iter: 25300 training loss: 0.69794
Global Iter: 25300 training acc: 0.46875
Global Iter: 25400 training loss: 0.681453
Global Iter: 25400 training acc: 0.59375
Global Iter: 25500 training loss: 0.692777
Global Iter: 25500 training acc: 0.5
Global Iter: 25600 training loss: 0.690991
Global Iter: 25600 training acc: 0.5625
Global Iter: 25700 training loss: 0.693027
Global Iter: 25700 training acc: 0.5
Global Iter: 25800 training loss: 0.682167
Global Iter: 25800 training acc: 0.59375
Global Iter: 25900 training loss: 0.658875
Global Iter: 25900 training acc: 0.75
Global Iter: 26000 training loss: 0.686674
Global Iter: 26000 training acc: 0.53125
Global Iter: 26100 training loss: 0.689706
Global Iter: 26100 training acc: 0.5625
Global Iter: 26200 training loss: 0.681355
Global Iter: 26200 training acc: 0.5625
Global Iter: 26300 training loss: 0.714905
Global Iter: 26300 training acc: 0.375
Global Iter: 26400 training loss: 0.695112
Global Iter: 26400 training acc: 0.5
Global Iter: 26500 training loss: 0.705096
Global Iter: 26500 training acc: 0.46875
Global Iter: 26600 training loss: 0.692939
Global Iter: 26600 training acc: 0.5
Global Iter: 26700 training loss: 0.700099
Global Iter: 26700 training acc: 0.5
Global Iter: 26800 training loss: 0.68125
Global Iter: 26800 training acc: 0.625
Global Iter: 26900 training loss: 0.6856
Global Iter: 26900 training acc: 0.53125
Global Iter: 27000 training loss: 0.680941
Global Iter: 27000 training acc: 0.625
Global Iter: 27100 training loss: 0.67752
Global Iter: 27100 training acc: 0.625
Global Iter: 27200 training loss: 0.694689
Global Iter: 27200 training acc: 0.53125
Global Iter: 27300 training loss: 0.685684
Global Iter: 27300 training acc: 0.59375
Global Iter: 27400 training loss: 0.701332
Global Iter: 27400 training acc: 0.4375
Global Iter: 27500 training loss: 0.703137
Global Iter: 27500 training acc: 0.4375
Global Iter: 27600 training loss: 0.70253
Global Iter: 27600 training acc: 0.46875
Global Iter: 27700 training loss: 0.691718
Global Iter: 27700 training acc: 0.53125
Global Iter: 27800 training loss: 0.716688
Global Iter: 27800 training acc: 0.375
Global Iter: 27900 training loss: 0.682557
Global Iter: 27900 training acc: 0.625
Global Iter: 28000 training loss: 0.690823
Global Iter: 28000 training acc: 0.53125
Global Iter: 28100 training loss: 0.699854
Global Iter: 28100 training acc: 0.4375
Global Iter: 28200 training loss: 0.685319
Global Iter: 28200 training acc: 0.59375
Global Iter: 28300 training loss: 0.688291
Global Iter: 28300 training acc: 0.5625
Global Iter: 28400 training loss: 0.698058
Global Iter: 28400 training acc: 0.53125
Global Iter: 28500 training loss: 0.694806
Global Iter: 28500 training acc: 0.5
Global Iter: 28600 training loss: 0.688431
Global Iter: 28600 training acc: 0.53125
Global Iter: 28700 training loss: 0.69936
Global Iter: 28700 training acc: 0.4375
Global Iter: 28800 training loss: 0.684361
Global Iter: 28800 training acc: 0.625
Global Iter: 28900 training loss: 0.710617
Global Iter: 28900 training acc: 0.40625
Global Iter: 29000 training loss: 0.671168
Global Iter: 29000 training acc: 0.6875
Global Iter: 29100 training loss: 0.677808
Global Iter: 29100 training acc: 0.65625
Global Iter: 29200 training loss: 0.689857
Global Iter: 29200 training acc: 0.53125
Global Iter: 29300 training loss: 0.689173
Global Iter: 29300 training acc: 0.53125
Global Iter: 29400 training loss: 0.689281
Global Iter: 29400 training acc: 0.53125
Global Iter: 29500 training loss: 0.699427
Global Iter: 29500 training acc: 0.46875
Global Iter: 29600 training loss: 0.685467
Global Iter: 29600 training acc: 0.59375
Global Iter: 29700 training loss: 0.713789
Global Iter: 29700 training acc: 0.4375
Global Iter: 29800 training loss: 0.707147
Global Iter: 29800 training acc: 0.40625
Global Iter: 29900 training loss: 0.668476
Global Iter: 29900 training acc: 0.75
Global Iter: 30000 training loss: 0.687019
Global Iter: 30000 training acc: 0.5625
Global Iter: 30100 training loss: 0.701787
Global Iter: 30100 training acc: 0.4375
Global Iter: 30200 training loss: 0.695151
Global Iter: 30200 training acc: 0.53125
Global Iter: 30300 training loss: 0.690817
Global Iter: 30300 training acc: 0.5625
Global Iter: 30400 training loss: 0.714972
Global Iter: 30400 training acc: 0.3125
Global Iter: 30500 training loss: 0.696597
Global Iter: 30500 training acc: 0.4375
Global Iter: 30600 training loss: 0.674764
Global Iter: 30600 training acc: 0.625
Global Iter: 30700 training loss: 0.698547
Global Iter: 30700 training acc: 0.5
Global Iter: 30800 training loss: 0.676795
Global Iter: 30800 training acc: 0.71875
Global Iter: 30900 training loss: 0.717177
Global Iter: 30900 training acc: 0.375
Global Iter: 31000 training loss: 0.676673
Global Iter: 31000 training acc: 0.71875
Global Iter: 31100 training loss: 0.681528
Global Iter: 31100 training acc: 0.59375
Global Iter: 31200 training loss: 0.7017
Global Iter: 31200 training acc: 0.4375
Global Iter: 31300 training loss: 0.684276
Global Iter: 31300 training acc: 0.625
Global Iter: 31400 training loss: 0.703409
Global Iter: 31400 training acc: 0.4375
Global Iter: 31500 training loss: 0.70673
Global Iter: 31500 training acc: 0.4375
Global Iter: 31600 training loss: 0.675468
Global Iter: 31600 training acc: 0.65625
Global Iter: 31700 training loss: 0.696401
Global Iter: 31700 training acc: 0.53125
Global Iter: 31800 training loss: 0.723878
Global Iter: 31800 training acc: 0.28125
Global Iter: 31900 training loss: 0.688651
Global Iter: 31900 training acc: 0.5625
Global Iter: 32000 training loss: 0.696168
Global Iter: 32000 training acc: 0.5
Global Iter: 32100 training loss: 0.688634
Global Iter: 32100 training acc: 0.5625
Global Iter: 32200 training loss: 0.681848
Global Iter: 32200 training acc: 0.65625
Global Iter: 32300 training loss: 0.695132
Global Iter: 32300 training acc: 0.5
Global Iter: 32400 training loss: 0.688391
Global Iter: 32400 training acc: 0.53125
Global Iter: 32500 training loss: 0.690762
Global Iter: 32500 training acc: 0.53125
Global Iter: 32600 training loss: 0.674725
Global Iter: 32600 training acc: 0.5625
Global Iter: 32700 training loss: 0.683499
Global Iter: 32700 training acc: 0.59375
Global Iter: 32800 training loss: 0.666716
Global Iter: 32800 training acc: 0.71875
Global Iter: 32900 training loss: 0.653678
Global Iter: 32900 training acc: 0.78125
Global Iter: 33000 training loss: 0.697471
Global Iter: 33000 training acc: 0.40625
Global Iter: 33100 training loss: 0.703917
Global Iter: 33100 training acc: 0.46875
Global Iter: 33200 training loss: 0.694764
Global Iter: 33200 training acc: 0.5
Global Iter: 33300 training loss: 0.736731
Global Iter: 33300 training acc: 0.21875
Global Iter: 33400 training loss: 0.695831
Global Iter: 33400 training acc: 0.5
Global Iter: 33500 training loss: 0.696306
Global Iter: 33500 training acc: 0.5
Global Iter: 33600 training loss: 0.692821
Global Iter: 33600 training acc: 0.53125
Global Iter: 33700 training loss: 0.684342
Global Iter: 33700 training acc: 0.5625
Global Iter: 33800 training loss: 0.68806
Global Iter: 33800 training acc: 0.5625
Global Iter: 33900 training loss: 0.68643
Global Iter: 33900 training acc: 0.59375
Global Iter: 34000 training loss: 0.700859
Global Iter: 34000 training acc: 0.4375
Global Iter: 34100 training loss: 0.68288
Global Iter: 34100 training acc: 0.59375
Global Iter: 34200 training loss: 0.696041
Global Iter: 34200 training acc: 0.5
Global Iter: 34300 training loss: 0.68494
Global Iter: 34300 training acc: 0.5625
Global Iter: 34400 training loss: 0.707295
Global Iter: 34400 training acc: 0.4375
Global Iter: 34500 training loss: 0.692953
Global Iter: 34500 training acc: 0.59375
Global Iter: 34600 training loss: 0.698478
Global Iter: 34600 training acc: 0.5
Global Iter: 34700 training loss: 0.706885
Global Iter: 34700 training acc: 0.4375
Global Iter: 34800 training loss: 0.701869
Global Iter: 34800 training acc: 0.40625
Global Iter: 34900 training loss: 0.689579
Global Iter: 34900 training acc: 0.53125
Global Iter: 35000 training loss: 0.675037
Global Iter: 35000 training acc: 0.625
Global Iter: 35100 training loss: 0.68144
Global Iter: 35100 training acc: 0.59375
Global Iter: 35200 training loss: 0.708769
Global Iter: 35200 training acc: 0.4375
Global Iter: 35300 training loss: 0.685373
Global Iter: 35300 training acc: 0.5625
Global Iter: 35400 training loss: 0.6937
Global Iter: 35400 training acc: 0.5
Global Iter: 35500 training loss: 0.682561
Global Iter: 35500 training acc: 0.625
Global Iter: 35600 training loss: 0.699905
Global Iter: 35600 training acc: 0.46875
Global Iter: 35700 training loss: 0.688012
Global Iter: 35700 training acc: 0.59375
Global Iter: 35800 training loss: 0.668341
Global Iter: 35800 training acc: 0.75
Global Iter: 35900 training loss: 0.693033
Global Iter: 35900 training acc: 0.53125
Global Iter: 36000 training loss: 0.68089
Global Iter: 36000 training acc: 0.5625
Global Iter: 36100 training loss: 0.693645
Global Iter: 36100 training acc: 0.53125
Global Iter: 36200 training loss: 0.721787
Global Iter: 36200 training acc: 0.34375
Global Iter: 36300 training loss: 0.689686
Global Iter: 36300 training acc: 0.5
Global Iter: 36400 training loss: 0.708424
Global Iter: 36400 training acc: 0.46875
Global Iter: 36500 training loss: 0.696404
Global Iter: 36500 training acc: 0.5
Global Iter: 36600 training loss: 0.688556
Global Iter: 36600 training acc: 0.46875
Global Iter: 36700 training loss: 0.667767
Global Iter: 36700 training acc: 0.625
Global Iter: 36800 training loss: 0.690667
Global Iter: 36800 training acc: 0.5
Global Iter: 36900 training loss: 0.686676
Global Iter: 36900 training acc: 0.59375
Global Iter: 37000 training loss: 0.680848
Global Iter: 37000 training acc: 0.59375
Global Iter: 37100 training loss: 0.693357
Global Iter: 37100 training acc: 0.53125
Global Iter: 37200 training loss: 0.67971
Global Iter: 37200 training acc: 0.59375
Global Iter: 37300 training loss: 0.7085
Global Iter: 37300 training acc: 0.46875
Global Iter: 37400 training loss: 0.696287
Global Iter: 37400 training acc: 0.46875
Global Iter: 37500 training loss: 0.703139
Global Iter: 37500 training acc: 0.4375
Global Iter: 37600 training loss: 0.699553
Global Iter: 37600 training acc: 0.5
Global Iter: 37700 training loss: 0.71704
Global Iter: 37700 training acc: 0.34375
Global Iter: 37800 training loss: 0.677833
Global Iter: 37800 training acc: 0.625
Global Iter: 37900 training loss: 0.679098
Global Iter: 37900 training acc: 0.59375
Global Iter: 38000 training loss: 0.697258
Global Iter: 38000 training acc: 0.46875
Global Iter: 38100 training loss: 0.684801
Global Iter: 38100 training acc: 0.59375
Global Iter: 38200 training loss: 0.687021
Global Iter: 38200 training acc: 0.5625
Global Iter: 38300 training loss: 0.693273
Global Iter: 38300 training acc: 0.5
Global Iter: 38400 training loss: 0.694559
Global Iter: 38400 training acc: 0.5
Global Iter: 38500 training loss: 0.685957
Global Iter: 38500 training acc: 0.5625
Global Iter: 38600 training loss: 0.703564
Global Iter: 38600 training acc: 0.4375
Global Iter: 38700 training loss: 0.684755
Global Iter: 38700 training acc: 0.59375
Global Iter: 38800 training loss: 0.707319
Global Iter: 38800 training acc: 0.4375
Global Iter: 38900 training loss: 0.672101
Global Iter: 38900 training acc: 0.6875
Global Iter: 39000 training loss: 0.679088
Global Iter: 39000 training acc: 0.65625
Global Iter: 39100 training loss: 0.694123
Global Iter: 39100 training acc: 0.5
Global Iter: 39200 training loss: 0.697196
Global Iter: 39200 training acc: 0.5
Global Iter: 39300 training loss: 0.689242
Global Iter: 39300 training acc: 0.53125
Global Iter: 39400 training loss: 0.691711
Global Iter: 39400 training acc: 0.53125
Global Iter: 39500 training loss: 0.696662
Global Iter: 39500 training acc: 0.53125
Global Iter: 39600 training loss: 0.707803
Global Iter: 39600 training acc: 0.40625
Global Iter: 39700 training loss: 0.706738
Global Iter: 39700 training acc: 0.4375
Global Iter: 39800 training loss: 0.674417
Global Iter: 39800 training acc: 0.75
Global Iter: 39900 training loss: 0.685975
Global Iter: 39900 training acc: 0.53125
Global Iter: 40000 training loss: 0.701218
Global Iter: 40000 training acc: 0.5
Global Iter: 40100 training loss: 0.681007
Global Iter: 40100 training acc: 0.59375
Global Iter: 40200 training loss: 0.687632
Global Iter: 40200 training acc: 0.5625
Global Iter: 40300 training loss: 0.7221
Global Iter: 40300 training acc: 0.28125
Global Iter: 40400 training loss: 0.709258
Global Iter: 40400 training acc: 0.40625
Global Iter: 40500 training loss: 0.679741
Global Iter: 40500 training acc: 0.625
Global Iter: 40600 training loss: 0.705706
Global Iter: 40600 training acc: 0.46875
Global Iter: 40700 training loss: 0.675732
Global Iter: 40700 training acc: 0.71875
Global Iter: 40800 training loss: 0.709846
Global Iter: 40800 training acc: 0.375
Global Iter: 40900 training loss: 0.6778
Global Iter: 40900 training acc: 0.65625
Global Iter: 41000 training loss: 0.683716
Global Iter: 41000 training acc: 0.59375
Global Iter: 41100 training loss: 0.69247
Global Iter: 41100 training acc: 0.5
Global Iter: 41200 training loss: 0.676022
Global Iter: 41200 training acc: 0.625
Global Iter: 41300 training loss: 0.698725
Global Iter: 41300 training acc: 0.46875
Global Iter: 41400 training loss: 0.696448
Global Iter: 41400 training acc: 0.46875
Global Iter: 41500 training loss: 0.669676
Global Iter: 41500 training acc: 0.65625
Global Iter: 41600 training loss: 0.690908
Global Iter: 41600 training acc: 0.5
Global Iter: 41700 training loss: 0.712329
Global Iter: 41700 training acc: 0.34375
Global Iter: 41800 training loss: 0.679219
Global Iter: 41800 training acc: 0.625
Global Iter: 41900 training loss: 0.692317
Global Iter: 41900 training acc: 0.59375
Global Iter: 42000 training loss: 0.687777
Global Iter: 42000 training acc: 0.53125
Global Iter: 42100 training loss: 0.681251
Global Iter: 42100 training acc: 0.625
Global Iter: 42200 training loss: 0.684367
Global Iter: 42200 training acc: 0.53125
Global Iter: 42300 training loss: 0.687741
Global Iter: 42300 training acc: 0.5625
Global Iter: 42400 training loss: 0.687709
Global Iter: 42400 training acc: 0.5625
Global Iter: 42500 training loss: 0.691576
Global Iter: 42500 training acc: 0.53125
Global Iter: 42600 training loss: 0.693532
Global Iter: 42600 training acc: 0.5625
Global Iter: 42700 training loss: 0.667215
Global Iter: 42700 training acc: 0.71875
Global Iter: 42800 training loss: 0.663757
Global Iter: 42800 training acc: 0.75
Global Iter: 42900 training loss: 0.706835
Global Iter: 42900 training acc: 0.375
Global Iter: 43000 training loss: 0.708571
Global Iter: 43000 training acc: 0.4375
Global Iter: 43100 training loss: 0.689022
Global Iter: 43100 training acc: 0.5625
Global Iter: 43200 training loss: 0.734463
Global Iter: 43200 training acc: 0.21875
Global Iter: 43300 training loss: 0.695268
Global Iter: 43300 training acc: 0.46875
Global Iter: 43400 training loss: 0.694777
Global Iter: 43400 training acc: 0.5
Global Iter: 43500 training loss: 0.692131
Global Iter: 43500 training acc: 0.53125
Global Iter: 43600 training loss: 0.684134
Global Iter: 43600 training acc: 0.59375
Global Iter: 43700 training loss: 0.681973
Global Iter: 43700 training acc: 0.59375
Global Iter: 43800 training loss: 0.682176
Global Iter: 43800 training acc: 0.65625
Global Iter: 43900 training loss: 0.706709
Global Iter: 43900 training acc: 0.4375
Global Iter: 44000 training loss: 0.682611
Global Iter: 44000 training acc: 0.5625
Global Iter: 44100 training loss: 0.684837
Global Iter: 44100 training acc: 0.5625
Global Iter: 44200 training loss: 0.69042
Global Iter: 44200 training acc: 0.53125
Global Iter: 44300 training loss: 0.700099
Global Iter: 44300 training acc: 0.46875
Global Iter: 44400 training loss: 0.68365
Global Iter: 44400 training acc: 0.59375
Global Iter: 44500 training loss: 0.705024
Global Iter: 44500 training acc: 0.5
Global Iter: 44600 training loss: 0.707669
Global Iter: 44600 training acc: 0.40625
Global Iter: 44700 training loss: 0.704835
Global Iter: 44700 training acc: 0.4375
Global Iter: 44800 training loss: 0.695161
Global Iter: 44800 training acc: 0.5
Global Iter: 44900 training loss: 0.684482
Global Iter: 44900 training acc: 0.59375
Global Iter: 45000 training loss: 0.683048
Global Iter: 45000 training acc: 0.59375
Global Iter: 45100 training loss: 0.704834
Global Iter: 45100 training acc: 0.4375
Global Iter: 45200 training loss: 0.686955
Global Iter: 45200 training acc: 0.5625
Global Iter: 45300 training loss: 0.693912
Global Iter: 45300 training acc: 0.53125
Global Iter: 45400 training loss: 0.680429
Global Iter: 45400 training acc: 0.5625
Global Iter: 45500 training loss: 0.693334
Global Iter: 45500 training acc: 0.5
Global Iter: 45600 training loss: 0.685878
Global Iter: 45600 training acc: 0.625
Global Iter: 45700 training loss: 0.667734
Global Iter: 45700 training acc: 0.75
Global Iter: 45800 training loss: 0.691495
Global Iter: 45800 training acc: 0.5625
Global Iter: 45900 training loss: 0.694579
Global Iter: 45900 training acc: 0.53125
Global Iter: 46000 training loss: 0.686596
Global Iter: 46000 training acc: 0.5625
Global Iter: 46100 training loss: 0.722231
Global Iter: 46100 training acc: 0.34375
Global Iter: 46200 training loss: 0.694584
Global Iter: 46200 training acc: 0.5
Global Iter: 46300 training loss: 0.708931
Global Iter: 46300 training acc: 0.4375
Global Iter: 46400 training loss: 0.69544
Global Iter: 46400 training acc: 0.5
Global Iter: 46500 training loss: 0.709765
Global Iter: 46500 training acc: 0.40625
Global Iter: 46600 training loss: 0.681305
Global Iter: 46600 training acc: 0.625
Global Iter: 46700 training loss: 0.691926
Global Iter: 46700 training acc: 0.5
Global Iter: 46800 training loss: 0.686014
Global Iter: 46800 training acc: 0.53125
Global Iter: 46900 training loss: 0.684589
Global Iter: 46900 training acc: 0.59375
Global Iter: 47000 training loss: 0.686006
Global Iter: 47000 training acc: 0.5
Global Iter: 47100 training loss: 0.6785
Global Iter: 47100 training acc: 0.59375
Global Iter: 47200 training loss: 0.703674
Global Iter: 47200 training acc: 0.46875
Global Iter: 47300 training loss: 0.693989
Global Iter: 47300 training acc: 0.5
Global Iter: 47400 training loss: 0.69982
Global Iter: 47400 training acc: 0.46875
Global Iter: 47500 training loss: 0.696713
Global Iter: 47500 training acc: 0.46875
Global Iter: 47600 training loss: 0.728895
Global Iter: 47600 training acc: 0.34375
Global Iter: 47700 training loss: 0.679324
Global Iter: 47700 training acc: 0.59375
Global Iter: 47800 training loss: 0.678004
Global Iter: 47800 training acc: 0.59375
Global Iter: 47900 training loss: 0.707204
Global Iter: 47900 training acc: 0.40625
Global Iter: 48000 training loss: 0.673317
Global Iter: 48000 training acc: 0.625
Global Iter: 48100 training loss: 0.687547
Global Iter: 48100 training acc: 0.5625
Global Iter: 48200 training loss: 0.690689
Global Iter: 48200 training acc: 0.53125
Global Iter: 48300 training loss: 0.679051
Global Iter: 48300 training acc: 0.5625
Global Iter: 48400 training loss: 0.69569
Global Iter: 48400 training acc: 0.5625
Global Iter: 48500 training loss: 0.704663
Global Iter: 48500 training acc: 0.4375
Global Iter: 48600 training loss: 0.679091
Global Iter: 48600 training acc: 0.625
Global Iter: 48700 training loss: 0.702666
Global Iter: 48700 training acc: 0.4375
Global Iter: 48800 training loss: 0.671594
Global Iter: 48800 training acc: 0.6875
Global Iter: 48900 training loss: 0.676192
Global Iter: 48900 training acc: 0.65625
Global Iter: 49000 training loss: 0.691211
Global Iter: 49000 training acc: 0.5
Global Iter: 49100 training loss: 0.69722
Global Iter: 49100 training acc: 0.5
Global Iter: 49200 training loss: 0.684368
Global Iter: 49200 training acc: 0.59375
Global Iter: 49300 training loss: 0.689461
Global Iter: 49300 training acc: 0.5
Global Iter: 49400 training loss: 0.690449
Global Iter: 49400 training acc: 0.53125
Global Iter: 49500 training loss: 0.714304
Global Iter: 49500 training acc: 0.40625
Global Iter: 49600 training loss: 0.701981
Global Iter: 49600 training acc: 0.4375
Global Iter: 49700 training loss: 0.657432
Global Iter: 49700 training acc: 0.78125
Global Iter: 49800 training loss: 0.691424
Global Iter: 49800 training acc: 0.5
Global Iter: 49900 training loss: 0.694895
Global Iter: 49900 training acc: 0.5
Global Iter: 50000 training loss: 0.685373
Global Iter: 50000 training acc: 0.59375
Global Iter: 50100 training loss: 0.690575
Global Iter: 50100 training acc: 0.5625
Global Iter: 50200 training loss: 0.719258
Global Iter: 50200 training acc: 0.3125
Global Iter: 50300 training loss: 0.711984
Global Iter: 50300 training acc: 0.375
Global Iter: 50400 training loss: 0.675282
Global Iter: 50400 training acc: 0.65625
Global Iter: 50500 training loss: 0.704935
Global Iter: 50500 training acc: 0.46875
Global Iter: 50600 training loss: 0.667896
Global Iter: 50600 training acc: 0.75
Global Iter: 50700 training loss: 0.715808
Global Iter: 50700 training acc: 0.375
Global Iter: 50800 training loss: 0.687397
Global Iter: 50800 training acc: 0.59375
Global Iter: 50900 training loss: 0.68757
Global Iter: 50900 training acc: 0.5625
Global Iter: 51000 training loss: 0.690931
Global Iter: 51000 training acc: 0.5625
Global Iter: 51100 training loss: 0.693894
Global Iter: 51100 training acc: 0.5625
Global Iter: 51200 training loss: 0.701136
Global Iter: 51200 training acc: 0.5
Global Iter: 51300 training loss: 0.691473
Global Iter: 51300 training acc: 0.53125
Global Iter: 51400 training loss: 0.685703
Global Iter: 51400 training acc: 0.5625
Global Iter: 51500 training loss: 0.69088
Global Iter: 51500 training acc: 0.5
Global Iter: 51600 training loss: 0.714595
Global Iter: 51600 training acc: 0.34375
Global Iter: 51700 training loss: 0.675934
Global Iter: 51700 training acc: 0.65625
Global Iter: 51800 training loss: 0.682204
Global Iter: 51800 training acc: 0.5625
Global Iter: 51900 training loss: 0.694704
Global Iter: 51900 training acc: 0.53125
Global Iter: 52000 training loss: 0.685384
Global Iter: 52000 training acc: 0.59375
Global Iter: 52100 training loss: 0.687707
Global Iter: 52100 training acc: 0.5625
Global Iter: 52200 training loss: 0.6965
Global Iter: 52200 training acc: 0.5
Global Iter: 52300 training loss: 0.680457
Global Iter: 52300 training acc: 0.625
Global Iter: 52400 training loss: 0.69867
Global Iter: 52400 training acc: 0.5
Global Iter: 52500 training loss: 0.695197
Global Iter: 52500 training acc: 0.53125
Global Iter: 52600 training loss: 0.673841
Global Iter: 52600 training acc: 0.71875
Global Iter: 52700 training loss: 0.668339
Global Iter: 52700 training acc: 0.6875
Global Iter: 52800 training loss: 0.708877
Global Iter: 52800 training acc: 0.375
Global Iter: 52900 training loss: 0.694266
Global Iter: 52900 training acc: 0.5
Global Iter: 53000 training loss: 0.686291
Global Iter: 53000 training acc: 0.59375
Global Iter: 53100 training loss: 0.733118
Global Iter: 53100 training acc: 0.21875
Global Iter: 53200 training loss: 0.701237
Global Iter: 53200 training acc: 0.46875
Global Iter: 53300 training loss: 0.685752
Global Iter: 53300 training acc: 0.53125
Global Iter: 53400 training loss: 0.696118
Global Iter: 53400 training acc: 0.46875
Global Iter: 53500 training loss: 0.682319
Global Iter: 53500 training acc: 0.625
Global Iter: 53600 training loss: 0.685035
Global Iter: 53600 training acc: 0.59375
Global Iter: 53700 training loss: 0.682264
Global Iter: 53700 training acc: 0.65625
Global Iter: 53800 training loss: 0.70785
Global Iter: 53800 training acc: 0.4375
Global Iter: 53900 training loss: 0.701321
Global Iter: 53900 training acc: 0.53125
Global Iter: 54000 training loss: 0.690998
Global Iter: 54000 training acc: 0.5625
Global Iter: 54100 training loss: 0.688489
Global Iter: 54100 training acc: 0.53125
Global Iter: 54200 training loss: 0.693036
Global Iter: 54200 training acc: 0.53125
Global Iter: 54300 training loss: 0.681169
Global Iter: 54300 training acc: 0.625
Global Iter: 54400 training loss: 0.684473
Global Iter: 54400 training acc: 0.5625
Global Iter: 54500 training loss: 0.709065
Global Iter: 54500 training acc: 0.40625
Global Iter: 54600 training loss: 0.702845
Global Iter: 54600 training acc: 0.4375
Global Iter: 54700 training loss: 0.696435
Global Iter: 54700 training acc: 0.5
Global Iter: 54800 training loss: 0.692177
Global Iter: 54800 training acc: 0.5625
Global Iter: 54900 training loss: 0.679382
Global Iter: 54900 training acc: 0.625
Global Iter: 55000 training loss: 0.69915
Global Iter: 55000 training acc: 0.46875
Global Iter: 55100 training loss: 0.686039
Global Iter: 55100 training acc: 0.5625
Global Iter: 55200 training loss: 0.691175
Global Iter: 55200 training acc: 0.5625
Global Iter: 55300 training loss: 0.674561
Global Iter: 55300 training acc: 0.625
Global Iter: 55400 training loss: 0.68849
Global Iter: 55400 training acc: 0.53125
Global Iter: 55500 training loss: 0.690497
Global Iter: 55500 training acc: 0.59375
Global Iter: 55600 training loss: 0.658865
Global Iter: 55600 training acc: 0.78125
Global Iter: 55700 training loss: 0.686867
Global Iter: 55700 training acc: 0.5625
Global Iter: 55800 training loss: 0.687623
Global Iter: 55800 training acc: 0.5625
Global Iter: 55900 training loss: 0.692488
Global Iter: 55900 training acc: 0.5625
Global Iter: 56000 training loss: 0.722129
Global Iter: 56000 training acc: 0.34375
Global Iter: 56100 training loss: 0.696559
Global Iter: 56100 training acc: 0.5
Global Iter: 56200 training loss: 0.703504
Global Iter: 56200 training acc: 0.4375
Global Iter: 56300 training loss: 0.700255
Global Iter: 56300 training acc: 0.46875
Global Iter: 56400 training loss: 0.704047
Global Iter: 56400 training acc: 0.40625
Global Iter: 56500 training loss: 0.688859
Global Iter: 56500 training acc: 0.59375
Global Iter: 56600 training loss: 0.691024
Global Iter: 56600 training acc: 0.5
Global Iter: 56700 training loss: 0.691055
Global Iter: 56700 training acc: 0.53125
Global Iter: 56800 training loss: 0.691195
Global Iter: 56800 training acc: 0.59375
Global Iter: 56900 training loss: 0.689487
Global Iter: 56900 training acc: 0.53125
Global Iter: 57000 training loss: 0.688253
Global Iter: 57000 training acc: 0.59375
Global Iter: 57100 training loss: 0.688635
Global Iter: 57100 training acc: 0.53125
Global Iter: 57200 training loss: 0.693121
Global Iter: 57200 training acc: 0.5
Global Iter: 57300 training loss: 0.693564
Global Iter: 57300 training acc: 0.5
Global Iter: 57400 training loss: 0.692702
Global Iter: 57400 training acc: 0.53125
Global Iter: 57500 training loss: 0.716768
Global Iter: 57500 training acc: 0.375
Global Iter: 57600 training loss: 0.691158
Global Iter: 57600 training acc: 0.5625
Global Iter: 57700 training loss: 0.671112
Global Iter: 57700 training acc: 0.65625
Global Iter: 57800 training loss: 0.70451
Global Iter: 57800 training acc: 0.40625
Global Iter: 57900 training loss: 0.690099
Global Iter: 57900 training acc: 0.5625
Global Iter: 58000 training loss: 0.696075
Global Iter: 58000 training acc: 0.53125
Global Iter: 58100 training loss: 0.700635
Global Iter: 58100 training acc: 0.5
Global Iter: 58200 training loss: 0.688367
Global Iter: 58200 training acc: 0.5625
Global Iter: 58300 training loss: 0.691224
Global Iter: 58300 training acc: 0.5625
Global Iter: 58400 training loss: 0.703878
Global Iter: 58400 training acc: 0.4375
Global Iter: 58500 training loss: 0.675475
Global Iter: 58500 training acc: 0.65625
Global Iter: 58600 training loss: 0.70333
Global Iter: 58600 training acc: 0.46875
Global Iter: 58700 training loss: 0.67023
Global Iter: 58700 training acc: 0.6875
Global Iter: 58800 training loss: 0.677215
Global Iter: 58800 training acc: 0.625
Global Iter: 58900 training loss: 0.698811
Global Iter: 58900 training acc: 0.46875
Global Iter: 59000 training loss: 0.696644
Global Iter: 59000 training acc: 0.5
Global Iter: 59100 training loss: 0.680744
Global Iter: 59100 training acc: 0.625
Global Iter: 59200 training loss: 0.695239
Global Iter: 59200 training acc: 0.5
Global Iter: 59300 training loss: 0.693465
Global Iter: 59300 training acc: 0.53125
Global Iter: 59400 training loss: 0.708888
Global Iter: 59400 training acc: 0.4375
Global Iter: 59500 training loss: 0.697726
Global Iter: 59500 training acc: 0.46875
Global Iter: 59600 training loss: 0.664952
Global Iter: 59600 training acc: 0.8125
Global Iter: 59700 training loss: 0.69255
Global Iter: 59700 training acc: 0.53125
Global Iter: 59800 training loss: 0.686041
Global Iter: 59800 training acc: 0.5625
Global Iter: 59900 training loss: 0.68731
Global Iter: 59900 training acc: 0.59375
Global Iter: 60000 training loss: 0.687739
Global Iter: 60000 training acc: 0.59375
Global Iter: 60100 training loss: 0.724035
Global Iter: 60100 training acc: 0.28125
Global Iter: 60200 training loss: 0.712466
Global Iter: 60200 training acc: 0.40625
Global Iter: 60300 training loss: 0.688588
Global Iter: 60300 training acc: 0.59375
Global Iter: 60400 training loss: 0.695537
Global Iter: 60400 training acc: 0.46875
Global Iter: 60500 training loss: 0.67296
Global Iter: 60500 training acc: 0.71875
Global Iter: 60600 training loss: 0.720082
Global Iter: 60600 training acc: 0.34375
Global Iter: 60700 training loss: 0.682537
Global Iter: 60700 training acc: 0.59375
Global Iter: 60800 training loss: 0.68388
Global Iter: 60800 training acc: 0.59375
Global Iter: 60900 training loss: 0.677639
Global Iter: 60900 training acc: 0.59375
Global Iter: 61000 training loss: 0.689612
Global Iter: 61000 training acc: 0.5625
Global Iter: 61100 training loss: 0.697876
Global Iter: 61100 training acc: 0.46875
Global Iter: 61200 training loss: 0.697059
Global Iter: 61200 training acc: 0.53125
Global Iter: 61300 training loss: 0.688029
Global Iter: 61300 training acc: 0.59375
Global Iter: 61400 training loss: 0.688041
Global Iter: 61400 training acc: 0.5
Global Iter: 61500 training loss: 0.717308
Global Iter: 61500 training acc: 0.3125
Global Iter: 61600 training loss: 0.671324
Global Iter: 61600 training acc: 0.65625
Global Iter: 61700 training loss: 0.689851
Global Iter: 61700 training acc: 0.5625
Global Iter: 61800 training loss: 0.694753
Global Iter: 61800 training acc: 0.5
Global Iter: 61900 training loss: 0.688204
Global Iter: 61900 training acc: 0.5625
Global Iter: 62000 training loss: 0.687768
Global Iter: 62000 training acc: 0.5625
Global Iter: 62100 training loss: 0.691787
Global Iter: 62100 training acc: 0.53125
Global Iter: 62200 training loss: 0.686643
Global Iter: 62200 training acc: 0.625
Global Iter: 62300 training loss: 0.683345
Global Iter: 62300 training acc: 0.59375
Global Iter: 62400 training loss: 0.695658
Global Iter: 62400 training acc: 0.5
Global Iter: 62500 training loss: 0.671245
Global Iter: 62500 training acc: 0.75
Global Iter: 62600 training loss: 0.674047
Global Iter: 62600 training acc: 0.65625
Global Iter: 62700 training loss: 0.701942
Global Iter: 62700 training acc: 0.46875
Global Iter: 62800 training loss: 0.707819
Global Iter: 62800 training acc: 0.46875
Global Iter: 62900 training loss: 0.694216
Global Iter: 62900 training acc: 0.53125
Global Iter: 63000 training loss: 0.731102
Global Iter: 63000 training acc: 0.21875
Global Iter: 63100 training loss: 0.694159
Global Iter: 63100 training acc: 0.5
Global Iter: 63200 training loss: 0.68978
Global Iter: 63200 training acc: 0.53125
Global Iter: 63300 training loss: 0.69587
Global Iter: 63300 training acc: 0.5
Global Iter: 63400 training loss: 0.680661
Global Iter: 63400 training acc: 0.59375
Global Iter: 63500 training loss: 0.686447
Global Iter: 63500 training acc: 0.5625
Global Iter: 63600 training loss: 0.680933
Global Iter: 63600 training acc: 0.625
Global Iter: 63700 training loss: 0.700668
Global Iter: 63700 training acc: 0.5
Global Iter: 63800 training loss: 0.691294
Global Iter: 63800 training acc: 0.53125
Global Iter: 63900 training loss: 0.69277
Global Iter: 63900 training acc: 0.53125
Global Iter: 64000 training loss: 0.69469
Global Iter: 64000 training acc: 0.5
Global Iter: 64100 training loss: 0.693674
Global Iter: 64100 training acc: 0.53125
Global Iter: 64200 training loss: 0.682652
Global Iter: 64200 training acc: 0.65625
Global Iter: 64300 training loss: 0.689222
Global Iter: 64300 training acc: 0.53125
Global Iter: 64400 training loss: 0.721181
Global Iter: 64400 training acc: 0.34375
Global Iter: 64500 training loss: 0.706516
Global Iter: 64500 training acc: 0.46875
Global Iter: 64600 training loss: 0.690903
Global Iter: 64600 training acc: 0.5
Global Iter: 64700 training loss: 0.687653
Global Iter: 64700 training acc: 0.53125
Global Iter: 64800 training loss: 0.675077
Global Iter: 64800 training acc: 0.65625
Global Iter: 64900 training loss: 0.705373
Global Iter: 64900 training acc: 0.4375
Global Iter: 65000 training loss: 0.682697
Global Iter: 65000 training acc: 0.5625
Global Iter: 65100 training loss: 0.689049
Global Iter: 65100 training acc: 0.5625
Global Iter: 65200 training loss: 0.678595
Global Iter: 65200 training acc: 0.65625
Global Iter: 65300 training loss: 0.686108
Global Iter: 65300 training acc: 0.5625
Global Iter: 65400 training loss: 0.682396
Global Iter: 65400 training acc: 0.6875
Global Iter: 65500 training loss: 0.658873
Global Iter: 65500 training acc: 0.75
Global Iter: 65600 training loss: 0.691246
Global Iter: 65600 training acc: 0.5
Global Iter: 65700 training loss: 0.690384
Global Iter: 65700 training acc: 0.5625
Global Iter: 65800 training loss: 0.69444
Global Iter: 65800 training acc: 0.53125
Global Iter: 65900 training loss: 0.726806
Global Iter: 65900 training acc: 0.28125
Global Iter: 66000 training loss: 0.695446
Global Iter: 66000 training acc: 0.46875
Global Iter: 66100 training loss: 0.703732
Global Iter: 66100 training acc: 0.4375
Global Iter: 66200 training loss: 0.697848
Global Iter: 66200 training acc: 0.5
Global Iter: 66300 training loss: 0.701278
Global Iter: 66300 training acc: 0.40625
Global Iter: 66400 training loss: 0.673443
Global Iter: 66400 training acc: 0.65625
Global Iter: 66500 training loss: 0.693987
Global Iter: 66500 training acc: 0.5625
Global Iter: 66600 training loss: 0.69228
Global Iter: 66600 training acc: 0.53125
Global Iter: 66700 training loss: 0.684899
Global Iter: 66700 training acc: 0.59375
Global Iter: 66800 training loss: 0.692966
Global Iter: 66800 training acc: 0.5
Global Iter: 66900 training loss: 0.690103
Global Iter: 66900 training acc: 0.53125
Global Iter: 67000 training loss: 0.69047
Global Iter: 67000 training acc: 0.5
Global Iter: 67100 training loss: 0.70267
Global Iter: 67100 training acc: 0.5
Global Iter: 67200 training loss: 0.694139
Global Iter: 67200 training acc: 0.53125
Global Iter: 67300 training loss: 0.689334
Global Iter: 67300 training acc: 0.53125
Global Iter: 67400 training loss: 0.715693
Global Iter: 67400 training acc: 0.375
Global Iter: 67500 training loss: 0.693676
Global Iter: 67500 training acc: 0.5625
Global Iter: 67600 training loss: 0.676513
Global Iter: 67600 training acc: 0.625
Global Iter: 67700 training loss: 0.704288
Global Iter: 67700 training acc: 0.4375
Global Iter: 67800 training loss: 0.685839
Global Iter: 67800 training acc: 0.5625
Global Iter: 67900 training loss: 0.683419
Global Iter: 67900 training acc: 0.5625
Global Iter: 68000 training loss: 0.690247
Global Iter: 68000 training acc: 0.5
Global Iter: 68100 training loss: 0.678773
Global Iter: 68100 training acc: 0.5625
Global Iter: 68200 training loss: 0.690324
Global Iter: 68200 training acc: 0.5625
Global Iter: 68300 training loss: 0.699495
Global Iter: 68300 training acc: 0.46875
Global Iter: 68400 training loss: 0.679495
Global Iter: 68400 training acc: 0.65625
Global Iter: 68500 training loss: 0.694767
Global Iter: 68500 training acc: 0.5
Global Iter: 68600 training loss: 0.671247
Global Iter: 68600 training acc: 0.65625
Global Iter: 68700 training loss: 0.676451
Global Iter: 68700 training acc: 0.625
Global Iter: 68800 training loss: 0.711112
Global Iter: 68800 training acc: 0.40625
Global Iter: 68900 training loss: 0.692558
Global Iter: 68900 training acc: 0.5
Global Iter: 69000 training loss: 0.681435
Global Iter: 69000 training acc: 0.59375
Global Iter: 69100 training loss: 0.700683
Global Iter: 69100 training acc: 0.53125
Global Iter: 69200 training loss: 0.691948
Global Iter: 69200 training acc: 0.5
Global Iter: 69300 training loss: 0.704429
Global Iter: 69300 training acc: 0.46875
Global Iter: 69400 training loss: 0.704357
Global Iter: 69400 training acc: 0.4375
Global Iter: 69500 training loss: 0.66268
Global Iter: 69500 training acc: 0.78125
Global Iter: 69600 training loss: 0.698099
Global Iter: 69600 training acc: 0.5
Global Iter: 69700 training loss: 0.686693
Global Iter: 69700 training acc: 0.53125
Global Iter: 69800 training loss: 0.684362
Global Iter: 69800 training acc: 0.625
Global Iter: 69900 training loss: 0.688784
Global Iter: 69900 training acc: 0.5625
Global Iter: 70000 training loss: 0.718
Global Iter: 70000 training acc: 0.3125
Global Iter: 70100 training loss: 0.703244
Global Iter: 70100 training acc: 0.40625
Global Iter: 70200 training loss: 0.677496
Global Iter: 70200 training acc: 0.625
Global Iter: 70300 training loss: 0.709762
Global Iter: 70300 training acc: 0.4375
Global Iter: 70400 training loss: 0.674944
Global Iter: 70400 training acc: 0.6875
Global Iter: 70500 training loss: 0.71365
Global Iter: 70500 training acc: 0.375
Global Iter: 70600 training loss: 0.683102
Global Iter: 70600 training acc: 0.59375
Global Iter: 70700 training loss: 0.685082
Global Iter: 70700 training acc: 0.59375
Global Iter: 70800 training loss: 0.690363
Global Iter: 70800 training acc: 0.5625
Global Iter: 70900 training loss: 0.685735
Global Iter: 70900 training acc: 0.5625
Global Iter: 71000 training loss: 0.696924
Global Iter: 71000 training acc: 0.5
Global Iter: 71100 training loss: 0.687672
Global Iter: 71100 training acc: 0.5625
Global Iter: 71200 training loss: 0.69176
Global Iter: 71200 training acc: 0.53125
Global Iter: 71300 training loss: 0.684505
Global Iter: 71300 training acc: 0.5625
Global Iter: 71400 training loss: 0.714862
Global Iter: 71400 training acc: 0.34375
Global Iter: 71500 training loss: 0.67089
Global Iter: 71500 training acc: 0.71875
Global Iter: 71600 training loss: 0.688737
Global Iter: 71600 training acc: 0.59375
Global Iter: 71700 training loss: 0.707079
Global Iter: 71700 training acc: 0.46875
Global Iter: 71800 training loss: 0.684018
Global Iter: 71800 training acc: 0.59375
Global Iter: 71900 training loss: 0.681833
Global Iter: 71900 training acc: 0.59375
Global Iter: 72000 training loss: 0.70194
Global Iter: 72000 training acc: 0.46875
Global Iter: 72100 training loss: 0.682166
Global Iter: 72100 training acc: 0.65625
Global Iter: 72200 training loss: 0.692674
Global Iter: 72200 training acc: 0.53125
Global Iter: 72300 training loss: 0.699655
Global Iter: 72300 training acc: 0.46875
Global Iter: 72400 training loss: 0.673125
Global Iter: 72400 training acc: 0.75
Global Iter: 72500 training loss: 0.685307
Global Iter: 72500 training acc: 0.625
Global Iter: 72600 training loss: 0.702559
Global Iter: 72600 training acc: 0.40625
Global Iter: 72700 training loss: 0.694787
Global Iter: 72700 training acc: 0.5
Global Iter: 72800 training loss: 0.686525
Global Iter: 72800 training acc: 0.5625
Global Iter: 72900 training loss: 0.73107
Global Iter: 72900 training acc: 0.21875
Global Iter: 73000 training loss: 0.702101
Global Iter: 73000 training acc: 0.4375
Global Iter: 73100 training loss: 0.686803
Global Iter: 73100 training acc: 0.5625
Global Iter: 73200 training loss: 0.69866
Global Iter: 73200 training acc: 0.53125
Global Iter: 73300 training loss: 0.689704
Global Iter: 73300 training acc: 0.59375
Global Iter: 73400 training loss: 0.691594
Global Iter: 73400 training acc: 0.53125
Global Iter: 73500 training loss: 0.672678
Global Iter: 73500 training acc: 0.65625
Global Iter: 73600 training loss: 0.703182
Global Iter: 73600 training acc: 0.46875
Global Iter: 73700 training loss: 0.696184
Global Iter: 73700 training acc: 0.5
Global Iter: 73800 training loss: 0.694156
Global Iter: 73800 training acc: 0.53125
Global Iter: 73900 training loss: 0.694797
Global Iter: 73900 training acc: 0.5
Global Iter: 74000 training loss: 0.684675
Global Iter: 74000 training acc: 0.5625
Global Iter: 74100 training loss: 0.675705
Global Iter: 74100 training acc: 0.71875
Global Iter: 74200 training loss: 0.687979
Global Iter: 74200 training acc: 0.53125
Global Iter: 74300 training loss: 0.710288
Global Iter: 74300 training acc: 0.375
Global Iter: 74400 training loss: 0.6944
Global Iter: 74400 training acc: 0.5
Global Iter: 74500 training loss: 0.696689
Global Iter: 74500 training acc: 0.5
Global Iter: 74600 training loss: 0.695359
Global Iter: 74600 training acc: 0.5
Global Iter: 74700 training loss: 0.664209
Global Iter: 74700 training acc: 0.71875
Global Iter: 74800 training loss: 0.703871
Global Iter: 74800 training acc: 0.46875
Global Iter: 74900 training loss: 0.68517
Global Iter: 74900 training acc: 0.59375
Global Iter: 75000 training loss: 0.678963
Global Iter: 75000 training acc: 0.625
Global Iter: 75100 training loss: 0.680499
Global Iter: 75100 training acc: 0.625
Global Iter: 75200 training loss: 0.693471
Global Iter: 75200 training acc: 0.53125
Global Iter: 75300 training loss: 0.682916
Global Iter: 75300 training acc: 0.65625
Global Iter: 75400 training loss: 0.669525
Global Iter: 75400 training acc: 0.75
Global Iter: 75500 training loss: 0.690405
Global Iter: 75500 training acc: 0.5
Global Iter: 75600 training loss: 0.690173
Global Iter: 75600 training acc: 0.5625
Global Iter: 75700 training loss: 0.69167
Global Iter: 75700 training acc: 0.53125
Global Iter: 75800 training loss: 0.729799
Global Iter: 75800 training acc: 0.25
Global Iter: 75900 training loss: 0.697338
Global Iter: 75900 training acc: 0.46875
Global Iter: 76000 training loss: 0.705557
Global Iter: 76000 training acc: 0.4375
Global Iter: 76100 training loss: 0.692556
Global Iter: 76100 training acc: 0.53125
Global Iter: 76200 training loss: 0.704377
Global Iter: 76200 training acc: 0.375
Global Iter: 76300 training loss: 0.660184
Global Iter: 76300 training acc: 0.71875
Global Iter: 76400 training loss: 0.688585
Global Iter: 76400 training acc: 0.5625
Global Iter: 76500 training loss: 0.695175
Global Iter: 76500 training acc: 0.5
Global Iter: 76600 training loss: 0.685965
Global Iter: 76600 training acc: 0.625
Global Iter: 76700 training loss: 0.701726
Global Iter: 76700 training acc: 0.46875
Global Iter: 76800 training loss: 0.690107
Global Iter: 76800 training acc: 0.5625
Global Iter: 76900 training loss: 0.689175
Global Iter: 76900 training acc: 0.53125
Global Iter: 77000 training loss: 0.69785
Global Iter: 77000 training acc: 0.5
Global Iter: 77100 training loss: 0.693252
Global Iter: 77100 training acc: 0.5
Global Iter: 77200 training loss: 0.688287
Global Iter: 77200 training acc: 0.53125
Global Iter: 77300 training loss: 0.715501
Global Iter: 77300 training acc: 0.375
Global Iter: 77400 training loss: 0.680487
Global Iter: 77400 training acc: 0.625
Global Iter: 77500 training loss: 0.674365
Global Iter: 77500 training acc: 0.625
Global Iter: 77600 training loss: 0.704173
Global Iter: 77600 training acc: 0.4375
Global Iter: 77700 training loss: 0.694829
Global Iter: 77700 training acc: 0.53125
Global Iter: 77800 training loss: 0.691474
Global Iter: 77800 training acc: 0.5625
Global Iter: 77900 training loss: 0.690646
Global Iter: 77900 training acc: 0.5625
Global Iter: 78000 training loss: 0.679236
Global Iter: 78000 training acc: 0.625
Global Iter: 78100 training loss: 0.687852
Global Iter: 78100 training acc: 0.53125
Global Iter: 78200 training loss: 0.698666
Global Iter: 78200 training acc: 0.4375
Global Iter: 78300 training loss: 0.682688
Global Iter: 78300 training acc: 0.625
Global Iter: 78400 training loss: 0.693384
Global Iter: 78400 training acc: 0.5
Global Iter: 78500 training loss: 0.672151
Global Iter: 78500 training acc: 0.6875
Global Iter: 78600 training loss: 0.682198
Global Iter: 78600 training acc: 0.59375
Global Iter: 78700 training loss: 0.713452
Global Iter: 78700 training acc: 0.40625
Global Iter: 78800 training loss: 0.69151
Global Iter: 78800 training acc: 0.53125
Global Iter: 78900 training loss: 0.683442
Global Iter: 78900 training acc: 0.59375
Global Iter: 79000 training loss: 0.693921
Global Iter: 79000 training acc: 0.53125
Global Iter: 79100 training loss: 0.691293
Global Iter: 79100 training acc: 0.53125
Global Iter: 79200 training loss: 0.696148
Global Iter: 79200 training acc: 0.5
Global Iter: 79300 training loss: 0.699332
Global Iter: 79300 training acc: 0.4375
Global Iter: 79400 training loss: 0.66293
Global Iter: 79400 training acc: 0.78125
Global Iter: 79500 training loss: 0.693927
Global Iter: 79500 training acc: 0.46875
Global Iter: 79600 training loss: 0.693773
Global Iter: 79600 training acc: 0.5625
Global Iter: 79700 training loss: 0.679141
Global Iter: 79700 training acc: 0.59375
Global Iter: 79800 training loss: 0.692929
Global Iter: 79800 training acc: 0.5625
Global Iter: 79900 training loss: 0.711531
Global Iter: 79900 training acc: 0.34375
Global Iter: 80000 training loss: 0.707908
Global Iter: 80000 training acc: 0.40625
Global Iter: 80100 training loss: 0.681007
Global Iter: 80100 training acc: 0.5625
Global Iter: 80200 training loss: 0.710249
Global Iter: 80200 training acc: 0.40625
Global Iter: 80300 training loss: 0.680339
Global Iter: 80300 training acc: 0.65625
Global Iter: 80400 training loss: 0.704899
Global Iter: 80400 training acc: 0.40625
Global Iter: 80500 training loss: 0.68884
Global Iter: 80500 training acc: 0.625
Global Iter: 80600 training loss: 0.687764
Global Iter: 80600 training acc: 0.5625
Global Iter: 80700 training loss: 0.693215
Global Iter: 80700 training acc: 0.5625
Global Iter: 80800 training loss: 0.693008
Global Iter: 80800 training acc: 0.5625
Global Iter: 80900 training loss: 0.700919
Global Iter: 80900 training acc: 0.5
Global Iter: 81000 training loss: 0.685452
Global Iter: 81000 training acc: 0.5625
Global Iter: 81100 training loss: 0.686545
Global Iter: 81100 training acc: 0.5625
Global Iter: 81200 training loss: 0.685276
Global Iter: 81200 training acc: 0.5625
Global Iter: 81300 training loss: 0.716191
Global Iter: 81300 training acc: 0.34375
Global Iter: 81400 training loss: 0.676688
Global Iter: 81400 training acc: 0.6875
Global Iter: 81500 training loss: 0.683705
Global Iter: 81500 training acc: 0.59375
Global Iter: 81600 training loss: 0.703616
Global Iter: 81600 training acc: 0.46875
Global Iter: 81700 training loss: 0.687175
Global Iter: 81700 training acc: 0.59375
Global Iter: 81800 training loss: 0.681242
Global Iter: 81800 training acc: 0.59375
Global Iter: 81900 training loss: 0.707404
Global Iter: 81900 training acc: 0.40625
Global Iter: 82000 training loss: 0.680615
Global Iter: 82000 training acc: 0.625
Global Iter: 82100 training loss: 0.687982
Global Iter: 82100 training acc: 0.53125
Global Iter: 82200 training loss: 0.697189
Global Iter: 82200 training acc: 0.5
Global Iter: 82300 training loss: 0.671025
Global Iter: 82300 training acc: 0.75
Global Iter: 82400 training loss: 0.678374
Global Iter: 82400 training acc: 0.65625
Global Iter: 82500 training loss: 0.701696
Global Iter: 82500 training acc: 0.4375
Global Iter: 82600 training loss: 0.69609
Global Iter: 82600 training acc: 0.5
Global Iter: 82700 training loss: 0.683967
Global Iter: 82700 training acc: 0.59375
Global Iter: 82800 training loss: 0.735621
Global Iter: 82800 training acc: 0.21875
Global Iter: 82900 training loss: 0.701367
Global Iter: 82900 training acc: 0.46875
Global Iter: 83000 training loss: 0.691427
Global Iter: 83000 training acc: 0.5625
Global Iter: 83100 training loss: 0.685524
Global Iter: 83100 training acc: 0.59375
Global Iter: 83200 training loss: 0.686205
Global Iter: 83200 training acc: 0.59375
Global Iter: 83300 training loss: 0.704357
Global Iter: 83300 training acc: 0.46875
Global Iter: 83400 training loss: 0.674238
Global Iter: 83400 training acc: 0.625
Global Iter: 83500 training loss: 0.701737
Global Iter: 83500 training acc: 0.4375
Global Iter: 83600 training loss: 0.703872
Global Iter: 83600 training acc: 0.46875
Global Iter: 83700 training loss: 0.685858
Global Iter: 83700 training acc: 0.59375
Global Iter: 83800 training loss: 0.695428
Global Iter: 83800 training acc: 0.5
Global Iter: 83900 training loss: 0.683789
Global Iter: 83900 training acc: 0.5625
Global Iter: 84000 training loss: 0.676966
Global Iter: 84000 training acc: 0.71875
Global Iter: 84100 training loss: 0.687381
Global Iter: 84100 training acc: 0.5625
Global Iter: 84200 training loss: 0.718809
Global Iter: 84200 training acc: 0.34375
Global Iter: 84300 training loss: 0.696622
Global Iter: 84300 training acc: 0.5
Global Iter: 84400 training loss: 0.696469
Global Iter: 84400 training acc: 0.5
Global Iter: 84500 training loss: 0.693731
Global Iter: 84500 training acc: 0.5
Global Iter: 84600 training loss: 0.668748
Global Iter: 84600 training acc: 0.71875
Global Iter: 84700 training loss: 0.699418
Global Iter: 84700 training acc: 0.46875
Global Iter: 84800 training loss: 0.679924
Global Iter: 84800 training acc: 0.59375
Global Iter: 84900 training loss: 0.683668
Global Iter: 84900 training acc: 0.625
Global Iter: 85000 training loss: 0.672284
Global Iter: 85000 training acc: 0.6875
Global Iter: 85100 training loss: 0.688269
Global Iter: 85100 training acc: 0.53125
Global Iter: 85200 training loss: 0.679766
Global Iter: 85200 training acc: 0.65625
Global Iter: 85300 training loss: 0.6702
Global Iter: 85300 training acc: 0.75
Global Iter: 85400 training loss: 0.701221
Global Iter: 85400 training acc: 0.46875
Global Iter: 85500 training loss: 0.699241
Global Iter: 85500 training acc: 0.5
Global Iter: 85600 training loss: 0.688403
Global Iter: 85600 training acc: 0.53125
Global Iter: 85700 training loss: 0.738427
Global Iter: 85700 training acc: 0.21875
Global Iter: 85800 training loss: 0.698175
Global Iter: 85800 training acc: 0.5
Global Iter: 85900 training loss: 0.699376
Global Iter: 85900 training acc: 0.4375
Global Iter: 86000 training loss: 0.692781
Global Iter: 86000 training acc: 0.5
Global Iter: 86100 training loss: 0.706637
Global Iter: 86100 training acc: 0.40625
Global Iter: 86200 training loss: 0.663215
Global Iter: 86200 training acc: 0.6875
Global Iter: 86300 training loss: 0.680763
Global Iter: 86300 training acc: 0.59375
Global Iter: 86400 training loss: 0.706371
Global Iter: 86400 training acc: 0.4375
Global Iter: 86500 training loss: 0.684479
Global Iter: 86500 training acc: 0.59375
Global Iter: 86600 training loss: 0.699274
Global Iter: 86600 training acc: 0.46875
Global Iter: 86700 training loss: 0.687558
Global Iter: 86700 training acc: 0.5625
Global Iter: 86800 training loss: 0.694326
Global Iter: 86800 training acc: 0.53125
Global Iter: 86900 training loss: 0.688497
Global Iter: 86900 training acc: 0.5
Global Iter: 87000 training loss: 0.691212
Global Iter: 87000 training acc: 0.5
Global Iter: 87100 training loss: 0.702487
Global Iter: 87100 training acc: 0.46875
Global Iter: 87200 training loss: 0.720944
Global Iter: 87200 training acc: 0.34375
Global Iter: 87300 training loss: 0.682149
Global Iter: 87300 training acc: 0.59375
Global Iter: 87400 training loss: 0.674359
Global Iter: 87400 training acc: 0.625
Global Iter: 87500 training loss: 0.703065
Global Iter: 87500 training acc: 0.46875
Global Iter: 87600 training loss: 0.695328
Global Iter: 87600 training acc: 0.46875
Global Iter: 87700 training loss: 0.676469
Global Iter: 87700 training acc: 0.59375
Global Iter: 87800 training loss: 0.694383
Global Iter: 87800 training acc: 0.5
Global Iter: 87900 training loss: 0.681957
Global Iter: 87900 training acc: 0.625
Global Iter: 88000 training loss: 0.694
Global Iter: 88000 training acc: 0.5
Global Iter: 88100 training loss: 0.696582
Global Iter: 88100 training acc: 0.46875
Global Iter: 88200 training loss: 0.673303
Global Iter: 88200 training acc: 0.6875
Global Iter: 88300 training loss: 0.696767
Global Iter: 88300 training acc: 0.53125
Global Iter: 88400 training loss: 0.668169
Global Iter: 88400 training acc: 0.71875
Global Iter: 88500 training loss: 0.678409
Global Iter: 88500 training acc: 0.625
Global Iter: 88600 training loss: 0.712977
Global Iter: 88600 training acc: 0.375
Global Iter: 88700 training loss: 0.700484
Global Iter: 88700 training acc: 0.5
Global Iter: 88800 training loss: 0.684524
Global Iter: 88800 training acc: 0.5625
Global Iter: 88900 training loss: 0.686642
Global Iter: 88900 training acc: 0.53125
Global Iter: 89000 training loss: 0.700298
Global Iter: 89000 training acc: 0.46875
Global Iter: 89100 training loss: 0.692673
Global Iter: 89100 training acc: 0.53125
Global Iter: 89200 training loss: 0.705303
Global Iter: 89200 training acc: 0.375
Global Iter: 89300 training loss: 0.666067
Global Iter: 89300 training acc: 0.75
Global Iter: 89400 training loss: 0.705208
Global Iter: 89400 training acc: 0.5
Global Iter: 89500 training loss: 0.690028
Global Iter: 89500 training acc: 0.53125
Global Iter: 89600 training loss: 0.684119
Global Iter: 89600 training acc: 0.59375
Global Iter: 89700 training loss: 0.697746
Global Iter: 89700 training acc: 0.5
Global Iter: 89800 training loss: 0.710294
Global Iter: 89800 training acc: 0.375
Global Iter: 89900 training loss: 0.707408
Global Iter: 89900 training acc: 0.40625
Global Iter: 90000 training loss: 0.680714
Global Iter: 90000 training acc: 0.5625
Global Iter: 90100 training loss: 0.720493
Global Iter: 90100 training acc: 0.375
Global Iter: 90200 training loss: 0.675781
Global Iter: 90200 training acc: 0.65625
Global Iter: 90300 training loss: 0.701638
Global Iter: 90300 training acc: 0.46875
Global Iter: 90400 training loss: 0.684586
Global Iter: 90400 training acc: 0.625
Global Iter: 90500 training loss: 0.683015
Global Iter: 90500 training acc: 0.625
Global Iter: 90600 training loss: 0.68284
Global Iter: 90600 training acc: 0.5625
Global Iter: 90700 training loss: 0.687386
Global Iter: 90700 training acc: 0.5625
Global Iter: 90800 training loss: 0.693123
Global Iter: 90800 training acc: 0.5
Global Iter: 90900 training loss: 0.68665
Global Iter: 90900 training acc: 0.53125
Global Iter: 91000 training loss: 0.685158
Global Iter: 91000 training acc: 0.5625
Global Iter: 91100 training loss: 0.68525
Global Iter: 91100 training acc: 0.59375
Global Iter: 91200 training loss: 0.717831
Global Iter: 91200 training acc: 0.3125
Global Iter: 91300 training loss: 0.66638
Global Iter: 91300 training acc: 0.71875
Global Iter: 91400 training loss: 0.691479
Global Iter: 91400 training acc: 0.53125
Global Iter: 91500 training loss: 0.711811
Global Iter: 91500 training acc: 0.4375
Global Iter: 91600 training loss: 0.682688
Global Iter: 91600 training acc: 0.5625
Global Iter: 91700 training loss: 0.681735
Global Iter: 91700 training acc: 0.625
Global Iter: 91800 training loss: 0.705116
Global Iter: 91800 training acc: 0.4375
Global Iter: 91900 training loss: 0.687178
Global Iter: 91900 training acc: 0.625
Global Iter: 92000 training loss: 0.701988
Global Iter: 92000 training acc: 0.46875
Global Iter: 92100 training loss: 0.69018
Global Iter: 92100 training acc: 0.5
Global Iter: 92200 training loss: 0.666413
Global Iter: 92200 training acc: 0.75
Global Iter: 92300 training loss: 0.675768
Global Iter: 92300 training acc: 0.65625
Global Iter: 92400 training loss: 0.69704
Global Iter: 92400 training acc: 0.4375
Global Iter: 92500 training loss: 0.68046
Global Iter: 92500 training acc: 0.5625
Global Iter: 92600 training loss: 0.688023
Global Iter: 92600 training acc: 0.59375
Global Iter: 92700 training loss: 0.729939
Global Iter: 92700 training acc: 0.21875
Global Iter: 92800 training loss: 0.698645
Global Iter: 92800 training acc: 0.46875
Global Iter: 92900 training loss: 0.687491
Global Iter: 92900 training acc: 0.59375
Global Iter: 93000 training loss: 0.678392
Global Iter: 93000 training acc: 0.625
Global Iter: 93100 training loss: 0.682578
Global Iter: 93100 training acc: 0.65625
Global Iter: 93200 training loss: 0.70319
Global Iter: 93200 training acc: 0.46875
Global Iter: 93300 training loss: 0.672869
Global Iter: 93300 training acc: 0.6875
Global Iter: 93400 training loss: 0.709126
Global Iter: 93400 training acc: 0.4375
Global Iter: 93500 training loss: 0.693119
Global Iter: 93500 training acc: 0.5
Global Iter: 93600 training loss: 0.690951
Global Iter: 93600 training acc: 0.5625
Global Iter: 93700 training loss: 0.694157
Global Iter: 93700 training acc: 0.46875
Global Iter: 93800 training loss: 0.696656
Global Iter: 93800 training acc: 0.5
Global Iter: 93900 training loss: 0.674287
Global Iter: 93900 training acc: 0.65625
Global Iter: 94000 training loss: 0.690152
Global Iter: 94000 training acc: 0.5625
Global Iter: 94100 training loss: 0.711911
Global Iter: 94100 training acc: 0.375
Global Iter: 94200 training loss: 0.695325
Global Iter: 94200 training acc: 0.5
Global Iter: 94300 training loss: 0.694623
Global Iter: 94300 training acc: 0.53125
Global Iter: 94400 training loss: 0.701713
Global Iter: 94400 training acc: 0.4375
Global Iter: 94500 training loss: 0.67467
Global Iter: 94500 training acc: 0.71875
Global Iter: 94600 training loss: 0.700943
Global Iter: 94600 training acc: 0.4375
Global Iter: 94700 training loss: 0.681015
Global Iter: 94700 training acc: 0.625
Global Iter: 94800 training loss: 0.689299
Global Iter: 94800 training acc: 0.59375
Global Iter: 94900 training loss: 0.667268
Global Iter: 94900 training acc: 0.71875
Global Iter: 95000 training loss: 0.697865
Global Iter: 95000 training acc: 0.5
Global Iter: 95100 training loss: 0.680136
Global Iter: 95100 training acc: 0.625
Global Iter: 95200 training loss: 0.665355
Global Iter: 95200 training acc: 0.78125
Global Iter: 95300 training loss: 0.698787
Global Iter: 95300 training acc: 0.4375
Global Iter: 95400 training loss: 0.699636
Global Iter: 95400 training acc: 0.5
Global Iter: 95500 training loss: 0.685383
Global Iter: 95500 training acc: 0.53125
Global Iter: 95600 training loss: 0.736228
Global Iter: 95600 training acc: 0.21875
Global Iter: 95700 training loss: 0.698353
Global Iter: 95700 training acc: 0.53125
Global Iter: 95800 training loss: 0.693106
Global Iter: 95800 training acc: 0.46875
Global Iter: 95900 training loss: 0.69881
Global Iter: 95900 training acc: 0.5
Global Iter: 96000 training loss: 0.703336
Global Iter: 96000 training acc: 0.4375
Global Iter: 96100 training loss: 0.679593
Global Iter: 96100 training acc: 0.65625
Global Iter: 96200 training loss: 0.688048
Global Iter: 96200 training acc: 0.59375
Global Iter: 96300 training loss: 0.700847
Global Iter: 96300 training acc: 0.46875
Global Iter: 96400 training loss: 0.685109
Global Iter: 96400 training acc: 0.59375
Global Iter: 96500 training loss: 0.695434
Global Iter: 96500 training acc: 0.5
Global Iter: 96600 training loss: 0.690671
Global Iter: 96600 training acc: 0.53125
Global Iter: 96700 training loss: 0.697248
Global Iter: 96700 training acc: 0.46875
Global Iter: 96800 training loss: 0.691528
Global Iter: 96800 training acc: 0.53125
Global Iter: 96900 training loss: 0.692687
Global Iter: 96900 training acc: 0.5
Global Iter: 97000 training loss: 0.702616
Global Iter: 97000 training acc: 0.4375
Global Iter: 97100 training loss: 0.713567
Global Iter: 97100 training acc: 0.375
Global Iter: 97200 training loss: 0.682906
Global Iter: 97200 training acc: 0.5625
Global Iter: 97300 training loss: 0.680747
Global Iter: 97300 training acc: 0.625
Global Iter: 97400 training loss: 0.69072
Global Iter: 97400 training acc: 0.53125
Global Iter: 97500 training loss: 0.698453
Global Iter: 97500 training acc: 0.46875
Global Iter: 97600 training loss: 0.677799
Global Iter: 97600 training acc: 0.59375
Global Iter: 97700 training loss: 0.6967
Global Iter: 97700 training acc: 0.46875
Global Iter: 97800 training loss: 0.678202
Global Iter: 97800 training acc: 0.625
Global Iter: 97900 training loss: 0.693333
Global Iter: 97900 training acc: 0.5
Global Iter: 98000 training loss: 0.697844
Global Iter: 98000 training acc: 0.5
Global Iter: 98100 training loss: 0.670349
Global Iter: 98100 training acc: 0.75
Global Iter: 98200 training loss: 0.696914
Global Iter: 98200 training acc: 0.5
Global Iter: 98300 training loss: 0.673179
Global Iter: 98300 training acc: 0.65625
Global Iter: 98400 training loss: 0.682853
Global Iter: 98400 training acc: 0.59375
Global Iter: 98500 training loss: 0.719292
Global Iter: 98500 training acc: 0.375
Global Iter: 98600 training loss: 0.700124
Global Iter: 98600 training acc: 0.5
Global Iter: 98700 training loss: 0.688108
Global Iter: 98700 training acc: 0.5625
Global Iter: 98800 training loss: 0.694022
Global Iter: 98800 training acc: 0.53125
Global Iter: 98900 training loss: 0.694125
Global Iter: 98900 training acc: 0.59375
Global Iter: 99000 training loss: 0.691882
Global Iter: 99000 training acc: 0.53125
Global Iter: 99100 training loss: 0.70086
Global Iter: 99100 training acc: 0.4375
Global Iter: 99200 training loss: 0.663787
Global Iter: 99200 training acc: 0.71875
Global Iter: 99300 training loss: 0.692993
Global Iter: 99300 training acc: 0.5
Global Iter: 99400 training loss: 0.686723
Global Iter: 99400 training acc: 0.5625
Global Iter: 99500 training loss: 0.682058
Global Iter: 99500 training acc: 0.59375
Global Iter: 99600 training loss: 0.697249
Global Iter: 99600 training acc: 0.4375
Global Iter: 99700 training loss: 0.704818
Global Iter: 99700 training acc: 0.34375
Global Iter: 99800 training loss: 0.705435
Global Iter: 99800 training acc: 0.40625
Global Iter: 99900 training loss: 0.684865
Global Iter: 99900 training acc: 0.5625
Global Iter: 100000 training loss: 0.707014
Global Iter: 100000 training acc: 0.375
Global Iter: 100100 training loss: 0.67771
Global Iter: 100100 training acc: 0.65625
Global Iter: 100200 training loss: 0.699792
Global Iter: 100200 training acc: 0.4375
Global Iter: 100300 training loss: 0.684106
Global Iter: 100300 training acc: 0.5625
Global Iter: 100400 training loss: 0.679901
Global Iter: 100400 training acc: 0.625
Global Iter: 100500 training loss: 0.685583
Global Iter: 100500 training acc: 0.5625
Global Iter: 100600 training loss: 0.696065
Global Iter: 100600 training acc: 0.5
Global Iter: 100700 training loss: 0.696856
Global Iter: 100700 training acc: 0.46875
Global Iter: 100800 training loss: 0.692986
Global Iter: 100800 training acc: 0.5625
Global Iter: 100900 training loss: 0.682023
Global Iter: 100900 training acc: 0.5625
Global Iter: 101000 training loss: 0.6836
Global Iter: 101000 training acc: 0.59375
Global Iter: 101100 training loss: 0.708227
Global Iter: 101100 training acc: 0.375
Global Iter: 101200 training loss: 0.661827
Global Iter: 101200 training acc: 0.71875
Global Iter: 101300 training loss: 0.692704
Global Iter: 101300 training acc: 0.53125
Global Iter: 101400 training loss: 0.700877
Global Iter: 101400 training acc: 0.4375
Global Iter: 101500 training loss: 0.693096
Global Iter: 101500 training acc: 0.53125
Global Iter: 101600 training loss: 0.681092
Global Iter: 101600 training acc: 0.59375
Global Iter: 101700 training loss: 0.700728
Global Iter: 101700 training acc: 0.46875
Global Iter: 101800 training loss: 0.683975
Global Iter: 101800 training acc: 0.625
Global Iter: 101900 training loss: 0.685066
Global Iter: 101900 training acc: 0.53125
Global Iter: 102000 training loss: 0.689525
Global Iter: 102000 training acc: 0.53125
Global Iter: 102100 training loss: 0.666243
Global Iter: 102100 training acc: 0.71875
Global Iter: 102200 training loss: 0.672602
Global Iter: 102200 training acc: 0.65625
Global Iter: 102300 training loss: 0.710103
Global Iter: 102300 training acc: 0.40625
Global Iter: 102400 training loss: 0.679083
Global Iter: 102400 training acc: 0.59375
Global Iter: 102500 training loss: 0.684713
Global Iter: 102500 training acc: 0.59375
Global Iter: 102600 training loss: 0.726836
Global Iter: 102600 training acc: 0.21875
Global Iter: 102700 training loss: 0.707063
Global Iter: 102700 training acc: 0.4375
Global Iter: 102800 training loss: 0.692871
Global Iter: 102800 training acc: 0.5625
Global Iter: 102900 training loss: 0.678157
Global Iter: 102900 training acc: 0.625
Global Iter: 103000 training loss: 0.676721
Global Iter: 103000 training acc: 0.6875
Global Iter: 103100 training loss: 0.701419
Global Iter: 103100 training acc: 0.46875
Global Iter: 103200 training loss: 0.669834
Global Iter: 103200 training acc: 0.71875
Global Iter: 103300 training loss: 0.696417
Global Iter: 103300 training acc: 0.5
Global Iter: 103400 training loss: 0.694734
Global Iter: 103400 training acc: 0.53125
Global Iter: 103500 training loss: 0.678652
Global Iter: 103500 training acc: 0.625
Global Iter: 103600 training loss: 0.699316
Global Iter: 103600 training acc: 0.46875
Global Iter: 103700 training loss: 0.704865
Global Iter: 103700 training acc: 0.4375
Global Iter: 103800 training loss: 0.678421
Global Iter: 103800 training acc: 0.65625
Global Iter: 103900 training loss: 0.694472
Global Iter: 103900 training acc: 0.53125
Global Iter: 104000 training loss: 0.714233
Global Iter: 104000 training acc: 0.34375
Global Iter: 104100 training loss: 0.690182
Global Iter: 104100 training acc: 0.53125
Global Iter: 104200 training loss: 0.68583
Global Iter: 104200 training acc: 0.5625
Global Iter: 104300 training loss: 0.702239
Global Iter: 104300 training acc: 0.4375
Global Iter: 104400 training loss: 0.667973
Global Iter: 104400 training acc: 0.71875
Global Iter: 104500 training loss: 0.707917
Global Iter: 104500 training acc: 0.40625
Global Iter: 104600 training loss: 0.685738
Global Iter: 104600 training acc: 0.5625
Global Iter: 104700 training loss: 0.67846
Global Iter: 104700 training acc: 0.625
Global Iter: 104800 training loss: 0.67291
Global Iter: 104800 training acc: 0.6875
Global Iter: 104900 training loss: 0.690464
Global Iter: 104900 training acc: 0.5
Global Iter: 105000 training loss: 0.680821
Global Iter: 105000 training acc: 0.65625
Global Iter: 105100 training loss: 0.65817
Global Iter: 105100 training acc: 0.75
Global Iter: 105200 training loss: 0.702237
Global Iter: 105200 training acc: 0.4375
Global Iter: 105300 training loss: 0.692516
Global Iter: 105300 training acc: 0.5
Global Iter: 105400 training loss: 0.693508
Global Iter: 105400 training acc: 0.5
Global Iter: 105500 training loss: 0.736057
Global Iter: 105500 training acc: 0.25
Global Iter: 105600 training loss: 0.695338
Global Iter: 105600 training acc: 0.5
Global Iter: 105700 training loss: 0.700472
Global Iter: 105700 training acc: 0.46875
Global Iter: 105800 training loss: 0.698963
Global Iter: 105800 training acc: 0.46875
Global Iter: 105900 training loss: 0.699491
Global Iter: 105900 training acc: 0.46875
Global Iter: 106000 training loss: 0.677451
Global Iter: 106000 training acc: 0.65625
Global Iter: 106100 training loss: 0.691508
Global Iter: 106100 training acc: 0.59375
Global Iter: 106200 training loss: 0.700942
Global Iter: 106200 training acc: 0.4375
Global Iter: 106300 training loss: 0.68112
Global Iter: 106300 training acc: 0.59375
Global Iter: 106400 training loss: 0.698102
Global Iter: 106400 training acc: 0.5
Global Iter: 106500 training loss: 0.685375
Global Iter: 106500 training acc: 0.5625
Global Iter: 106600 training loss: 0.698734
Global Iter: 106600 training acc: 0.46875
Global Iter: 106700 training loss: 0.689876
Global Iter: 106700 training acc: 0.53125
Global Iter: 106800 training loss: 0.690629
Global Iter: 106800 training acc: 0.5
Global Iter: 106900 training loss: 0.7012
Global Iter: 106900 training acc: 0.4375
Global Iter: 107000 training loss: 0.702597
Global Iter: 107000 training acc: 0.375
Global Iter: 107100 training loss: 0.69533
Global Iter: 107100 training acc: 0.53125
Global Iter: 107200 training loss: 0.675696
Global Iter: 107200 training acc: 0.65625
Global Iter: 107300 training loss: 0.681829
Global Iter: 107300 training acc: 0.59375
Global Iter: 107400 training loss: 0.699411
Global Iter: 107400 training acc: 0.5
Global Iter: 107500 training loss: 0.685229
Global Iter: 107500 training acc: 0.5625
Global Iter: 107600 training loss: 0.692044
Global Iter: 107600 training acc: 0.46875
Global Iter: 107700 training loss: 0.690995
Global Iter: 107700 training acc: 0.5625
Global Iter: 107800 training loss: 0.698942
Global Iter: 107800 training acc: 0.5
Global Iter: 107900 training loss: 0.687771
Global Iter: 107900 training acc: 0.5
Global Iter: 108000 training loss: 0.667715
Global Iter: 108000 training acc: 0.75
Global Iter: 108100 training loss: 0.692893
Global Iter: 108100 training acc: 0.46875
Global Iter: 108200 training loss: 0.67901
Global Iter: 108200 training acc: 0.625
Global Iter: 108300 training loss: 0.684451
Global Iter: 108300 training acc: 0.59375
Global Iter: 108400 training loss: 0.715267
Global Iter: 108400 training acc: 0.34375
Global Iter: 108500 training loss: 0.702749
Global Iter: 108500 training acc: 0.46875
Global Iter: 108600 training loss: 0.6889
Global Iter: 108600 training acc: 0.5625
Global Iter: 108700 training loss: 0.696265
Global Iter: 108700 training acc: 0.53125
Global Iter: 108800 training loss: 0.692268
Global Iter: 108800 training acc: 0.5
Global Iter: 108900 training loss: 0.688025
Global Iter: 108900 training acc: 0.59375
Global Iter: 109000 training loss: 0.691541
Global Iter: 109000 training acc: 0.5
Global Iter: 109100 training loss: 0.681967
Global Iter: 109100 training acc: 0.65625
Global Iter: 109200 training loss: 0.688207
Global Iter: 109200 training acc: 0.5625
Global Iter: 109300 training loss: 0.694682
Global Iter: 109300 training acc: 0.5625
Global Iter: 109400 training loss: 0.683736
Global Iter: 109400 training acc: 0.59375
Global Iter: 109500 training loss: 0.703105
Global Iter: 109500 training acc: 0.40625
Global Iter: 109600 training loss: 0.710414
Global Iter: 109600 training acc: 0.375
Global Iter: 109700 training loss: 0.709914
Global Iter: 109700 training acc: 0.40625
Global Iter: 109800 training loss: 0.689156
Global Iter: 109800 training acc: 0.5625
Global Iter: 109900 training loss: 0.70542
Global Iter: 109900 training acc: 0.40625
Global Iter: 110000 training loss: 0.681515
Global Iter: 110000 training acc: 0.59375
Global Iter: 110100 training loss: 0.694685
Global Iter: 110100 training acc: 0.5
Global Iter: 110200 training loss: 0.695797
Global Iter: 110200 training acc: 0.5
Global Iter: 110300 training loss: 0.68428
Global Iter: 110300 training acc: 0.625
Global Iter: 110400 training loss: 0.68631
Global Iter: 110400 training acc: 0.59375
Global Iter: 110500 training loss: 0.689744
Global Iter: 110500 training acc: 0.53125
Global Iter: 110600 training loss: 0.696238
Global Iter: 110600 training acc: 0.5
Global Iter: 110700 training loss: 0.68784
Global Iter: 110700 training acc: 0.53125
Global Iter: 110800 training loss: 0.69741
Global Iter: 110800 training acc: 0.5
Global Iter: 110900 training loss: 0.681884
Global Iter: 110900 training acc: 0.59375
Global Iter: 111000 training loss: 0.70738
Global Iter: 111000 training acc: 0.375
Global Iter: 111100 training loss: 0.665677
Global Iter: 111100 training acc: 0.71875
Global Iter: 111200 training loss: 0.69005
Global Iter: 111200 training acc: 0.5625
Global Iter: 111300 training loss: 0.696108
Global Iter: 111300 training acc: 0.5
Global Iter: 111400 training loss: 0.677898
Global Iter: 111400 training acc: 0.5625
Global Iter: 111500 training loss: 0.689006
Global Iter: 111500 training acc: 0.53125
Global Iter: 111600 training loss: 0.704356
Global Iter: 111600 training acc: 0.46875
Global Iter: 111700 training loss: 0.680506
Global Iter: 111700 training acc: 0.65625
Global Iter: 111800 training loss: 0.706796
Global Iter: 111800 training acc: 0.46875
Global Iter: 111900 training loss: 0.696789
Global Iter: 111900 training acc: 0.46875
Global Iter: 112000 training loss: 0.659856
Global Iter: 112000 training acc: 0.75
Global Iter: 112100 training loss: 0.678539
Global Iter: 112100 training acc: 0.625
Global Iter: 112200 training loss: 0.701906
Global Iter: 112200 training acc: 0.40625
Global Iter: 112300 training loss: 0.686239
Global Iter: 112300 training acc: 0.59375
Global Iter: 112400 training loss: 0.681633
Global Iter: 112400 training acc: 0.625
Global Iter: 112500 training loss: 0.730508
Global Iter: 112500 training acc: 0.21875
Global Iter: 112600 training loss: 0.709279
Global Iter: 112600 training acc: 0.40625
Global Iter: 112700 training loss: 0.689408
Global Iter: 112700 training acc: 0.5625
Global Iter: 112800 training loss: 0.683476
Global Iter: 112800 training acc: 0.59375
Global Iter: 112900 training loss: 0.67608
Global Iter: 112900 training acc: 0.6875
Global Iter: 113000 training loss: 0.696839
Global Iter: 113000 training acc: 0.46875
Global Iter: 113100 training loss: 0.671147
Global Iter: 113100 training acc: 0.71875
Global Iter: 113200 training loss: 0.702975
Global Iter: 113200 training acc: 0.53125
Global Iter: 113300 training loss: 0.69917
Global Iter: 113300 training acc: 0.46875
Global Iter: 113400 training loss: 0.686596
Global Iter: 113400 training acc: 0.59375
Global Iter: 113500 training loss: 0.703925
Global Iter: 113500 training acc: 0.4375
Global Iter: 113600 training loss: 0.705936
Global Iter: 113600 training acc: 0.40625
Global Iter: 113700 training loss: 0.681192
Global Iter: 113700 training acc: 0.625
Global Iter: 113800 training loss: 0.688474
Global Iter: 113800 training acc: 0.53125
Global Iter: 113900 training loss: 0.721571
Global Iter: 113900 training acc: 0.3125
Global Iter: 114000 training loss: 0.695369
Global Iter: 114000 training acc: 0.5
Global Iter: 114100 training loss: 0.688842
Global Iter: 114100 training acc: 0.5625
Global Iter: 114200 training loss: 0.710712
Global Iter: 114200 training acc: 0.4375
Global Iter: 114300 training loss: 0.675972
Global Iter: 114300 training acc: 0.6875
Global Iter: 114400 training loss: 0.701482
Global Iter: 114400 training acc: 0.4375
Global Iter: 114500 training loss: 0.691417
Global Iter: 114500 training acc: 0.53125
Global Iter: 114600 training loss: 0.685016
Global Iter: 114600 training acc: 0.59375
Global Iter: 114700 training loss: 0.676506
Global Iter: 114700 training acc: 0.625
Global Iter: 114800 training loss: 0.683917
Global Iter: 114800 training acc: 0.5625
Global Iter: 114900 training loss: 0.670972
Global Iter: 114900 training acc: 0.65625
Global Iter: 115000 training loss: 0.665593
Global Iter: 115000 training acc: 0.75
Global Iter: 115100 training loss: 0.703625
Global Iter: 115100 training acc: 0.4375
Global Iter: 115200 training loss: 0.703413
Global Iter: 115200 training acc: 0.46875
Global Iter: 115300 training loss: 0.697348
Global Iter: 115300 training acc: 0.5
Global Iter: 115400 training loss: 0.730664
Global Iter: 115400 training acc: 0.25
Global Iter: 115500 training loss: 0.696944
Global Iter: 115500 training acc: 0.5
Global Iter: 115600 training loss: 0.701689
Global Iter: 115600 training acc: 0.46875
Global Iter: 115700 training loss: 0.69215
Global Iter: 115700 training acc: 0.53125
Global Iter: 115800 training loss: 0.698433
Global Iter: 115800 training acc: 0.5
Global Iter: 115900 training loss: 0.683697
Global Iter: 115900 training acc: 0.625
Global Iter: 116000 training loss: 0.681805
Global Iter: 116000 training acc: 0.59375
Global Iter: 116100 training loss: 0.703461
Global Iter: 116100 training acc: 0.4375
Global Iter: 116200 training loss: 0.678705
Global Iter: 116200 training acc: 0.59375
Global Iter: 116300 training loss: 0.696738
Global Iter: 116300 training acc: 0.46875
Global Iter: 116400 training loss: 0.687949
Global Iter: 116400 training acc: 0.5625
Global Iter: 116500 training loss: 0.707615
Global Iter: 116500 training acc: 0.40625
Global Iter: 116600 training loss: 0.681125
Global Iter: 116600 training acc: 0.59375
Global Iter: 116700 training loss: 0.69567
Global Iter: 116700 training acc: 0.5
Global Iter: 116800 training loss: 0.707352
Global Iter: 116800 training acc: 0.4375
Global Iter: 116900 training loss: 0.715709
Global Iter: 116900 training acc: 0.375
Global Iter: 117000 training loss: 0.702052
Global Iter: 117000 training acc: 0.46875
Global Iter: 117100 training loss: 0.677351
Global Iter: 117100 training acc: 0.65625
Global Iter: 117200 training loss: 0.686401
Global Iter: 117200 training acc: 0.5625
Global Iter: 117300 training loss: 0.702239
Global Iter: 117300 training acc: 0.46875
Global Iter: 117400 training loss: 0.690459
Global Iter: 117400 training acc: 0.5625
Global Iter: 117500 training loss: 0.703096
Global Iter: 117500 training acc: 0.4375
Global Iter: 117600 training loss: 0.69051
Global Iter: 117600 training acc: 0.59375
Global Iter: 117700 training loss: 0.701774
Global Iter: 117700 training acc: 0.5
Global Iter: 117800 training loss: 0.68163
Global Iter: 117800 training acc: 0.59375
Global Iter: 117900 training loss: 0.664908
Global Iter: 117900 training acc: 0.78125
Global Iter: 118000 training loss: 0.689595
Global Iter: 118000 training acc: 0.5
Global Iter: 118100 training loss: 0.680509
Global Iter: 118100 training acc: 0.59375
Global Iter: 118200 training loss: 0.68337
Global Iter: 118200 training acc: 0.59375
Global Iter: 118300 training loss: 0.720147
Global Iter: 118300 training acc: 0.34375
Global Iter: 118400 training loss: 0.695966
Global Iter: 118400 training acc: 0.5
Global Iter: 118500 training loss: 0.701499
Global Iter: 118500 training acc: 0.5
Global Iter: 118600 training loss: 0.689323
Global Iter: 118600 training acc: 0.5
Global Iter: 118700 training loss: 0.69725
Global Iter: 118700 training acc: 0.5
Global Iter: 118800 training loss: 0.686123
Global Iter: 118800 training acc: 0.59375
Global Iter: 118900 training loss: 0.687724
Global Iter: 118900 training acc: 0.5
Global Iter: 119000 training loss: 0.677959
Global Iter: 119000 training acc: 0.625
Global Iter: 119100 training loss: 0.679897
Global Iter: 119100 training acc: 0.59375
Global Iter: 119200 training loss: 0.692048
Global Iter: 119200 training acc: 0.53125
Global Iter: 119300 training loss: 0.676319
Global Iter: 119300 training acc: 0.625
Global Iter: 119400 training loss: 0.704124
Global Iter: 119400 training acc: 0.4375
Global Iter: 119500 training loss: 0.709877
Global Iter: 119500 training acc: 0.375
Global Iter: 119600 training loss: 0.699802
Global Iter: 119600 training acc: 0.4375
Global Iter: 119700 training loss: 0.690431
Global Iter: 119700 training acc: 0.53125
Global Iter: 119800 training loss: 0.710633
Global Iter: 119800 training acc: 0.40625
Global Iter: 119900 training loss: 0.673642
Global Iter: 119900 training acc: 0.625
Global Iter: 120000 training loss: 0.698098
Global Iter: 120000 training acc: 0.53125
Global Iter: 120100 training loss: 0.701467
Global Iter: 120100 training acc: 0.46875
Global Iter: 120200 training loss: 0.679409
Global Iter: 120200 training acc: 0.59375
Global Iter: 120300 training loss: 0.685348
Global Iter: 120300 training acc: 0.5625
Global Iter: 120400 training loss: 0.690702
Global Iter: 120400 training acc: 0.46875
Global Iter: 120500 training loss: 0.688685
Global Iter: 120500 training acc: 0.53125
Global Iter: 120600 training loss: 0.687647
Global Iter: 120600 training acc: 0.5625
Global Iter: 120700 training loss: 0.698774
Global Iter: 120700 training acc: 0.46875
Global Iter: 120800 training loss: 0.682474
Global Iter: 120800 training acc: 0.65625
Global Iter: 120900 training loss: 0.708178
Global Iter: 120900 training acc: 0.375
Global Iter: 121000 training loss: 0.667817
Global Iter: 121000 training acc: 0.6875
Global Iter: 121100 training loss: 0.685906
Global Iter: 121100 training acc: 0.625
Global Iter: 121200 training loss: 0.691659
Global Iter: 121200 training acc: 0.5
Global Iter: 121300 training loss: 0.687402
Global Iter: 121300 training acc: 0.5625
Global Iter: 121400 training loss: 0.6859
Global Iter: 121400 training acc: 0.5625
Global Iter: 121500 training loss: 0.701725
Global Iter: 121500 training acc: 0.46875
Global Iter: 121600 training loss: 0.68404
Global Iter: 121600 training acc: 0.625
Global Iter: 121700 training loss: 0.706253
Global Iter: 121700 training acc: 0.40625
Global Iter: 121800 training loss: 0.707037
Global Iter: 121800 training acc: 0.40625
Global Iter: 121900 training loss: 0.670947
Global Iter: 121900 training acc: 0.71875
Global Iter: 122000 training loss: 0.677171
Global Iter: 122000 training acc: 0.59375
Global Iter: 122100 training loss: 0.692948
Global Iter: 122100 training acc: 0.46875
Global Iter: 122200 training loss: 0.684638
Global Iter: 122200 training acc: 0.5625
Global Iter: 122300 training loss: 0.688501
Global Iter: 122300 training acc: 0.59375
Global Iter: 122400 training loss: 0.724884
Global Iter: 122400 training acc: 0.25
Global Iter: 122500 training loss: 0.702146
Global Iter: 122500 training acc: 0.4375
Global Iter: 122600 training loss: 0.680269
Global Iter: 122600 training acc: 0.59375
Global Iter: 122700 training loss: 0.690243
Global Iter: 122700 training acc: 0.53125
Global Iter: 122800 training loss: 0.676179
Global Iter: 122800 training acc: 0.65625
Global Iter: 122900 training loss: 0.707601
Global Iter: 122900 training acc: 0.40625
Global Iter: 123000 training loss: 0.674246
Global Iter: 123000 training acc: 0.6875
Global Iter: 123100 training loss: 0.690759
Global Iter: 123100 training acc: 0.53125
Global Iter: 123200 training loss: 0.699411
Global Iter: 123200 training acc: 0.46875
Global Iter: 123300 training loss: 0.685245
Global Iter: 123300 training acc: 0.59375
Global Iter: 123400 training loss: 0.704351
Global Iter: 123400 training acc: 0.4375
Global Iter: 123500 training loss: 0.705384
Global Iter: 123500 training acc: 0.4375
Global Iter: 123600 training loss: 0.675795
Global Iter: 123600 training acc: 0.65625
Global Iter: 123700 training loss: 0.682901
Global Iter: 123700 training acc: 0.5625
Global Iter: 123800 training loss: 0.720532
Global Iter: 123800 training acc: 0.3125
Global Iter: 123900 training loss: 0.68654
Global Iter: 123900 training acc: 0.5625
Global Iter: 124000 training loss: 0.686882
Global Iter: 124000 training acc: 0.5625
Global Iter: 124100 training loss: 0.699023
Global Iter: 124100 training acc: 0.5
Global Iter: 124200 training loss: 0.679704
Global Iter: 124200 training acc: 0.625
Global Iter: 124300 training loss: 0.701635
Global Iter: 124300 training acc: 0.4375
Global Iter: 124400 training loss: 0.691101
Global Iter: 124400 training acc: 0.53125
Global Iter: 124500 training loss: 0.685065
Global Iter: 124500 training acc: 0.5625
Global Iter: 124600 training loss: 0.679153
Global Iter: 124600 training acc: 0.625
Global Iter: 124700 training loss: 0.689538
Global Iter: 124700 training acc: 0.5625
Global Iter: 124800 training loss: 0.676389
Global Iter: 124800 training acc: 0.65625
Global Iter: 124900 training loss: 0.656889
Global Iter: 124900 training acc: 0.78125
Global Iter: 125000 training loss: 0.706547
Global Iter: 125000 training acc: 0.40625
Global Iter: 125100 training loss: 0.701428
Global Iter: 125100 training acc: 0.46875
Global Iter: 125200 training loss: 0.694456
Global Iter: 125200 training acc: 0.46875
Global Iter: 125300 training loss: 0.725019
Global Iter: 125300 training acc: 0.25
Global Iter: 125400 training loss: 0.697654
Global Iter: 125400 training acc: 0.5
Global Iter: 125500 training loss: 0.688445
Global Iter: 125500 training acc: 0.53125
Global Iter: 125600 training loss: 0.689099
Global Iter: 125600 training acc: 0.53125
Global Iter: 125700 training loss: 0.688029
Global Iter: 125700 training acc: 0.5625
Global Iter: 125800 training loss: 0.67599
Global Iter: 125800 training acc: 0.625
Global Iter: 125900 training loss: 0.683915
Global Iter: 125900 training acc: 0.5625
Global Iter: 126000 training loss: 0.699646
Global Iter: 126000 training acc: 0.5
Global Iter: 126100 training loss: 0.687349
Global Iter: 126100 training acc: 0.59375
Global Iter: 126200 training loss: 0.695097
Global Iter: 126200 training acc: 0.53125
Global Iter: 126300 training loss: 0.695306
Global Iter: 126300 training acc: 0.5
Global Iter: 126400 training loss: 0.705929
Global Iter: 126400 training acc: 0.4375
Global Iter: 126500 training loss: 0.687606
Global Iter: 126500 training acc: 0.59375
Global Iter: 126600 training loss: 0.697783
Global Iter: 126600 training acc: 0.5
Global Iter: 126700 training loss: 0.710083
Global Iter: 126700 training acc: 0.375
Global Iter: 126800 training loss: 0.708973
Global Iter: 126800 training acc: 0.375
Global Iter: 126900 training loss: 0.693788
Global Iter: 126900 training acc: 0.5
Global Iter: 127000 training loss: 0.6782
Global Iter: 127000 training acc: 0.625
Global Iter: 127100 training loss: 0.687057
Global Iter: 127100 training acc: 0.5625
Global Iter: 127200 training loss: 0.708334
Global Iter: 127200 training acc: 0.4375
Global Iter: 127300 training loss: 0.682564
Global Iter: 127300 training acc: 0.59375
Global Iter: 127400 training loss: 0.692764
Global Iter: 127400 training acc: 0.5
Global Iter: 127500 training loss: 0.682593
Global Iter: 127500 training acc: 0.5625
Global Iter: 127600 training loss: 0.694088
Global Iter: 127600 training acc: 0.5
Global Iter: 127700 training loss: 0.691001
Global Iter: 127700 training acc: 0.59375
Global Iter: 127800 training loss: 0.663807
Global Iter: 127800 training acc: 0.75
Global Iter: 127900 training loss: 0.694669
Global Iter: 127900 training acc: 0.53125
Global Iter: 128000 training loss: 0.690764
Global Iter: 128000 training acc: 0.5625
Global Iter: 128100 training loss: 0.691153
Global Iter: 128100 training acc: 0.5625
Global Iter: 128200 training loss: 0.718018
Global Iter: 128200 training acc: 0.375
Global Iter: 128300 training loss: 0.700262
Global Iter: 128300 training acc: 0.5
Global Iter: 128400 training loss: 0.699845
Global Iter: 128400 training acc: 0.46875
Global Iter: 128500 training loss: 0.688788
Global Iter: 128500 training acc: 0.5
Global Iter: 128600 training loss: 0.696499
Global Iter: 128600 training acc: 0.5
Global Iter: 128700 training loss: 0.671491
Global Iter: 128700 training acc: 0.625
Global Iter: 128800 training loss: 0.692309
Global Iter: 128800 training acc: 0.5625
Global Iter: 128900 training loss: 0.68539
Global Iter: 128900 training acc: 0.625
Global Iter: 129000 training loss: 0.682773
Global Iter: 129000 training acc: 0.625
Global Iter: 129100 training loss: 0.684367
Global Iter: 129100 training acc: 0.5625
Global Iter: 129200 training loss: 0.686716
Global Iter: 129200 training acc: 0.59375
Global Iter: 129300 training loss: 0.702328
Global Iter: 129300 training acc: 0.4375
Global Iter: 129400 training loss: 0.704754
Global Iter: 129400 training acc: 0.4375
Global Iter: 129500 training loss: 0.697592
Global Iter: 129500 training acc: 0.46875
Global Iter: 129600 training loss: 0.689741
Global Iter: 129600 training acc: 0.53125
Global Iter: 129700 training loss: 0.717773
Global Iter: 129700 training acc: 0.375
Global Iter: 129800 training loss: 0.684598
Global Iter: 129800 training acc: 0.625
Global Iter: 129900 training loss: 0.693576
Global Iter: 129900 training acc: 0.53125
Global Iter: 130000 training loss: 0.705649
Global Iter: 130000 training acc: 0.4375
Global Iter: 130100 training loss: 0.682613
Global Iter: 130100 training acc: 0.59375
Global Iter: 130200 training loss: 0.687907
Global Iter: 130200 training acc: 0.5625
Global Iter: 130300 training loss: 0.689426
Global Iter: 130300 training acc: 0.53125
Global Iter: 130400 training loss: 0.701681
Global Iter: 130400 training acc: 0.5
Global Iter: 130500 training loss: 0.69973
Global Iter: 130500 training acc: 0.53125
Global Iter: 130600 training loss: 0.704695
Global Iter: 130600 training acc: 0.4375
Global Iter: 130700 training loss: 0.681758
Global Iter: 130700 training acc: 0.625
Global Iter: 130800 training loss: 0.709896
Global Iter: 130800 training acc: 0.40625
Global Iter: 130900 training loss: 0.673573
Global Iter: 130900 training acc: 0.6875
Global Iter: 131000 training loss: 0.68161
Global Iter: 131000 training acc: 0.65625
Global Iter: 131100 training loss: 0.685789
Global Iter: 131100 training acc: 0.53125
Global Iter: 131200 training loss: 0.692616
Global Iter: 131200 training acc: 0.53125
Global Iter: 131300 training loss: 0.696527
Global Iter: 131300 training acc: 0.53125
Global Iter: 131400 training loss: 0.697465
Global Iter: 131400 training acc: 0.46875
Global Iter: 131500 training loss: 0.689036
Global Iter: 131500 training acc: 0.5625
Global Iter: 131600 training loss: 0.705245
Global Iter: 131600 training acc: 0.4375
Global Iter: 131700 training loss: 0.702734
Global Iter: 131700 training acc: 0.40625
Global Iter: 131800 training loss: 0.667992
Global Iter: 131800 training acc: 0.75
Global Iter: 131900 training loss: 0.688485
Global Iter: 131900 training acc: 0.5625
Global Iter: 132000 training loss: 0.701391
Global Iter: 132000 training acc: 0.46875
Global Iter: 132100 training loss: 0.691997
Global Iter: 132100 training acc: 0.53125
Global Iter: 132200 training loss: 0.686999
Global Iter: 132200 training acc: 0.5625
Global Iter: 132300 training loss: 0.721819
Global Iter: 132300 training acc: 0.28125
Global Iter: 132400 training loss: 0.704059
Global Iter: 132400 training acc: 0.4375
Global Iter: 132500 training loss: 0.678825
Global Iter: 132500 training acc: 0.625
Global Iter: 132600 training loss: 0.699942
Global Iter: 132600 training acc: 0.5
Global Iter: 132700 training loss: 0.671963
Global Iter: 132700 training acc: 0.71875
Global Iter: 132800 training loss: 0.717809
Global Iter: 132800 training acc: 0.375
Global Iter: 132900 training loss: 0.672091
Global Iter: 132900 training acc: 0.71875
Global Iter: 133000 training loss: 0.687865
Global Iter: 133000 training acc: 0.5625
Global Iter: 133100 training loss: 0.706953
Global Iter: 133100 training acc: 0.4375
Global Iter: 133200 training loss: 0.687807
Global Iter: 133200 training acc: 0.625
Global Iter: 133300 training loss: 0.699522
Global Iter: 133300 training acc: 0.4375
Global Iter: 133400 training loss: 0.705576
Global Iter: 133400 training acc: 0.4375
Global Iter: 133500 training loss: 0.68479
Global Iter: 133500 training acc: 0.65625
Global Iter: 133600 training loss: 0.690596
Global Iter: 133600 training acc: 0.53125
Global Iter: 133700 training loss: 0.730088
Global Iter: 133700 training acc: 0.28125
Global Iter: 133800 training loss: 0.686229
Global Iter: 133800 training acc: 0.5625
Global Iter: 133900 training loss: 0.691852
Global Iter: 133900 training acc: 0.5
Global Iter: 134000 training loss: 0.686871
Global Iter: 134000 training acc: 0.5625
Global Iter: 134100 training loss: 0.67535
Global Iter: 134100 training acc: 0.65625
Global Iter: 134200 training loss: 0.695936
Global Iter: 134200 training acc: 0.5
Global Iter: 134300 training loss: 0.693543
Global Iter: 134300 training acc: 0.53125
Global Iter: 134400 training loss: 0.69542
Global Iter: 134400 training acc: 0.53125
Global Iter: 134500 training loss: 0.687647
Global Iter: 134500 training acc: 0.5625
Global Iter: 134600 training loss: 0.680167
Global Iter: 134600 training acc: 0.59375
Global Iter: 134700 training loss: 0.674508
Global Iter: 134700 training acc: 0.71875
Global Iter: 134800 training loss: 0.659653
Global Iter: 134800 training acc: 0.78125
Global Iter: 134900 training loss: 0.703666
Global Iter: 134900 training acc: 0.40625
Global Iter: 135000 training loss: 0.701506
Global Iter: 135000 training acc: 0.46875
Global Iter: 135100 training loss: 0.68538
Global Iter: 135100 training acc: 0.5
Global Iter: 135200 training loss: 0.73033
Global Iter: 135200 training acc: 0.21875
Global Iter: 135300 training loss: 0.692068
Global Iter: 135300 training acc: 0.5
Global Iter: 135400 training loss: 0.693184
Global Iter: 135400 training acc: 0.5
Global Iter: 135500 training loss: 0.692723
Global Iter: 135500 training acc: 0.53125
Global Iter: 135600 training loss: 0.692962
Global Iter: 135600 training acc: 0.53125
Global Iter: 135700 training loss: 0.689109
Global Iter: 135700 training acc: 0.5625
Global Iter: 135800 training loss: 0.681128
Global Iter: 135800 training acc: 0.59375
Global Iter: 135900 training loss: 0.703399
Global Iter: 135900 training acc: 0.4375
Global Iter: 136000 training loss: 0.679601
Global Iter: 136000 training acc: 0.59375
Global Iter: 136100 training loss: 0.696157
Global Iter: 136100 training acc: 0.5
Global Iter: 136200 training loss: 0.687464
Global Iter: 136200 training acc: 0.5625
Global Iter: 136300 training loss: 0.700273
Global Iter: 136300 training acc: 0.4375
Global Iter: 136400 training loss: 0.688203
Global Iter: 136400 training acc: 0.59375
Global Iter: 136500 training loss: 0.699411
Global Iter: 136500 training acc: 0.5
Global Iter: 136600 training loss: 0.705208
Global Iter: 136600 training acc: 0.4375
Global Iter: 136700 training loss: 0.707791
Global Iter: 136700 training acc: 0.40625
Global Iter: 136800 training loss: 0.692234
Global Iter: 136800 training acc: 0.5
Global Iter: 136900 training loss: 0.674556
Global Iter: 136900 training acc: 0.625
Global Iter: 137000 training loss: 0.682681
Global Iter: 137000 training acc: 0.59375
Global Iter: 137100 training loss: 0.704086
Global Iter: 137100 training acc: 0.4375
Global Iter: 137200 training loss: 0.683449
Global Iter: 137200 training acc: 0.5625
Global Iter: 137300 training loss: 0.692717
Global Iter: 137300 training acc: 0.5
Global Iter: 137400 training loss: 0.681406
Global Iter: 137400 training acc: 0.625
Global Iter: 137500 training loss: 0.695186
Global Iter: 137500 training acc: 0.46875
Global Iter: 137600 training loss: 0.689231
Global Iter: 137600 training acc: 0.59375
Global Iter: 137700 training loss: 0.664958
Global Iter: 137700 training acc: 0.75
Global Iter: 137800 training loss: 0.691454
Global Iter: 137800 training acc: 0.53125
Global Iter: 137900 training loss: 0.690372
Global Iter: 137900 training acc: 0.5625
Global Iter: 138000 training loss: 0.688422
Global Iter: 138000 training acc: 0.53125
Global Iter: 138100 training loss: 0.717593
Global Iter: 138100 training acc: 0.34375
Global Iter: 138200 training loss: 0.690389
Global Iter: 138200 training acc: 0.5
Global Iter: 138300 training loss: 0.701679
Global Iter: 138300 training acc: 0.46875
Global Iter: 138400 training loss: 0.697633
Global Iter: 138400 training acc: 0.5
Global Iter: 138500 training loss: 0.701643
Global Iter: 138500 training acc: 0.46875
Global Iter: 138600 training loss: 0.675167
Global Iter: 138600 training acc: 0.625
Global Iter: 138700 training loss: 0.694829
Global Iter: 138700 training acc: 0.5
Global Iter: 138800 training loss: 0.684058
Global Iter: 138800 training acc: 0.5625
Global Iter: 138900 training loss: 0.682193
Global Iter: 138900 training acc: 0.59375
Global Iter: 139000 training loss: 0.68932
Global Iter: 139000 training acc: 0.53125
Global Iter: 139100 training loss: 0.678768
Global Iter: 139100 training acc: 0.59375
Global Iter: 139200 training loss: 0.701452
Global Iter: 139200 training acc: 0.46875
Global Iter: 139300 training loss: 0.696398
Global Iter: 139300 training acc: 0.46875
Global Iter: 139400 training loss: 0.702153
Global Iter: 139400 training acc: 0.4375
Global Iter: 139500 training loss: 0.694694
Global Iter: 139500 training acc: 0.5
Global Iter: 139600 training loss: 0.720524
Global Iter: 139600 training acc: 0.34375
Global Iter: 139700 training loss: 0.682144
Global Iter: 139700 training acc: 0.625
Global Iter: 139800 training loss: 0.677636
Global Iter: 139800 training acc: 0.59375
Global Iter: 139900 training loss: 0.707755
Global Iter: 139900 training acc: 0.4375
Global Iter: 140000 training loss: 0.680042
Global Iter: 140000 training acc: 0.59375
Global Iter: 140100 training loss: 0.684682
Global Iter: 140100 training acc: 0.5625
Global Iter: 140200 training loss: 0.697853
Global Iter: 140200 training acc: 0.5
Global Iter: 140300 training loss: 0.692986
Global Iter: 140300 training acc: 0.5
Global Iter: 140400 training loss: 0.688105
Global Iter: 140400 training acc: 0.5625
Global Iter: 140500 training loss: 0.692751
Global Iter: 140500 training acc: 0.4375
Global Iter: 140600 training loss: 0.684843
Global Iter: 140600 training acc: 0.59375
Global Iter: 140700 training loss: 0.703385
Global Iter: 140700 training acc: 0.4375
Global Iter: 140800 training loss: 0.667501
Global Iter: 140800 training acc: 0.6875
Global Iter: 140900 training loss: 0.681848
Global Iter: 140900 training acc: 0.65625
Global Iter: 141000 training loss: 0.691713
Global Iter: 141000 training acc: 0.5
Global Iter: 141100 training loss: 0.694857
Global Iter: 141100 training acc: 0.5
Global Iter: 141200 training loss: 0.691593
Global Iter: 141200 training acc: 0.53125
Global Iter: 141300 training loss: 0.691914
Global Iter: 141300 training acc: 0.53125
Global Iter: 141400 training loss: 0.686097
Global Iter: 141400 training acc: 0.5625
Global Iter: 141500 training loss: 0.708244
Global Iter: 141500 training acc: 0.40625
Global Iter: 141600 training loss: 0.706175
Global Iter: 141600 training acc: 0.4375
Global Iter: 141700 training loss: 0.667111
Global Iter: 141700 training acc: 0.75
Global Iter: 141800 training loss: 0.700501
Global Iter: 141800 training acc: 0.53125
Global Iter: 141900 training loss: 0.696358
Global Iter: 141900 training acc: 0.5
Global Iter: 142000 training loss: 0.681387
Global Iter: 142000 training acc: 0.59375
Global Iter: 142100 training loss: 0.690218
Global Iter: 142100 training acc: 0.5625
Global Iter: 142200 training loss: 0.721595
Global Iter: 142200 training acc: 0.28125
Global Iter: 142300 training loss: 0.711206
Global Iter: 142300 training acc: 0.40625
Global Iter: 142400 training loss: 0.682899
Global Iter: 142400 training acc: 0.625
Global Iter: 142500 training loss: 0.695068
Global Iter: 142500 training acc: 0.46875
Global Iter: 142600 training loss: 0.668673
Global Iter: 142600 training acc: 0.71875
Global Iter: 142700 training loss: 0.714384
Global Iter: 142700 training acc: 0.375
Global Iter: 142800 training loss: 0.674431
Global Iter: 142800 training acc: 0.65625
Global Iter: 142900 training loss: 0.680627
Global Iter: 142900 training acc: 0.59375
Global Iter: 143000 training loss: 0.698794
Global Iter: 143000 training acc: 0.5
Global Iter: 143100 training loss: 0.675328
Global Iter: 143100 training acc: 0.625
Global Iter: 143200 training loss: 0.704272
Global Iter: 143200 training acc: 0.46875
Global Iter: 143300 training loss: 0.699182
Global Iter: 143300 training acc: 0.46875
Global Iter: 143400 training loss: 0.677216
Global Iter: 143400 training acc: 0.65625
Global Iter: 143500 training loss: 0.692798
Global Iter: 143500 training acc: 0.5
Global Iter: 143600 training loss: 0.719301
Global Iter: 143600 training acc: 0.34375
Global Iter: 143700 training loss: 0.677712
Global Iter: 143700 training acc: 0.625
Global Iter: 143800 training loss: 0.689935
Global Iter: 143800 training acc: 0.53125
Global Iter: 143900 training loss: 0.682793
Global Iter: 143900 training acc: 0.53125
Global Iter: 144000 training loss: 0.681683
Global Iter: 144000 training acc: 0.625
Global Iter: 144100 training loss: 0.689119
Global Iter: 144100 training acc: 0.53125
Global Iter: 144200 training loss: 0.681839
Global Iter: 144200 training acc: 0.5625
Global Iter: 144300 training loss: 0.68736
Global Iter: 144300 training acc: 0.5625
Global Iter: 144400 training loss: 0.687213
Global Iter: 144400 training acc: 0.53125
Global Iter: 144500 training loss: 0.687172
Global Iter: 144500 training acc: 0.5625
Global Iter: 144600 training loss: 0.673008
Global Iter: 144600 training acc: 0.71875
Global Iter: 144700 training loss: 0.666109
Global Iter: 144700 training acc: 0.75
Global Iter: 144800 training loss: 0.709602
Global Iter: 144800 training acc: 0.375
Global Iter: 144900 training loss: 0.704584
Global Iter: 144900 training acc: 0.4375
Global Iter: 145000 training loss: 0.69015
Global Iter: 145000 training acc: 0.5625
Global Iter: 145100 training loss: 0.730093
Global Iter: 145100 training acc: 0.21875
Global Iter: 145200 training loss: 0.704691
Global Iter: 145200 training acc: 0.46875
Global Iter: 145300 training loss: 0.698634
Global Iter: 145300 training acc: 0.5
Global Iter: 145400 training loss: 0.692652
Global Iter: 145400 training acc: 0.53125
Global Iter: 145500 training loss: 0.686471
Global Iter: 145500 training acc: 0.59375
Global Iter: 145600 training loss: 0.681224
Global Iter: 145600 training acc: 0.59375
Global Iter: 145700 training loss: 0.675424
Global Iter: 145700 training acc: 0.65625
Global Iter: 145800 training loss: 0.706806
Global Iter: 145800 training acc: 0.4375
Global Iter: 145900 training loss: 0.682303
Global Iter: 145900 training acc: 0.5625
Global Iter: 146000 training loss: 0.6855
Global Iter: 146000 training acc: 0.5625
Global Iter: 146100 training loss: 0.693653
Global Iter: 146100 training acc: 0.53125
Global Iter: 146200 training loss: 0.701379
Global Iter: 146200 training acc: 0.46875
Global Iter: 146300 training loss: 0.690288
Global Iter: 146300 training acc: 0.5625
Global Iter: 146400 training loss: 0.696464
Global Iter: 146400 training acc: 0.5
Global Iter: 146500 training loss: 0.707246
Global Iter: 146500 training acc: 0.40625
Global Iter: 146600 training loss: 0.706718
Global Iter: 146600 training acc: 0.4375
Global Iter: 146700 training loss: 0.696945
Global Iter: 146700 training acc: 0.5
Global Iter: 146800 training loss: 0.681089
Global Iter: 146800 training acc: 0.59375
Global Iter: 146900 training loss: 0.677933
Global Iter: 146900 training acc: 0.59375
Global Iter: 147000 training loss: 0.699595
Global Iter: 147000 training acc: 0.4375
Global Iter: 147100 training loss: 0.689
Global Iter: 147100 training acc: 0.5625
Global Iter: 147200 training loss: 0.691624
Global Iter: 147200 training acc: 0.5
Global Iter: 147300 training loss: 0.681827
Global Iter: 147300 training acc: 0.5625
Global Iter: 147400 training loss: 0.696638
Global Iter: 147400 training acc: 0.5
Global Iter: 147500 training loss: 0.68426
Global Iter: 147500 training acc: 0.625
Global Iter: 147600 training loss: 0.667789
Global Iter: 147600 training acc: 0.75
Global Iter: 147700 training loss: 0.687242
Global Iter: 147700 training acc: 0.5625
Global Iter: 147800 training loss: 0.69864
Global Iter: 147800 training acc: 0.53125
Global Iter: 147900 training loss: 0.688483
Global Iter: 147900 training acc: 0.5625
Global Iter: 148000 training loss: 0.718126
Global Iter: 148000 training acc: 0.34375
Global Iter: 148100 training loss: 0.69507
Global Iter: 148100 training acc: 0.5
Global Iter: 148200 training loss: 0.702475
Global Iter: 148200 training acc: 0.4375
Global Iter: 148300 training loss: 0.69767
Global Iter: 148300 training acc: 0.5
Global Iter: 148400 training loss: 0.702387
Global Iter: 148400 training acc: 0.40625
Global Iter: 148500 training loss: 0.677706
Global Iter: 148500 training acc: 0.625
Global Iter: 148600 training loss: 0.701392
Global Iter: 148600 training acc: 0.5
Global Iter: 148700 training loss: 0.683095
Global Iter: 148700 training acc: 0.53125
Global Iter: 148800 training loss: 0.685441
Global Iter: 148800 training acc: 0.59375
Global Iter: 148900 training loss: 0.686176
Global Iter: 148900 training acc: 0.5
Global Iter: 149000 training loss: 0.688355
Global Iter: 149000 training acc: 0.59375
Global Iter: 149100 training loss: 0.692651
Global Iter: 149100 training acc: 0.5
Global Iter: 149200 training loss: 0.699565
Global Iter: 149200 training acc: 0.5
Global Iter: 149300 training loss: 0.702145
Global Iter: 149300 training acc: 0.46875
Global Iter: 149400 training loss: 0.70158
Global Iter: 149400 training acc: 0.46875
Global Iter: 149500 training loss: 0.718915
Global Iter: 149500 training acc: 0.34375
Global Iter: 149600 training loss: 0.68375
Global Iter: 149600 training acc: 0.59375
Global Iter: 149700 training loss: 0.680273
Global Iter: 149700 training acc: 0.59375
Global Iter: 149800 training loss: 0.708406
Global Iter: 149800 training acc: 0.40625
Global Iter: 149900 training loss: 0.677698
Global Iter: 149900 training acc: 0.625
Global Iter: 150000 training loss: 0.686621
Global Iter: 150000 training acc: 0.5625
Global Iter: 150100 training loss: 0.693104
Global Iter: 150100 training acc: 0.53125
Global Iter: 150200 training loss: 0.683783
Global Iter: 150200 training acc: 0.5625
Global Iter: 150300 training loss: 0.6879
Global Iter: 150300 training acc: 0.5625
Global Iter: 150400 training loss: 0.69725
Global Iter: 150400 training acc: 0.4375
Global Iter: 150500 training loss: 0.683764
Global Iter: 150500 training acc: 0.625
Global Iter: 150600 training loss: 0.70132
Global Iter: 150600 training acc: 0.4375
Global Iter: 150700 training loss: 0.668418
Global Iter: 150700 training acc: 0.6875
Global Iter: 150800 training loss: 0.676108
Global Iter: 150800 training acc: 0.65625
Global Iter: 150900 training loss: 0.698416
Global Iter: 150900 training acc: 0.5
Global Iter: 151000 training loss: 0.696337
Global Iter: 151000 training acc: 0.5
Global Iter: 151100 training loss: 0.679361
Global Iter: 151100 training acc: 0.59375
Global Iter: 151200 training loss: 0.694615
Global Iter: 151200 training acc: 0.5
Global Iter: 151300 training loss: 0.689481
Global Iter: 151300 training acc: 0.53125
Global Iter: 151400 training loss: 0.714471
Global Iter: 151400 training acc: 0.40625
Global Iter: 151500 training loss: 0.699759
Global Iter: 151500 training acc: 0.4375
Global Iter: 151600 training loss: 0.661865
Global Iter: 151600 training acc: 0.78125
Global Iter: 151700 training loss: 0.69479
Global Iter: 151700 training acc: 0.5
Global Iter: 151800 training loss: 0.688727
Global Iter: 151800 training acc: 0.53125
Global Iter: 151900 training loss: 0.679953
Global Iter: 151900 training acc: 0.59375
Global Iter: 152000 training loss: 0.690286
Global Iter: 152000 training acc: 0.5625
Global Iter: 152100 training loss: 0.721684
Global Iter: 152100 training acc: 0.3125
Global Iter: 152200 training loss: 0.71098
Global Iter: 152200 training acc: 0.375
Global Iter: 152300 training loss: 0.6813
Global Iter: 152300 training acc: 0.65625
Global Iter: 152400 training loss: 0.704973
Global Iter: 152400 training acc: 0.46875
Global Iter: 152500 training loss: 0.674247
Global Iter: 152500 training acc: 0.71875
Global Iter: 152600 training loss: 0.714186
Global Iter: 152600 training acc: 0.375
Global Iter: 152700 training loss: 0.680012
Global Iter: 152700 training acc: 0.59375
Global Iter: 152800 training loss: 0.690559
Global Iter: 152800 training acc: 0.5625
Global Iter: 152900 training loss: 0.688423
Global Iter: 152900 training acc: 0.5625
Global Iter: 153000 training loss: 0.686465
Global Iter: 153000 training acc: 0.5625
Global Iter: 153100 training loss: 0.698837
Global Iter: 153100 training acc: 0.5
Global Iter: 153200 training loss: 0.691075
Global Iter: 153200 training acc: 0.53125
Global Iter: 153300 training loss: 0.683522
Global Iter: 153300 training acc: 0.59375
Global Iter: 153400 training loss: 0.69643
Global Iter: 153400 training acc: 0.5
Global Iter: 153500 training loss: 0.713768
Global Iter: 153500 training acc: 0.34375
Global Iter: 153600 training loss: 0.677722
Global Iter: 153600 training acc: 0.65625
Global Iter: 153700 training loss: 0.683523
Global Iter: 153700 training acc: 0.5625
Global Iter: 153800 training loss: 0.688584
Global Iter: 153800 training acc: 0.53125
Global Iter: 153900 training loss: 0.683451
Global Iter: 153900 training acc: 0.59375
Global Iter: 154000 training loss: 0.684903
Global Iter: 154000 training acc: 0.5625
Global Iter: 154100 training loss: 0.697399
Global Iter: 154100 training acc: 0.5
Global Iter: 154200 training loss: 0.683354
Global Iter: 154200 training acc: 0.625
Global Iter: 154300 training loss: 0.698305
Global Iter: 154300 training acc: 0.5
Global Iter: 154400 training loss: 0.693089
Global Iter: 154400 training acc: 0.53125
Global Iter: 154500 training loss: 0.667844
Global Iter: 154500 training acc: 0.71875
Global Iter: 154600 training loss: 0.667072
Global Iter: 154600 training acc: 0.6875
Global Iter: 154700 training loss: 0.711788
Global Iter: 154700 training acc: 0.375
Global Iter: 154800 training loss: 0.69478
Global Iter: 154800 training acc: 0.5
Global Iter: 154900 training loss: 0.684482
Global Iter: 154900 training acc: 0.59375
Global Iter: 155000 training loss: 0.733887
Global Iter: 155000 training acc: 0.21875
Global Iter: 155100 training loss: 0.700727
Global Iter: 155100 training acc: 0.46875
Global Iter: 155200 training loss: 0.686914
Global Iter: 155200 training acc: 0.53125
Global Iter: 155300 training loss: 0.710912
Global Iter: 155300 training acc: 0.46875
Global Iter: 155400 training loss: 0.680088
Global Iter: 155400 training acc: 0.625
Global Iter: 155500 training loss: 0.681633
Global Iter: 155500 training acc: 0.59375
Global Iter: 155600 training loss: 0.678917
Global Iter: 155600 training acc: 0.65625
Global Iter: 155700 training loss: 0.705805
Global Iter: 155700 training acc: 0.4375
Global Iter: 155800 training loss: 0.692715
Global Iter: 155800 training acc: 0.53125
Global Iter: 155900 training loss: 0.685127
Global Iter: 155900 training acc: 0.5625
Global Iter: 156000 training loss: 0.688697
Global Iter: 156000 training acc: 0.53125
Global Iter: 156100 training loss: 0.691067
Global Iter: 156100 training acc: 0.53125
Global Iter: 156200 training loss: 0.678121
Global Iter: 156200 training acc: 0.625
Global Iter: 156300 training loss: 0.689607
Global Iter: 156300 training acc: 0.5625
Global Iter: 156400 training loss: 0.704157
Global Iter: 156400 training acc: 0.40625
Global Iter: 156500 training loss: 0.699218
Global Iter: 156500 training acc: 0.4375
Global Iter: 156600 training loss: 0.697086
Global Iter: 156600 training acc: 0.5
Global Iter: 156700 training loss: 0.684333
Global Iter: 156700 training acc: 0.5625
Global Iter: 156800 training loss: 0.681045
Global Iter: 156800 training acc: 0.625
Global Iter: 156900 training loss: 0.700753
Global Iter: 156900 training acc: 0.46875
Global Iter: 157000 training loss: 0.687918
Global Iter: 157000 training acc: 0.5625
Global Iter: 157100 training loss: 0.690337
Global Iter: 157100 training acc: 0.5625
Global Iter: 157200 training loss: 0.681314
Global Iter: 157200 training acc: 0.625
Global Iter: 157300 training loss: 0.692016
Global Iter: 157300 training acc: 0.53125
Global Iter: 157400 training loss: 0.689397
Global Iter: 157400 training acc: 0.59375
Global Iter: 157500 training loss: 0.655364
Global Iter: 157500 training acc: 0.78125
Global Iter: 157600 training loss: 0.692326
Global Iter: 157600 training acc: 0.53125
Global Iter: 157700 training loss: 0.683677
Global Iter: 157700 training acc: 0.5625
Global Iter: 157800 training loss: 0.688594
Global Iter: 157800 training acc: 0.5625
Global Iter: 157900 training loss: 0.721086
Global Iter: 157900 training acc: 0.34375
Global Iter: 158000 training loss: 0.69607
Global Iter: 158000 training acc: 0.5
Global Iter: 158100 training loss: 0.708993
Global Iter: 158100 training acc: 0.4375
Global Iter: 158200 training loss: 0.70008
Global Iter: 158200 training acc: 0.46875
Global Iter: 158300 training loss: 0.711389
Global Iter: 158300 training acc: 0.40625
Global Iter: 158400 training loss: 0.682424
Global Iter: 158400 training acc: 0.59375
Global Iter: 158500 training loss: 0.69746
Global Iter: 158500 training acc: 0.5
Global Iter: 158600 training loss: 0.686077
Global Iter: 158600 training acc: 0.53125
Global Iter: 158700 training loss: 0.680204
Global Iter: 158700 training acc: 0.59375
Global Iter: 158800 training loss: 0.692809
Global Iter: 158800 training acc: 0.53125
Global Iter: 158900 training loss: 0.679811
Global Iter: 158900 training acc: 0.59375
Global Iter: 159000 training loss: 0.69011
Global Iter: 159000 training acc: 0.53125
Global Iter: 159100 training loss: 0.693525
Global Iter: 159100 training acc: 0.53125
Global Iter: 159200 training loss: 0.697915
Global Iter: 159200 training acc: 0.5
Global Iter: 159300 training loss: 0.68929
Global Iter: 159300 training acc: 0.53125
Global Iter: 159400 training loss: 0.715794
Global Iter: 159400 training acc: 0.375
Global Iter: 159500 training loss: 0.687534
Global Iter: 159500 training acc: 0.5625
Global Iter: 159600 training loss: 0.677306
Global Iter: 159600 training acc: 0.65625
Global Iter: 159700 training loss: 0.710755
Global Iter: 159700 training acc: 0.40625
Global Iter: 159800 training loss: 0.696856
Global Iter: 159800 training acc: 0.5625
Global Iter: 159900 training loss: 0.693312
Global Iter: 159900 training acc: 0.53125
Global Iter: 160000 training loss: 0.698667
Global Iter: 160000 training acc: 0.5
Global Iter: 160100 training loss: 0.68932
Global Iter: 160100 training acc: 0.5625
Global Iter: 160200 training loss: 0.688997
Global Iter: 160200 training acc: 0.5625
Global Iter: 160300 training loss: 0.70474
Global Iter: 160300 training acc: 0.4375
Global Iter: 160400 training loss: 0.675887
Global Iter: 160400 training acc: 0.65625
Global Iter: 160500 training loss: 0.695112
Global Iter: 160500 training acc: 0.46875
Global Iter: 160600 training loss: 0.670383
Global Iter: 160600 training acc: 0.6875
Global Iter: 160700 training loss: 0.684242
Global Iter: 160700 training acc: 0.625
Global Iter: 160800 training loss: 0.697491
Global Iter: 160800 training acc: 0.46875
Global Iter: 160900 training loss: 0.691732
Global Iter: 160900 training acc: 0.5
Global Iter: 161000 training loss: 0.679802
Global Iter: 161000 training acc: 0.625
Global Iter: 161100 training loss: 0.692912
Global Iter: 161100 training acc: 0.5
Global Iter: 161200 training loss: 0.683107
Global Iter: 161200 training acc: 0.53125
Global Iter: 161300 training loss: 0.707347
Global Iter: 161300 training acc: 0.4375
Global Iter: 161400 training loss: 0.696442
Global Iter: 161400 training acc: 0.46875
Global Iter: 161500 training loss: 0.660187
Global Iter: 161500 training acc: 0.8125
Global Iter: 161600 training loss: 0.69469
Global Iter: 161600 training acc: 0.53125
Global Iter: 161700 training loss: 0.687449
Global Iter: 161700 training acc: 0.5625
Global Iter: 161800 training loss: 0.680308
Global Iter: 161800 training acc: 0.59375
Global Iter: 161900 training loss: 0.681205
Global Iter: 161900 training acc: 0.59375
Global Iter: 162000 training loss: 0.720831
Global Iter: 162000 training acc: 0.28125
Global Iter: 162100 training loss: 0.705418
Global Iter: 162100 training acc: 0.40625
Global Iter: 162200 training loss: 0.681767
Global Iter: 162200 training acc: 0.59375
Global Iter: 162300 training loss: 0.700538
Global Iter: 162300 training acc: 0.46875
Global Iter: 162400 training loss: 0.667934
Global Iter: 162400 training acc: 0.71875
Global Iter: 162500 training loss: 0.718102
Global Iter: 162500 training acc: 0.34375
Global Iter: 162600 training loss: 0.678717
Global Iter: 162600 training acc: 0.59375
Global Iter: 162700 training loss: 0.687126
Global Iter: 162700 training acc: 0.59375
Global Iter: 162800 training loss: 0.685375
Global Iter: 162800 training acc: 0.59375
Global Iter: 162900 training loss: 0.687977
Global Iter: 162900 training acc: 0.59375
Global Iter: 163000 training loss: 0.697676
Global Iter: 163000 training acc: 0.46875
Global Iter: 163100 training loss: 0.691036
Global Iter: 163100 training acc: 0.53125
Global Iter: 163200 training loss: 0.682864
Global Iter: 163200 training acc: 0.5625
Global Iter: 163300 training loss: 0.694911
Global Iter: 163300 training acc: 0.53125
Global Iter: 163400 training loss: 0.723418
Global Iter: 163400 training acc: 0.3125
Global Iter: 163500 training loss: 0.677136
Global Iter: 163500 training acc: 0.65625
Global Iter: 163600 training loss: 0.686823
Global Iter: 163600 training acc: 0.5625
Global Iter: 163700 training loss: 0.696052
Global Iter: 163700 training acc: 0.5
Global Iter: 163800 training loss: 0.69155
Global Iter: 163800 training acc: 0.5625
Global Iter: 163900 training loss: 0.690038
Global Iter: 163900 training acc: 0.5625
Global Iter: 164000 training loss: 0.689175
Global Iter: 164000 training acc: 0.53125
Global Iter: 164100 training loss: 0.684556
Global Iter: 164100 training acc: 0.625
Global Iter: 164200 training loss: 0.685155
Global Iter: 164200 training acc: 0.5625
Global Iter: 164300 training loss: 0.691408
Global Iter: 164300 training acc: 0.5
Global Iter: 164400 training loss: 0.666184
Global Iter: 164400 training acc: 0.75
Global Iter: 164500 training loss: 0.679916
Global Iter: 164500 training acc: 0.65625
Global Iter: 164600 training loss: 0.705693
Global Iter: 164600 training acc: 0.4375
Global Iter: 164700 training loss: 0.694837
Global Iter: 164700 training acc: 0.46875
Global Iter: 164800 training loss: 0.694404
Global Iter: 164800 training acc: 0.53125
Global Iter: 164900 training loss: 0.733488
Global Iter: 164900 training acc: 0.21875
Global Iter: 165000 training loss: 0.693526
Global Iter: 165000 training acc: 0.5
Global Iter: 165100 training loss: 0.685862
Global Iter: 165100 training acc: 0.53125
Global Iter: 165200 training loss: 0.6964
Global Iter: 165200 training acc: 0.5
Global Iter: 165300 training loss: 0.686702
Global Iter: 165300 training acc: 0.59375
Global Iter: 165400 training loss: 0.686054
Global Iter: 165400 training acc: 0.5625
Global Iter: 165500 training loss: 0.676741
Global Iter: 165500 training acc: 0.625
Global Iter: 165600 training loss: 0.694724
Global Iter: 165600 training acc: 0.5
Global Iter: 165700 training loss: 0.69749
Global Iter: 165700 training acc: 0.53125
Global Iter: 165800 training loss: 0.691802
Global Iter: 165800 training acc: 0.53125
Global Iter: 165900 training loss: 0.69618
Global Iter: 165900 training acc: 0.5
Global Iter: 166000 training loss: 0.687569
Global Iter: 166000 training acc: 0.53125
Global Iter: 166100 training loss: 0.6847
Global Iter: 166100 training acc: 0.65625
Global Iter: 166200 training loss: 0.691852
Global Iter: 166200 training acc: 0.53125
Global Iter: 166300 training loss: 0.709694
Global Iter: 166300 training acc: 0.34375
Global Iter: 166400 training loss: 0.702085
Global Iter: 166400 training acc: 0.46875
Global Iter: 166500 training loss: 0.699925
Global Iter: 166500 training acc: 0.5
Global Iter: 166600 training loss: 0.684114
Global Iter: 166600 training acc: 0.53125
Global Iter: 166700 training loss: 0.673929
Global Iter: 166700 training acc: 0.65625
Global Iter: 166800 training loss: 0.703066
Global Iter: 166800 training acc: 0.4375
Global Iter: 166900 training loss: 0.688257
Global Iter: 166900 training acc: 0.5625
Global Iter: 167000 training loss: 0.691037
Global Iter: 167000 training acc: 0.5625
Global Iter: 167100 training loss: 0.67037
Global Iter: 167100 training acc: 0.65625
Global Iter: 167200 training loss: 0.691944
Global Iter: 167200 training acc: 0.5625
Global Iter: 167300 training loss: 0.672632
Global Iter: 167300 training acc: 0.65625
Global Iter: 167400 training loss: 0.666911
Global Iter: 167400 training acc: 0.75
Global Iter: 167500 training loss: 0.695127
Global Iter: 167500 training acc: 0.5
Global Iter: 167600 training loss: 0.686114
Global Iter: 167600 training acc: 0.5625
Global Iter: 167700 training loss: 0.691846
Global Iter: 167700 training acc: 0.53125
Global Iter: 167800 training loss: 0.72334
Global Iter: 167800 training acc: 0.28125
Global Iter: 167900 training loss: 0.696967
Global Iter: 167900 training acc: 0.46875
Global Iter: 168000 training loss: 0.699929
Global Iter: 168000 training acc: 0.4375
Global Iter: 168100 training loss: 0.690255
Global Iter: 168100 training acc: 0.5
Global Iter: 168200 training loss: 0.70351
Global Iter: 168200 training acc: 0.40625
Global Iter: 168300 training loss: 0.671208
Global Iter: 168300 training acc: 0.65625
Global Iter: 168400 training loss: 0.69236
Global Iter: 168400 training acc: 0.5625
Global Iter: 168500 training loss: 0.691711
Global Iter: 168500 training acc: 0.53125
Global Iter: 168600 training loss: 0.688774
Global Iter: 168600 training acc: 0.59375
Global Iter: 168700 training loss: 0.696365
Global Iter: 168700 training acc: 0.5
Global Iter: 168800 training loss: 0.690807
Global Iter: 168800 training acc: 0.53125
Global Iter: 168900 training loss: 0.692239
Global Iter: 168900 training acc: 0.5
Global Iter: 169000 training loss: 0.687561
Global Iter: 169000 training acc: 0.5
Global Iter: 169100 training loss: 0.692092
Global Iter: 169100 training acc: 0.53125
Global Iter: 169200 training loss: 0.693112
Global Iter: 169200 training acc: 0.53125
Global Iter: 169300 training loss: 0.715092
Global Iter: 169300 training acc: 0.375
Global Iter: 169400 training loss: 0.689758
Global Iter: 169400 training acc: 0.5625
Global Iter: 169500 training loss: 0.67883
Global Iter: 169500 training acc: 0.625
Global Iter: 169600 training loss: 0.702449
Global Iter: 169600 training acc: 0.4375
Global Iter: 169700 training loss: 0.690809
Global Iter: 169700 training acc: 0.5625
Global Iter: 169800 training loss: 0.688631
Global Iter: 169800 training acc: 0.5625
Global Iter: 169900 training loss: 0.690916
Global Iter: 169900 training acc: 0.53125
Global Iter: 170000 training loss: 0.68841
Global Iter: 170000 training acc: 0.5625
Global Iter: 170100 training loss: 0.683986
Global Iter: 170100 training acc: 0.5625
Global Iter: 170200 training loss: 0.697285
Global Iter: 170200 training acc: 0.46875
Global Iter: 170300 training loss: 0.675069
Global Iter: 170300 training acc: 0.65625
Global Iter: 170400 training loss: 0.690834
Global Iter: 170400 training acc: 0.5
Global Iter: 170500 training loss: 0.67294
Global Iter: 170500 training acc: 0.65625
Global Iter: 170600 training loss: 0.683064
Global Iter: 170600 training acc: 0.625
Global Iter: 170700 training loss: 0.707488
Global Iter: 170700 training acc: 0.40625
Global Iter: 170800 training loss: 0.690148
Global Iter: 170800 training acc: 0.5
Global Iter: 170900 training loss: 0.687756
Global Iter: 170900 training acc: 0.59375
Global Iter: 171000 training loss: 0.693894
Global Iter: 171000 training acc: 0.53125
Global Iter: 171100 training loss: 0.690178
Global Iter: 171100 training acc: 0.5
Global Iter: 171200 training loss: 0.701715
Global Iter: 171200 training acc: 0.46875
Global Iter: 171300 training loss: 0.705959
Global Iter: 171300 training acc: 0.4375
Global Iter: 171400 training loss: 0.65873
Global Iter: 171400 training acc: 0.78125
Global Iter: 171500 training loss: 0.694934
Global Iter: 171500 training acc: 0.5
Global Iter: 171600 training loss: 0.691889
Global Iter: 171600 training acc: 0.53125
Global Iter: 171700 training loss: 0.682863
Global Iter: 171700 training acc: 0.625
Global Iter: 171800 training loss: 0.694006
Global Iter: 171800 training acc: 0.5625
Global Iter: 171900 training loss: 0.719582
Global Iter: 171900 training acc: 0.3125
Global Iter: 172000 training loss: 0.712416
Global Iter: 172000 training acc: 0.40625
Global Iter: 172100 training loss: 0.680448
Global Iter: 172100 training acc: 0.625
Global Iter: 172200 training loss: 0.707386
Global Iter: 172200 training acc: 0.4375
Global Iter: 172300 training loss: 0.671897
Global Iter: 172300 training acc: 0.6875
Global Iter: 172400 training loss: 0.713284
Global Iter: 172400 training acc: 0.375
Global Iter: 172500 training loss: 0.683147
Global Iter: 172500 training acc: 0.59375
Global Iter: 172600 training loss: 0.683675
Global Iter: 172600 training acc: 0.59375
Global Iter: 172700 training loss: 0.68295
Global Iter: 172700 training acc: 0.5625
Global Iter: 172800 training loss: 0.682938
Global Iter: 172800 training acc: 0.5625
Global Iter: 172900 training loss: 0.693809
Global Iter: 172900 training acc: 0.5
Global Iter: 173000 training loss: 0.684367
Global Iter: 173000 training acc: 0.5625
Global Iter: 173100 training loss: 0.688042
Global Iter: 173100 training acc: 0.5
Global Iter: 173200 training loss: 0.686199
Global Iter: 173200 training acc: 0.5625
Global Iter: 173300 training loss: 0.719646
Global Iter: 173300 training acc: 0.34375
Global Iter: 173400 training loss: 0.670818
Global Iter: 173400 training acc: 0.71875
Global Iter: 173500 training loss: 0.680611
Global Iter: 173500 training acc: 0.59375
Global Iter: 173600 training loss: 0.703328
Global Iter: 173600 training acc: 0.46875
Global Iter: 173700 training loss: 0.688456
Global Iter: 173700 training acc: 0.59375
Global Iter: 173800 training loss: 0.683531
Global Iter: 173800 training acc: 0.59375
Global Iter: 173900 training loss: 0.704444
Global Iter: 173900 training acc: 0.46875
Global Iter: 174000 training loss: 0.680772
Global Iter: 174000 training acc: 0.65625
Global Iter: 174100 training loss: 0.684093
Global Iter: 174100 training acc: 0.53125
Global Iter: 174200 training loss: 0.706539
Global Iter: 174200 training acc: 0.46875
Global Iter: 174300 training loss: 0.667059
Global Iter: 174300 training acc: 0.75
Global Iter: 174400 training loss: 0.681759
Global Iter: 174400 training acc: 0.625
Global Iter: 174500 training loss: 0.703809
Global Iter: 174500 training acc: 0.40625
Global Iter: 174600 training loss: 0.695686
Global Iter: 174600 training acc: 0.5
Global Iter: 174700 training loss: 0.684192
Global Iter: 174700 training acc: 0.5625
Global Iter: 174800 training loss: 0.73312
Global Iter: 174800 training acc: 0.21875
Global Iter: 174900 training loss: 0.704167
Global Iter: 174900 training acc: 0.4375
Global Iter: 175000 training loss: 0.683379
Global Iter: 175000 training acc: 0.5625
Global Iter: 175100 training loss: 0.686074
Global Iter: 175100 training acc: 0.53125
Global Iter: 175200 training loss: 0.675946
Global Iter: 175200 training acc: 0.625
Global Iter: 175300 training loss: 0.69039
Global Iter: 175300 training acc: 0.53125
Global Iter: 175400 training loss: 0.678058
Global Iter: 175400 training acc: 0.65625
Global Iter: 175500 training loss: 0.697146
Global Iter: 175500 training acc: 0.46875
Global Iter: 175600 training loss: 0.698907
Global Iter: 175600 training acc: 0.5
Global Iter: 175700 training loss: 0.691108
Global Iter: 175700 training acc: 0.53125
Global Iter: 175800 training loss: 0.700423
Global Iter: 175800 training acc: 0.5
Global Iter: 175900 training loss: 0.685251
Global Iter: 175900 training acc: 0.5625
Global Iter: 176000 training loss: 0.67387
Global Iter: 176000 training acc: 0.6875
Global Iter: 176100 training loss: 0.691112
Global Iter: 176100 training acc: 0.53125
Global Iter: 176200 training loss: 0.714612
Global Iter: 176200 training acc: 0.375
Global Iter: 176300 training loss: 0.693829
Global Iter: 176300 training acc: 0.5
Global Iter: 176400 training loss: 0.68931
Global Iter: 176400 training acc: 0.5
Global Iter: 176500 training loss: 0.692361
Global Iter: 176500 training acc: 0.5
Global Iter: 176600 training loss: 0.669462
Global Iter: 176600 training acc: 0.71875
Global Iter: 176700 training loss: 0.700676
Global Iter: 176700 training acc: 0.46875
Global Iter: 176800 training loss: 0.67955
Global Iter: 176800 training acc: 0.59375
Global Iter: 176900 training loss: 0.678683
Global Iter: 176900 training acc: 0.625
Global Iter: 177000 training loss: 0.677747
Global Iter: 177000 training acc: 0.625
Global Iter: 177100 training loss: 0.694777
Global Iter: 177100 training acc: 0.53125
Global Iter: 177200 training loss: 0.680437
Global Iter: 177200 training acc: 0.65625
Global Iter: 177300 training loss: 0.665501
Global Iter: 177300 training acc: 0.75
Global Iter: 177400 training loss: 0.691199
Global Iter: 177400 training acc: 0.53125
Global Iter: 177500 training loss: 0.687029
Global Iter: 177500 training acc: 0.5625
Global Iter: 177600 training loss: 0.694054
Global Iter: 177600 training acc: 0.53125
Global Iter: 177700 training loss: 0.733282
Global Iter: 177700 training acc: 0.25
Global Iter: 177800 training loss: 0.701828
Global Iter: 177800 training acc: 0.46875
Global Iter: 177900 training loss: 0.703804
Global Iter: 177900 training acc: 0.4375
Global Iter: 178000 training loss: 0.696125
Global Iter: 178000 training acc: 0.53125
Global Iter: 178100 training loss: 0.709519
Global Iter: 178100 training acc: 0.375
Global Iter: 178200 training loss: 0.6649
Global Iter: 178200 training acc: 0.71875
Global Iter: 178300 training loss: 0.688841
Global Iter: 178300 training acc: 0.5625
Global Iter: 178400 training loss: 0.697691
Global Iter: 178400 training acc: 0.5
Global Iter: 178500 training loss: 0.68268
Global Iter: 178500 training acc: 0.625
Global Iter: 178600 training loss: 0.702683
Global Iter: 178600 training acc: 0.46875
Global Iter: 178700 training loss: 0.689564
Global Iter: 178700 training acc: 0.5625
Global Iter: 178800 training loss: 0.68505
Global Iter: 178800 training acc: 0.53125
Global Iter: 178900 training loss: 0.690918
Global Iter: 178900 training acc: 0.5
Global Iter: 179000 training loss: 0.694324
Global Iter: 179000 training acc: 0.5
Global Iter: 179100 training loss: 0.6926
Global Iter: 179100 training acc: 0.53125
Global Iter: 179200 training loss: 0.71565
Global Iter: 179200 training acc: 0.375
Global Iter: 179300 training loss: 0.685332
Global Iter: 179300 training acc: 0.625
Global Iter: 179400 training loss: 0.678168
Global Iter: 179400 training acc: 0.625
Global Iter: 179500 training loss: 0.706864
Global Iter: 179500 training acc: 0.4375
Global Iter: 179600 training loss: 0.687865
Global Iter: 179600 training acc: 0.53125
Global Iter: 179700 training loss: 0.690536
Global Iter: 179700 training acc: 0.5625
Global Iter: 179800 training loss: 0.686249
Global Iter: 179800 training acc: 0.5625
Global Iter: 179900 training loss: 0.674643
Global Iter: 179900 training acc: 0.625
Global Iter: 180000 training loss: 0.68473
Global Iter: 180000 training acc: 0.53125
Global Iter: 180100 training loss: 0.701643
Global Iter: 180100 training acc: 0.4375
Global Iter: 180200 training loss: 0.674666
Global Iter: 180200 training acc: 0.625
Global Iter: 180300 training loss: 0.695407
Global Iter: 180300 training acc: 0.5
Global Iter: 180400 training loss: 0.671192
Global Iter: 180400 training acc: 0.6875
Global Iter: 180500 training loss: 0.684159
Global Iter: 180500 training acc: 0.625
Global Iter: 180600 training loss: 0.708963
Global Iter: 180600 training acc: 0.40625
Global Iter: 180700 training loss: 0.691484
Global Iter: 180700 training acc: 0.53125
Global Iter: 180800 training loss: 0.682901
Global Iter: 180800 training acc: 0.59375
Global Iter: 180900 training loss: 0.693512
Global Iter: 180900 training acc: 0.53125
Global Iter: 181000 training loss: 0.68778
Global Iter: 181000 training acc: 0.53125
Global Iter: 181100 training loss: 0.696287
Global Iter: 181100 training acc: 0.5
Global Iter: 181200 training loss: 0.707015
Global Iter: 181200 training acc: 0.4375
Global Iter: 181300 training loss: 0.660144
Global Iter: 181300 training acc: 0.78125
Global Iter: 181400 training loss: 0.702022
Global Iter: 181400 training acc: 0.46875
Global Iter: 181500 training loss: 0.685286
Global Iter: 181500 training acc: 0.5625
Global Iter: 181600 training loss: 0.680209
Global Iter: 181600 training acc: 0.59375
Global Iter: 181700 training loss: 0.682624
Global Iter: 181700 training acc: 0.5625
Global Iter: 181800 training loss: 0.713417
Global Iter: 181800 training acc: 0.34375
Global Iter: 181900 training loss: 0.70557
Global Iter: 181900 training acc: 0.40625
Global Iter: 182000 training loss: 0.690645
Global Iter: 182000 training acc: 0.5625
Global Iter: 182100 training loss: 0.709221
Global Iter: 182100 training acc: 0.40625
Global Iter: 182200 training loss: 0.678173
Global Iter: 182200 training acc: 0.65625
Global Iter: 182300 training loss: 0.711715
Global Iter: 182300 training acc: 0.40625
Global Iter: 182400 training loss: 0.68209
Global Iter: 182400 training acc: 0.625
Global Iter: 182500 training loss: 0.689743
Global Iter: 182500 training acc: 0.5625
Global Iter: 182600 training loss: 0.685672
Global Iter: 182600 training acc: 0.5625
Global Iter: 182700 training loss: 0.685885
Global Iter: 182700 training acc: 0.5625
Global Iter: 182800 training loss: 0.691267
Global Iter: 182800 training acc: 0.5
Global Iter: 182900 training loss: 0.68419
Global Iter: 182900 training acc: 0.5625
Global Iter: 183000 training loss: 0.687242
Global Iter: 183000 training acc: 0.5625
Global Iter: 183100 training loss: 0.691345
Global Iter: 183100 training acc: 0.5625
Global Iter: 183200 training loss: 0.713865
Global Iter: 183200 training acc: 0.34375
Global Iter: 183300 training loss: 0.668807
Global Iter: 183300 training acc: 0.6875
Global Iter: 183400 training loss: 0.684928
Global Iter: 183400 training acc: 0.59375
Global Iter: 183500 training loss: 0.704683
Global Iter: 183500 training acc: 0.46875
Global Iter: 183600 training loss: 0.687372
Global Iter: 183600 training acc: 0.59375
Global Iter: 183700 training loss: 0.680301
Global Iter: 183700 training acc: 0.59375
Global Iter: 183800 training loss: 0.715204
Global Iter: 183800 training acc: 0.40625
Global Iter: 183900 training loss: 0.681964
Global Iter: 183900 training acc: 0.625
Global Iter: 184000 training loss: 0.688462
Global Iter: 184000 training acc: 0.53125
Global Iter: 184100 training loss: 0.701452
Global Iter: 184100 training acc: 0.5
Global Iter: 184200 training loss: 0.668603
Global Iter: 184200 training acc: 0.78125
Global Iter: 184300 training loss: 0.678452
Global Iter: 184300 training acc: 0.65625
Global Iter: 184400 training loss: 0.699966
Global Iter: 184400 training acc: 0.4375
Global Iter: 184500 training loss: 0.696178
Global Iter: 184500 training acc: 0.5
Global Iter: 184600 training loss: 0.687893
Global Iter: 184600 training acc: 0.5625
Global Iter: 184700 training loss: 0.725134
Global Iter: 184700 training acc: 0.21875
Global Iter: 184800 training loss: 0.701977
Global Iter: 184800 training acc: 0.46875
Global Iter: 184900 training loss: 0.687472
Global Iter: 184900 training acc: 0.5625
Global Iter: 185000 training loss: 0.679354
Global Iter: 185000 training acc: 0.59375
Global Iter: 185100 training loss: 0.684606
Global Iter: 185100 training acc: 0.5625
Global Iter: 185200 training loss: 0.699956
Global Iter: 185200 training acc: 0.46875
Global Iter: 185300 training loss: 0.679187
Global Iter: 185300 training acc: 0.625
Global Iter: 185400 training loss: 0.699349
Global Iter: 185400 training acc: 0.4375
Global Iter: 185500 training loss: 0.700353
Global Iter: 185500 training acc: 0.46875
Global Iter: 185600 training loss: 0.685354
Global Iter: 185600 training acc: 0.5625
Global Iter: 185700 training loss: 0.697798
Global Iter: 185700 training acc: 0.5
Global Iter: 185800 training loss: 0.68361
Global Iter: 185800 training acc: 0.5625
Global Iter: 185900 training loss: 0.675017
Global Iter: 185900 training acc: 0.71875
Global Iter: 186000 training loss: 0.681876
Global Iter: 186000 training acc: 0.5625
Global Iter: 186100 training loss: 0.71748
Global Iter: 186100 training acc: 0.34375
Global Iter: 186200 training loss: 0.704102
Global Iter: 186200 training acc: 0.5
Global Iter: 186300 training loss: 0.697454
Global Iter: 186300 training acc: 0.5
Global Iter: 186400 training loss: 0.694124
Global Iter: 186400 training acc: 0.5
Global Iter: 186500 training loss: 0.66476
Global Iter: 186500 training acc: 0.71875
Global Iter: 186600 training loss: 0.701233
Global Iter: 186600 training acc: 0.46875
Global Iter: 186700 training loss: 0.685961
Global Iter: 186700 training acc: 0.59375
Global Iter: 186800 training loss: 0.681131
Global Iter: 186800 training acc: 0.625
Global Iter: 186900 training loss: 0.671557
Global Iter: 186900 training acc: 0.6875
Global Iter: 187000 training loss: 0.685887
Global Iter: 187000 training acc: 0.53125
Global Iter: 187100 training loss: 0.680325
Global Iter: 187100 training acc: 0.65625
Global Iter: 187200 training loss: 0.668672
Global Iter: 187200 training acc: 0.75
Global Iter: 187300 training loss: 0.697021
Global Iter: 187300 training acc: 0.46875
Global Iter: 187400 training loss: 0.698605
Global Iter: 187400 training acc: 0.5
Global Iter: 187500 training loss: 0.693818
Global Iter: 187500 training acc: 0.53125
Global Iter: 187600 training loss: 0.7363
Global Iter: 187600 training acc: 0.21875
Global Iter: 187700 training loss: 0.694509
Global Iter: 187700 training acc: 0.5
Global Iter: 187800 training loss: 0.697538
Global Iter: 187800 training acc: 0.4375
Global Iter: 187900 training loss: 0.695828
Global Iter: 187900 training acc: 0.5
Global Iter: 188000 training loss: 0.7013
Global Iter: 188000 training acc: 0.4375
Global Iter: 188100 training loss: 0.666669
Global Iter: 188100 training acc: 0.6875
Global Iter: 188200 training loss: 0.688357
Global Iter: 188200 training acc: 0.59375
Global Iter: 188300 training loss: 0.702239
Global Iter: 188300 training acc: 0.4375
Global Iter: 188400 training loss: 0.682538
Global Iter: 188400 training acc: 0.59375
Global Iter: 188500 training loss: 0.693538
Global Iter: 188500 training acc: 0.46875
Global Iter: 188600 training loss: 0.691388
Global Iter: 188600 training acc: 0.5625
Global Iter: 188700 training loss: 0.688694
Global Iter: 188700 training acc: 0.53125
Global Iter: 188800 training loss: 0.692121
Global Iter: 188800 training acc: 0.5
Global Iter: 188900 training loss: 0.699673
Global Iter: 188900 training acc: 0.5
Global Iter: 189000 training loss: 0.700122
Global Iter: 189000 training acc: 0.46875
Global Iter: 189100 training loss: 0.71185
Global Iter: 189100 training acc: 0.34375
Global Iter: 189200 training loss: 0.680074
Global Iter: 189200 training acc: 0.59375
Global Iter: 189300 training loss: 0.68283
Global Iter: 189300 training acc: 0.625
Global Iter: 189400 training loss: 0.703949
Global Iter: 189400 training acc: 0.46875
Global Iter: 189500 training loss: 0.700007
Global Iter: 189500 training acc: 0.46875
Global Iter: 189600 training loss: 0.678779
Global Iter: 189600 training acc: 0.59375
Global Iter: 189700 training loss: 0.690949
Global Iter: 189700 training acc: 0.5
Global Iter: 189800 training loss: 0.675341
Global Iter: 189800 training acc: 0.625
Global Iter: 189900 training loss: 0.691552
Global Iter: 189900 training acc: 0.53125
Global Iter: 190000 training loss: 0.697049
Global Iter: 190000 training acc: 0.46875
Global Iter: 190100 training loss: 0.673031
Global Iter: 190100 training acc: 0.6875
Global Iter: 190200 training loss: 0.686247
Global Iter: 190200 training acc: 0.53125
Global Iter: 190300 training loss: 0.665213
Global Iter: 190300 training acc: 0.71875
Global Iter: 190400 training loss: 0.6782
Global Iter: 190400 training acc: 0.625
Global Iter: 190500 training loss: 0.709732
Global Iter: 190500 training acc: 0.375
Global Iter: 190600 training loss: 0.696075
Global Iter: 190600 training acc: 0.5
Global Iter: 190700 training loss: 0.690058
Global Iter: 190700 training acc: 0.5625
Global Iter: 190800 training loss: 0.69515
Global Iter: 190800 training acc: 0.53125
Global Iter: 190900 training loss: 0.69386
Global Iter: 190900 training acc: 0.53125
Global Iter: 191000 training loss: 0.691229
Global Iter: 191000 training acc: 0.53125
Global Iter: 191100 training loss: 0.715401
Global Iter: 191100 training acc: 0.375
Global Iter: 191200 training loss: 0.668389
Global Iter: 191200 training acc: 0.75
Global Iter: 191300 training loss: 0.697013
Global Iter: 191300 training acc: 0.5
Global Iter: 191400 training loss: 0.690129
Global Iter: 191400 training acc: 0.53125
Global Iter: 191500 training loss: 0.685883
Global Iter: 191500 training acc: 0.59375
Global Iter: 191600 training loss: 0.698652
Global Iter: 191600 training acc: 0.5
Global Iter: 191700 training loss: 0.710143
Global Iter: 191700 training acc: 0.375
Global Iter: 191800 training loss: 0.70965
Global Iter: 191800 training acc: 0.40625
Global Iter: 191900 training loss: 0.691389
Global Iter: 191900 training acc: 0.5625
Global Iter: 192000 training loss: 0.713635
Global Iter: 192000 training acc: 0.375
Global Iter: 192100 training loss: 0.675732
Global Iter: 192100 training acc: 0.65625
Global Iter: 192200 training loss: 0.697733
Global Iter: 192200 training acc: 0.46875
Global Iter: 192300 training loss: 0.680104
Global Iter: 192300 training acc: 0.625
Global Iter: 192400 training loss: 0.682754
Global Iter: 192400 training acc: 0.625
Global Iter: 192500 training loss: 0.687711
Global Iter: 192500 training acc: 0.5625
Global Iter: 192600 training loss: 0.688843
Global Iter: 192600 training acc: 0.5625
Global Iter: 192700 training loss: 0.693808
Global Iter: 192700 training acc: 0.5
Global Iter: 192800 training loss: 0.684849
Global Iter: 192800 training acc: 0.5625
Global Iter: 192900 training loss: 0.693117
Global Iter: 192900 training acc: 0.5625
Global Iter: 193000 training loss: 0.683947
Global Iter: 193000 training acc: 0.59375
Global Iter: 193100 training loss: 0.720233
Global Iter: 193100 training acc: 0.3125
Global Iter: 193200 training loss: 0.668023
Global Iter: 193200 training acc: 0.71875
Global Iter: 193300 training loss: 0.695069
Global Iter: 193300 training acc: 0.53125
Global Iter: 193400 training loss: 0.706066
Global Iter: 193400 training acc: 0.4375
Global Iter: 193500 training loss: 0.688733
Global Iter: 193500 training acc: 0.5625
Global Iter: 193600 training loss: 0.679484
Global Iter: 193600 training acc: 0.625
Global Iter: 193700 training loss: 0.705275
Global Iter: 193700 training acc: 0.4375
Global Iter: 193800 training loss: 0.68341
Global Iter: 193800 training acc: 0.625
Global Iter: 193900 training loss: 0.695319
Global Iter: 193900 training acc: 0.46875
Global Iter: 194000 training loss: 0.691773
Global Iter: 194000 training acc: 0.53125
Global Iter: 194100 training loss: 0.668061
Global Iter: 194100 training acc: 0.75
Global Iter: 194200 training loss: 0.673864
Global Iter: 194200 training acc: 0.65625
Global Iter: 194300 training loss: 0.698795
Global Iter: 194300 training acc: 0.4375
Global Iter: 194400 training loss: 0.684714
Global Iter: 194400 training acc: 0.5625
Global Iter: 194500 training loss: 0.685064
Global Iter: 194500 training acc: 0.59375
Global Iter: 194600 training loss: 0.729515
Global Iter: 194600 training acc: 0.21875
Global Iter: 194700 training loss: 0.69946
Global Iter: 194700 training acc: 0.46875
Global Iter: 194800 training loss: 0.681191
Global Iter: 194800 training acc: 0.59375
Global Iter: 194900 training loss: 0.677419
Global Iter: 194900 training acc: 0.625
Global Iter: 195000 training loss: 0.674956
Global Iter: 195000 training acc: 0.6875
Global Iter: 195100 training loss: 0.700792
Global Iter: 195100 training acc: 0.46875
Global Iter: 195200 training loss: 0.676524
Global Iter: 195200 training acc: 0.65625
Global Iter: 195300 training loss: 0.703944
Global Iter: 195300 training acc: 0.4375
Global Iter: 195400 training loss: 0.692881
Global Iter: 195400 training acc: 0.5
Global Iter: 195500 training loss: 0.691234
Global Iter: 195500 training acc: 0.5625
Global Iter: 195600 training loss: 0.702376
Global Iter: 195600 training acc: 0.46875
Global Iter: 195700 training loss: 0.698304
Global Iter: 195700 training acc: 0.5
Global Iter: 195800 training loss: 0.67314
Global Iter: 195800 training acc: 0.65625
Global Iter: 195900 training loss: 0.687083
Global Iter: 195900 training acc: 0.5625
Global Iter: 196000 training loss: 0.710597
Global Iter: 196000 training acc: 0.375
Global Iter: 196100 training loss: 0.700558
Global Iter: 196100 training acc: 0.5
Global Iter: 196200 training loss: 0.686754
Global Iter: 196200 training acc: 0.53125
Global Iter: 196300 training loss: 0.704365
Global Iter: 196300 training acc: 0.4375
Global Iter: 196400 training loss: 0.672082
Global Iter: 196400 training acc: 0.6875
Global Iter: 196500 training loss: 0.707263
Global Iter: 196500 training acc: 0.4375
Global Iter: 196600 training loss: 0.675386
Global Iter: 196600 training acc: 0.625
Global Iter: 196700 training loss: 0.68047
Global Iter: 196700 training acc: 0.625
Global Iter: 196800 training loss: 0.662786
Global Iter: 196800 training acc: 0.71875
Global Iter: 196900 training loss: 0.693826
Global Iter: 196900 training acc: 0.5
Global Iter: 197000 training loss: 0.684771
Global Iter: 197000 training acc: 0.625
Global Iter: 197100 training loss: 0.663409
Global Iter: 197100 training acc: 0.78125
Global Iter: 197200 training loss: 0.705541
Global Iter: 197200 training acc: 0.4375
Global Iter: 197300 training loss: 0.699125
Global Iter: 197300 training acc: 0.5
Global Iter: 197400 training loss: 0.695189
Global Iter: 197400 training acc: 0.53125
Global Iter: 197500 training loss: 0.736476
Global Iter: 197500 training acc: 0.21875
Global Iter: 197600 training loss: 0.691875
Global Iter: 197600 training acc: 0.53125
Global Iter: 197700 training loss: 0.700128
Global Iter: 197700 training acc: 0.46875
Global Iter: 197800 training loss: 0.69642
Global Iter: 197800 training acc: 0.5
Global Iter: 197900 training loss: 0.695197
Global Iter: 197900 training acc: 0.4375
Global Iter: 198000 training loss: 0.674155
Global Iter: 198000 training acc: 0.65625
Global Iter: 198100 training loss: 0.685749
Global Iter: 198100 training acc: 0.59375
Global Iter: 198200 training loss: 0.700805
Global Iter: 198200 training acc: 0.46875
Global Iter: 198300 training loss: 0.683907
Global Iter: 198300 training acc: 0.59375
Global Iter: 198400 training loss: 0.695783
Global Iter: 198400 training acc: 0.5
Global Iter: 198500 training loss: 0.687584
Global Iter: 198500 training acc: 0.53125
Global Iter: 198600 training loss: 0.698371
Global Iter: 198600 training acc: 0.46875
Global Iter: 198700 training loss: 0.689877
Global Iter: 198700 training acc: 0.53125
Global Iter: 198800 training loss: 0.698635
Global Iter: 198800 training acc: 0.5
Global Iter: 198900 training loss: 0.700592
Global Iter: 198900 training acc: 0.46875
Global Iter: 199000 training loss: 0.717086
Global Iter: 199000 training acc: 0.375
Global Iter: 199100 training loss: 0.683804
Global Iter: 199100 training acc: 0.5625
Global Iter: 199200 training loss: 0.676088
Global Iter: 199200 training acc: 0.625
Global Iter: 199300 training loss: 0.690963
Global Iter: 199300 training acc: 0.53125
Global Iter: 199400 training loss: 0.701052
Global Iter: 199400 training acc: 0.46875
Global Iter: 199500 training loss: 0.682982
Global Iter: 199500 training acc: 0.59375
Global Iter: 199600 training loss: 0.698133
Global Iter: 199600 training acc: 0.46875
Global Iter: 199700 training loss: 0.68
Global Iter: 199700 training acc: 0.625
Global Iter: 199800 training loss: 0.696978
Global Iter: 199800 training acc: 0.5
Global Iter: 199900 training loss: 0.694761
Global Iter: 199900 training acc: 0.5
Global Iter: 200000 training loss: 0.669892
Global Iter: 200000 training acc: 0.75
Global Iter: 200100 training loss: 0.689674
Global Iter: 200100 training acc: 0.5
Global Iter: 200200 training loss: 0.67343
Global Iter: 200200 training acc: 0.65625
Global Iter: 200300 training loss: 0.681958
Global Iter: 200300 training acc: 0.59375
Global Iter: 200400 training loss: 0.719876
Global Iter: 200400 training acc: 0.375
Global Iter: 200500 training loss: 0.69278
Global Iter: 200500 training acc: 0.5
Global Iter: 200600 training loss: 0.689767
Global Iter: 200600 training acc: 0.5625
Global Iter: 200700 training loss: 0.688174
Global Iter: 200700 training acc: 0.53125
Global Iter: 200800 training loss: 0.689769
Global Iter: 200800 training acc: 0.53125
Global Iter: 200900 training loss: 0.68959
Global Iter: 200900 training acc: 0.53125
Global Iter: 201000 training loss: 0.706252
Global Iter: 201000 training acc: 0.4375
Global Iter: 201100 training loss: 0.670784
Global Iter: 201100 training acc: 0.71875
Global Iter: 201200 training loss: 0.695183
Global Iter: 201200 training acc: 0.5
Global Iter: 201300 training loss: 0.688392
Global Iter: 201300 training acc: 0.5
Global Iter: 201400 training loss: 0.683909
Global Iter: 201400 training acc: 0.59375
Global Iter: 201500 training loss: 0.700062
Global Iter: 201500 training acc: 0.4375
Global Iter: 201600 training loss: 0.71041
Global Iter: 201600 training acc: 0.34375
Global Iter: 201700 training loss: 0.701538
Global Iter: 201700 training acc: 0.40625
Global Iter: 201800 training loss: 0.685665
Global Iter: 201800 training acc: 0.5625
Global Iter: 201900 training loss: 0.720235
Global Iter: 201900 training acc: 0.375
Global Iter: 202000 training loss: 0.676623
Global Iter: 202000 training acc: 0.65625
Global Iter: 202100 training loss: 0.700908
Global Iter: 202100 training acc: 0.4375
Global Iter: 202200 training loss: 0.683825
Global Iter: 202200 training acc: 0.5625
Global Iter: 202300 training loss: 0.680382
Global Iter: 202300 training acc: 0.625
Global Iter: 202400 training loss: 0.694851
Global Iter: 202400 training acc: 0.5625
Global Iter: 202500 training loss: 0.689344
Global Iter: 202500 training acc: 0.5
Global Iter: 202600 training loss: 0.700666
Global Iter: 202600 training acc: 0.46875
Global Iter: 202700 training loss: 0.690283
Global Iter: 202700 training acc: 0.5625
Global Iter: 202800 training loss: 0.682838
Global Iter: 202800 training acc: 0.5625
Global Iter: 202900 training loss: 0.679415
Global Iter: 202900 training acc: 0.59375
Global Iter: 203000 training loss: 0.71038
Global Iter: 203000 training acc: 0.375
Global Iter: 203100 training loss: 0.665744
Global Iter: 203100 training acc: 0.71875
Global Iter: 203200 training loss: 0.692657
Global Iter: 203200 training acc: 0.53125
Global Iter: 203300 training loss: 0.70452
Global Iter: 203300 training acc: 0.4375
Global Iter: 203400 training loss: 0.694385
Global Iter: 203400 training acc: 0.53125
Global Iter: 203500 training loss: 0.686947
Global Iter: 203500 training acc: 0.59375
Global Iter: 203600 training loss: 0.702086
Global Iter: 203600 training acc: 0.46875
Global Iter: 203700 training loss: 0.6832
Global Iter: 203700 training acc: 0.625
Global Iter: 203800 training loss: 0.68925
Global Iter: 203800 training acc: 0.53125
Global Iter: 203900 training loss: 0.693527
Global Iter: 203900 training acc: 0.53125
Global Iter: 204000 training loss: 0.66736
Global Iter: 204000 training acc: 0.71875
Global Iter: 204100 training loss: 0.679785
Global Iter: 204100 training acc: 0.65625
Global Iter: 204200 training loss: 0.711243
Global Iter: 204200 training acc: 0.375
Global Iter: 204300 training loss: 0.687052
Global Iter: 204300 training acc: 0.59375
Global Iter: 204400 training loss: 0.686687
Global Iter: 204400 training acc: 0.59375
Global Iter: 204500 training loss: 0.731607
Global Iter: 204500 training acc: 0.21875
Global Iter: 204600 training loss: 0.706936
Global Iter: 204600 training acc: 0.4375
Global Iter: 204700 training loss: 0.690139
Global Iter: 204700 training acc: 0.5625
Global Iter: 204800 training loss: 0.682565
Global Iter: 204800 training acc: 0.625
Global Iter: 204900 training loss: 0.67165
Global Iter: 204900 training acc: 0.71875
Global Iter: 205000 training loss: 0.699306
Global Iter: 205000 training acc: 0.46875
Global Iter: 205100 training loss: 0.663582
Global Iter: 205100 training acc: 0.71875
Global Iter: 205200 training loss: 0.705524
Global Iter: 205200 training acc: 0.5
Global Iter: 205300 training loss: 0.685046
Global Iter: 205300 training acc: 0.53125
Global Iter: 205400 training loss: 0.685656
Global Iter: 205400 training acc: 0.625
Global Iter: 205500 training loss: 0.696864
Global Iter: 205500 training acc: 0.46875
Global Iter: 205600 training loss: 0.705444
Global Iter: 205600 training acc: 0.4375
Global Iter: 205700 training loss: 0.677098
Global Iter: 205700 training acc: 0.65625
Global Iter: 205800 training loss: 0.689961
Global Iter: 205800 training acc: 0.53125
Global Iter: 205900 training loss: 0.714384
Global Iter: 205900 training acc: 0.34375
Global Iter: 206000 training loss: 0.696964
Global Iter: 206000 training acc: 0.53125
Global Iter: 206100 training loss: 0.688591
Global Iter: 206100 training acc: 0.5625
Global Iter: 206200 training loss: 0.705082
Global Iter: 206200 training acc: 0.4375
Global Iter: 206300 training loss: 0.667359
Global Iter: 206300 training acc: 0.71875
Global Iter: 206400 training loss: 0.705355
Global Iter: 206400 training acc: 0.40625
Global Iter: 206500 training loss: 0.686375
Global Iter: 206500 training acc: 0.5625
Global Iter: 206600 training loss: 0.684572
Global Iter: 206600 training acc: 0.59375
Global Iter: 206700 training loss: 0.668002
Global Iter: 206700 training acc: 0.6875
Global Iter: 206800 training loss: 0.697085
Global Iter: 206800 training acc: 0.5
Global Iter: 206900 training loss: 0.675532
Global Iter: 206900 training acc: 0.65625
Global Iter: 207000 training loss: 0.66429
Global Iter: 207000 training acc: 0.75
Global Iter: 207100 training loss: 0.70286
Global Iter: 207100 training acc: 0.4375
Global Iter: 207200 training loss: 0.694005
Global Iter: 207200 training acc: 0.5
Global Iter: 207300 training loss: 0.696447
Global Iter: 207300 training acc: 0.5
Global Iter: 207400 training loss: 0.732446
Global Iter: 207400 training acc: 0.25
Global Iter: 207500 training loss: 0.698658
Global Iter: 207500 training acc: 0.5
Global Iter: 207600 training loss: 0.696122
Global Iter: 207600 training acc: 0.46875
Global Iter: 207700 training loss: 0.701832
Global Iter: 207700 training acc: 0.46875
Global Iter: 207800 training loss: 0.699254
Global Iter: 207800 training acc: 0.46875
Global Iter: 207900 training loss: 0.672634
Global Iter: 207900 training acc: 0.65625
Global Iter: 208000 training loss: 0.68579
Global Iter: 208000 training acc: 0.59375
Global Iter: 208100 training loss: 0.70543
Global Iter: 208100 training acc: 0.4375
Global Iter: 208200 training loss: 0.681798
Global Iter: 208200 training acc: 0.59375
Global Iter: 208300 training loss: 0.69141
Global Iter: 208300 training acc: 0.5
Global Iter: 208400 training loss: 0.690912
Global Iter: 208400 training acc: 0.5625
Global Iter: 208500 training loss: 0.694686
Global Iter: 208500 training acc: 0.46875
Global Iter: 208600 training loss: 0.695805
Global Iter: 208600 training acc: 0.53125
Global Iter: 208700 training loss: 0.697957
Global Iter: 208700 training acc: 0.5
Global Iter: 208800 training loss: 0.70225
Global Iter: 208800 training acc: 0.4375
Global Iter: 208900 training loss: 0.7163
Global Iter: 208900 training acc: 0.375
Global Iter: 209000 training loss: 0.697127
Global Iter: 209000 training acc: 0.53125
Global Iter: 209100 training loss: 0.675695
Global Iter: 209100 training acc: 0.65625
Global Iter: 209200 training loss: 0.68069
Global Iter: 209200 training acc: 0.59375
Global Iter: 209300 training loss: 0.692905
Global Iter: 209300 training acc: 0.5
Global Iter: 209400 training loss: 0.690449
Global Iter: 209400 training acc: 0.5625
Global Iter: 209500 training loss: 0.695904
Global Iter: 209500 training acc: 0.5
Global Iter: 209600 training loss: 0.690225
Global Iter: 209600 training acc: 0.5625
Global Iter: 209700 training loss: 0.69573
Global Iter: 209700 training acc: 0.5
Global Iter: 209800 training loss: 0.691928
Global Iter: 209800 training acc: 0.53125
Global Iter: 209900 training loss: 0.664953
Global Iter: 209900 training acc: 0.75
Global Iter: 210000 training loss: 0.699028
Global Iter: 210000 training acc: 0.4375
Global Iter: 210100 training loss: 0.681831
Global Iter: 210100 training acc: 0.625
Global Iter: 210200 training loss: 0.681322
Global Iter: 210200 training acc: 0.59375
Global Iter: 210300 training loss: 0.717084
Global Iter: 210300 training acc: 0.34375
Global Iter: 210400 training loss: 0.698862
Global Iter: 210400 training acc: 0.46875
Global Iter: 210500 training loss: 0.68452
Global Iter: 210500 training acc: 0.5625
Global Iter: 210600 training loss: 0.687697
Global Iter: 210600 training acc: 0.53125
Global Iter: 210700 training loss: 0.695529
Global Iter: 210700 training acc: 0.5
Global Iter: 210800 training loss: 0.678456
Global Iter: 210800 training acc: 0.59375
Global Iter: 210900 training loss: 0.695812
Global Iter: 210900 training acc: 0.5
Global Iter: 211000 training loss: 0.671465
Global Iter: 211000 training acc: 0.65625
Global Iter: 211100 training loss: 0.680487
Global Iter: 211100 training acc: 0.5625
Global Iter: 211200 training loss: 0.690422
Global Iter: 211200 training acc: 0.5625
Global Iter: 211300 training loss: 0.683802
Global Iter: 211300 training acc: 0.59375
Global Iter: 211400 training loss: 0.710803
Global Iter: 211400 training acc: 0.40625
Global Iter: 211500 training loss: 0.710166
Global Iter: 211500 training acc: 0.375
Global Iter: 211600 training loss: 0.712401
Global Iter: 211600 training acc: 0.40625
Global Iter: 211700 training loss: 0.681981
Global Iter: 211700 training acc: 0.5625
Global Iter: 211800 training loss: 0.714654
Global Iter: 211800 training acc: 0.40625
Global Iter: 211900 training loss: 0.684503
Global Iter: 211900 training acc: 0.59375
Global Iter: 212000 training loss: 0.69389
Global Iter: 212000 training acc: 0.5
Global Iter: 212100 training loss: 0.696066
Global Iter: 212100 training acc: 0.5
Global Iter: 212200 training loss: 0.680752
Global Iter: 212200 training acc: 0.625
Global Iter: 212300 training loss: 0.678339
Global Iter: 212300 training acc: 0.59375
Global Iter: 212400 training loss: 0.685994
Global Iter: 212400 training acc: 0.53125
Global Iter: 212500 training loss: 0.695164
Global Iter: 212500 training acc: 0.5
Global Iter: 212600 training loss: 0.691723
Global Iter: 212600 training acc: 0.53125
Global Iter: 212700 training loss: 0.694224
Global Iter: 212700 training acc: 0.5
Global Iter: 212800 training loss: 0.683786
Global Iter: 212800 training acc: 0.59375
Global Iter: 212900 training loss: 0.705819
Global Iter: 212900 training acc: 0.375
Global Iter: 213000 training loss: 0.667421
Global Iter: 213000 training acc: 0.71875
Global Iter: 213100 training loss: 0.689693
Global Iter: 213100 training acc: 0.5625
Global Iter: 213200 training loss: 0.692092
Global Iter: 213200 training acc: 0.5
Global Iter: 213300 training loss: 0.683486
Global Iter: 213300 training acc: 0.5625
Global Iter: 213400 training loss: 0.694562
Global Iter: 213400 training acc: 0.53125
Global Iter: 213500 training loss: 0.705535
Global Iter: 213500 training acc: 0.46875
Global Iter: 213600 training loss: 0.679612
Global Iter: 213600 training acc: 0.625
Global Iter: 213700 training loss: 0.698523
Global Iter: 213700 training acc: 0.46875
Global Iter: 213800 training loss: 0.703093
Global Iter: 213800 training acc: 0.46875
Global Iter: 213900 training loss: 0.666008
Global Iter: 213900 training acc: 0.75
Global Iter: 214000 training loss: 0.678092
Global Iter: 214000 training acc: 0.625
Global Iter: 214100 training loss: 0.704773
Global Iter: 214100 training acc: 0.40625
Global Iter: 214200 training loss: 0.682879
Global Iter: 214200 training acc: 0.59375
Global Iter: 214300 training loss: 0.679289
Global Iter: 214300 training acc: 0.625
Global Iter: 214400 training loss: 0.735054
Global Iter: 214400 training acc: 0.21875
Global Iter: 214500 training loss: 0.714322
Global Iter: 214500 training acc: 0.40625
Global Iter: 214600 training loss: 0.686454
Global Iter: 214600 training acc: 0.5625
Global Iter: 214700 training loss: 0.684238
Global Iter: 214700 training acc: 0.59375
Global Iter: 214800 training loss: 0.669884
Global Iter: 214800 training acc: 0.6875
Global Iter: 214900 training loss: 0.696937
Global Iter: 214900 training acc: 0.46875
Global Iter: 215000 training loss: 0.666499
Global Iter: 215000 training acc: 0.71875
Global Iter: 215100 training loss: 0.689393
Global Iter: 215100 training acc: 0.53125
Global Iter: 215200 training loss: 0.706061
Global Iter: 215200 training acc: 0.46875
Global Iter: 215300 training loss: 0.684589
Global Iter: 215300 training acc: 0.59375
Global Iter: 215400 training loss: 0.708212
Global Iter: 215400 training acc: 0.4375
Global Iter: 215500 training loss: 0.708517
Global Iter: 215500 training acc: 0.40625
Global Iter: 215600 training loss: 0.680202
Global Iter: 215600 training acc: 0.65625
Global Iter: 215700 training loss: 0.694862
Global Iter: 215700 training acc: 0.53125
Global Iter: 215800 training loss: 0.719396
Global Iter: 215800 training acc: 0.3125
Global Iter: 215900 training loss: 0.693149
Global Iter: 215900 training acc: 0.53125
Global Iter: 216000 training loss: 0.686774
Global Iter: 216000 training acc: 0.5625
Global Iter: 216100 training loss: 0.707253
Global Iter: 216100 training acc: 0.4375
Global Iter: 216200 training loss: 0.670217
Global Iter: 216200 training acc: 0.6875
Global Iter: 216300 training loss: 0.705525
Global Iter: 216300 training acc: 0.4375
Global Iter: 216400 training loss: 0.693316
Global Iter: 216400 training acc: 0.53125
Global Iter: 216500 training loss: 0.687123
Global Iter: 216500 training acc: 0.59375
Global Iter: 216600 training loss: 0.676666
Global Iter: 216600 training acc: 0.625
Global Iter: 216700 training loss: 0.684319
Global Iter: 216700 training acc: 0.5625
Global Iter: 216800 training loss: 0.681583
Global Iter: 216800 training acc: 0.65625
Global Iter: 216900 training loss: 0.662335
Global Iter: 216900 training acc: 0.75
Global Iter: 217000 training loss: 0.699191
Global Iter: 217000 training acc: 0.4375
Global Iter: 217100 training loss: 0.700696
Global Iter: 217100 training acc: 0.46875
Global Iter: 217200 training loss: 0.696231
Global Iter: 217200 training acc: 0.5
Global Iter: 217300 training loss: 0.731659
Global Iter: 217300 training acc: 0.25
Global Iter: 217400 training loss: 0.695559
Global Iter: 217400 training acc: 0.5
Global Iter: 217500 training loss: 0.70024
Global Iter: 217500 training acc: 0.46875
Global Iter: 217600 training loss: 0.693953
Global Iter: 217600 training acc: 0.53125
Global Iter: 217700 training loss: 0.698532
Global Iter: 217700 training acc: 0.5
Global Iter: 217800 training loss: 0.676469
Global Iter: 217800 training acc: 0.625
Global Iter: 217900 training loss: 0.684442
Global Iter: 217900 training acc: 0.59375
Global Iter: 218000 training loss: 0.702911
Global Iter: 218000 training acc: 0.4375
Global Iter: 218100 training loss: 0.676344
Global Iter: 218100 training acc: 0.59375
Global Iter: 218200 training loss: 0.700674
Global Iter: 218200 training acc: 0.46875
Global Iter: 218300 training loss: 0.683623
Global Iter: 218300 training acc: 0.5625
Global Iter: 218400 training loss: 0.708178
Global Iter: 218400 training acc: 0.40625
Global Iter: 218500 training loss: 0.684534
Global Iter: 218500 training acc: 0.59375
Global Iter: 218600 training loss: 0.698604
Global Iter: 218600 training acc: 0.5
Global Iter: 218700 training loss: 0.703088
Global Iter: 218700 training acc: 0.4375
Global Iter: 218800 training loss: 0.7136
Global Iter: 218800 training acc: 0.375
Global Iter: 218900 training loss: 0.696583
Global Iter: 218900 training acc: 0.46875
Global Iter: 219000 training loss: 0.676392
Global Iter: 219000 training acc: 0.65625
Global Iter: 219100 training loss: 0.688273
Global Iter: 219100 training acc: 0.5625
Global Iter: 219200 training loss: 0.701373
Global Iter: 219200 training acc: 0.46875
Global Iter: 219300 training loss: 0.682021
Global Iter: 219300 training acc: 0.5625
Global Iter: 219400 training loss: 0.707406
Global Iter: 219400 training acc: 0.4375
Global Iter: 219500 training loss: 0.68159
Global Iter: 219500 training acc: 0.59375
Global Iter: 219600 training loss: 0.690049
Global Iter: 219600 training acc: 0.5
Global Iter: 219700 training loss: 0.685263
Global Iter: 219700 training acc: 0.59375
Global Iter: 219800 training loss: 0.660813
Global Iter: 219800 training acc: 0.75
Global Iter: 219900 training loss: 0.694287
Global Iter: 219900 training acc: 0.5
Global Iter: 220000 training loss: 0.681333
Global Iter: 220000 training acc: 0.59375
Global Iter: 220100 training loss: 0.685907
Global Iter: 220100 training acc: 0.59375
Global Iter: 220200 training loss: 0.713587
Global Iter: 220200 training acc: 0.34375
Global Iter: 220300 training loss: 0.694502
Global Iter: 220300 training acc: 0.5
Global Iter: 220400 training loss: 0.699334
Global Iter: 220400 training acc: 0.5
Global Iter: 220500 training loss: 0.694265
Global Iter: 220500 training acc: 0.5
Global Iter: 220600 training loss: 0.689394
Global Iter: 220600 training acc: 0.53125
Global Iter: 220700 training loss: 0.682486
Global Iter: 220700 training acc: 0.59375
Global Iter: 220800 training loss: 0.694023
Global Iter: 220800 training acc: 0.5
Global Iter: 220900 training loss: 0.678064
Global Iter: 220900 training acc: 0.625
Global Iter: 221000 training loss: 0.686417
Global Iter: 221000 training acc: 0.59375
Global Iter: 221100 training loss: 0.688248
Global Iter: 221100 training acc: 0.5625
Global Iter: 221200 training loss: 0.679684
Global Iter: 221200 training acc: 0.625
Global Iter: 221300 training loss: 0.703729
Global Iter: 221300 training acc: 0.4375
Global Iter: 221400 training loss: 0.711638
Global Iter: 221400 training acc: 0.375
Global Iter: 221500 training loss: 0.695266
Global Iter: 221500 training acc: 0.4375
Global Iter: 221600 training loss: 0.6901
Global Iter: 221600 training acc: 0.53125
Global Iter: 221700 training loss: 0.708381
Global Iter: 221700 training acc: 0.40625
Global Iter: 221800 training loss: 0.680544
Global Iter: 221800 training acc: 0.625
Global Iter: 221900 training loss: 0.692449
Global Iter: 221900 training acc: 0.53125
Global Iter: 222000 training loss: 0.70043
Global Iter: 222000 training acc: 0.46875
Global Iter: 222100 training loss: 0.676756
Global Iter: 222100 training acc: 0.59375
Global Iter: 222200 training loss: 0.687078
Global Iter: 222200 training acc: 0.5625
Global Iter: 222300 training loss: 0.691949
Global Iter: 222300 training acc: 0.5
Global Iter: 222400 training loss: 0.694091
Global Iter: 222400 training acc: 0.53125
Global Iter: 222500 training loss: 0.688358
Global Iter: 222500 training acc: 0.5625
Global Iter: 222600 training loss: 0.696418
Global Iter: 222600 training acc: 0.46875
Global Iter: 222700 training loss: 0.675288
Global Iter: 222700 training acc: 0.65625
Global Iter: 222800 training loss: 0.710323
Global Iter: 222800 training acc: 0.375
Global Iter: 222900 training loss: 0.67037
Global Iter: 222900 training acc: 0.6875
Global Iter: 223000 training loss: 0.67887
Global Iter: 223000 training acc: 0.625
Global Iter: 223100 training loss: 0.686979
Global Iter: 223100 training acc: 0.5
Global Iter: 223200 training loss: 0.684899
Global Iter: 223200 training acc: 0.5625
Global Iter: 223300 training loss: 0.68586
Global Iter: 223300 training acc: 0.5625
Global Iter: 223400 training loss: 0.703725
Global Iter: 223400 training acc: 0.46875
Global Iter: 223500 training loss: 0.681442
Global Iter: 223500 training acc: 0.625
Global Iter: 223600 training loss: 0.712038
Global Iter: 223600 training acc: 0.40625
Global Iter: 223700 training loss: 0.701254
Global Iter: 223700 training acc: 0.40625
Global Iter: 223800 training loss: 0.670328
Global Iter: 223800 training acc: 0.71875
Global Iter: 223900 training loss: 0.683726
Global Iter: 223900 training acc: 0.59375
Global Iter: 224000 training loss: 0.696873
Global Iter: 224000 training acc: 0.46875
Global Iter: 224100 training loss: 0.688018
Global Iter: 224100 training acc: 0.5625
Global Iter: 224200 training loss: 0.681966
Global Iter: 224200 training acc: 0.59375
Global Iter: 224300 training loss: 0.729063
Global Iter: 224300 training acc: 0.25
Global Iter: 224400 training loss: 0.702517
Global Iter: 224400 training acc: 0.4375
Global Iter: 224500 training loss: 0.683498
Global Iter: 224500 training acc: 0.59375
Global Iter: 224600 training loss: 0.691477
Global Iter: 224600 training acc: 0.53125
Global Iter: 224700 training loss: 0.681897
Global Iter: 224700 training acc: 0.65625
Global Iter: 224800 training loss: 0.711645
Global Iter: 224800 training acc: 0.40625
Global Iter: 224900 training loss: 0.674836
Global Iter: 224900 training acc: 0.6875
Global Iter: 225000 training loss: 0.690853
Global Iter: 225000 training acc: 0.53125
Global Iter: 225100 training loss: 0.70366
Global Iter: 225100 training acc: 0.46875
Global Iter: 225200 training loss: 0.689819
Global Iter: 225200 training acc: 0.59375
Global Iter: 225300 training loss: 0.707672
Global Iter: 225300 training acc: 0.4375
Global Iter: 225400 training loss: 0.703931
Global Iter: 225400 training acc: 0.4375
Global Iter: 225500 training loss: 0.675578
Global Iter: 225500 training acc: 0.65625
Global Iter: 225600 training loss: 0.684875
Global Iter: 225600 training acc: 0.5625
Global Iter: 225700 training loss: 0.720226
Global Iter: 225700 training acc: 0.3125
Global Iter: 225800 training loss: 0.687027
Global Iter: 225800 training acc: 0.5625
Global Iter: 225900 training loss: 0.690104
Global Iter: 225900 training acc: 0.5625
Global Iter: 226000 training loss: 0.695896
Global Iter: 226000 training acc: 0.5
Global Iter: 226100 training loss: 0.680911
Global Iter: 226100 training acc: 0.625
Global Iter: 226200 training loss: 0.704358
Global Iter: 226200 training acc: 0.4375
Global Iter: 226300 training loss: 0.686836
Global Iter: 226300 training acc: 0.53125
Global Iter: 226400 training loss: 0.686124
Global Iter: 226400 training acc: 0.5625
Global Iter: 226500 training loss: 0.674853
Global Iter: 226500 training acc: 0.625
Global Iter: 226600 training loss: 0.688466
Global Iter: 226600 training acc: 0.5625
Global Iter: 226700 training loss: 0.681945
Global Iter: 226700 training acc: 0.65625
Global Iter: 226800 training loss: 0.665979
Global Iter: 226800 training acc: 0.78125
Global Iter: 226900 training loss: 0.705844
Global Iter: 226900 training acc: 0.40625
Global Iter: 227000 training loss: 0.69992
Global Iter: 227000 training acc: 0.46875
Global Iter: 227100 training loss: 0.701002
Global Iter: 227100 training acc: 0.46875
Global Iter: 227200 training loss: 0.732502
Global Iter: 227200 training acc: 0.25
Global Iter: 227300 training loss: 0.696825
Global Iter: 227300 training acc: 0.5
Global Iter: 227400 training loss: 0.690294
Global Iter: 227400 training acc: 0.53125
Global Iter: 227500 training loss: 0.690389
Global Iter: 227500 training acc: 0.53125
Global Iter: 227600 training loss: 0.686081
Global Iter: 227600 training acc: 0.5625
Global Iter: 227700 training loss: 0.682906
Global Iter: 227700 training acc: 0.625
Global Iter: 227800 training loss: 0.688365
Global Iter: 227800 training acc: 0.5625
Global Iter: 227900 training loss: 0.696255
Global Iter: 227900 training acc: 0.5
Global Iter: 228000 training loss: 0.682181
Global Iter: 228000 training acc: 0.59375
Global Iter: 228100 training loss: 0.694564
Global Iter: 228100 training acc: 0.53125
Global Iter: 228200 training loss: 0.698357
Global Iter: 228200 training acc: 0.5
Global Iter: 228300 training loss: 0.704032
Global Iter: 228300 training acc: 0.4375
Global Iter: 228400 training loss: 0.681409
Global Iter: 228400 training acc: 0.59375
Global Iter: 228500 training loss: 0.695845
Global Iter: 228500 training acc: 0.5
Global Iter: 228600 training loss: 0.712402
Global Iter: 228600 training acc: 0.375
Global Iter: 228700 training loss: 0.71707
Global Iter: 228700 training acc: 0.375
Global Iter: 228800 training loss: 0.695593
Global Iter: 228800 training acc: 0.5
Global Iter: 228900 training loss: 0.677
Global Iter: 228900 training acc: 0.625
Global Iter: 229000 training loss: 0.689511
Global Iter: 229000 training acc: 0.5625
Global Iter: 229100 training loss: 0.698723
Global Iter: 229100 training acc: 0.4375
Global Iter: 229200 training loss: 0.691175
Global Iter: 229200 training acc: 0.59375
Global Iter: 229300 training loss: 0.693907
Global Iter: 229300 training acc: 0.5
Global Iter: 229400 training loss: 0.68885
Global Iter: 229400 training acc: 0.5625
Global Iter: 229500 training loss: 0.68946
Global Iter: 229500 training acc: 0.5
Global Iter: 229600 training loss: 0.681031
Global Iter: 229600 training acc: 0.59375
Global Iter: 229700 training loss: 0.665446
Global Iter: 229700 training acc: 0.75
Global Iter: 229800 training loss: 0.694442
Global Iter: 229800 training acc: 0.5
Global Iter: 229900 training loss: 0.682206
Global Iter: 229900 training acc: 0.5625
Global Iter: 230000 training loss: 0.688651
Global Iter: 230000 training acc: 0.5625
Global Iter: 230100 training loss: 0.716844
Global Iter: 230100 training acc: 0.375
Global Iter: 230200 training loss: 0.7008
Global Iter: 230200 training acc: 0.5
Global Iter: 230300 training loss: 0.70236
Global Iter: 230300 training acc: 0.46875
Global Iter: 230400 training loss: 0.693985
Global Iter: 230400 training acc: 0.5
Global Iter: 230500 training loss: 0.699642
Global Iter: 230500 training acc: 0.5
Global Iter: 230600 training loss: 0.678319
Global Iter: 230600 training acc: 0.625
Global Iter: 230700 training loss: 0.691943
Global Iter: 230700 training acc: 0.5625
Global Iter: 230800 training loss: 0.676012
Global Iter: 230800 training acc: 0.625
Global Iter: 230900 training loss: 0.678941
Global Iter: 230900 training acc: 0.625
Global Iter: 231000 training loss: 0.681792
Global Iter: 231000 training acc: 0.59375
Global Iter: 231100 training loss: 0.679896
Global Iter: 231100 training acc: 0.59375
Global Iter: 231200 training loss: 0.706452
Global Iter: 231200 training acc: 0.46875
Global Iter: 231300 training loss: 0.704201
Global Iter: 231300 training acc: 0.4375
Global Iter: 231400 training loss: 0.697847
Global Iter: 231400 training acc: 0.46875
Global Iter: 231500 training loss: 0.690542
Global Iter: 231500 training acc: 0.53125
Global Iter: 231600 training loss: 0.714232
Global Iter: 231600 training acc: 0.375
Global Iter: 231700 training loss: 0.679968
Global Iter: 231700 training acc: 0.625
Global Iter: 231800 training loss: 0.69545
Global Iter: 231800 training acc: 0.53125
Global Iter: 231900 training loss: 0.703136
Global Iter: 231900 training acc: 0.4375
Global Iter: 232000 training loss: 0.686129
Global Iter: 232000 training acc: 0.59375
Global Iter: 232100 training loss: 0.687196
Global Iter: 232100 training acc: 0.5625
Global Iter: 232200 training loss: 0.692287
Global Iter: 232200 training acc: 0.53125
Global Iter: 232300 training loss: 0.691797
Global Iter: 232300 training acc: 0.5
Global Iter: 232400 training loss: 0.690221
Global Iter: 232400 training acc: 0.53125
Global Iter: 232500 training loss: 0.706151
Global Iter: 232500 training acc: 0.4375
Global Iter: 232600 training loss: 0.681764
Global Iter: 232600 training acc: 0.625
Global Iter: 232700 training loss: 0.706968
Global Iter: 232700 training acc: 0.40625
Global Iter: 232800 training loss: 0.670134
Global Iter: 232800 training acc: 0.6875
Global Iter: 232900 training loss: 0.678043
Global Iter: 232900 training acc: 0.65625
Global Iter: 233000 training loss: 0.690774
Global Iter: 233000 training acc: 0.53125
Global Iter: 233100 training loss: 0.691547
Global Iter: 233100 training acc: 0.53125
Global Iter: 233200 training loss: 0.695538
Global Iter: 233200 training acc: 0.53125
Global Iter: 233300 training loss: 0.701148
Global Iter: 233300 training acc: 0.46875
Global Iter: 233400 training loss: 0.691197
Global Iter: 233400 training acc: 0.5625
Global Iter: 233500 training loss: 0.707482
Global Iter: 233500 training acc: 0.4375
Global Iter: 233600 training loss: 0.70804
Global Iter: 233600 training acc: 0.40625
Global Iter: 233700 training loss: 0.66338
Global Iter: 233700 training acc: 0.75
Global Iter: 233800 training loss: 0.683877
Global Iter: 233800 training acc: 0.5625
Global Iter: 233900 training loss: 0.695217
Global Iter: 233900 training acc: 0.46875
Global Iter: 234000 training loss: 0.691208
Global Iter: 234000 training acc: 0.53125
Global Iter: 234100 training loss: 0.682293
Global Iter: 234100 training acc: 0.5625
Global Iter: 234200 training loss: 0.727611
Global Iter: 234200 training acc: 0.28125
Global Iter: 234300 training loss: 0.705486
Global Iter: 234300 training acc: 0.4375
Global Iter: 234400 training loss: 0.678297
Global Iter: 234400 training acc: 0.625
Global Iter: 234500 training loss: 0.697271
Global Iter: 234500 training acc: 0.5
Global Iter: 234600 training loss: 0.668704
Global Iter: 234600 training acc: 0.71875
Global Iter: 234700 training loss: 0.707502
Global Iter: 234700 training acc: 0.375
Global Iter: 234800 training loss: 0.672748
Global Iter: 234800 training acc: 0.71875
Global Iter: 234900 training loss: 0.687144
Global Iter: 234900 training acc: 0.59375
Global Iter: 235000 training loss: 0.703811
Global Iter: 235000 training acc: 0.4375
Global Iter: 235100 training loss: 0.678163
Global Iter: 235100 training acc: 0.625
Global Iter: 235200 training loss: 0.707141
Global Iter: 235200 training acc: 0.4375
Global Iter: 235300 training loss: 0.70078
Global Iter: 235300 training acc: 0.4375
Global Iter: 235400 training loss: 0.675846
Global Iter: 235400 training acc: 0.65625
Global Iter: 235500 training loss: 0.694685
Global Iter: 235500 training acc: 0.53125
Global Iter: 235600 training loss: 0.727199
Global Iter: 235600 training acc: 0.28125
Global Iter: 235700 training loss: 0.686779
Global Iter: 235700 training acc: 0.5625
Global Iter: 235800 training loss: 0.699113
Global Iter: 235800 training acc: 0.5
Global Iter: 235900 training loss: 0.680591
Global Iter: 235900 training acc: 0.5625
Global Iter: 236000 training loss: 0.675734
Global Iter: 236000 training acc: 0.65625
Global Iter: 236100 training loss: 0.696907
Global Iter: 236100 training acc: 0.5
Global Iter: 236200 training loss: 0.693377
Global Iter: 236200 training acc: 0.53125
Global Iter: 236300 training loss: 0.690287
Global Iter: 236300 training acc: 0.53125
Global Iter: 236400 training loss: 0.68449
Global Iter: 236400 training acc: 0.5625
Global Iter: 236500 training loss: 0.682706
Global Iter: 236500 training acc: 0.59375
Global Iter: 236600 training loss: 0.671548
Global Iter: 236600 training acc: 0.71875
Global Iter: 236700 training loss: 0.660272
Global Iter: 236700 training acc: 0.78125
Global Iter: 236800 training loss: 0.707826
Global Iter: 236800 training acc: 0.40625
Global Iter: 236900 training loss: 0.70121
Global Iter: 236900 training acc: 0.46875
Global Iter: 237000 training loss: 0.695113
Global Iter: 237000 training acc: 0.5
Global Iter: 237100 training loss: 0.736968
Global Iter: 237100 training acc: 0.21875
Global Iter: 237200 training loss: 0.696369
Global Iter: 237200 training acc: 0.5
Global Iter: 237300 training loss: 0.695006
Global Iter: 237300 training acc: 0.5
Global Iter: 237400 training loss: 0.692154
Global Iter: 237400 training acc: 0.53125
Global Iter: 237500 training loss: 0.688251
Global Iter: 237500 training acc: 0.5625
Global Iter: 237600 training loss: 0.68478
Global Iter: 237600 training acc: 0.5625
Global Iter: 237700 training loss: 0.683419
Global Iter: 237700 training acc: 0.59375
Global Iter: 237800 training loss: 0.707558
Global Iter: 237800 training acc: 0.4375
Global Iter: 237900 training loss: 0.679346
Global Iter: 237900 training acc: 0.59375
Global Iter: 238000 training loss: 0.698695
Global Iter: 238000 training acc: 0.5
Global Iter: 238100 training loss: 0.683519
Global Iter: 238100 training acc: 0.5625
Global Iter: 238200 training loss: 0.707967
Global Iter: 238200 training acc: 0.4375
Global Iter: 238300 training loss: 0.684893
Global Iter: 238300 training acc: 0.59375
Global Iter: 238400 training loss: 0.700846
Global Iter: 238400 training acc: 0.5
Global Iter: 238500 training loss: 0.704136
Global Iter: 238500 training acc: 0.4375
Global Iter: 238600 training loss: 0.708997
Global Iter: 238600 training acc: 0.40625
Global Iter: 238700 training loss: 0.690465
Global Iter: 238700 training acc: 0.53125
Global Iter: 238800 training loss: 0.677307
Global Iter: 238800 training acc: 0.625
Global Iter: 238900 training loss: 0.683055
Global Iter: 238900 training acc: 0.59375
Global Iter: 239000 training loss: 0.705837
Global Iter: 239000 training acc: 0.4375
Global Iter: 239100 training loss: 0.678893
Global Iter: 239100 training acc: 0.5625
Global Iter: 239200 training loss: 0.697759
Global Iter: 239200 training acc: 0.5
Global Iter: 239300 training loss: 0.684124
Global Iter: 239300 training acc: 0.625
Global Iter: 239400 training loss: 0.698814
Global Iter: 239400 training acc: 0.46875
Global Iter: 239500 training loss: 0.687766
Global Iter: 239500 training acc: 0.59375
Global Iter: 239600 training loss: 0.664481
Global Iter: 239600 training acc: 0.75
Global Iter: 239700 training loss: 0.690047
Global Iter: 239700 training acc: 0.53125
Global Iter: 239800 training loss: 0.683749
Global Iter: 239800 training acc: 0.5625
Global Iter: 239900 training loss: 0.690691
Global Iter: 239900 training acc: 0.53125
Global Iter: 240000 training loss: 0.71647
Global Iter: 240000 training acc: 0.34375
Global Iter: 240100 training loss: 0.699502
Global Iter: 240100 training acc: 0.5
Global Iter: 240200 training loss: 0.699247
Global Iter: 240200 training acc: 0.46875
Global Iter: 240300 training loss: 0.695961
Global Iter: 240300 training acc: 0.5
Global Iter: 240400 training loss: 0.696649
Global Iter: 240400 training acc: 0.46875
Global Iter: 240500 training loss: 0.675461
Global Iter: 240500 training acc: 0.625
Global Iter: 240600 training loss: 0.691055
Global Iter: 240600 training acc: 0.5
Global Iter: 240700 training loss: 0.686028
Global Iter: 240700 training acc: 0.59375
Global Iter: 240800 training loss: 0.683702
Global Iter: 240800 training acc: 0.59375
Global Iter: 240900 training loss: 0.691736
Global Iter: 240900 training acc: 0.53125
Global Iter: 241000 training loss: 0.680157
Global Iter: 241000 training acc: 0.59375
Global Iter: 241100 training loss: 0.702025
Global Iter: 241100 training acc: 0.46875
Global Iter: 241200 training loss: 0.693925
Global Iter: 241200 training acc: 0.46875
Global Iter: 241300 training loss: 0.707581
Global Iter: 241300 training acc: 0.4375
Global Iter: 241400 training loss: 0.697695
Global Iter: 241400 training acc: 0.5
Global Iter: 241500 training loss: 0.711478
Global Iter: 241500 training acc: 0.34375
Global Iter: 241600 training loss: 0.676778
Global Iter: 241600 training acc: 0.625
Global Iter: 241700 training loss: 0.684679
Global Iter: 241700 training acc: 0.59375
Global Iter: 241800 training loss: 0.705721
Global Iter: 241800 training acc: 0.4375
Global Iter: 241900 training loss: 0.683587
Global Iter: 241900 training acc: 0.59375
Global Iter: 242000 training loss: 0.685712
Global Iter: 242000 training acc: 0.5625
Global Iter: 242100 training loss: 0.693273
Global Iter: 242100 training acc: 0.5
Global Iter: 242200 training loss: 0.69672
Global Iter: 242200 training acc: 0.5
Global Iter: 242300 training loss: 0.684672
Global Iter: 242300 training acc: 0.5625
Global Iter: 242400 training loss: 0.703728
Global Iter: 242400 training acc: 0.4375
Global Iter: 242500 training loss: 0.691633
Global Iter: 242500 training acc: 0.59375
Global Iter: 242600 training loss: 0.699566
Global Iter: 242600 training acc: 0.4375
Global Iter: 242700 training loss: 0.675395
Global Iter: 242700 training acc: 0.6875
Global Iter: 242800 training loss: 0.672278
Global Iter: 242800 training acc: 0.65625
Global Iter: 242900 training loss: 0.697878
Global Iter: 242900 training acc: 0.5
Global Iter: 243000 training loss: 0.690623
Global Iter: 243000 training acc: 0.5
Global Iter: 243100 training loss: 0.694577
Global Iter: 243100 training acc: 0.53125
Global Iter: 243200 training loss: 0.689239
Global Iter: 243200 training acc: 0.53125
Global Iter: 243300 training loss: 0.686512
Global Iter: 243300 training acc: 0.5625
Global Iter: 243400 training loss: 0.709234
Global Iter: 243400 training acc: 0.40625
Global Iter: 243500 training loss: 0.702072
Global Iter: 243500 training acc: 0.4375
Global Iter: 243600 training loss: 0.667201
Global Iter: 243600 training acc: 0.75
Global Iter: 243700 training loss: 0.692507
Global Iter: 243700 training acc: 0.53125
Global Iter: 243800 training loss: 0.69483
Global Iter: 243800 training acc: 0.5
Global Iter: 243900 training loss: 0.673673
Global Iter: 243900 training acc: 0.59375
Global Iter: 244000 training loss: 0.685293
Global Iter: 244000 training acc: 0.5625
Global Iter: 244100 training loss: 0.722838
Global Iter: 244100 training acc: 0.28125
Global Iter: 244200 training loss: 0.708076
Global Iter: 244200 training acc: 0.40625
Global Iter: 244300 training loss: 0.682037
Global Iter: 244300 training acc: 0.625
Global Iter: 244400 training loss: 0.701194
Global Iter: 244400 training acc: 0.46875
Global Iter: 244500 training loss: 0.670654
Global Iter: 244500 training acc: 0.71875
Global Iter: 244600 training loss: 0.710988
Global Iter: 244600 training acc: 0.375
Global Iter: 244700 training loss: 0.6781
Global Iter: 244700 training acc: 0.65625
Global Iter: 244800 training loss: 0.684813
Global Iter: 244800 training acc: 0.59375
Global Iter: 244900 training loss: 0.694131
Global Iter: 244900 training acc: 0.5
Global Iter: 245000 training loss: 0.688392
Global Iter: 245000 training acc: 0.59375
Global Iter: 245100 training loss: 0.696388
Global Iter: 245100 training acc: 0.46875
Global Iter: 245200 training loss: 0.694528
Global Iter: 245200 training acc: 0.46875
Global Iter: 245300 training loss: 0.679187
Global Iter: 245300 training acc: 0.65625
Global Iter: 245400 training loss: 0.694549
Global Iter: 245400 training acc: 0.5
Global Iter: 245500 training loss: 0.714394
Global Iter: 245500 training acc: 0.34375
Global Iter: 245600 training loss: 0.681815
Global Iter: 245600 training acc: 0.625
Global Iter: 245700 training loss: 0.688966
Global Iter: 245700 training acc: 0.53125
Global Iter: 245800 training loss: 0.69319
Global Iter: 245800 training acc: 0.53125
Global Iter: 245900 training loss: 0.683097
Global Iter: 245900 training acc: 0.625
Global Iter: 246000 training loss: 0.685104
Global Iter: 246000 training acc: 0.53125
Global Iter: 246100 training loss: 0.684247
Global Iter: 246100 training acc: 0.5625
Global Iter: 246200 training loss: 0.693202
Global Iter: 246200 training acc: 0.5625
Global Iter: 246300 training loss: 0.697879
Global Iter: 246300 training acc: 0.53125
Global Iter: 246400 training loss: 0.688667
Global Iter: 246400 training acc: 0.5625
Global Iter: 246500 training loss: 0.669337
Global Iter: 246500 training acc: 0.71875
Global Iter: 246600 training loss: 0.658021
Global Iter: 246600 training acc: 0.75
Global Iter: 246700 training loss: 0.707863
Global Iter: 246700 training acc: 0.375
Global Iter: 246800 training loss: 0.702189
Global Iter: 246800 training acc: 0.4375
Global Iter: 246900 training loss: 0.68676
Global Iter: 246900 training acc: 0.5625
Global Iter: 247000 training loss: 0.730523
Global Iter: 247000 training acc: 0.21875
Global Iter: 247100 training loss: 0.701395
Global Iter: 247100 training acc: 0.46875
Global Iter: 247200 training loss: 0.69535
Global Iter: 247200 training acc: 0.5
Global Iter: 247300 training loss: 0.691084
Global Iter: 247300 training acc: 0.53125
Global Iter: 247400 training loss: 0.688685
Global Iter: 247400 training acc: 0.59375
Global Iter: 247500 training loss: 0.685749
Global Iter: 247500 training acc: 0.59375
Global Iter: 247600 training loss: 0.680936
Global Iter: 247600 training acc: 0.65625
Global Iter: 247700 training loss: 0.700643
Global Iter: 247700 training acc: 0.4375
Global Iter: 247800 training loss: 0.685768
Global Iter: 247800 training acc: 0.5625
Global Iter: 247900 training loss: 0.689142
Global Iter: 247900 training acc: 0.5625
Global Iter: 248000 training loss: 0.686896
Global Iter: 248000 training acc: 0.53125
Global Iter: 248100 training loss: 0.701104
Global Iter: 248100 training acc: 0.46875
Global Iter: 248200 training loss: 0.691863
Global Iter: 248200 training acc: 0.5625
Global Iter: 248300 training loss: 0.691231
Global Iter: 248300 training acc: 0.5
Global Iter: 248400 training loss: 0.705443
Global Iter: 248400 training acc: 0.40625
Global Iter: 248500 training loss: 0.70272
Global Iter: 248500 training acc: 0.4375
Global Iter: 248600 training loss: 0.696317
Global Iter: 248600 training acc: 0.5
Global Iter: 248700 training loss: 0.683857
Global Iter: 248700 training acc: 0.59375
Global Iter: 248800 training loss: 0.68413
Global Iter: 248800 training acc: 0.59375
Global Iter: 248900 training loss: 0.708314
Global Iter: 248900 training acc: 0.4375
Global Iter: 249000 training loss: 0.693563
Global Iter: 249000 training acc: 0.5625
Global Iter: 249100 training loss: 0.687549
Global Iter: 249100 training acc: 0.53125
Global Iter: 249200 training loss: 0.685804
Global Iter: 249200 training acc: 0.5625
Global Iter: 249300 training loss: 0.693926
Global Iter: 249300 training acc: 0.5
Global Iter: 249400 training loss: 0.681325
Global Iter: 249400 training acc: 0.625
Global Iter: 249500 training loss: 0.671426
Global Iter: 249500 training acc: 0.75
Global Iter: 249600 training loss: 0.689737
Global Iter: 249600 training acc: 0.5625
Global Iter: 249700 training loss: 0.692006
Global Iter: 249700 training acc: 0.53125
Global Iter: 249800 training loss: 0.68731
Global Iter: 249800 training acc: 0.5625
Global Iter: 249900 training loss: 0.715001
Global Iter: 249900 training acc: 0.34375
Global Iter: 250000 training loss: 0.689839
Global Iter: 250000 training acc: 0.5
Global Iter: 250100 training loss: 0.707811
Global Iter: 250100 training acc: 0.4375
Global Iter: 250200 training loss: 0.697988
Global Iter: 250200 training acc: 0.5
Global Iter: 250300 training loss: 0.702084
Global Iter: 250300 training acc: 0.4375
Global Iter: 250400 training loss: 0.675358
Global Iter: 250400 training acc: 0.625
Global Iter: 250500 training loss: 0.69346
Global Iter: 250500 training acc: 0.5
Global Iter: 250600 training loss: 0.688319
Global Iter: 250600 training acc: 0.53125
Global Iter: 250700 training loss: 0.678609
Global Iter: 250700 training acc: 0.59375
Global Iter: 250800 training loss: 0.691406
Global Iter: 250800 training acc: 0.5
Global Iter: 250900 training loss: 0.680944
Global Iter: 250900 training acc: 0.59375
Global Iter: 251000 training loss: 0.698134
Global Iter: 251000 training acc: 0.5
Global Iter: 251100 training loss: 0.691428
Global Iter: 251100 training acc: 0.5
Global Iter: 251200 training loss: 0.698288
Global Iter: 251200 training acc: 0.46875
Global Iter: 251300 training loss: 0.698327
Global Iter: 251300 training acc: 0.46875
Global Iter: 251400 training loss: 0.718024
Global Iter: 251400 training acc: 0.34375
Global Iter: 251500 training loss: 0.685089
Global Iter: 251500 training acc: 0.59375
Global Iter: 251600 training loss: 0.681295
Global Iter: 251600 training acc: 0.59375
Global Iter: 251700 training loss: 0.710005
Global Iter: 251700 training acc: 0.40625
Global Iter: 251800 training loss: 0.680197
Global Iter: 251800 training acc: 0.625
Global Iter: 251900 training loss: 0.684805
Global Iter: 251900 training acc: 0.5625
Global Iter: 252000 training loss: 0.685673
Global Iter: 252000 training acc: 0.53125
Global Iter: 252100 training loss: 0.689573
Global Iter: 252100 training acc: 0.5625
Global Iter: 252200 training loss: 0.691644
Global Iter: 252200 training acc: 0.5625
Global Iter: 252300 training loss: 0.700907
Global Iter: 252300 training acc: 0.4375
Global Iter: 252400 training loss: 0.680381
Global Iter: 252400 training acc: 0.625
Global Iter: 252500 training loss: 0.703275
Global Iter: 252500 training acc: 0.4375
Global Iter: 252600 training loss: 0.667297
Global Iter: 252600 training acc: 0.6875
Global Iter: 252700 training loss: 0.680266
Global Iter: 252700 training acc: 0.65625
Global Iter: 252800 training loss: 0.69417
Global Iter: 252800 training acc: 0.5
Global Iter: 252900 training loss: 0.700489
Global Iter: 252900 training acc: 0.5
Global Iter: 253000 training loss: 0.683299
Global Iter: 253000 training acc: 0.59375
Global Iter: 253100 training loss: 0.696952
Global Iter: 253100 training acc: 0.5
Global Iter: 253200 training loss: 0.696961
Global Iter: 253200 training acc: 0.53125
Global Iter: 253300 training loss: 0.709255
Global Iter: 253300 training acc: 0.40625
Global Iter: 253400 training loss: 0.702253
Global Iter: 253400 training acc: 0.4375
Global Iter: 253500 training loss: 0.664872
Global Iter: 253500 training acc: 0.78125
Global Iter: 253600 training loss: 0.691935
Global Iter: 253600 training acc: 0.5
Global Iter: 253700 training loss: 0.68989
Global Iter: 253700 training acc: 0.53125
Global Iter: 253800 training loss: 0.685196
Global Iter: 253800 training acc: 0.59375
Global Iter: 253900 training loss: 0.688704
Global Iter: 253900 training acc: 0.5625
Global Iter: 254000 training loss: 0.71302
Global Iter: 254000 training acc: 0.34375
Global Iter: 254100 training loss: 0.717033
Global Iter: 254100 training acc: 0.375
Global Iter: 254200 training loss: 0.674535
Global Iter: 254200 training acc: 0.65625
Global Iter: 254300 training loss: 0.699138
Global Iter: 254300 training acc: 0.46875
Global Iter: 254400 training loss: 0.671486
Global Iter: 254400 training acc: 0.71875
Global Iter: 254500 training loss: 0.717462
Global Iter: 254500 training acc: 0.375
Global Iter: 254600 training loss: 0.688791
Global Iter: 254600 training acc: 0.59375
Global Iter: 254700 training loss: 0.686582
Global Iter: 254700 training acc: 0.5625
Global Iter: 254800 training loss: 0.686743
Global Iter: 254800 training acc: 0.5625
Global Iter: 254900 training loss: 0.691719
Global Iter: 254900 training acc: 0.5625
Global Iter: 255000 training loss: 0.701712
Global Iter: 255000 training acc: 0.5
Global Iter: 255100 training loss: 0.686192
Global Iter: 255100 training acc: 0.53125
Global Iter: 255200 training loss: 0.682604
Global Iter: 255200 training acc: 0.59375
Global Iter: 255300 training loss: 0.690917
Global Iter: 255300 training acc: 0.5
Global Iter: 255400 training loss: 0.713097
Global Iter: 255400 training acc: 0.34375
Global Iter: 255500 training loss: 0.674944
Global Iter: 255500 training acc: 0.65625
Global Iter: 255600 training loss: 0.685612
Global Iter: 255600 training acc: 0.5625
Global Iter: 255700 training loss: 0.691937
Global Iter: 255700 training acc: 0.53125
Global Iter: 255800 training loss: 0.685091
Global Iter: 255800 training acc: 0.59375
Global Iter: 255900 training loss: 0.683423
Global Iter: 255900 training acc: 0.5625
Global Iter: 256000 training loss: 0.69786
Global Iter: 256000 training acc: 0.5
Global Iter: 256100 training loss: 0.67871
Global Iter: 256100 training acc: 0.625
Global Iter: 256200 training loss: 0.698574
Global Iter: 256200 training acc: 0.5
Global Iter: 256300 training loss: 0.691104
Global Iter: 256300 training acc: 0.53125
Global Iter: 256400 training loss: 0.668371
Global Iter: 256400 training acc: 0.71875
Global Iter: 256500 training loss: 0.665477
Global Iter: 256500 training acc: 0.6875
Global Iter: 256600 training loss: 0.712928
Global Iter: 256600 training acc: 0.375
Global Iter: 256700 training loss: 0.692955
Global Iter: 256700 training acc: 0.5
Global Iter: 256800 training loss: 0.687371
Global Iter: 256800 training acc: 0.59375
Global Iter: 256900 training loss: 0.738799
Global Iter: 256900 training acc: 0.21875
Global Iter: 257000 training loss: 0.700057
Global Iter: 257000 training acc: 0.46875
Global Iter: 257100 training loss: 0.69022
Global Iter: 257100 training acc: 0.53125
Global Iter: 257200 training loss: 0.701084
Global Iter: 257200 training acc: 0.46875
Global Iter: 257300 training loss: 0.677596
Global Iter: 257300 training acc: 0.625
Global Iter: 257400 training loss: 0.684496
Global Iter: 257400 training acc: 0.59375
Global Iter: 257500 training loss: 0.673062
Global Iter: 257500 training acc: 0.65625
Global Iter: 257600 training loss: 0.704681
Global Iter: 257600 training acc: 0.4375
Global Iter: 257700 training loss: 0.690631
Global Iter: 257700 training acc: 0.53125
Global Iter: 257800 training loss: 0.692059
Global Iter: 257800 training acc: 0.5625
Global Iter: 257900 training loss: 0.693188
Global Iter: 257900 training acc: 0.53125
Global Iter: 258000 training loss: 0.696999
Global Iter: 258000 training acc: 0.53125
Global Iter: 258100 training loss: 0.685672
Global Iter: 258100 training acc: 0.625
Global Iter: 258200 training loss: 0.691468
Global Iter: 258200 training acc: 0.5625
Global Iter: 258300 training loss: 0.711024
Global Iter: 258300 training acc: 0.40625
Global Iter: 258400 training loss: 0.702477
Global Iter: 258400 training acc: 0.4375
Global Iter: 258500 training loss: 0.699126
Global Iter: 258500 training acc: 0.5
Global Iter: 258600 training loss: 0.687267
Global Iter: 258600 training acc: 0.5625
Global Iter: 258700 training loss: 0.67988
Global Iter: 258700 training acc: 0.625
Global Iter: 258800 training loss: 0.699761
Global Iter: 258800 training acc: 0.46875
Global Iter: 258900 training loss: 0.68641
Global Iter: 258900 training acc: 0.5625
Global Iter: 259000 training loss: 0.690421
Global Iter: 259000 training acc: 0.53125
Global Iter: 259100 training loss: 0.67719
Global Iter: 259100 training acc: 0.625
Global Iter: 259200 training loss: 0.68489
Global Iter: 259200 training acc: 0.53125
Global Iter: 259300 training loss: 0.684938
Global Iter: 259300 training acc: 0.59375
Global Iter: 259400 training loss: 0.662645
Global Iter: 259400 training acc: 0.78125
Global Iter: 259500 training loss: 0.697947
Global Iter: 259500 training acc: 0.53125
Global Iter: 259600 training loss: 0.68396
Global Iter: 259600 training acc: 0.5625
Global Iter: 259700 training loss: 0.690112
Global Iter: 259700 training acc: 0.5625
Global Iter: 259800 training loss: 0.722929
Global Iter: 259800 training acc: 0.34375
Global Iter: 259900 training loss: 0.693451
Global Iter: 259900 training acc: 0.5
Global Iter: 260000 training loss: 0.70591
Global Iter: 260000 training acc: 0.4375
Global Iter: 260100 training loss: 0.703957
Global Iter: 260100 training acc: 0.46875
Global Iter: 260200 training loss: 0.705372
Global Iter: 260200 training acc: 0.40625
Global Iter: 260300 training loss: 0.684016
Global Iter: 260300 training acc: 0.59375
Global Iter: 260400 training loss: 0.696928
Global Iter: 260400 training acc: 0.46875
Global Iter: 260500 training loss: 0.690107
Global Iter: 260500 training acc: 0.53125
Global Iter: 260600 training loss: 0.681193
Global Iter: 260600 training acc: 0.59375
Global Iter: 260700 training loss: 0.692255
Global Iter: 260700 training acc: 0.53125
Global Iter: 260800 training loss: 0.683289
Global Iter: 260800 training acc: 0.59375
Global Iter: 260900 training loss: 0.688139
Global Iter: 260900 training acc: 0.53125
Global Iter: 261000 training loss: 0.690381
Global Iter: 261000 training acc: 0.5
Global Iter: 261100 training loss: 0.695224
Global Iter: 261100 training acc: 0.5
Global Iter: 261200 training loss: 0.690041
Global Iter: 261200 training acc: 0.53125
Global Iter: 261300 training loss: 0.715702
Global Iter: 261300 training acc: 0.375
Global Iter: 261400 training loss: 0.682774
Global Iter: 261400 training acc: 0.5625
Global Iter: 261500 training loss: 0.673457
Global Iter: 261500 training acc: 0.65625
Global Iter: 261600 training loss: 0.709582
Global Iter: 261600 training acc: 0.40625
Global Iter: 261700 training loss: 0.692829
Global Iter: 261700 training acc: 0.5625
Global Iter: 261800 training loss: 0.691869
Global Iter: 261800 training acc: 0.53125
Global Iter: 261900 training loss: 0.694643
Global Iter: 261900 training acc: 0.5
Global Iter: 262000 training loss: 0.684663
Global Iter: 262000 training acc: 0.5625
Global Iter: 262100 training loss: 0.688222
Global Iter: 262100 training acc: 0.5625
Global Iter: 262200 training loss: 0.704826
Global Iter: 262200 training acc: 0.4375
Global Iter: 262300 training loss: 0.677507
Global Iter: 262300 training acc: 0.65625
Global Iter: 262400 training loss: 0.697062
Global Iter: 262400 training acc: 0.46875
Global Iter: 262500 training loss: 0.674809
Global Iter: 262500 training acc: 0.6875
Global Iter: 262600 training loss: 0.682468
Global Iter: 262600 training acc: 0.625
Global Iter: 262700 training loss: 0.701613
Global Iter: 262700 training acc: 0.46875
Global Iter: 262800 training loss: 0.696431
Global Iter: 262800 training acc: 0.5
Global Iter: 262900 training loss: 0.682683
Global Iter: 262900 training acc: 0.625
Global Iter: 263000 training loss: 0.694548
Global Iter: 263000 training acc: 0.5
Global Iter: 263100 training loss: 0.690489
Global Iter: 263100 training acc: 0.53125
Global Iter: 263200 training loss: 0.705479
Global Iter: 263200 training acc: 0.4375
Global Iter: 263300 training loss: 0.703233
Global Iter: 263300 training acc: 0.46875
Global Iter: 263400 training loss: 0.65507
Global Iter: 263400 training acc: 0.8125
Global Iter: 263500 training loss: 0.692489
Global Iter: 263500 training acc: 0.53125
Global Iter: 263600 training loss: 0.691387
Global Iter: 263600 training acc: 0.5625
Global Iter: 263700 training loss: 0.684197
Global Iter: 263700 training acc: 0.59375
Global Iter: 263800 training loss: 0.683321
Global Iter: 263800 training acc: 0.59375
Global Iter: 263900 training loss: 0.717351
Global Iter: 263900 training acc: 0.28125
Global Iter: 264000 training loss: 0.706693
Global Iter: 264000 training acc: 0.40625
Global Iter: 264100 training loss: 0.681297
Global Iter: 264100 training acc: 0.59375
Global Iter: 264200 training loss: 0.695246
Global Iter: 264200 training acc: 0.46875
Global Iter: 264300 training loss: 0.672948
Global Iter: 264300 training acc: 0.71875
Global Iter: 264400 training loss: 0.718986
Global Iter: 264400 training acc: 0.34375
Global Iter: 264500 training loss: 0.679289
Global Iter: 264500 training acc: 0.59375
Global Iter: 264600 training loss: 0.682591
Global Iter: 264600 training acc: 0.59375
Global Iter: 264700 training loss: 0.678983
Global Iter: 264700 training acc: 0.59375
Global Iter: 264800 training loss: 0.679832
Global Iter: 264800 training acc: 0.59375
Global Iter: 264900 training loss: 0.702594
Global Iter: 264900 training acc: 0.46875
Global Iter: 265000 training loss: 0.695472
Global Iter: 265000 training acc: 0.53125
Global Iter: 265100 training loss: 0.686975
Global Iter: 265100 training acc: 0.5625
Global Iter: 265200 training loss: 0.690312
Global Iter: 265200 training acc: 0.53125
Global Iter: 265300 training loss: 0.724028
Global Iter: 265300 training acc: 0.3125
Global Iter: 265400 training loss: 0.673872
Global Iter: 265400 training acc: 0.65625
Global Iter: 265500 training loss: 0.692595
Global Iter: 265500 training acc: 0.5625
Global Iter: 265600 training loss: 0.697854
Global Iter: 265600 training acc: 0.5
Global Iter: 265700 training loss: 0.683678
Global Iter: 265700 training acc: 0.5625
Global Iter: 265800 training loss: 0.685561
Global Iter: 265800 training acc: 0.5625
Global Iter: 265900 training loss: 0.696915
Global Iter: 265900 training acc: 0.53125
Global Iter: 266000 training loss: 0.686389
Global Iter: 266000 training acc: 0.625
Global Iter: 266100 training loss: 0.687245
Global Iter: 266100 training acc: 0.5625
Global Iter: 266200 training loss: 0.693546
Global Iter: 266200 training acc: 0.5
Global Iter: 266300 training loss: 0.664697
Global Iter: 266300 training acc: 0.75
Global Iter: 266400 training loss: 0.676204
Global Iter: 266400 training acc: 0.65625
Global Iter: 266500 training loss: 0.700087
Global Iter: 266500 training acc: 0.4375
Global Iter: 266600 training loss: 0.704701
Global Iter: 266600 training acc: 0.46875
Global Iter: 266700 training loss: 0.695544
Global Iter: 266700 training acc: 0.53125
Global Iter: 266800 training loss: 0.729184
Global Iter: 266800 training acc: 0.21875
Global Iter: 266900 training loss: 0.693245
Global Iter: 266900 training acc: 0.5
Global Iter: 267000 training loss: 0.696814
Global Iter: 267000 training acc: 0.53125
Global Iter: 267100 training loss: 0.697823
Global Iter: 267100 training acc: 0.5
Global Iter: 267200 training loss: 0.685019
Global Iter: 267200 training acc: 0.59375
Global Iter: 267300 training loss: 0.687209
Global Iter: 267300 training acc: 0.5625
Global Iter: 267400 training loss: 0.682669
Global Iter: 267400 training acc: 0.625
Global Iter: 267500 training loss: 0.688752
Global Iter: 267500 training acc: 0.5
Global Iter: 267600 training loss: 0.691008
Global Iter: 267600 training acc: 0.53125
Global Iter: 267700 training loss: 0.689706
Global Iter: 267700 training acc: 0.53125
Global Iter: 267800 training loss: 0.698145
Global Iter: 267800 training acc: 0.5
Global Iter: 267900 training loss: 0.694275
Global Iter: 267900 training acc: 0.53125
Global Iter: 268000 training loss: 0.676868
Global Iter: 268000 training acc: 0.65625
Global Iter: 268100 training loss: 0.690648
Global Iter: 268100 training acc: 0.53125
Global Iter: 268200 training loss: 0.713832
Global Iter: 268200 training acc: 0.34375
Global Iter: 268300 training loss: 0.706884
Global Iter: 268300 training acc: 0.46875
Global Iter: 268400 training loss: 0.690348
Global Iter: 268400 training acc: 0.5
Global Iter: 268500 training loss: 0.688243
Global Iter: 268500 training acc: 0.53125
Global Iter: 268600 training loss: 0.675493
Global Iter: 268600 training acc: 0.65625
Global Iter: 268700 training loss: 0.704511
Global Iter: 268700 training acc: 0.4375
Global Iter: 268800 training loss: 0.689297
Global Iter: 268800 training acc: 0.5625
Global Iter: 268900 training loss: 0.684362
Global Iter: 268900 training acc: 0.5625
Global Iter: 269000 training loss: 0.675544
Global Iter: 269000 training acc: 0.65625
Global Iter: 269100 training loss: 0.689018
Global Iter: 269100 training acc: 0.5625
Global Iter: 269200 training loss: 0.677679
Global Iter: 269200 training acc: 0.65625
Global Iter: 269300 training loss: 0.663208
Global Iter: 269300 training acc: 0.75
Global Iter: 269400 training loss: 0.696409
Global Iter: 269400 training acc: 0.5
Global Iter: 269500 training loss: 0.684214
Global Iter: 269500 training acc: 0.5625
Global Iter: 269600 training loss: 0.693696
Global Iter: 269600 training acc: 0.53125
Global Iter: 269700 training loss: 0.726073
Global Iter: 269700 training acc: 0.28125
Global Iter: 269800 training loss: 0.698949
Global Iter: 269800 training acc: 0.46875
Global Iter: 269900 training loss: 0.703535
Global Iter: 269900 training acc: 0.4375
Global Iter: 270000 training loss: 0.693822
Global Iter: 270000 training acc: 0.5
Global Iter: 270100 training loss: 0.703377
Global Iter: 270100 training acc: 0.40625
Global Iter: 270200 training loss: 0.672752
Global Iter: 270200 training acc: 0.65625
Global Iter: 270300 training loss: 0.684208
Global Iter: 270300 training acc: 0.5625
Global Iter: 270400 training loss: 0.688683
Global Iter: 270400 training acc: 0.53125
Global Iter: 270500 training loss: 0.676067
Global Iter: 270500 training acc: 0.59375
Global Iter: 270600 training loss: 0.692311
Global Iter: 270600 training acc: 0.5
Global Iter: 270700 training loss: 0.689525
Global Iter: 270700 training acc: 0.53125
Global Iter: 270800 training loss: 0.694852
Global Iter: 270800 training acc: 0.5
Global Iter: 270900 training loss: 0.694016
Global Iter: 270900 training acc: 0.5
Global Iter: 271000 training loss: 0.692465
Global Iter: 271000 training acc: 0.53125
Global Iter: 271100 training loss: 0.689064
Global Iter: 271100 training acc: 0.53125
Global Iter: 271200 training loss: 0.714963
Global Iter: 271200 training acc: 0.375
Global Iter: 271300 training loss: 0.688794
Global Iter: 271300 training acc: 0.5625
Global Iter: 271400 training loss: 0.682876
Global Iter: 271400 training acc: 0.625
Global Iter: 271500 training loss: 0.708875
Global Iter: 271500 training acc: 0.4375
Global Iter: 271600 training loss: 0.684594
Global Iter: 271600 training acc: 0.5625
Global Iter: 271700 training loss: 0.688382
Global Iter: 271700 training acc: 0.5625
Global Iter: 271800 training loss: 0.68873
Global Iter: 271800 training acc: 0.53125
Global Iter: 271900 training loss: 0.691388
Global Iter: 271900 training acc: 0.5625
Global Iter: 272000 training loss: 0.686361
Global Iter: 272000 training acc: 0.5625
Global Iter: 272100 training loss: 0.694646
Global Iter: 272100 training acc: 0.46875
Global Iter: 272200 training loss: 0.675916
Global Iter: 272200 training acc: 0.65625
Global Iter: 272300 training loss: 0.692889
Global Iter: 272300 training acc: 0.5
Global Iter: 272400 training loss: 0.674115
Global Iter: 272400 training acc: 0.65625
Global Iter: 272500 training loss: 0.679509
Global Iter: 272500 training acc: 0.625
Global Iter: 272600 training loss: 0.709926
Global Iter: 272600 training acc: 0.40625
Global Iter: 272700 training loss: 0.695789
Global Iter: 272700 training acc: 0.5
Global Iter: 272800 training loss: 0.685109
Global Iter: 272800 training acc: 0.59375
Global Iter: 272900 training loss: 0.688814
Global Iter: 272900 training acc: 0.53125
Global Iter: 273000 training loss: 0.692389
Global Iter: 273000 training acc: 0.5
Global Iter: 273100 training loss: 0.702031
Global Iter: 273100 training acc: 0.46875
Global Iter: 273200 training loss: 0.701019
Global Iter: 273200 training acc: 0.4375
Global Iter: 273300 training loss: 0.660833
Global Iter: 273300 training acc: 0.78125
Global Iter: 273400 training loss: 0.695141
Global Iter: 273400 training acc: 0.5
Global Iter: 273500 training loss: 0.689085
Global Iter: 273500 training acc: 0.53125
Global Iter: 273600 training loss: 0.681265
Global Iter: 273600 training acc: 0.625
Global Iter: 273700 training loss: 0.68731
Global Iter: 273700 training acc: 0.5625
Global Iter: 273800 training loss: 0.715368
Global Iter: 273800 training acc: 0.3125
Global Iter: 273900 training loss: 0.708827
Global Iter: 273900 training acc: 0.40625
Global Iter: 274000 training loss: 0.677798
Global Iter: 274000 training acc: 0.625
Global Iter: 274100 training loss: 0.701909
Global Iter: 274100 training acc: 0.4375
Global Iter: 274200 training loss: 0.669974
Global Iter: 274200 training acc: 0.6875
Global Iter: 274300 training loss: 0.718765
Global Iter: 274300 training acc: 0.375
Global Iter: 274400 training loss: 0.686027
Global Iter: 274400 training acc: 0.59375
Global Iter: 274500 training loss: 0.678876
Global Iter: 274500 training acc: 0.59375
Global Iter: 274600 training loss: 0.685731
Global Iter: 274600 training acc: 0.5625
Global Iter: 274700 training loss: 0.690114
Global Iter: 274700 training acc: 0.5625
Global Iter: 274800 training loss: 0.695648
Global Iter: 274800 training acc: 0.5
Global Iter: 274900 training loss: 0.689618
Global Iter: 274900 training acc: 0.5625
Global Iter: 275000 training loss: 0.691709
Global Iter: 275000 training acc: 0.53125
Global Iter: 275100 training loss: 0.682955
Global Iter: 275100 training acc: 0.5625
Global Iter: 275200 training loss: 0.712948
Global Iter: 275200 training acc: 0.34375
Global Iter: 275300 training loss: 0.665439
Global Iter: 275300 training acc: 0.71875
Global Iter: 275400 training loss: 0.683105
Global Iter: 275400 training acc: 0.59375
Global Iter: 275500 training loss: 0.699507
Global Iter: 275500 training acc: 0.46875
Global Iter: 275600 training loss: 0.684269
Global Iter: 275600 training acc: 0.59375
Global Iter: 275700 training loss: 0.683658
Global Iter: 275700 training acc: 0.59375
Global Iter: 275800 training loss: 0.704647
Global Iter: 275800 training acc: 0.46875
Global Iter: 275900 training loss: 0.673951
Global Iter: 275900 training acc: 0.65625
Global Iter: 276000 training loss: 0.69048
Global Iter: 276000 training acc: 0.53125
Global Iter: 276100 training loss: 0.702312
Global Iter: 276100 training acc: 0.46875
Global Iter: 276200 training loss: 0.669465
Global Iter: 276200 training acc: 0.75
Global Iter: 276300 training loss: 0.676986
Global Iter: 276300 training acc: 0.625
Global Iter: 276400 training loss: 0.70512
Global Iter: 276400 training acc: 0.40625
Global Iter: 276500 training loss: 0.698455
Global Iter: 276500 training acc: 0.5
Global Iter: 276600 training loss: 0.68958
Global Iter: 276600 training acc: 0.5625
Global Iter: 276700 training loss: 0.734446
Global Iter: 276700 training acc: 0.21875
Global Iter: 276800 training loss: 0.705937
Global Iter: 276800 training acc: 0.4375
Global Iter: 276900 training loss: 0.687747
Global Iter: 276900 training acc: 0.5625
Global Iter: 277000 training loss: 0.692303
Global Iter: 277000 training acc: 0.53125
Global Iter: 277100 training loss: 0.678461
Global Iter: 277100 training acc: 0.625
Global Iter: 277200 training loss: 0.689996
Global Iter: 277200 training acc: 0.53125
Global Iter: 277300 training loss: 0.680092
Global Iter: 277300 training acc: 0.65625
Global Iter: 277400 training loss: 0.695575
Global Iter: 277400 training acc: 0.46875
Global Iter: 277500 training loss: 0.694791
Global Iter: 277500 training acc: 0.5
Global Iter: 277600 training loss: 0.680839
Global Iter: 277600 training acc: 0.53125
Global Iter: 277700 training loss: 0.696674
Global Iter: 277700 training acc: 0.5
Global Iter: 277800 training loss: 0.688623
Global Iter: 277800 training acc: 0.5625
Global Iter: 277900 training loss: 0.67075
Global Iter: 277900 training acc: 0.71875
Global Iter: 278000 training loss: 0.693678
Global Iter: 278000 training acc: 0.53125
Global Iter: 278100 training loss: 0.709821
Global Iter: 278100 training acc: 0.375
Global Iter: 278200 training loss: 0.700367
Global Iter: 278200 training acc: 0.5
Global Iter: 278300 training loss: 0.698928
Global Iter: 278300 training acc: 0.5
Global Iter: 278400 training loss: 0.701159
Global Iter: 278400 training acc: 0.5
Global Iter: 278500 training loss: 0.669777
Global Iter: 278500 training acc: 0.71875
Global Iter: 278600 training loss: 0.697479
Global Iter: 278600 training acc: 0.46875
Global Iter: 278700 training loss: 0.680138
Global Iter: 278700 training acc: 0.59375
Global Iter: 278800 training loss: 0.681043
Global Iter: 278800 training acc: 0.625
Global Iter: 278900 training loss: 0.679883
Global Iter: 278900 training acc: 0.625
Global Iter: 279000 training loss: 0.688415
Global Iter: 279000 training acc: 0.53125
Global Iter: 279100 training loss: 0.680371
Global Iter: 279100 training acc: 0.65625
Global Iter: 279200 training loss: 0.662803
Global Iter: 279200 training acc: 0.75
Global Iter: 279300 training loss: 0.689111
Global Iter: 279300 training acc: 0.53125
Global Iter: 279400 training loss: 0.68489
Global Iter: 279400 training acc: 0.5625
Global Iter: 279500 training loss: 0.686744
Global Iter: 279500 training acc: 0.53125
Global Iter: 279600 training loss: 0.729789
Global Iter: 279600 training acc: 0.25
Global Iter: 279700 training loss: 0.701504
Global Iter: 279700 training acc: 0.46875
Global Iter: 279800 training loss: 0.701592
Global Iter: 279800 training acc: 0.4375
Global Iter: 279900 training loss: 0.692194
Global Iter: 279900 training acc: 0.53125
Global Iter: 280000 training loss: 0.707117
Global Iter: 280000 training acc: 0.375
Global Iter: 280100 training loss: 0.66529
Global Iter: 280100 training acc: 0.71875
Global Iter: 280200 training loss: 0.689725
Global Iter: 280200 training acc: 0.5625
Global Iter: 280300 training loss: 0.69101
Global Iter: 280300 training acc: 0.5
Global Iter: 280400 training loss: 0.672898
Global Iter: 280400 training acc: 0.625
Global Iter: 280500 training loss: 0.699404
Global Iter: 280500 training acc: 0.46875
Global Iter: 280600 training loss: 0.687117
Global Iter: 280600 training acc: 0.5625
Global Iter: 280700 training loss: 0.689023
Global Iter: 280700 training acc: 0.53125
Global Iter: 280800 training loss: 0.694349
Global Iter: 280800 training acc: 0.5
Global Iter: 280900 training loss: 0.693405
Global Iter: 280900 training acc: 0.5
Global Iter: 281000 training loss: 0.692346
Global Iter: 281000 training acc: 0.53125
Global Iter: 281100 training loss: 0.713942
Global Iter: 281100 training acc: 0.375
Global Iter: 281200 training loss: 0.680197
Global Iter: 281200 training acc: 0.625
Global Iter: 281300 training loss: 0.672419
Global Iter: 281300 training acc: 0.625
Global Iter: 281400 training loss: 0.700977
Global Iter: 281400 training acc: 0.4375
Global Iter: 281500 training loss: 0.696618
Global Iter: 281500 training acc: 0.53125
Global Iter: 281600 training loss: 0.685379
Global Iter: 281600 training acc: 0.5625
Global Iter: 281700 training loss: 0.686195
Global Iter: 281700 training acc: 0.5625
Global Iter: 281800 training loss: 0.680736
Global Iter: 281800 training acc: 0.625
Global Iter: 281900 training loss: 0.691673
Global Iter: 281900 training acc: 0.53125
Global Iter: 282000 training loss: 0.695987
Global Iter: 282000 training acc: 0.4375
Global Iter: 282100 training loss: 0.676162
Global Iter: 282100 training acc: 0.625
Global Iter: 282200 training loss: 0.700004
Global Iter: 282200 training acc: 0.5
Global Iter: 282300 training loss: 0.672304
Global Iter: 282300 training acc: 0.6875
Global Iter: 282400 training loss: 0.677618
Global Iter: 282400 training acc: 0.625
Global Iter: 282500 training loss: 0.70912
Global Iter: 282500 training acc: 0.40625
Global Iter: 282600 training loss: 0.692832
Global Iter: 282600 training acc: 0.53125
Global Iter: 282700 training loss: 0.682162
Global Iter: 282700 training acc: 0.59375
Global Iter: 282800 training loss: 0.694366
Global Iter: 282800 training acc: 0.53125
Global Iter: 282900 training loss: 0.696949
Global Iter: 282900 training acc: 0.53125
Global Iter: 283000 training loss: 0.697788
Global Iter: 283000 training acc: 0.5
Global Iter: 283100 training loss: 0.702451
Global Iter: 283100 training acc: 0.4375
Global Iter: 283200 training loss: 0.661918
Global Iter: 283200 training acc: 0.78125
Global Iter: 283300 training loss: 0.697408
Global Iter: 283300 training acc: 0.46875
Global Iter: 283400 training loss: 0.686683
Global Iter: 283400 training acc: 0.5625
Global Iter: 283500 training loss: 0.682204
Global Iter: 283500 training acc: 0.59375
Global Iter: 283600 training loss: 0.683586
Global Iter: 283600 training acc: 0.5625
Global Iter: 283700 training loss: 0.714738
Global Iter: 283700 training acc: 0.34375
Global Iter: 283800 training loss: 0.702561
Global Iter: 283800 training acc: 0.40625
Global Iter: 283900 training loss: 0.688327
Global Iter: 283900 training acc: 0.5625
Global Iter: 284000 training loss: 0.709716
Global Iter: 284000 training acc: 0.40625
Global Iter: 284100 training loss: 0.679543
Global Iter: 284100 training acc: 0.65625
Global Iter: 284200 training loss: 0.708098
Global Iter: 284200 training acc: 0.40625
Global Iter: 284300 training loss: 0.674119
Global Iter: 284300 training acc: 0.625
Global Iter: 284400 training loss: 0.68363
Global Iter: 284400 training acc: 0.5625
Global Iter: 284500 training loss: 0.687097
Global Iter: 284500 training acc: 0.5625
Global Iter: 284600 training loss: 0.685132
Global Iter: 284600 training acc: 0.5625
Global Iter: 284700 training loss: 0.690914
Global Iter: 284700 training acc: 0.5
Global Iter: 284800 training loss: 0.688893
Global Iter: 284800 training acc: 0.5625
Global Iter: 284900 training loss: 0.691914
Global Iter: 284900 training acc: 0.5625
Global Iter: 285000 training loss: 0.689366
Global Iter: 285000 training acc: 0.5625
Global Iter: 285100 training loss: 0.716152
Global Iter: 285100 training acc: 0.34375
Global Iter: 285200 training loss: 0.673298
Global Iter: 285200 training acc: 0.6875
Global Iter: 285300 training loss: 0.683554
Global Iter: 285300 training acc: 0.59375
Global Iter: 285400 training loss: 0.699277
Global Iter: 285400 training acc: 0.46875
Global Iter: 285500 training loss: 0.685801
Global Iter: 285500 training acc: 0.59375
Global Iter: 285600 training loss: 0.686043
Global Iter: 285600 training acc: 0.59375
Global Iter: 285700 training loss: 0.713656
Global Iter: 285700 training acc: 0.40625
Global Iter: 285800 training loss: 0.6798
Global Iter: 285800 training acc: 0.625
Global Iter: 285900 training loss: 0.692962
Global Iter: 285900 training acc: 0.53125
Global Iter: 286000 training loss: 0.695116
Global Iter: 286000 training acc: 0.5
Global Iter: 286100 training loss: 0.667837
Global Iter: 286100 training acc: 0.75
Global Iter: 286200 training loss: 0.675556
Global Iter: 286200 training acc: 0.65625
Global Iter: 286300 training loss: 0.702493
Global Iter: 286300 training acc: 0.4375
Global Iter: 286400 training loss: 0.694339
Global Iter: 286400 training acc: 0.5
Global Iter: 286500 training loss: 0.683502
Global Iter: 286500 training acc: 0.59375
Global Iter: 286600 training loss: 0.728395
Global Iter: 286600 training acc: 0.21875
Global Iter: 286700 training loss: 0.70064
Global Iter: 286700 training acc: 0.46875
Global Iter: 286800 training loss: 0.689692
Global Iter: 286800 training acc: 0.5625
Global Iter: 286900 training loss: 0.68034
Global Iter: 286900 training acc: 0.59375
Global Iter: 287000 training loss: 0.678944
Global Iter: 287000 training acc: 0.59375
Global Iter: 287100 training loss: 0.698675
Global Iter: 287100 training acc: 0.46875
Global Iter: 287200 training loss: 0.680125
Global Iter: 287200 training acc: 0.625
Global Iter: 287300 training loss: 0.705222
Global Iter: 287300 training acc: 0.4375
Global Iter: 287400 training loss: 0.701165
Global Iter: 287400 training acc: 0.46875
Global Iter: 287500 training loss: 0.687987
Global Iter: 287500 training acc: 0.5625
Global Iter: 287600 training loss: 0.695859
Global Iter: 287600 training acc: 0.5
Global Iter: 287700 training loss: 0.693421
Global Iter: 287700 training acc: 0.5625
Global Iter: 287800 training loss: 0.66985
Global Iter: 287800 training acc: 0.71875
Global Iter: 287900 training loss: 0.687139
Global Iter: 287900 training acc: 0.5625
Global Iter: 288000 training loss: 0.710774
Global Iter: 288000 training acc: 0.34375
Global Iter: 288100 training loss: 0.695049
Global Iter: 288100 training acc: 0.5
Global Iter: 288200 training loss: 0.691669
Global Iter: 288200 training acc: 0.5
Global Iter: 288300 training loss: 0.691995
Global Iter: 288300 training acc: 0.5
Global Iter: 288400 training loss: 0.667416
Global Iter: 288400 training acc: 0.71875
Global Iter: 288500 training loss: 0.70036
Global Iter: 288500 training acc: 0.46875
Global Iter: 288600 training loss: 0.682075
Global Iter: 288600 training acc: 0.59375
Global Iter: 288700 training loss: 0.682939
Global Iter: 288700 training acc: 0.59375
Global Iter: 288800 training loss: 0.67336
Global Iter: 288800 training acc: 0.6875
Global Iter: 288900 training loss: 0.692667
Global Iter: 288900 training acc: 0.53125
Global Iter: 289000 training loss: 0.677629
Global Iter: 289000 training acc: 0.65625
Global Iter: 289100 training loss: 0.659143
Global Iter: 289100 training acc: 0.75
Global Iter: 289200 training loss: 0.698846
Global Iter: 289200 training acc: 0.46875
Global Iter: 289300 training loss: 0.692445
Global Iter: 289300 training acc: 0.5
Global Iter: 289400 training loss: 0.691961
Global Iter: 289400 training acc: 0.53125
Global Iter: 289500 training loss: 0.738374
Global Iter: 289500 training acc: 0.21875
Global Iter: 289600 training loss: 0.691716
Global Iter: 289600 training acc: 0.5
Global Iter: 289700 training loss: 0.705719
Global Iter: 289700 training acc: 0.4375
Global Iter: 289800 training loss: 0.690414
Global Iter: 289800 training acc: 0.5
Global Iter: 289900 training loss: 0.704318
Global Iter: 289900 training acc: 0.4375
Global Iter: 290000 training loss: 0.668507
Global Iter: 290000 training acc: 0.6875
Global Iter: 290100 training loss: 0.685309
Global Iter: 290100 training acc: 0.59375
Global Iter: 290200 training loss: 0.708026
Global Iter: 290200 training acc: 0.4375
Global Iter: 290300 training loss: 0.681025
Global Iter: 290300 training acc: 0.59375
Global Iter: 290400 training loss: 0.700252
Global Iter: 290400 training acc: 0.46875
Global Iter: 290500 training loss: 0.68283
Global Iter: 290500 training acc: 0.5625
Global Iter: 290600 training loss: 0.690428
Global Iter: 290600 training acc: 0.53125
Global Iter: 290700 training loss: 0.693928
Global Iter: 290700 training acc: 0.5
Global Iter: 290800 training loss: 0.689693
Global Iter: 290800 training acc: 0.5
Global Iter: 290900 training loss: 0.699056
Global Iter: 290900 training acc: 0.46875
Global Iter: 291000 training loss: 0.720433
Global Iter: 291000 training acc: 0.34375
Global Iter: 291100 training loss: 0.685925
Global Iter: 291100 training acc: 0.59375
Global Iter: 291200 training loss: 0.680652
Global Iter: 291200 training acc: 0.625
Global Iter: 291300 training loss: 0.70412
Global Iter: 291300 training acc: 0.46875
Global Iter: 291400 training loss: 0.69563
Global Iter: 291400 training acc: 0.46875
Global Iter: 291500 training loss: 0.688541
Global Iter: 291500 training acc: 0.59375
Global Iter: 291600 training loss: 0.694656
Global Iter: 291600 training acc: 0.5
Global Iter: 291700 training loss: 0.68448
Global Iter: 291700 training acc: 0.625
Global Iter: 291800 training loss: 0.688791
Global Iter: 291800 training acc: 0.53125
Global Iter: 291900 training loss: 0.694345
Global Iter: 291900 training acc: 0.46875
Global Iter: 292000 training loss: 0.671059
Global Iter: 292000 training acc: 0.6875
Global Iter: 292100 training loss: 0.691228
Global Iter: 292100 training acc: 0.53125
Global Iter: 292200 training loss: 0.663251
Global Iter: 292200 training acc: 0.71875
Global Iter: 292300 training loss: 0.682761
Global Iter: 292300 training acc: 0.625
Global Iter: 292400 training loss: 0.709217
Global Iter: 292400 training acc: 0.375
Global Iter: 292500 training loss: 0.69942
Global Iter: 292500 training acc: 0.5
Global Iter: 292600 training loss: 0.684642
Global Iter: 292600 training acc: 0.5625
Global Iter: 292700 training loss: 0.691352
Global Iter: 292700 training acc: 0.53125
Global Iter: 292800 training loss: 0.689747
Global Iter: 292800 training acc: 0.53125
Global Iter: 292900 training loss: 0.692655
Global Iter: 292900 training acc: 0.53125
Global Iter: 293000 training loss: 0.709136
Global Iter: 293000 training acc: 0.375
Global Iter: 293100 training loss: 0.666361
Global Iter: 293100 training acc: 0.75
Global Iter: 293200 training loss: 0.696367
Global Iter: 293200 training acc: 0.5
Global Iter: 293300 training loss: 0.686541
Global Iter: 293300 training acc: 0.53125
Global Iter: 293400 training loss: 0.682119
Global Iter: 293400 training acc: 0.59375
Global Iter: 293500 training loss: 0.698563
Global Iter: 293500 training acc: 0.5
Global Iter: 293600 training loss: 0.710461
Global Iter: 293600 training acc: 0.375
Global Iter: 293700 training loss: 0.705682
Global Iter: 293700 training acc: 0.40625
Global Iter: 293800 training loss: 0.686796
Global Iter: 293800 training acc: 0.5625
Global Iter: 293900 training loss: 0.713954
Global Iter: 293900 training acc: 0.375
Global Iter: 294000 training loss: 0.679609
Global Iter: 294000 training acc: 0.65625
Global Iter: 294100 training loss: 0.70066
Global Iter: 294100 training acc: 0.46875
Global Iter: 294200 training loss: 0.682851
Global Iter: 294200 training acc: 0.625
Global Iter: 294300 training loss: 0.677564
Global Iter: 294300 training acc: 0.625
Global Iter: 294400 training loss: 0.688012
Global Iter: 294400 training acc: 0.5625
Global Iter: 294500 training loss: 0.685192
Global Iter: 294500 training acc: 0.5625
Global Iter: 294600 training loss: 0.695857
Global Iter: 294600 training acc: 0.5
Global Iter: 294700 training loss: 0.686818
Global Iter: 294700 training acc: 0.5625
Global Iter: 294800 training loss: 0.685306
Global Iter: 294800 training acc: 0.5625
Global Iter: 294900 training loss: 0.680826
Global Iter: 294900 training acc: 0.59375
Global Iter: 295000 training loss: 0.714993
Global Iter: 295000 training acc: 0.3125
Global Iter: 295100 training loss: 0.668558
Global Iter: 295100 training acc: 0.71875
Global Iter: 295200 training loss: 0.691565
Global Iter: 295200 training acc: 0.53125
Global Iter: 295300 training loss: 0.706193
Global Iter: 295300 training acc: 0.4375
Global Iter: 295400 training loss: 0.687075
Global Iter: 295400 training acc: 0.5625
Global Iter: 295500 training loss: 0.681112
Global Iter: 295500 training acc: 0.625
Global Iter: 295600 training loss: 0.705133
Global Iter: 295600 training acc: 0.4375
Global Iter: 295700 training loss: 0.683599
Global Iter: 295700 training acc: 0.625
Global Iter: 295800 training loss: 0.696655
Global Iter: 295800 training acc: 0.46875
Global Iter: 295900 training loss: 0.691127
Global Iter: 295900 training acc: 0.53125
Global Iter: 296000 training loss: 0.662429
Global Iter: 296000 training acc: 0.75
Global Iter: 296100 training loss: 0.674074
Global Iter: 296100 training acc: 0.65625
Global Iter: 296200 training loss: 0.701773
Global Iter: 296200 training acc: 0.4375
Global Iter: 296300 training loss: 0.683603
Global Iter: 296300 training acc: 0.5625
Global Iter: 296400 training loss: 0.683582
Global Iter: 296400 training acc: 0.59375
Global Iter: 296500 training loss: 0.729779
Global Iter: 296500 training acc: 0.21875
Global Iter: 296600 training loss: 0.697397
Global Iter: 296600 training acc: 0.46875
Global Iter: 296700 training loss: 0.679422
Global Iter: 296700 training acc: 0.59375
Global Iter: 296800 training loss: 0.678014
Global Iter: 296800 training acc: 0.625
Global Iter: 296900 training loss: 0.67784
Global Iter: 296900 training acc: 0.65625
Global Iter: 297000 training loss: 0.701391
Global Iter: 297000 training acc: 0.46875
Global Iter: 297100 training loss: 0.67008
Global Iter: 297100 training acc: 0.6875
Global Iter: 297200 training loss: 0.704359
Global Iter: 297200 training acc: 0.4375
Global Iter: 297300 training loss: 0.700294
Global Iter: 297300 training acc: 0.5
Global Iter: 297400 training loss: 0.688759
Global Iter: 297400 training acc: 0.5625
Global Iter: 297500 training loss: 0.701851
Global Iter: 297500 training acc: 0.46875
Global Iter: 297600 training loss: 0.695846
Global Iter: 297600 training acc: 0.5
Global Iter: 297700 training loss: 0.674817
Global Iter: 297700 training acc: 0.65625
Global Iter: 297800 training loss: 0.684852
Global Iter: 297800 training acc: 0.5625
Global Iter: 297900 training loss: 0.716374
Global Iter: 297900 training acc: 0.375
Global Iter: 298000 training loss: 0.698864
Global Iter: 298000 training acc: 0.5
Global Iter: 298100 training loss: 0.687411
Global Iter: 298100 training acc: 0.53125
Global Iter: 298200 training loss: 0.70702
Global Iter: 298200 training acc: 0.4375
Global Iter: 298300 training loss: 0.667749
Global Iter: 298300 training acc: 0.71875
Global Iter: 298400 training loss: 0.700999
Global Iter: 298400 training acc: 0.4375
Global Iter: 298500 training loss: 0.682188
Global Iter: 298500 training acc: 0.625
Global Iter: 298600 training loss: 0.677472
Global Iter: 298600 training acc: 0.625
Global Iter: 298700 training loss: 0.669702
Global Iter: 298700 training acc: 0.71875
Global Iter: 298800 training loss: 0.692487
Global Iter: 298800 training acc: 0.5
Global Iter: 298900 training loss: 0.682951
Global Iter: 298900 training acc: 0.625
Global Iter: 299000 training loss: 0.660932
Global Iter: 299000 training acc: 0.78125
Global Iter: 299100 training loss: 0.702156
Global Iter: 299100 training acc: 0.4375
Global Iter: 299200 training loss: 0.697675
Global Iter: 299200 training acc: 0.5
Global Iter: 299300 training loss: 0.68884
Global Iter: 299300 training acc: 0.53125
Global Iter: 299400 training loss: 0.734295
Global Iter: 299400 training acc: 0.21875
Global Iter: 299500 training loss: 0.68791
Global Iter: 299500 training acc: 0.53125
Global Iter: 299600 training loss: 0.701042
Global Iter: 299600 training acc: 0.46875
Global Iter: 299700 training loss: 0.695693
Global Iter: 299700 training acc: 0.5
Global Iter: 299800 training loss: 0.705025
Global Iter: 299800 training acc: 0.4375
Global Iter: 299900 training loss: 0.668745
Global Iter: 299900 training acc: 0.65625
Global Iter: 300000 training loss: 0.682172
Global Iter: 300000 training acc: 0.59375
Global Iter: 300100 training loss: 0.69924
Global Iter: 300100 training acc: 0.46875
Global Iter: 300200 training loss: 0.686571
Global Iter: 300200 training acc: 0.59375
Global Iter: 300300 training loss: 0.695102
Global Iter: 300300 training acc: 0.5
Global Iter: 300400 training loss: 0.693854
Global Iter: 300400 training acc: 0.53125
Global Iter: 300500 training loss: 0.702408
Global Iter: 300500 training acc: 0.46875
Global Iter: 300600 training loss: 0.692768
Global Iter: 300600 training acc: 0.53125
Global Iter: 300700 training loss: 0.692798
Global Iter: 300700 training acc: 0.5
Global Iter: 300800 training loss: 0.704219
Global Iter: 300800 training acc: 0.46875
Global Iter: 300900 training loss: 0.715858
Global Iter: 300900 training acc: 0.375
Global Iter: 301000 training loss: 0.684015
Global Iter: 301000 training acc: 0.5625
Global Iter: 301100 training loss: 0.681864
Global Iter: 301100 training acc: 0.625
Global Iter: 301200 training loss: 0.69385
Global Iter: 301200 training acc: 0.53125
Global Iter: 301300 training loss: 0.697973
Global Iter: 301300 training acc: 0.46875
Global Iter: 301400 training loss: 0.679691
Global Iter: 301400 training acc: 0.59375
Global Iter: 301500 training loss: 0.695205
Global Iter: 301500 training acc: 0.46875
Global Iter: 301600 training loss: 0.680423
Global Iter: 301600 training acc: 0.625
Global Iter: 301700 training loss: 0.694254
Global Iter: 301700 training acc: 0.5
Global Iter: 301800 training loss: 0.695921
Global Iter: 301800 training acc: 0.5
Global Iter: 301900 training loss: 0.665856
Global Iter: 301900 training acc: 0.75
Global Iter: 302000 training loss: 0.693761
Global Iter: 302000 training acc: 0.5
Global Iter: 302100 training loss: 0.675498
Global Iter: 302100 training acc: 0.65625
Global Iter: 302200 training loss: 0.688424
Global Iter: 302200 training acc: 0.59375
Global Iter: 302300 training loss: 0.717824
Global Iter: 302300 training acc: 0.375
Global Iter: 302400 training loss: 0.694629
Global Iter: 302400 training acc: 0.5
Global Iter: 302500 training loss: 0.690236
Global Iter: 302500 training acc: 0.5625
Global Iter: 302600 training loss: 0.690989
Global Iter: 302600 training acc: 0.53125
Global Iter: 302700 training loss: 0.694266
Global Iter: 302700 training acc: 0.53125
Global Iter: 302800 training loss: 0.687532
Global Iter: 302800 training acc: 0.53125
Global Iter: 302900 training loss: 0.703814
Global Iter: 302900 training acc: 0.4375
Global Iter: 303000 training loss: 0.658152
Global Iter: 303000 training acc: 0.71875
Global Iter: 303100 training loss: 0.698365
Global Iter: 303100 training acc: 0.5
Global Iter: 303200 training loss: 0.691142
Global Iter: 303200 training acc: 0.53125
Global Iter: 303300 training loss: 0.684255
Global Iter: 303300 training acc: 0.59375
Global Iter: 303400 training loss: 0.702925
Global Iter: 303400 training acc: 0.4375
Global Iter: 303500 training loss: 0.710108
Global Iter: 303500 training acc: 0.34375
Global Iter: 303600 training loss: 0.711621
Global Iter: 303600 training acc: 0.40625
Global Iter: 303700 training loss: 0.687019
Global Iter: 303700 training acc: 0.5625
Global Iter: 303800 training loss: 0.716713
Global Iter: 303800 training acc: 0.375
Global Iter: 303900 training loss: 0.676499
Global Iter: 303900 training acc: 0.65625
Global Iter: 304000 training loss: 0.705649
Global Iter: 304000 training acc: 0.4375
Global Iter: 304100 training loss: 0.690202
Global Iter: 304100 training acc: 0.5625
Global Iter: 304200 training loss: 0.682204
Global Iter: 304200 training acc: 0.625
Global Iter: 304300 training loss: 0.687856
Global Iter: 304300 training acc: 0.5625
Global Iter: 304400 training loss: 0.694455
Global Iter: 304400 training acc: 0.5
Global Iter: 304500 training loss: 0.696153
Global Iter: 304500 training acc: 0.46875
Global Iter: 304600 training loss: 0.685042
Global Iter: 304600 training acc: 0.5625
Global Iter: 304700 training loss: 0.688766
Global Iter: 304700 training acc: 0.5625
Global Iter: 304800 training loss: 0.684968
Global Iter: 304800 training acc: 0.59375
Global Iter: 304900 training loss: 0.706423
Global Iter: 304900 training acc: 0.375
Global Iter: 305000 training loss: 0.661261
Global Iter: 305000 training acc: 0.71875
Global Iter: 305100 training loss: 0.688282
Global Iter: 305100 training acc: 0.53125
Global Iter: 305200 training loss: 0.706849
Global Iter: 305200 training acc: 0.4375
Global Iter: 305300 training loss: 0.691111
Global Iter: 305300 training acc: 0.53125
Global Iter: 305400 training loss: 0.68581
Global Iter: 305400 training acc: 0.59375
Global Iter: 305500 training loss: 0.703176
Global Iter: 305500 training acc: 0.46875
Global Iter: 305600 training loss: 0.677901
Global Iter: 305600 training acc: 0.625
Global Iter: 305700 training loss: 0.698214
Global Iter: 305700 training acc: 0.53125
Global Iter: 305800 training loss: 0.687499
Global Iter: 305800 training acc: 0.53125
Global Iter: 305900 training loss: 0.67046
Global Iter: 305900 training acc: 0.71875
Global Iter: 306000 training loss: 0.674061
Global Iter: 306000 training acc: 0.65625
Global Iter: 306100 training loss: 0.707916
Global Iter: 306100 training acc: 0.40625
Global Iter: 306200 training loss: 0.686312
Global Iter: 306200 training acc: 0.59375
Global Iter: 306300 training loss: 0.68379
Global Iter: 306300 training acc: 0.59375
Global Iter: 306400 training loss: 0.729496
Global Iter: 306400 training acc: 0.21875
Global Iter: 306500 training loss: 0.711082
Global Iter: 306500 training acc: 0.4375
Global Iter: 306600 training loss: 0.689857
Global Iter: 306600 training acc: 0.5625
Global Iter: 306700 training loss: 0.68135
Global Iter: 306700 training acc: 0.625
Global Iter: 306800 training loss: 0.672271
Global Iter: 306800 training acc: 0.71875
Global Iter: 306900 training loss: 0.699916
Global Iter: 306900 training acc: 0.46875
Global Iter: 307000 training loss: 0.669769
Global Iter: 307000 training acc: 0.71875
Global Iter: 307100 training loss: 0.691711
Global Iter: 307100 training acc: 0.5
Global Iter: 307200 training loss: 0.690767
Global Iter: 307200 training acc: 0.53125
Global Iter: 307300 training loss: 0.681415
Global Iter: 307300 training acc: 0.625
Global Iter: 307400 training loss: 0.698632
Global Iter: 307400 training acc: 0.46875
Global Iter: 307500 training loss: 0.70803
Global Iter: 307500 training acc: 0.4375
Global Iter: 307600 training loss: 0.674247
Global Iter: 307600 training acc: 0.65625
Global Iter: 307700 training loss: 0.692179
Global Iter: 307700 training acc: 0.53125
Global Iter: 307800 training loss: 0.714285
Global Iter: 307800 training acc: 0.34375
Global Iter: 307900 training loss: 0.695268
Global Iter: 307900 training acc: 0.53125
Global Iter: 308000 training loss: 0.68631
Global Iter: 308000 training acc: 0.5625
Global Iter: 308100 training loss: 0.707555
Global Iter: 308100 training acc: 0.4375
Global Iter: 308200 training loss: 0.671272
Global Iter: 308200 training acc: 0.71875
Global Iter: 308300 training loss: 0.709175
Global Iter: 308300 training acc: 0.40625
Global Iter: 308400 training loss: 0.692736
Global Iter: 308400 training acc: 0.5625
Global Iter: 308500 training loss: 0.686338
Global Iter: 308500 training acc: 0.59375
Global Iter: 308600 training loss: 0.667941
Global Iter: 308600 training acc: 0.6875
Global Iter: 308700 training loss: 0.68943
Global Iter: 308700 training acc: 0.5
Global Iter: 308800 training loss: 0.678578
Global Iter: 308800 training acc: 0.65625
Global Iter: 308900 training loss: 0.669312
Global Iter: 308900 training acc: 0.75
Global Iter: 309000 training loss: 0.7044
Global Iter: 309000 training acc: 0.4375
Global Iter: 309100 training loss: 0.691933
Global Iter: 309100 training acc: 0.5
Global Iter: 309200 training loss: 0.693032
Global Iter: 309200 training acc: 0.5
Global Iter: 309300 training loss: 0.726031
Global Iter: 309300 training acc: 0.25
Global Iter: 309400 training loss: 0.69847
Global Iter: 309400 training acc: 0.5
Global Iter: 309500 training loss: 0.700746
Global Iter: 309500 training acc: 0.46875
Global Iter: 309600 training loss: 0.703028
Global Iter: 309600 training acc: 0.46875
Global Iter: 309700 training loss: 0.696285
Global Iter: 309700 training acc: 0.46875
Global Iter: 309800 training loss: 0.673335
Global Iter: 309800 training acc: 0.65625
Global Iter: 309900 training loss: 0.684665
Global Iter: 309900 training acc: 0.59375
Global Iter: 310000 training loss: 0.701653
Global Iter: 310000 training acc: 0.4375
Global Iter: 310100 training loss: 0.684522
Global Iter: 310100 training acc: 0.59375
Global Iter: 310200 training loss: 0.694525
Global Iter: 310200 training acc: 0.5
Global Iter: 310300 training loss: 0.687818
Global Iter: 310300 training acc: 0.5625
Global Iter: 310400 training loss: 0.70357
Global Iter: 310400 training acc: 0.46875
Global Iter: 310500 training loss: 0.690295
Global Iter: 310500 training acc: 0.53125
Global Iter: 310600 training loss: 0.696895
Global Iter: 310600 training acc: 0.5
Global Iter: 310700 training loss: 0.710527
Global Iter: 310700 training acc: 0.4375
Global Iter: 310800 training loss: 0.708065
Global Iter: 310800 training acc: 0.375
Global Iter: 310900 training loss: 0.690017
Global Iter: 310900 training acc: 0.53125
Global Iter: 311000 training loss: 0.67416
Global Iter: 311000 training acc: 0.65625
Global Iter: 311100 training loss: 0.680617
Global Iter: 311100 training acc: 0.59375
Global Iter: 311200 training loss: 0.69153
Global Iter: 311200 training acc: 0.5
Global Iter: 311300 training loss: 0.681114
Global Iter: 311300 training acc: 0.5625
Global Iter: 311400 training loss: 0.695582
Global Iter: 311400 training acc: 0.46875
Global Iter: 311500 training loss: 0.688392
Global Iter: 311500 training acc: 0.5625
Global Iter: 311600 training loss: 0.693191
Global Iter: 311600 training acc: 0.5
Global Iter: 311700 training loss: 0.692182
Global Iter: 311700 training acc: 0.53125
Global Iter: 311800 training loss: 0.665554
Global Iter: 311800 training acc: 0.75
Global Iter: 311900 training loss: 0.693326
Global Iter: 311900 training acc: 0.46875
Global Iter: 312000 training loss: 0.676083
Global Iter: 312000 training acc: 0.625
Global Iter: 312100 training loss: 0.685299
Global Iter: 312100 training acc: 0.59375
Global Iter: 312200 training loss: 0.713131
Global Iter: 312200 training acc: 0.34375
Global Iter: 312300 training loss: 0.699745
Global Iter: 312300 training acc: 0.46875
Global Iter: 312400 training loss: 0.687045
Global Iter: 312400 training acc: 0.5625
Global Iter: 312500 training loss: 0.689026
Global Iter: 312500 training acc: 0.53125
Global Iter: 312600 training loss: 0.689788
Global Iter: 312600 training acc: 0.5
Global Iter: 312700 training loss: 0.68061
Global Iter: 312700 training acc: 0.59375
Global Iter: 312800 training loss: 0.69195
Global Iter: 312800 training acc: 0.5
Global Iter: 312900 training loss: 0.682485
Global Iter: 312900 training acc: 0.65625
Global Iter: 313000 training loss: 0.690867
Global Iter: 313000 training acc: 0.5625
Global Iter: 313100 training loss: 0.690452
Global Iter: 313100 training acc: 0.5625
Global Iter: 313200 training loss: 0.680713
Global Iter: 313200 training acc: 0.59375
Global Iter: 313300 training loss: 0.707024
Global Iter: 313300 training acc: 0.40625
Global Iter: 313400 training loss: 0.706751
Global Iter: 313400 training acc: 0.375
Global Iter: 313500 training loss: 0.709522
Global Iter: 313500 training acc: 0.40625
Global Iter: 313600 training loss: 0.688532
Global Iter: 313600 training acc: 0.5625
Global Iter: 313700 training loss: 0.704823
Global Iter: 313700 training acc: 0.40625
Global Iter: 313800 training loss: 0.683581
Global Iter: 313800 training acc: 0.59375
Global Iter: 313900 training loss: 0.695355
Global Iter: 313900 training acc: 0.5
Global Iter: 314000 training loss: 0.696114
Global Iter: 314000 training acc: 0.5
Global Iter: 314100 training loss: 0.679203
Global Iter: 314100 training acc: 0.625
Global Iter: 314200 training loss: 0.683554
Global Iter: 314200 training acc: 0.59375
Global Iter: 314300 training loss: 0.687277
Global Iter: 314300 training acc: 0.53125
Global Iter: 314400 training loss: 0.695196
Global Iter: 314400 training acc: 0.5
Global Iter: 314500 training loss: 0.694324
Global Iter: 314500 training acc: 0.53125
Global Iter: 314600 training loss: 0.695069
Global Iter: 314600 training acc: 0.5
Global Iter: 314700 training loss: 0.686058
Global Iter: 314700 training acc: 0.59375
Global Iter: 314800 training loss: 0.712733
Global Iter: 314800 training acc: 0.375
Global Iter: 314900 training loss: 0.667993
Global Iter: 314900 training acc: 0.71875
Global Iter: 315000 training loss: 0.684902
Global Iter: 315000 training acc: 0.5625
Global Iter: 315100 training loss: 0.694839
Global Iter: 315100 training acc: 0.5
Global Iter: 315200 training loss: 0.687695
Global Iter: 315200 training acc: 0.5625
Global Iter: 315300 training loss: 0.698842
Global Iter: 315300 training acc: 0.53125
Global Iter: 315400 training loss: 0.701212
Global Iter: 315400 training acc: 0.46875
Global Iter: 315500 training loss: 0.676078
Global Iter: 315500 training acc: 0.625
Global Iter: 315600 training loss: 0.699141
Global Iter: 315600 training acc: 0.46875
Global Iter: 315700 training loss: 0.700997
Global Iter: 315700 training acc: 0.46875
Global Iter: 315800 training loss: 0.661007
Global Iter: 315800 training acc: 0.75
Global Iter: 315900 training loss: 0.67799
Global Iter: 315900 training acc: 0.625
Global Iter: 316000 training loss: 0.703008
Global Iter: 316000 training acc: 0.40625
Global Iter: 316100 training loss: 0.682723
Global Iter: 316100 training acc: 0.59375
Global Iter: 316200 training loss: 0.679989
Global Iter: 316200 training acc: 0.625
Global Iter: 316300 training loss: 0.729972
Global Iter: 316300 training acc: 0.21875
Global Iter: 316400 training loss: 0.706577
Global Iter: 316400 training acc: 0.40625
Global Iter: 316500 training loss: 0.686089
Global Iter: 316500 training acc: 0.5625
Global Iter: 316600 training loss: 0.687206
Global Iter: 316600 training acc: 0.59375
Global Iter: 316700 training loss: 0.672502
Global Iter: 316700 training acc: 0.6875
Global Iter: 316800 training loss: 0.698259
Global Iter: 316800 training acc: 0.46875
Global Iter: 316900 training loss: 0.674379
Global Iter: 316900 training acc: 0.71875
Global Iter: 317000 training loss: 0.689088
Global Iter: 317000 training acc: 0.53125
Global Iter: 317100 training loss: 0.699595
Global Iter: 317100 training acc: 0.46875
Global Iter: 317200 training loss: 0.68331
Global Iter: 317200 training acc: 0.59375
Global Iter: 317300 training loss: 0.707449
Global Iter: 317300 training acc: 0.4375
Global Iter: 317400 training loss: 0.704316
Global Iter: 317400 training acc: 0.40625
Global Iter: 317500 training loss: 0.677763
Global Iter: 317500 training acc: 0.65625
Global Iter: 317600 training loss: 0.688528
Global Iter: 317600 training acc: 0.53125
Global Iter: 317700 training loss: 0.715499
Global Iter: 317700 training acc: 0.3125
Global Iter: 317800 training loss: 0.690761
Global Iter: 317800 training acc: 0.53125
Global Iter: 317900 training loss: 0.685795
Global Iter: 317900 training acc: 0.5625
Global Iter: 318000 training loss: 0.709821
Global Iter: 318000 training acc: 0.4375
Global Iter: 318100 training loss: 0.672015
Global Iter: 318100 training acc: 0.6875
Global Iter: 318200 training loss: 0.705897
Global Iter: 318200 training acc: 0.4375
Global Iter: 318300 training loss: 0.686743
Global Iter: 318300 training acc: 0.53125
Global Iter: 318400 training loss: 0.680508
Global Iter: 318400 training acc: 0.59375
Global Iter: 318500 training loss: 0.676896
Global Iter: 318500 training acc: 0.625
Global Iter: 318600 training loss: 0.690186
Global Iter: 318600 training acc: 0.5625
Global Iter: 318700 training loss: 0.674429
Global Iter: 318700 training acc: 0.65625
Global Iter: 318800 training loss: 0.662568
Global Iter: 318800 training acc: 0.75
Global Iter: 318900 training loss: 0.703745
Global Iter: 318900 training acc: 0.4375
Global Iter: 319000 training loss: 0.700643
Global Iter: 319000 training acc: 0.46875
Global Iter: 319100 training loss: 0.69399
Global Iter: 319100 training acc: 0.5
Global Iter: 319200 training loss: 0.728069
Global Iter: 319200 training acc: 0.25
Global Iter: 319300 training loss: 0.695055
Global Iter: 319300 training acc: 0.5
Global Iter: 319400 training loss: 0.700642
Global Iter: 319400 training acc: 0.46875
Global Iter: 319500 training loss: 0.689547
Global Iter: 319500 training acc: 0.53125
Global Iter: 319600 training loss: 0.694384
Global Iter: 319600 training acc: 0.5
Global Iter: 319700 training loss: 0.676505
Global Iter: 319700 training acc: 0.625
Global Iter: 319800 training loss: 0.688727
Global Iter: 319800 training acc: 0.59375
Global Iter: 319900 training loss: 0.705886
Global Iter: 319900 training acc: 0.4375
Global Iter: 320000 training loss: 0.684085
Global Iter: 320000 training acc: 0.59375
Global Iter: 320100 training loss: 0.695341
Global Iter: 320100 training acc: 0.46875
Global Iter: 320200 training loss: 0.687582
Global Iter: 320200 training acc: 0.5625
Global Iter: 320300 training loss: 0.706156
Global Iter: 320300 training acc: 0.40625
Global Iter: 320400 training loss: 0.684453
Global Iter: 320400 training acc: 0.59375
Global Iter: 320500 training loss: 0.695829
Global Iter: 320500 training acc: 0.5
Global Iter: 320600 training loss: 0.702123
Global Iter: 320600 training acc: 0.4375
Global Iter: 320700 training loss: 0.715908
Global Iter: 320700 training acc: 0.375
Global Iter: 320800 training loss: 0.696174
Global Iter: 320800 training acc: 0.46875
Global Iter: 320900 training loss: 0.674657
Global Iter: 320900 training acc: 0.65625
Global Iter: 321000 training loss: 0.683634
Global Iter: 321000 training acc: 0.5625
Global Iter: 321100 training loss: 0.699268
Global Iter: 321100 training acc: 0.46875
Global Iter: 321200 training loss: 0.685366
Global Iter: 321200 training acc: 0.5625
Global Iter: 321300 training loss: 0.696581
Global Iter: 321300 training acc: 0.4375
Global Iter: 321400 training loss: 0.686896
Global Iter: 321400 training acc: 0.59375
Global Iter: 321500 training loss: 0.698173
Global Iter: 321500 training acc: 0.5
Global Iter: 321600 training loss: 0.687151
Global Iter: 321600 training acc: 0.59375
Global Iter: 321700 training loss: 0.655384
Global Iter: 321700 training acc: 0.78125
Global Iter: 321800 training loss: 0.70163
Global Iter: 321800 training acc: 0.5
Global Iter: 321900 training loss: 0.683132
Global Iter: 321900 training acc: 0.59375
Global Iter: 322000 training loss: 0.684709
Global Iter: 322000 training acc: 0.59375
Global Iter: 322100 training loss: 0.715732
Global Iter: 322100 training acc: 0.34375
Global Iter: 322200 training loss: 0.699588
Global Iter: 322200 training acc: 0.5
Global Iter: 322300 training loss: 0.69621
Global Iter: 322300 training acc: 0.5
Global Iter: 322400 training loss: 0.700964
Global Iter: 322400 training acc: 0.5
Global Iter: 322500 training loss: 0.695259
Global Iter: 322500 training acc: 0.5
Global Iter: 322600 training loss: 0.681266
Global Iter: 322600 training acc: 0.59375
Global Iter: 322700 training loss: 0.694992
Global Iter: 322700 training acc: 0.5
Global Iter: 322800 training loss: 0.679995
Global Iter: 322800 training acc: 0.625
Global Iter: 322900 training loss: 0.681485
Global Iter: 322900 training acc: 0.59375
Global Iter: 323000 training loss: 0.687181
Global Iter: 323000 training acc: 0.53125
Global Iter: 323100 training loss: 0.674395
Global Iter: 323100 training acc: 0.625
Global Iter: 323200 training loss: 0.699421
Global Iter: 323200 training acc: 0.4375
Global Iter: 323300 training loss: 0.709194
Global Iter: 323300 training acc: 0.375
Global Iter: 323400 training loss: 0.706831
Global Iter: 323400 training acc: 0.4375
Global Iter: 323500 training loss: 0.691278
Global Iter: 323500 training acc: 0.53125
Global Iter: 323600 training loss: 0.709163
Global Iter: 323600 training acc: 0.40625
Global Iter: 323700 training loss: 0.678147
Global Iter: 323700 training acc: 0.625
Global Iter: 323800 training loss: 0.689017
Global Iter: 323800 training acc: 0.53125
Global Iter: 323900 training loss: 0.706378
Global Iter: 323900 training acc: 0.46875
Global Iter: 324000 training loss: 0.682614
Global Iter: 324000 training acc: 0.59375
Global Iter: 324100 training loss: 0.686118
Global Iter: 324100 training acc: 0.5625
Global Iter: 324200 training loss: 0.693806
Global Iter: 324200 training acc: 0.5
Global Iter: 324300 training loss: 0.695161
Global Iter: 324300 training acc: 0.53125
Global Iter: 324400 training loss: 0.688206
Global Iter: 324400 training acc: 0.5625
Global Iter: 324500 training loss: 0.704768
Global Iter: 324500 training acc: 0.46875
Global Iter: 324600 training loss: 0.67606
Global Iter: 324600 training acc: 0.65625
Global Iter: 324700 training loss: 0.709046
Global Iter: 324700 training acc: 0.375
Global Iter: 324800 training loss: 0.670084
Global Iter: 324800 training acc: 0.6875
Global Iter: 324900 training loss: 0.680388
Global Iter: 324900 training acc: 0.625
Global Iter: 325000 training loss: 0.696525
Global Iter: 325000 training acc: 0.5
Global Iter: 325100 training loss: 0.687128
Global Iter: 325100 training acc: 0.5625
Global Iter: 325200 training loss: 0.686077
Global Iter: 325200 training acc: 0.5625
Global Iter: 325300 training loss: 0.705633
Global Iter: 325300 training acc: 0.46875
Global Iter: 325400 training loss: 0.683891
Global Iter: 325400 training acc: 0.625
Global Iter: 325500 training loss: 0.71036
Global Iter: 325500 training acc: 0.40625
Global Iter: 325600 training loss: 0.700964
Global Iter: 325600 training acc: 0.40625
Global Iter: 325700 training loss: 0.668642
Global Iter: 325700 training acc: 0.71875
Global Iter: 325800 training loss: 0.682809
Global Iter: 325800 training acc: 0.59375
Global Iter: 325900 training loss: 0.700423
Global Iter: 325900 training acc: 0.46875
Global Iter: 326000 training loss: 0.688912
Global Iter: 326000 training acc: 0.5625
Global Iter: 326100 training loss: 0.687602
Global Iter: 326100 training acc: 0.59375
Global Iter: 326200 training loss: 0.72349
Global Iter: 326200 training acc: 0.25
Global Iter: 326300 training loss: 0.704324
Global Iter: 326300 training acc: 0.4375
Global Iter: 326400 training loss: 0.680989
Global Iter: 326400 training acc: 0.59375
Global Iter: 326500 training loss: 0.69207
Global Iter: 326500 training acc: 0.53125
Global Iter: 326600 training loss: 0.678192
Global Iter: 326600 training acc: 0.65625
Global Iter: 326700 training loss: 0.706765
Global Iter: 326700 training acc: 0.40625
Global Iter: 326800 training loss: 0.670368
Global Iter: 326800 training acc: 0.6875
Global Iter: 326900 training loss: 0.693591
Global Iter: 326900 training acc: 0.53125
Global Iter: 327000 training loss: 0.699742
Global Iter: 327000 training acc: 0.46875
Global Iter: 327100 training loss: 0.686871
Global Iter: 327100 training acc: 0.59375
Global Iter: 327200 training loss: 0.705764
Global Iter: 327200 training acc: 0.4375
Global Iter: 327300 training loss: 0.707369
Global Iter: 327300 training acc: 0.4375
Global Iter: 327400 training loss: 0.67746
Global Iter: 327400 training acc: 0.65625
Global Iter: 327500 training loss: 0.68183
Global Iter: 327500 training acc: 0.5625
Global Iter: 327600 training loss: 0.718209
Global Iter: 327600 training acc: 0.3125
Global Iter: 327700 training loss: 0.690413
Global Iter: 327700 training acc: 0.5625
Global Iter: 327800 training loss: 0.685709
Global Iter: 327800 training acc: 0.5625
Global Iter: 327900 training loss: 0.695239
Global Iter: 327900 training acc: 0.5
Global Iter: 328000 training loss: 0.67536
Global Iter: 328000 training acc: 0.625
Global Iter: 328100 training loss: 0.698618
Global Iter: 328100 training acc: 0.4375
Global Iter: 328200 training loss: 0.693753
Global Iter: 328200 training acc: 0.53125
Global Iter: 328300 training loss: 0.689992
Global Iter: 328300 training acc: 0.5625
Global Iter: 328400 training loss: 0.676676
Global Iter: 328400 training acc: 0.625
Global Iter: 328500 training loss: 0.687541
Global Iter: 328500 training acc: 0.5625
Global Iter: 328600 training loss: 0.675246
Global Iter: 328600 training acc: 0.65625
Global Iter: 328700 training loss: 0.65636
Global Iter: 328700 training acc: 0.78125
Global Iter: 328800 training loss: 0.708217
Global Iter: 328800 training acc: 0.40625
Global Iter: 328900 training loss: 0.700692
Global Iter: 328900 training acc: 0.46875
Global Iter: 329000 training loss: 0.700509
Global Iter: 329000 training acc: 0.46875
Global Iter: 329100 training loss: 0.732602
Global Iter: 329100 training acc: 0.25
Global Iter: 329200 training loss: 0.701269
Global Iter: 329200 training acc: 0.5
Global Iter: 329300 training loss: 0.69038
Global Iter: 329300 training acc: 0.53125
Global Iter: 329400 training loss: 0.691791
Global Iter: 329400 training acc: 0.53125
Global Iter: 329500 training loss: 0.685777
Global Iter: 329500 training acc: 0.5625
Global Iter: 329600 training loss: 0.677684
Global Iter: 329600 training acc: 0.625
Global Iter: 329700 training loss: 0.687207
Global Iter: 329700 training acc: 0.5625
Global Iter: 329800 training loss: 0.698232
Global Iter: 329800 training acc: 0.5
Global Iter: 329900 training loss: 0.68753
Global Iter: 329900 training acc: 0.59375
Global Iter: 330000 training loss: 0.695879
Global Iter: 330000 training acc: 0.53125
Global Iter: 330100 training loss: 0.697137
Global Iter: 330100 training acc: 0.5
Global Iter: 330200 training loss: 0.70436
Global Iter: 330200 training acc: 0.4375
Global Iter: 330300 training loss: 0.682526
Global Iter: 330300 training acc: 0.59375
Global Iter: 330400 training loss: 0.694596
Global Iter: 330400 training acc: 0.5
Global Iter: 330500 training loss: 0.706572
Global Iter: 330500 training acc: 0.375
Global Iter: 330600 training loss: 0.714868
Global Iter: 330600 training acc: 0.375
Global Iter: 330700 training loss: 0.695761
Global Iter: 330700 training acc: 0.5
Global Iter: 330800 training loss: 0.681721
Global Iter: 330800 training acc: 0.625
Global Iter: 330900 training loss: 0.690818
Global Iter: 330900 training acc: 0.5625
Global Iter: 331000 training loss: 0.700963
Global Iter: 331000 training acc: 0.4375
Global Iter: 331100 training loss: 0.681923
Global Iter: 331100 training acc: 0.59375
Global Iter: 331200 training loss: 0.693053
Global Iter: 331200 training acc: 0.5
Global Iter: 331300 training loss: 0.687675
Global Iter: 331300 training acc: 0.5625
Global Iter: 331400 training loss: 0.694156
Global Iter: 331400 training acc: 0.5
Global Iter: 331500 training loss: 0.687899
Global Iter: 331500 training acc: 0.59375
Global Iter: 331600 training loss: 0.66364
Global Iter: 331600 training acc: 0.75
Global Iter: 331700 training loss: 0.694687
Global Iter: 331700 training acc: 0.5
Global Iter: 331800 training loss: 0.684374
Global Iter: 331800 training acc: 0.5625
Global Iter: 331900 training loss: 0.683526
Global Iter: 331900 training acc: 0.5625
Global Iter: 332000 training loss: 0.714039
Global Iter: 332000 training acc: 0.375
Global Iter: 332100 training loss: 0.691723
Global Iter: 332100 training acc: 0.5
Global Iter: 332200 training loss: 0.698784
Global Iter: 332200 training acc: 0.46875
Global Iter: 332300 training loss: 0.695307
Global Iter: 332300 training acc: 0.5
Global Iter: 332400 training loss: 0.697909
Global Iter: 332400 training acc: 0.5
Global Iter: 332500 training loss: 0.671573
Global Iter: 332500 training acc: 0.625
Global Iter: 332600 training loss: 0.691471
Global Iter: 332600 training acc: 0.5625
Global Iter: 332700 training loss: 0.681858
Global Iter: 332700 training acc: 0.625
Global Iter: 332800 training loss: 0.680284
Global Iter: 332800 training acc: 0.625
Global Iter: 332900 training loss: 0.688726
Global Iter: 332900 training acc: 0.5625
Global Iter: 333000 training loss: 0.680755
Global Iter: 333000 training acc: 0.59375
Global Iter: 333100 training loss: 0.701171
Global Iter: 333100 training acc: 0.4375
Global Iter: 333200 training loss: 0.707893
Global Iter: 333200 training acc: 0.4375
Global Iter: 333300 training loss: 0.695229
Global Iter: 333300 training acc: 0.46875
Global Iter: 333400 training loss: 0.69092
Global Iter: 333400 training acc: 0.53125
Global Iter: 333500 training loss: 0.715009
Global Iter: 333500 training acc: 0.375
Global Iter: 333600 training loss: 0.680116
Global Iter: 333600 training acc: 0.625
Global Iter: 333700 training loss: 0.689947
Global Iter: 333700 training acc: 0.53125
Global Iter: 333800 training loss: 0.705527
Global Iter: 333800 training acc: 0.4375
Global Iter: 333900 training loss: 0.681522
Global Iter: 333900 training acc: 0.59375
Global Iter: 334000 training loss: 0.687771
Global Iter: 334000 training acc: 0.5625
Global Iter: 334100 training loss: 0.692196
Global Iter: 334100 training acc: 0.53125
Global Iter: 334200 training loss: 0.693995
Global Iter: 334200 training acc: 0.5
Global Iter: 334300 training loss: 0.693163
Global Iter: 334300 training acc: 0.53125
Global Iter: 334400 training loss: 0.702385
Global Iter: 334400 training acc: 0.4375
Global Iter: 334500 training loss: 0.684688
Global Iter: 334500 training acc: 0.625
Global Iter: 334600 training loss: 0.705963
Global Iter: 334600 training acc: 0.40625
Global Iter: 334700 training loss: 0.675405
Global Iter: 334700 training acc: 0.6875
Global Iter: 334800 training loss: 0.679456
Global Iter: 334800 training acc: 0.65625
Global Iter: 334900 training loss: 0.689606
Global Iter: 334900 training acc: 0.53125
Global Iter: 335000 training loss: 0.694914
Global Iter: 335000 training acc: 0.53125
Global Iter: 335100 training loss: 0.688344
Global Iter: 335100 training acc: 0.53125
Global Iter: 335200 training loss: 0.702297
Global Iter: 335200 training acc: 0.46875
Global Iter: 335300 training loss: 0.688475
Global Iter: 335300 training acc: 0.5625
Global Iter: 335400 training loss: 0.703184
Global Iter: 335400 training acc: 0.4375
Global Iter: 335500 training loss: 0.706765
Global Iter: 335500 training acc: 0.40625
Global Iter: 335600 training loss: 0.664888
Global Iter: 335600 training acc: 0.75
Global Iter: 335700 training loss: 0.686716
Global Iter: 335700 training acc: 0.5625
Global Iter: 335800 training loss: 0.700794
Global Iter: 335800 training acc: 0.46875
Global Iter: 335900 training loss: 0.691442
Global Iter: 335900 training acc: 0.53125
Global Iter: 336000 training loss: 0.691373
Global Iter: 336000 training acc: 0.5625
Global Iter: 336100 training loss: 0.723782
Global Iter: 336100 training acc: 0.28125
Global Iter: 336200 training loss: 0.700557
Global Iter: 336200 training acc: 0.4375
Global Iter: 336300 training loss: 0.6761
Global Iter: 336300 training acc: 0.625
Global Iter: 336400 training loss: 0.699548
Global Iter: 336400 training acc: 0.5
Global Iter: 336500 training loss: 0.675383
Global Iter: 336500 training acc: 0.71875
Global Iter: 336600 training loss: 0.708054
Global Iter: 336600 training acc: 0.375
Global Iter: 336700 training loss: 0.665564
Global Iter: 336700 training acc: 0.71875
Global Iter: 336800 training loss: 0.683403
Global Iter: 336800 training acc: 0.59375
Global Iter: 336900 training loss: 0.708387
Global Iter: 336900 training acc: 0.4375
Global Iter: 337000 training loss: 0.681445
Global Iter: 337000 training acc: 0.625
Global Iter: 337100 training loss: 0.703787
Global Iter: 337100 training acc: 0.4375
Global Iter: 337200 training loss: 0.700839
Global Iter: 337200 training acc: 0.4375
Global Iter: 337300 training loss: 0.675836
Global Iter: 337300 training acc: 0.65625
Global Iter: 337400 training loss: 0.689298
Global Iter: 337400 training acc: 0.53125
Global Iter: 337500 training loss: 0.723005
Global Iter: 337500 training acc: 0.28125
Global Iter: 337600 training loss: 0.69169
Global Iter: 337600 training acc: 0.5625
Global Iter: 337700 training loss: 0.698231
Global Iter: 337700 training acc: 0.5
Global Iter: 337800 training loss: 0.691178
Global Iter: 337800 training acc: 0.5625
Global Iter: 337900 training loss: 0.670541
Global Iter: 337900 training acc: 0.65625
Global Iter: 338000 training loss: 0.695892
Global Iter: 338000 training acc: 0.5
Global Iter: 338100 training loss: 0.693219
Global Iter: 338100 training acc: 0.53125
Global Iter: 338200 training loss: 0.694814
Global Iter: 338200 training acc: 0.53125
Global Iter: 338300 training loss: 0.688031
Global Iter: 338300 training acc: 0.5625
Global Iter: 338400 training loss: 0.683095
Global Iter: 338400 training acc: 0.59375
Global Iter: 338500 training loss: 0.67751
Global Iter: 338500 training acc: 0.71875
Global Iter: 338600 training loss: 0.662028
Global Iter: 338600 training acc: 0.78125
Global Iter: 338700 training loss: 0.70879
Global Iter: 338700 training acc: 0.40625
Global Iter: 338800 training loss: 0.700051
Global Iter: 338800 training acc: 0.46875
Global Iter: 338900 training loss: 0.695794
Global Iter: 338900 training acc: 0.5
Global Iter: 339000 training loss: 0.731494
Global Iter: 339000 training acc: 0.21875
Global Iter: 339100 training loss: 0.69527
Global Iter: 339100 training acc: 0.5
Global Iter: 339200 training loss: 0.695978
Global Iter: 339200 training acc: 0.5
Global Iter: 339300 training loss: 0.690135
Global Iter: 339300 training acc: 0.53125
Global Iter: 339400 training loss: 0.691544
Global Iter: 339400 training acc: 0.5625
Global Iter: 339500 training loss: 0.687467
Global Iter: 339500 training acc: 0.5625
Global Iter: 339600 training loss: 0.683168
Global Iter: 339600 training acc: 0.59375
Global Iter: 339700 training loss: 0.703187
Global Iter: 339700 training acc: 0.4375
Global Iter: 339800 training loss: 0.681698
Global Iter: 339800 training acc: 0.59375
Global Iter: 339900 training loss: 0.694777
Global Iter: 339900 training acc: 0.5
Global Iter: 340000 training loss: 0.689274
Global Iter: 340000 training acc: 0.5625
Global Iter: 340100 training loss: 0.702575
Global Iter: 340100 training acc: 0.4375
Global Iter: 340200 training loss: 0.68252
Global Iter: 340200 training acc: 0.59375
Global Iter: 340300 training loss: 0.693381
Global Iter: 340300 training acc: 0.5
Global Iter: 340400 training loss: 0.703783
Global Iter: 340400 training acc: 0.4375
Global Iter: 340500 training loss: 0.706898
Global Iter: 340500 training acc: 0.40625
Global Iter: 340600 training loss: 0.687105
Global Iter: 340600 training acc: 0.53125
Global Iter: 340700 training loss: 0.676848
Global Iter: 340700 training acc: 0.625
Global Iter: 340800 training loss: 0.679937
Global Iter: 340800 training acc: 0.59375
Global Iter: 340900 training loss: 0.702703
Global Iter: 340900 training acc: 0.4375
Global Iter: 341000 training loss: 0.683176
Global Iter: 341000 training acc: 0.5625
Global Iter: 341100 training loss: 0.693953
Global Iter: 341100 training acc: 0.5
Global Iter: 341200 training loss: 0.674687
Global Iter: 341200 training acc: 0.625
Global Iter: 341300 training loss: 0.690904
Global Iter: 341300 training acc: 0.46875
Global Iter: 341400 training loss: 0.685586
Global Iter: 341400 training acc: 0.59375
Global Iter: 341500 training loss: 0.667955
Global Iter: 341500 training acc: 0.75
Global Iter: 341600 training loss: 0.691208
Global Iter: 341600 training acc: 0.53125
Global Iter: 341700 training loss: 0.684029
Global Iter: 341700 training acc: 0.5625
Global Iter: 341800 training loss: 0.694931
Global Iter: 341800 training acc: 0.53125
Global Iter: 341900 training loss: 0.716277
Global Iter: 341900 training acc: 0.34375
Global Iter: 342000 training loss: 0.698457
Global Iter: 342000 training acc: 0.5
Global Iter: 342100 training loss: 0.700548
Global Iter: 342100 training acc: 0.46875
Global Iter: 342200 training loss: 0.692981
Global Iter: 342200 training acc: 0.5
Global Iter: 342300 training loss: 0.696671
Global Iter: 342300 training acc: 0.46875
Global Iter: 342400 training loss: 0.679443
Global Iter: 342400 training acc: 0.625
Global Iter: 342500 training loss: 0.694001
Global Iter: 342500 training acc: 0.5
Global Iter: 342600 training loss: 0.685277
Global Iter: 342600 training acc: 0.59375
Global Iter: 342700 training loss: 0.686322
Global Iter: 342700 training acc: 0.59375
Global Iter: 342800 training loss: 0.691959
Global Iter: 342800 training acc: 0.53125
Global Iter: 342900 training loss: 0.688318
Global Iter: 342900 training acc: 0.59375
Global Iter: 343000 training loss: 0.698723
Global Iter: 343000 training acc: 0.46875
Global Iter: 343100 training loss: 0.696277
Global Iter: 343100 training acc: 0.46875
Global Iter: 343200 training loss: 0.705468
Global Iter: 343200 training acc: 0.4375
Global Iter: 343300 training loss: 0.694382
Global Iter: 343300 training acc: 0.5
Global Iter: 343400 training loss: 0.7141
Global Iter: 343400 training acc: 0.34375
Global Iter: 343500 training loss: 0.68351
Global Iter: 343500 training acc: 0.625
Global Iter: 343600 training loss: 0.685051
Global Iter: 343600 training acc: 0.59375
Global Iter: 343700 training loss: 0.706118
Global Iter: 343700 training acc: 0.4375
Global Iter: 343800 training loss: 0.680384
Global Iter: 343800 training acc: 0.59375
Global Iter: 343900 training loss: 0.691165
Global Iter: 343900 training acc: 0.5625
Global Iter: 344000 training loss: 0.696723
Global Iter: 344000 training acc: 0.5
Global Iter: 344100 training loss: 0.692856
Global Iter: 344100 training acc: 0.5
Global Iter: 344200 training loss: 0.690188
Global Iter: 344200 training acc: 0.5625
Global Iter: 344300 training loss: 0.706515
Global Iter: 344300 training acc: 0.4375
Global Iter: 344400 training loss: 0.682506
Global Iter: 344400 training acc: 0.59375
Global Iter: 344500 training loss: 0.70084
Global Iter: 344500 training acc: 0.4375
Global Iter: 344600 training loss: 0.668557
Global Iter: 344600 training acc: 0.6875
Global Iter: 344700 training loss: 0.680968
Global Iter: 344700 training acc: 0.65625
Global Iter: 344800 training loss: 0.694962
Global Iter: 344800 training acc: 0.5
Global Iter: 344900 training loss: 0.693096
Global Iter: 344900 training acc: 0.5
Global Iter: 345000 training loss: 0.691659
Global Iter: 345000 training acc: 0.53125
Global Iter: 345100 training loss: 0.693398
Global Iter: 345100 training acc: 0.53125
Global Iter: 345200 training loss: 0.687834
Global Iter: 345200 training acc: 0.5625
Global Iter: 345300 training loss: 0.713617
Global Iter: 345300 training acc: 0.40625
Global Iter: 345400 training loss: 0.702257
Global Iter: 345400 training acc: 0.4375
Global Iter: 345500 training loss: 0.664204
Global Iter: 345500 training acc: 0.75
Global Iter: 345600 training loss: 0.695741
Global Iter: 345600 training acc: 0.53125
Global Iter: 345700 training loss: 0.700232
Global Iter: 345700 training acc: 0.5
Global Iter: 345800 training loss: 0.684811
Global Iter: 345800 training acc: 0.59375
Global Iter: 345900 training loss: 0.684725
Global Iter: 345900 training acc: 0.5625
Global Iter: 346000 training loss: 0.721947
Global Iter: 346000 training acc: 0.28125
Global Iter: 346100 training loss: 0.709634
Global Iter: 346100 training acc: 0.40625
Global Iter: 346200 training loss: 0.682385
Global Iter: 346200 training acc: 0.625
Global Iter: 346300 training loss: 0.697757
Global Iter: 346300 training acc: 0.46875
Global Iter: 346400 training loss: 0.672482
Global Iter: 346400 training acc: 0.71875
Global Iter: 346500 training loss: 0.713982
Global Iter: 346500 training acc: 0.375
Global Iter: 346600 training loss: 0.671847
Global Iter: 346600 training acc: 0.65625
Global Iter: 346700 training loss: 0.689565
Global Iter: 346700 training acc: 0.59375
Global Iter: 346800 training loss: 0.698075
Global Iter: 346800 training acc: 0.5
Global Iter: 346900 training loss: 0.686833
Global Iter: 346900 training acc: 0.625
Global Iter: 347000 training loss: 0.700604
Global Iter: 347000 training acc: 0.46875
Global Iter: 347100 training loss: 0.695451
Global Iter: 347100 training acc: 0.46875
Global Iter: 347200 training loss: 0.678313
Global Iter: 347200 training acc: 0.65625
Global Iter: 347300 training loss: 0.693588
Global Iter: 347300 training acc: 0.5
Global Iter: 347400 training loss: 0.717846
Global Iter: 347400 training acc: 0.34375
Global Iter: 347500 training loss: 0.678245
Global Iter: 347500 training acc: 0.625
Global Iter: 347600 training loss: 0.6944
Global Iter: 347600 training acc: 0.53125
Global Iter: 347700 training loss: 0.693637
Global Iter: 347700 training acc: 0.53125
Global Iter: 347800 training loss: 0.686359
Global Iter: 347800 training acc: 0.625
Global Iter: 347900 training loss: 0.696474
Global Iter: 347900 training acc: 0.53125
Global Iter: 348000 training loss: 0.685233
Global Iter: 348000 training acc: 0.5625
Global Iter: 348100 training loss: 0.688678
Global Iter: 348100 training acc: 0.5625
Global Iter: 348200 training loss: 0.690723
Global Iter: 348200 training acc: 0.53125
Global Iter: 348300 training loss: 0.680364
Global Iter: 348300 training acc: 0.5625
Global Iter: 348400 training loss: 0.667776
Global Iter: 348400 training acc: 0.71875
Global Iter: 348500 training loss: 0.662093
Global Iter: 348500 training acc: 0.75
Global Iter: 348600 training loss: 0.709167
Global Iter: 348600 training acc: 0.375
Global Iter: 348700 training loss: 0.70744
Global Iter: 348700 training acc: 0.4375
Global Iter: 348800 training loss: 0.684504
Global Iter: 348800 training acc: 0.5625
Global Iter: 348900 training loss: 0.730328
Global Iter: 348900 training acc: 0.21875
Global Iter: 349000 training loss: 0.703389
Global Iter: 349000 training acc: 0.46875
Global Iter: 349100 training loss: 0.694695
Global Iter: 349100 training acc: 0.5
Global Iter: 349200 training loss: 0.689248
Global Iter: 349200 training acc: 0.53125
Global Iter: 349300 training loss: 0.684769
Global Iter: 349300 training acc: 0.59375
Global Iter: 349400 training loss: 0.680367
Global Iter: 349400 training acc: 0.59375
Global Iter: 349500 training loss: 0.677055
Global Iter: 349500 training acc: 0.65625
Global Iter: 349600 training loss: 0.70554
Global Iter: 349600 training acc: 0.4375
Global Iter: 349700 training loss: 0.690652
Global Iter: 349700 training acc: 0.5625
Global Iter: 349800 training loss: 0.691384
Global Iter: 349800 training acc: 0.5625
Global Iter: 349900 training loss: 0.690884
Global Iter: 349900 training acc: 0.53125
Global Iter: 350000 training loss: 0.697107
Global Iter: 350000 training acc: 0.46875
Global Iter: 350100 training loss: 0.683142
Global Iter: 350100 training acc: 0.5625
Global Iter: 350200 training loss: 0.693192
Global Iter: 350200 training acc: 0.5
Global Iter: 350300 training loss: 0.704078
Global Iter: 350300 training acc: 0.40625
Global Iter: 350400 training loss: 0.705749
Global Iter: 350400 training acc: 0.4375
Global Iter: 350500 training loss: 0.689987
Global Iter: 350500 training acc: 0.5
Global Iter: 350600 training loss: 0.68373
Global Iter: 350600 training acc: 0.59375
Global Iter: 350700 training loss: 0.686234
Global Iter: 350700 training acc: 0.59375
Global Iter: 350800 training loss: 0.702659
Global Iter: 350800 training acc: 0.4375
Global Iter: 350900 training loss: 0.686573
Global Iter: 350900 training acc: 0.5625
Global Iter: 351000 training loss: 0.697912
Global Iter: 351000 training acc: 0.53125
Global Iter: 351100 training loss: 0.684957
Global Iter: 351100 training acc: 0.5625
Global Iter: 351200 training loss: 0.69098
Global Iter: 351200 training acc: 0.5
Global Iter: 351300 training loss: 0.681444
Global Iter: 351300 training acc: 0.625
Global Iter: 351400 training loss: 0.66317
Global Iter: 351400 training acc: 0.75
Global Iter: 351500 training loss: 0.686914
Global Iter: 351500 training acc: 0.5625
Global Iter: 351600 training loss: 0.689048
Global Iter: 351600 training acc: 0.53125
Global Iter: 351700 training loss: 0.686921
Global Iter: 351700 training acc: 0.5625
Global Iter: 351800 training loss: 0.720911
Global Iter: 351800 training acc: 0.34375
Global Iter: 351900 training loss: 0.696634
Global Iter: 351900 training acc: 0.5
Global Iter: 352000 training loss: 0.701405
Global Iter: 352000 training acc: 0.4375
Global Iter: 352100 training loss: 0.69147
Global Iter: 352100 training acc: 0.5
Global Iter: 352200 training loss: 0.702005
Global Iter: 352200 training acc: 0.40625
Global Iter: 352300 training loss: 0.680317
Global Iter: 352300 training acc: 0.625
Global Iter: 352400 training loss: 0.695072
Global Iter: 352400 training acc: 0.5
Global Iter: 352500 training loss: 0.68689
Global Iter: 352500 training acc: 0.53125
Global Iter: 352600 training loss: 0.684567
Global Iter: 352600 training acc: 0.59375
Global Iter: 352700 training loss: 0.699287
Global Iter: 352700 training acc: 0.5
Global Iter: 352800 training loss: 0.683565
Global Iter: 352800 training acc: 0.59375
Global Iter: 352900 training loss: 0.697799
Global Iter: 352900 training acc: 0.5
Global Iter: 353000 training loss: 0.697202
Global Iter: 353000 training acc: 0.5
Global Iter: 353100 training loss: 0.701579
Global Iter: 353100 training acc: 0.46875
Global Iter: 353200 training loss: 0.707817
Global Iter: 353200 training acc: 0.46875
Global Iter: 353300 training loss: 0.719845
Global Iter: 353300 training acc: 0.34375
Global Iter: 353400 training loss: 0.68608
Global Iter: 353400 training acc: 0.59375
Global Iter: 353500 training loss: 0.682747
Global Iter: 353500 training acc: 0.59375
Global Iter: 353600 training loss: 0.708575
Global Iter: 353600 training acc: 0.40625
Global Iter: 353700 training loss: 0.678151
Global Iter: 353700 training acc: 0.625
Global Iter: 353800 training loss: 0.68635
Global Iter: 353800 training acc: 0.5625
Global Iter: 353900 training loss: 0.691165
Global Iter: 353900 training acc: 0.53125
Global Iter: 354000 training loss: 0.690811
Global Iter: 354000 training acc: 0.5625
Global Iter: 354100 training loss: 0.685972
Global Iter: 354100 training acc: 0.5625
Global Iter: 354200 training loss: 0.701914
Global Iter: 354200 training acc: 0.4375
Global Iter: 354300 training loss: 0.682542
Global Iter: 354300 training acc: 0.625
Global Iter: 354400 training loss: 0.711287
Global Iter: 354400 training acc: 0.4375
Global Iter: 354500 training loss: 0.669861
Global Iter: 354500 training acc: 0.6875
Global Iter: 354600 training loss: 0.677324
Global Iter: 354600 training acc: 0.65625
Global Iter: 354700 training loss: 0.700779
Global Iter: 354700 training acc: 0.5
Global Iter: 354800 training loss: 0.693997
Global Iter: 354800 training acc: 0.5
Global Iter: 354900 training loss: 0.681785
Global Iter: 354900 training acc: 0.59375
Global Iter: 355000 training loss: 0.696932
Global Iter: 355000 training acc: 0.5
Global Iter: 355100 training loss: 0.690048
Global Iter: 355100 training acc: 0.53125
Global Iter: 355200 training loss: 0.703522
Global Iter: 355200 training acc: 0.40625
Global Iter: 355300 training loss: 0.699066
Global Iter: 355300 training acc: 0.4375
Global Iter: 355400 training loss: 0.660619
Global Iter: 355400 training acc: 0.78125
Global Iter: 355500 training loss: 0.690961
Global Iter: 355500 training acc: 0.5
Global Iter: 355600 training loss: 0.694238
Global Iter: 355600 training acc: 0.53125
Global Iter: 355700 training loss: 0.684954
Global Iter: 355700 training acc: 0.59375
Global Iter: 355800 training loss: 0.687127
Global Iter: 355800 training acc: 0.5625
Global Iter: 355900 training loss: 0.716959
Global Iter: 355900 training acc: 0.3125
Global Iter: 356000 training loss: 0.71404
Global Iter: 356000 training acc: 0.375
Global Iter: 356100 training loss: 0.677372
Global Iter: 356100 training acc: 0.65625
Global Iter: 356200 training loss: 0.699311
Global Iter: 356200 training acc: 0.46875
Global Iter: 356300 training loss: 0.676193
Global Iter: 356300 training acc: 0.71875
Global Iter: 356400 training loss: 0.71215
Global Iter: 356400 training acc: 0.375
Global Iter: 356500 training loss: 0.682558
Global Iter: 356500 training acc: 0.59375
Global Iter: 356600 training loss: 0.68783
Global Iter: 356600 training acc: 0.5625
Global Iter: 356700 training loss: 0.685923
Global Iter: 356700 training acc: 0.5625
Global Iter: 356800 training loss: 0.688642
Global Iter: 356800 training acc: 0.5625
Global Iter: 356900 training loss: 0.695577
Global Iter: 356900 training acc: 0.5
Global Iter: 357000 training loss: 0.69411
Global Iter: 357000 training acc: 0.53125
Global Iter: 357100 training loss: 0.687851
Global Iter: 357100 training acc: 0.59375
Global Iter: 357200 training loss: 0.694827
Global Iter: 357200 training acc: 0.5
Global Iter: 357300 training loss: 0.718128
Global Iter: 357300 training acc: 0.34375
Global Iter: 357400 training loss: 0.672332
Global Iter: 357400 training acc: 0.65625
Global Iter: 357500 training loss: 0.6885
Global Iter: 357500 training acc: 0.5625
Global Iter: 357600 training loss: 0.690954
Global Iter: 357600 training acc: 0.53125
Global Iter: 357700 training loss: 0.679928
Global Iter: 357700 training acc: 0.59375
Global Iter: 357800 training loss: 0.685905
Global Iter: 357800 training acc: 0.5625
Global Iter: 357900 training loss: 0.698619
Global Iter: 357900 training acc: 0.5
Global Iter: 358000 training loss: 0.680801
Global Iter: 358000 training acc: 0.625
Global Iter: 358100 training loss: 0.697329
Global Iter: 358100 training acc: 0.5
Global Iter: 358200 training loss: 0.688097
Global Iter: 358200 training acc: 0.53125
Global Iter: 358300 training loss: 0.665675
Global Iter: 358300 training acc: 0.71875
Global Iter: 358400 training loss: 0.669935
Global Iter: 358400 training acc: 0.6875
Global Iter: 358500 training loss: 0.713418
Global Iter: 358500 training acc: 0.375
Global Iter: 358600 training loss: 0.69178
Global Iter: 358600 training acc: 0.5
Global Iter: 358700 training loss: 0.686523
Global Iter: 358700 training acc: 0.59375
Global Iter: 358800 training loss: 0.735148
Global Iter: 358800 training acc: 0.21875
Global Iter: 358900 training loss: 0.699699
Global Iter: 358900 training acc: 0.46875
Global Iter: 359000 training loss: 0.693793
Global Iter: 359000 training acc: 0.53125
Global Iter: 359100 training loss: 0.69122
Global Iter: 359100 training acc: 0.46875
Global Iter: 359200 training loss: 0.682069
Global Iter: 359200 training acc: 0.625
Global Iter: 359300 training loss: 0.683247
Global Iter: 359300 training acc: 0.59375
Global Iter: 359400 training loss: 0.676777
Global Iter: 359400 training acc: 0.65625
Global Iter: 359500 training loss: 0.708394
Global Iter: 359500 training acc: 0.4375
Global Iter: 359600 training loss: 0.693914
Global Iter: 359600 training acc: 0.53125
Global Iter: 359700 training loss: 0.686865
Global Iter: 359700 training acc: 0.5625
Global Iter: 359800 training loss: 0.692768
Global Iter: 359800 training acc: 0.53125
Global Iter: 359900 training loss: 0.687515
Global Iter: 359900 training acc: 0.53125
Global Iter: 360000 training loss: 0.679936
Global Iter: 360000 training acc: 0.625
Global Iter: 360100 training loss: 0.689817
Global Iter: 360100 training acc: 0.5625
Global Iter: 360200 training loss: 0.712242
Global Iter: 360200 training acc: 0.40625
Global Iter: 360300 training loss: 0.704513
Global Iter: 360300 training acc: 0.4375
Global Iter: 360400 training loss: 0.695421
Global Iter: 360400 training acc: 0.5
Global Iter: 360500 training loss: 0.682849
Global Iter: 360500 training acc: 0.5625
Global Iter: 360600 training loss: 0.679385
Global Iter: 360600 training acc: 0.625
Global Iter: 360700 training loss: 0.697098
Global Iter: 360700 training acc: 0.46875
Global Iter: 360800 training loss: 0.685024
Global Iter: 360800 training acc: 0.5625
Global Iter: 360900 training loss: 0.686104
Global Iter: 360900 training acc: 0.5625
Global Iter: 361000 training loss: 0.676328
Global Iter: 361000 training acc: 0.625
Global Iter: 361100 training loss: 0.690456
Global Iter: 361100 training acc: 0.53125
Global Iter: 361200 training loss: 0.684176
Global Iter: 361200 training acc: 0.59375
Global Iter: 361300 training loss: 0.661725
Global Iter: 361300 training acc: 0.78125
Global Iter: 361400 training loss: 0.695916
Global Iter: 361400 training acc: 0.53125
Global Iter: 361500 training loss: 0.689647
Global Iter: 361500 training acc: 0.5625
Global Iter: 361600 training loss: 0.688376
Global Iter: 361600 training acc: 0.5625
Global Iter: 361700 training loss: 0.718117
Global Iter: 361700 training acc: 0.34375
Global Iter: 361800 training loss: 0.696668
Global Iter: 361800 training acc: 0.5
Global Iter: 361900 training loss: 0.701616
Global Iter: 361900 training acc: 0.4375
Global Iter: 362000 training loss: 0.695823
Global Iter: 362000 training acc: 0.46875
Global Iter: 362100 training loss: 0.704152
Global Iter: 362100 training acc: 0.40625
Global Iter: 362200 training loss: 0.68091
Global Iter: 362200 training acc: 0.59375
Global Iter: 362300 training loss: 0.689627
Global Iter: 362300 training acc: 0.5
Global Iter: 362400 training loss: 0.693765
Global Iter: 362400 training acc: 0.53125
Global Iter: 362500 training loss: 0.682133
Global Iter: 362500 training acc: 0.59375
Global Iter: 362600 training loss: 0.689816
Global Iter: 362600 training acc: 0.53125
Global Iter: 362700 training loss: 0.682877
Global Iter: 362700 training acc: 0.59375
Global Iter: 362800 training loss: 0.690518
Global Iter: 362800 training acc: 0.53125
Global Iter: 362900 training loss: 0.694971
Global Iter: 362900 training acc: 0.5
Global Iter: 363000 training loss: 0.692657
Global Iter: 363000 training acc: 0.5
Global Iter: 363100 training loss: 0.696687
Global Iter: 363100 training acc: 0.53125
Global Iter: 363200 training loss: 0.710966
Global Iter: 363200 training acc: 0.375
Global Iter: 363300 training loss: 0.686049
Global Iter: 363300 training acc: 0.5625
Global Iter: 363400 training loss: 0.674292
Global Iter: 363400 training acc: 0.65625
Global Iter: 363500 training loss: 0.708731
Global Iter: 363500 training acc: 0.40625
Global Iter: 363600 training loss: 0.687746
Global Iter: 363600 training acc: 0.5625
Global Iter: 363700 training loss: 0.694218
Global Iter: 363700 training acc: 0.53125
Global Iter: 363800 training loss: 0.693665
Global Iter: 363800 training acc: 0.5
Global Iter: 363900 training loss: 0.690448
Global Iter: 363900 training acc: 0.5625
Global Iter: 364000 training loss: 0.685678
Global Iter: 364000 training acc: 0.5625
Global Iter: 364100 training loss: 0.701736
Global Iter: 364100 training acc: 0.4375
Global Iter: 364200 training loss: 0.674255
Global Iter: 364200 training acc: 0.65625
Global Iter: 364300 training loss: 0.697393
Global Iter: 364300 training acc: 0.46875
Global Iter: 364400 training loss: 0.667274
Global Iter: 364400 training acc: 0.6875
Global Iter: 364500 training loss: 0.673847
Global Iter: 364500 training acc: 0.625
Global Iter: 364600 training loss: 0.706
Global Iter: 364600 training acc: 0.46875
Global Iter: 364700 training loss: 0.693246
Global Iter: 364700 training acc: 0.5
Global Iter: 364800 training loss: 0.676179
Global Iter: 364800 training acc: 0.625
Global Iter: 364900 training loss: 0.694285
Global Iter: 364900 training acc: 0.5
Global Iter: 365000 training loss: 0.69287
Global Iter: 365000 training acc: 0.53125
Global Iter: 365100 training loss: 0.70274
Global Iter: 365100 training acc: 0.4375
Global Iter: 365200 training loss: 0.700827
Global Iter: 365200 training acc: 0.46875
Global Iter: 365300 training loss: 0.655593
Global Iter: 365300 training acc: 0.8125
Global Iter: 365400 training loss: 0.691602
Global Iter: 365400 training acc: 0.53125
Global Iter: 365500 training loss: 0.689828
Global Iter: 365500 training acc: 0.5625
Global Iter: 365600 training loss: 0.689466
Global Iter: 365600 training acc: 0.59375
Global Iter: 365700 training loss: 0.688154
Global Iter: 365700 training acc: 0.59375
Global Iter: 365800 training loss: 0.717708
Global Iter: 365800 training acc: 0.28125
Global Iter: 365900 training loss: 0.704474
Global Iter: 365900 training acc: 0.40625
Global Iter: 366000 training loss: 0.678319
Global Iter: 366000 training acc: 0.59375
Global Iter: 366100 training loss: 0.702856
Global Iter: 366100 training acc: 0.46875
Global Iter: 366200 training loss: 0.668569
Global Iter: 366200 training acc: 0.71875
Global Iter: 366300 training loss: 0.727879
Global Iter: 366300 training acc: 0.34375
Global Iter: 366400 training loss: 0.687088
Global Iter: 366400 training acc: 0.59375
Global Iter: 366500 training loss: 0.683671
Global Iter: 366500 training acc: 0.59375
Global Iter: 366600 training loss: 0.684703
Global Iter: 366600 training acc: 0.59375
Global Iter: 366700 training loss: 0.68763
Global Iter: 366700 training acc: 0.59375
Global Iter: 366800 training loss: 0.697563
Global Iter: 366800 training acc: 0.46875
Global Iter: 366900 training loss: 0.690765
Global Iter: 366900 training acc: 0.53125
Global Iter: 367000 training loss: 0.685345
Global Iter: 367000 training acc: 0.5625
Global Iter: 367100 training loss: 0.692757
Global Iter: 367100 training acc: 0.53125
Global Iter: 367200 training loss: 0.717769
Global Iter: 367200 training acc: 0.3125
Global Iter: 367300 training loss: 0.674711
Global Iter: 367300 training acc: 0.65625
Global Iter: 367400 training loss: 0.688209
Global Iter: 367400 training acc: 0.5625
Global Iter: 367500 training loss: 0.697197
Global Iter: 367500 training acc: 0.5
Global Iter: 367600 training loss: 0.683853
Global Iter: 367600 training acc: 0.5625
Global Iter: 367700 training loss: 0.686586
Global Iter: 367700 training acc: 0.5625
Global Iter: 367800 training loss: 0.692347
Global Iter: 367800 training acc: 0.53125
Global Iter: 367900 training loss: 0.680957
Global Iter: 367900 training acc: 0.625
Global Iter: 368000 training loss: 0.684481
Global Iter: 368000 training acc: 0.5625
Global Iter: 368100 training loss: 0.694948
Global Iter: 368100 training acc: 0.5
Global Iter: 368200 training loss: 0.669156
Global Iter: 368200 training acc: 0.75
Global Iter: 368300 training loss: 0.678139
Global Iter: 368300 training acc: 0.65625
Global Iter: 368400 training loss: 0.700521
Global Iter: 368400 training acc: 0.4375
Global Iter: 368500 training loss: 0.701877
Global Iter: 368500 training acc: 0.46875
Global Iter: 368600 training loss: 0.688293
Global Iter: 368600 training acc: 0.53125
Global Iter: 368700 training loss: 0.732922
Global Iter: 368700 training acc: 0.21875
Global Iter: 368800 training loss: 0.694018
Global Iter: 368800 training acc: 0.5
Global Iter: 368900 training loss: 0.693466
Global Iter: 368900 training acc: 0.53125
Global Iter: 369000 training loss: 0.697546
Global Iter: 369000 training acc: 0.5
Global Iter: 369100 training loss: 0.69199
Global Iter: 369100 training acc: 0.59375
Global Iter: 369200 training loss: 0.691443
Global Iter: 369200 training acc: 0.5625
Global Iter: 369300 training loss: 0.684092
Global Iter: 369300 training acc: 0.625
Global Iter: 369400 training loss: 0.692231
Global Iter: 369400 training acc: 0.5
Global Iter: 369500 training loss: 0.686944
Global Iter: 369500 training acc: 0.53125
Global Iter: 369600 training loss: 0.692323
Global Iter: 369600 training acc: 0.53125
Global Iter: 369700 training loss: 0.691549
Global Iter: 369700 training acc: 0.5
Global Iter: 369800 training loss: 0.690711
Global Iter: 369800 training acc: 0.53125
Global Iter: 369900 training loss: 0.683166
Global Iter: 369900 training acc: 0.65625
Global Iter: 370000 training loss: 0.687918
Global Iter: 370000 training acc: 0.53125
Global Iter: 370100 training loss: 0.717121
Global Iter: 370100 training acc: 0.34375
Global Iter: 370200 training loss: 0.695617
Global Iter: 370200 training acc: 0.46875
Global Iter: 370300 training loss: 0.696813
Global Iter: 370300 training acc: 0.5
Global Iter: 370400 training loss: 0.69235
Global Iter: 370400 training acc: 0.53125
Global Iter: 370500 training loss: 0.678361
Global Iter: 370500 training acc: 0.65625
Global Iter: 370600 training loss: 0.705225
Global Iter: 370600 training acc: 0.4375
Global Iter: 370700 training loss: 0.685329
Global Iter: 370700 training acc: 0.5625
Global Iter: 370800 training loss: 0.688808
Global Iter: 370800 training acc: 0.5625
Global Iter: 370900 training loss: 0.671674
Global Iter: 370900 training acc: 0.65625
Global Iter: 371000 training loss: 0.687211
Global Iter: 371000 training acc: 0.5625
Global Iter: 371100 training loss: 0.680458
Global Iter: 371100 training acc: 0.65625
Global Iter: 371200 training loss: 0.661425
Global Iter: 371200 training acc: 0.75
Global Iter: 371300 training loss: 0.695637
Global Iter: 371300 training acc: 0.5
Global Iter: 371400 training loss: 0.685995
Global Iter: 371400 training acc: 0.5625
Global Iter: 371500 training loss: 0.698277
Global Iter: 371500 training acc: 0.53125
Global Iter: 371600 training loss: 0.726218
Global Iter: 371600 training acc: 0.28125
Global Iter: 371700 training loss: 0.703754
Global Iter: 371700 training acc: 0.46875
Global Iter: 371800 training loss: 0.698734
Global Iter: 371800 training acc: 0.4375
Global Iter: 371900 training loss: 0.699032
Global Iter: 371900 training acc: 0.5
Global Iter: 372000 training loss: 0.705149
Global Iter: 372000 training acc: 0.40625
Global Iter: 372100 training loss: 0.669571
Global Iter: 372100 training acc: 0.65625
Global Iter: 372200 training loss: 0.687054
Global Iter: 372200 training acc: 0.5625
Global Iter: 372300 training loss: 0.692446
Global Iter: 372300 training acc: 0.53125
Global Iter: 372400 training loss: 0.684845
Global Iter: 372400 training acc: 0.59375
Global Iter: 372500 training loss: 0.697863
Global Iter: 372500 training acc: 0.5
Global Iter: 372600 training loss: 0.692094
Global Iter: 372600 training acc: 0.53125
Global Iter: 372700 training loss: 0.690822
Global Iter: 372700 training acc: 0.5
Global Iter: 372800 training loss: 0.69545
Global Iter: 372800 training acc: 0.5
Global Iter: 372900 training loss: 0.689792
Global Iter: 372900 training acc: 0.53125
Global Iter: 373000 training loss: 0.688603
Global Iter: 373000 training acc: 0.53125
Global Iter: 373100 training loss: 0.710289
Global Iter: 373100 training acc: 0.375
Global Iter: 373200 training loss: 0.685102
Global Iter: 373200 training acc: 0.5625
Global Iter: 373300 training loss: 0.678608
Global Iter: 373300 training acc: 0.625
Global Iter: 373400 training loss: 0.705566
Global Iter: 373400 training acc: 0.4375
Global Iter: 373500 training loss: 0.69052
Global Iter: 373500 training acc: 0.5625
Global Iter: 373600 training loss: 0.688706
Global Iter: 373600 training acc: 0.5625
Global Iter: 373700 training loss: 0.689611
Global Iter: 373700 training acc: 0.53125
Global Iter: 373800 training loss: 0.680543
Global Iter: 373800 training acc: 0.5625
Global Iter: 373900 training loss: 0.692662
Global Iter: 373900 training acc: 0.5625
Global Iter: 374000 training loss: 0.699595
Global Iter: 374000 training acc: 0.46875
Global Iter: 374100 training loss: 0.67414
Global Iter: 374100 training acc: 0.65625
Global Iter: 374200 training loss: 0.688017
Global Iter: 374200 training acc: 0.5
Global Iter: 374300 training loss: 0.671857
Global Iter: 374300 training acc: 0.65625
Global Iter: 374400 training loss: 0.681543
Global Iter: 374400 training acc: 0.625
Global Iter: 374500 training loss: 0.708822
Global Iter: 374500 training acc: 0.40625
Global Iter: 374600 training loss: 0.695967
Global Iter: 374600 training acc: 0.5
Global Iter: 374700 training loss: 0.684903
Global Iter: 374700 training acc: 0.59375
Global Iter: 374800 training loss: 0.688733
Global Iter: 374800 training acc: 0.53125
Global Iter: 374900 training loss: 0.694106
Global Iter: 374900 training acc: 0.5
Global Iter: 375000 training loss: 0.69686
Global Iter: 375000 training acc: 0.46875
Global Iter: 375100 training loss: 0.708926
Global Iter: 375100 training acc: 0.4375
Global Iter: 375200 training loss: 0.66149
Global Iter: 375200 training acc: 0.78125
Global Iter: 375300 training loss: 0.694738
Global Iter: 375300 training acc: 0.5
Global Iter: 375400 training loss: 0.690486
Global Iter: 375400 training acc: 0.53125
Global Iter: 375500 training loss: 0.673236
Global Iter: 375500 training acc: 0.625
Global Iter: 375600 training loss: 0.689066
Global Iter: 375600 training acc: 0.5625
Global Iter: 375700 training loss: 0.719817
Global Iter: 375700 training acc: 0.3125
Global Iter: 375800 training loss: 0.709116
Global Iter: 375800 training acc: 0.40625
Global Iter: 375900 training loss: 0.68171
Global Iter: 375900 training acc: 0.625
Global Iter: 376000 training loss: 0.701712
Global Iter: 376000 training acc: 0.4375
Global Iter: 376100 training loss: 0.675894
Global Iter: 376100 training acc: 0.6875
Global Iter: 376200 training loss: 0.714977
Global Iter: 376200 training acc: 0.375
Global Iter: 376300 training loss: 0.684242
Global Iter: 376300 training acc: 0.59375
Global Iter: 376400 training loss: 0.678014
Global Iter: 376400 training acc: 0.59375
Global Iter: 376500 training loss: 0.691289
Global Iter: 376500 training acc: 0.5625
Global Iter: 376600 training loss: 0.685622
Global Iter: 376600 training acc: 0.5625
Global Iter: 376700 training loss: 0.696753
Global Iter: 376700 training acc: 0.5
Global Iter: 376800 training loss: 0.694441
Global Iter: 376800 training acc: 0.5625
Global Iter: 376900 training loss: 0.692159
Global Iter: 376900 training acc: 0.53125
Global Iter: 377000 training loss: 0.689847
Global Iter: 377000 training acc: 0.5625
Global Iter: 377100 training loss: 0.713788
Global Iter: 377100 training acc: 0.34375
Global Iter: 377200 training loss: 0.664178
Global Iter: 377200 training acc: 0.71875
Global Iter: 377300 training loss: 0.679898
Global Iter: 377300 training acc: 0.59375
Global Iter: 377400 training loss: 0.698746
Global Iter: 377400 training acc: 0.46875
Global Iter: 377500 training loss: 0.681537
Global Iter: 377500 training acc: 0.59375
Global Iter: 377600 training loss: 0.681464
Global Iter: 377600 training acc: 0.59375
Global Iter: 377700 training loss: 0.696929
Global Iter: 377700 training acc: 0.46875
Global Iter: 377800 training loss: 0.675454
Global Iter: 377800 training acc: 0.65625
Global Iter: 377900 training loss: 0.686394
Global Iter: 377900 training acc: 0.53125
Global Iter: 378000 training loss: 0.699365
Global Iter: 378000 training acc: 0.46875
Global Iter: 378100 training loss: 0.666734
Global Iter: 378100 training acc: 0.75
Global Iter: 378200 training loss: 0.679649
Global Iter: 378200 training acc: 0.625
Global Iter: 378300 training loss: 0.699564
Global Iter: 378300 training acc: 0.40625
Global Iter: 378400 training loss: 0.699005
Global Iter: 378400 training acc: 0.5
Global Iter: 378500 training loss: 0.690918
Global Iter: 378500 training acc: 0.5625
Global Iter: 378600 training loss: 0.733554
Global Iter: 378600 training acc: 0.21875
Global Iter: 378700 training loss: 0.704059
Global Iter: 378700 training acc: 0.4375
Global Iter: 378800 training loss: 0.686785
Global Iter: 378800 training acc: 0.5625
Global Iter: 378900 training loss: 0.694167
Global Iter: 378900 training acc: 0.53125
Global Iter: 379000 training loss: 0.684253
Global Iter: 379000 training acc: 0.625
Global Iter: 379100 training loss: 0.690252
Global Iter: 379100 training acc: 0.53125
Global Iter: 379200 training loss: 0.67633
Global Iter: 379200 training acc: 0.65625
Global Iter: 379300 training loss: 0.700628
Global Iter: 379300 training acc: 0.46875
Global Iter: 379400 training loss: 0.696292
Global Iter: 379400 training acc: 0.5
Global Iter: 379500 training loss: 0.684858
Global Iter: 379500 training acc: 0.53125
Global Iter: 379600 training loss: 0.694521
Global Iter: 379600 training acc: 0.5
Global Iter: 379700 training loss: 0.688252
Global Iter: 379700 training acc: 0.5625
Global Iter: 379800 training loss: 0.676702
Global Iter: 379800 training acc: 0.71875
Global Iter: 379900 training loss: 0.69221
Global Iter: 379900 training acc: 0.53125
Global Iter: 380000 training loss: 0.70845
Global Iter: 380000 training acc: 0.375
Global Iter: 380100 training loss: 0.694641
Global Iter: 380100 training acc: 0.5
Global Iter: 380200 training loss: 0.692909
Global Iter: 380200 training acc: 0.5
Global Iter: 380300 training loss: 0.692479
Global Iter: 380300 training acc: 0.5
Global Iter: 380400 training loss: 0.669672
Global Iter: 380400 training acc: 0.71875
Global Iter: 380500 training loss: 0.697605
Global Iter: 380500 training acc: 0.46875
Global Iter: 380600 training loss: 0.681221
Global Iter: 380600 training acc: 0.59375
Global Iter: 380700 training loss: 0.676389
Global Iter: 380700 training acc: 0.625
Global Iter: 380800 training loss: 0.677732
Global Iter: 380800 training acc: 0.625
Global Iter: 380900 training loss: 0.687349
Global Iter: 380900 training acc: 0.53125
Global Iter: 381000 training loss: 0.684029
Global Iter: 381000 training acc: 0.65625
Global Iter: 381100 training loss: 0.664672
Global Iter: 381100 training acc: 0.75
Global Iter: 381200 training loss: 0.69218
Global Iter: 381200 training acc: 0.53125
Global Iter: 381300 training loss: 0.685352
Global Iter: 381300 training acc: 0.5625
Global Iter: 381400 training loss: 0.690636
Global Iter: 381400 training acc: 0.53125
Global Iter: 381500 training loss: 0.730222
Global Iter: 381500 training acc: 0.25
Global Iter: 381600 training loss: 0.702094
Global Iter: 381600 training acc: 0.46875
Global Iter: 381700 training loss: 0.703193
Global Iter: 381700 training acc: 0.4375
Global Iter: 381800 training loss: 0.693795
Global Iter: 381800 training acc: 0.53125
Global Iter: 381900 training loss: 0.706049
Global Iter: 381900 training acc: 0.375
Global Iter: 382000 training loss: 0.661353
Global Iter: 382000 training acc: 0.71875
Global Iter: 382100 training loss: 0.684351
Global Iter: 382100 training acc: 0.5625
Global Iter: 382200 training loss: 0.694422
Global Iter: 382200 training acc: 0.5
Global Iter: 382300 training loss: 0.678127
Global Iter: 382300 training acc: 0.625
Global Iter: 382400 training loss: 0.698303
Global Iter: 382400 training acc: 0.46875
Global Iter: 382500 training loss: 0.686562
Global Iter: 382500 training acc: 0.5625
Global Iter: 382600 training loss: 0.691357
Global Iter: 382600 training acc: 0.53125
Global Iter: 382700 training loss: 0.690098
Global Iter: 382700 training acc: 0.5
Global Iter: 382800 training loss: 0.697213
Global Iter: 382800 training acc: 0.5
Global Iter: 382900 training loss: 0.692082
Global Iter: 382900 training acc: 0.53125
Global Iter: 383000 training loss: 0.717114
Global Iter: 383000 training acc: 0.375
Global Iter: 383100 training loss: 0.68216
Global Iter: 383100 training acc: 0.625
Global Iter: 383200 training loss: 0.679959
Global Iter: 383200 training acc: 0.625
Global Iter: 383300 training loss: 0.703318
Global Iter: 383300 training acc: 0.4375
Global Iter: 383400 training loss: 0.688741
Global Iter: 383400 training acc: 0.53125
Global Iter: 383500 training loss: 0.683899
Global Iter: 383500 training acc: 0.5625
Global Iter: 383600 training loss: 0.686207
Global Iter: 383600 training acc: 0.5625
Global Iter: 383700 training loss: 0.679016
Global Iter: 383700 training acc: 0.625
Global Iter: 383800 training loss: 0.689236
Global Iter: 383800 training acc: 0.53125
Global Iter: 383900 training loss: 0.699036
Global Iter: 383900 training acc: 0.4375
Global Iter: 384000 training loss: 0.681839
Global Iter: 384000 training acc: 0.625
Global Iter: 384100 training loss: 0.696903
Global Iter: 384100 training acc: 0.5
Global Iter: 384200 training loss: 0.669796
Global Iter: 384200 training acc: 0.6875
Global Iter: 384300 training loss: 0.681023
Global Iter: 384300 training acc: 0.625
Global Iter: 384400 training loss: 0.71329
Global Iter: 384400 training acc: 0.40625
Global Iter: 384500 training loss: 0.689985
Global Iter: 384500 training acc: 0.53125
Global Iter: 384600 training loss: 0.683874
Global Iter: 384600 training acc: 0.59375
Global Iter: 384700 training loss: 0.69483
Global Iter: 384700 training acc: 0.53125
Global Iter: 384800 training loss: 0.693014
Global Iter: 384800 training acc: 0.53125
Global Iter: 384900 training loss: 0.695567
Global Iter: 384900 training acc: 0.5
Global Iter: 385000 training loss: 0.703701
Global Iter: 385000 training acc: 0.4375
Global Iter: 385100 training loss: 0.661637
Global Iter: 385100 training acc: 0.78125
Global Iter: 385200 training loss: 0.696706
Global Iter: 385200 training acc: 0.46875
Global Iter: 385300 training loss: 0.684867
Global Iter: 385300 training acc: 0.5625
Global Iter: 385400 training loss: 0.687348
Global Iter: 385400 training acc: 0.59375
Global Iter: 385500 training loss: 0.690968
Global Iter: 385500 training acc: 0.5625
Global Iter: 385600 training loss: 0.709754
Global Iter: 385600 training acc: 0.34375
Global Iter: 385700 training loss: 0.706264
Global Iter: 385700 training acc: 0.40625
Global Iter: 385800 training loss: 0.691927
Global Iter: 385800 training acc: 0.5625
Global Iter: 385900 training loss: 0.708784
Global Iter: 385900 training acc: 0.40625
Global Iter: 386000 training loss: 0.675515
Global Iter: 386000 training acc: 0.65625
Global Iter: 386100 training loss: 0.712149
Global Iter: 386100 training acc: 0.40625
Global Iter: 386200 training loss: 0.674236
Global Iter: 386200 training acc: 0.625
Global Iter: 386300 training loss: 0.688614
Global Iter: 386300 training acc: 0.5625
Global Iter: 386400 training loss: 0.689802
Global Iter: 386400 training acc: 0.5625
Global Iter: 386500 training loss: 0.692384
Global Iter: 386500 training acc: 0.5625
Global Iter: 386600 training loss: 0.692573
Global Iter: 386600 training acc: 0.5
Global Iter: 386700 training loss: 0.681241
Global Iter: 386700 training acc: 0.5625
Global Iter: 386800 training loss: 0.69047
Global Iter: 386800 training acc: 0.5625
Global Iter: 386900 training loss: 0.688506
Global Iter: 386900 training acc: 0.5625
Global Iter: 387000 training loss: 0.716604
Global Iter: 387000 training acc: 0.34375
Global Iter: 387100 training loss: 0.67165
Global Iter: 387100 training acc: 0.6875
Global Iter: 387200 training loss: 0.681508
Global Iter: 387200 training acc: 0.59375
Global Iter: 387300 training loss: 0.696982
Global Iter: 387300 training acc: 0.46875
Global Iter: 387400 training loss: 0.685869
Global Iter: 387400 training acc: 0.59375
Global Iter: 387500 training loss: 0.683031
Global Iter: 387500 training acc: 0.59375
Global Iter: 387600 training loss: 0.712859
Global Iter: 387600 training acc: 0.40625
Global Iter: 387700 training loss: 0.684551
Global Iter: 387700 training acc: 0.625
Global Iter: 387800 training loss: 0.690591
Global Iter: 387800 training acc: 0.53125
Global Iter: 387900 training loss: 0.694003
Global Iter: 387900 training acc: 0.5
Global Iter: 388000 training loss: 0.667806
Global Iter: 388000 training acc: 0.75
Global Iter: 388100 training loss: 0.671126
Global Iter: 388100 training acc: 0.65625
Global Iter: 388200 training loss: 0.705781
Global Iter: 388200 training acc: 0.4375
Global Iter: 388300 training loss: 0.69018
Global Iter: 388300 training acc: 0.5
Global Iter: 388400 training loss: 0.684752
Global Iter: 388400 training acc: 0.59375
Global Iter: 388500 training loss: 0.732068
Global Iter: 388500 training acc: 0.21875
Global Iter: 388600 training loss: 0.700209
Global Iter: 388600 training acc: 0.46875
Global Iter: 388700 training loss: 0.692194
Global Iter: 388700 training acc: 0.5625
Global Iter: 388800 training loss: 0.679815
Global Iter: 388800 training acc: 0.59375
Global Iter: 388900 training loss: 0.686449
Global Iter: 388900 training acc: 0.59375
Global Iter: 389000 training loss: 0.704983
Global Iter: 389000 training acc: 0.46875
Global Iter: 389100 training loss: 0.677991
Global Iter: 389100 training acc: 0.625
Global Iter: 389200 training loss: 0.700523
Global Iter: 389200 training acc: 0.4375
Global Iter: 389300 training loss: 0.700405
Global Iter: 389300 training acc: 0.46875
Global Iter: 389400 training loss: 0.692445
Global Iter: 389400 training acc: 0.5625
Global Iter: 389500 training loss: 0.698362
Global Iter: 389500 training acc: 0.5
Global Iter: 389600 training loss: 0.687696
Global Iter: 389600 training acc: 0.5625
Global Iter: 389700 training loss: 0.678854
Global Iter: 389700 training acc: 0.71875
Global Iter: 389800 training loss: 0.689699
Global Iter: 389800 training acc: 0.5625
Global Iter: 389900 training loss: 0.717311
Global Iter: 389900 training acc: 0.34375
Global Iter: 390000 training loss: 0.693065
Global Iter: 390000 training acc: 0.5
Global Iter: 390100 training loss: 0.700573
Global Iter: 390100 training acc: 0.5
Global Iter: 390200 training loss: 0.694255
Global Iter: 390200 training acc: 0.5
Global Iter: 390300 training loss: 0.671077
Global Iter: 390300 training acc: 0.71875
Global Iter: 390400 training loss: 0.699552
Global Iter: 390400 training acc: 0.46875
Global Iter: 390500 training loss: 0.680574
Global Iter: 390500 training acc: 0.59375
Global Iter: 390600 training loss: 0.680531
Global Iter: 390600 training acc: 0.59375
Global Iter: 390700 training loss: 0.674408
Global Iter: 390700 training acc: 0.6875
Global Iter: 390800 training loss: 0.688257
Global Iter: 390800 training acc: 0.53125
Global Iter: 390900 training loss: 0.676738
Global Iter: 390900 training acc: 0.65625
Global Iter: 391000 training loss: 0.666464
Global Iter: 391000 training acc: 0.75
Global Iter: 391100 training loss: 0.69513
Global Iter: 391100 training acc: 0.46875
Global Iter: 391200 training loss: 0.700375
Global Iter: 391200 training acc: 0.5
Global Iter: 391300 training loss: 0.692207
Global Iter: 391300 training acc: 0.53125
Global Iter: 391400 training loss: 0.733861
Global Iter: 391400 training acc: 0.21875
Global Iter: 391500 training loss: 0.694508
Global Iter: 391500 training acc: 0.5
Global Iter: 391600 training loss: 0.703876
Global Iter: 391600 training acc: 0.4375
Global Iter: 391700 training loss: 0.695204
Global Iter: 391700 training acc: 0.5
Global Iter: 391800 training loss: 0.69934
Global Iter: 391800 training acc: 0.4375
Global Iter: 391900 training loss: 0.669023
Global Iter: 391900 training acc: 0.6875
Global Iter: 392000 training loss: 0.68817
Global Iter: 392000 training acc: 0.59375
Global Iter: 392100 training loss: 0.704596
Global Iter: 392100 training acc: 0.4375
Global Iter: 392200 training loss: 0.680991
Global Iter: 392200 training acc: 0.59375
Global Iter: 392300 training loss: 0.700919
Global Iter: 392300 training acc: 0.46875
Global Iter: 392400 training loss: 0.68535
Global Iter: 392400 training acc: 0.5625
Global Iter: 392500 training loss: 0.689253
Global Iter: 392500 training acc: 0.53125
Global Iter: 392600 training loss: 0.692029
Global Iter: 392600 training acc: 0.5
Global Iter: 392700 training loss: 0.695009
Global Iter: 392700 training acc: 0.5
Global Iter: 392800 training loss: 0.699824
Global Iter: 392800 training acc: 0.46875
Global Iter: 392900 training loss: 0.721579
Global Iter: 392900 training acc: 0.34375
Global Iter: 393000 training loss: 0.689952
Global Iter: 393000 training acc: 0.59375
Global Iter: 393100 training loss: 0.681903
Global Iter: 393100 training acc: 0.625
Global Iter: 393200 training loss: 0.700622
Global Iter: 393200 training acc: 0.46875
Global Iter: 393300 training loss: 0.703737
Global Iter: 393300 training acc: 0.46875
Global Iter: 393400 training loss: 0.682581
Global Iter: 393400 training acc: 0.59375
Global Iter: 393500 training loss: 0.695014
Global Iter: 393500 training acc: 0.5
Global Iter: 393600 training loss: 0.681396
Global Iter: 393600 training acc: 0.625
Global Iter: 393700 training loss: 0.693835
Global Iter: 393700 training acc: 0.53125
Global Iter: 393800 training loss: 0.697594
Global Iter: 393800 training acc: 0.46875
Global Iter: 393900 training loss: 0.67137
Global Iter: 393900 training acc: 0.6875
Global Iter: 394000 training loss: 0.695337
Global Iter: 394000 training acc: 0.53125
Global Iter: 394100 training loss: 0.66453
Global Iter: 394100 training acc: 0.71875
Global Iter: 394200 training loss: 0.680199
Global Iter: 394200 training acc: 0.625
Global Iter: 394300 training loss: 0.716331
Global Iter: 394300 training acc: 0.375
Global Iter: 394400 training loss: 0.68985
Global Iter: 394400 training acc: 0.5
Global Iter: 394500 training loss: 0.687587
Global Iter: 394500 training acc: 0.5625
Global Iter: 394600 training loss: 0.691069
Global Iter: 394600 training acc: 0.53125
Global Iter: 394700 training loss: 0.692597
Global Iter: 394700 training acc: 0.53125
Global Iter: 394800 training loss: 0.694143
Global Iter: 394800 training acc: 0.53125
Global Iter: 394900 training loss: 0.708265
Global Iter: 394900 training acc: 0.375
Global Iter: 395000 training loss: 0.662453
Global Iter: 395000 training acc: 0.75
Global Iter: 395100 training loss: 0.696785
Global Iter: 395100 training acc: 0.5
Global Iter: 395200 training loss: 0.690817
Global Iter: 395200 training acc: 0.53125
Global Iter: 395300 training loss: 0.683648
Global Iter: 395300 training acc: 0.59375
Global Iter: 395400 training loss: 0.699507
Global Iter: 395400 training acc: 0.5
Global Iter: 395500 training loss: 0.704877
Global Iter: 395500 training acc: 0.375
Global Iter: 395600 training loss: 0.708161
Global Iter: 395600 training acc: 0.40625
Global Iter: 395700 training loss: 0.6856
Global Iter: 395700 training acc: 0.5625
Global Iter: 395800 training loss: 0.712289
Global Iter: 395800 training acc: 0.375
Global Iter: 395900 training loss: 0.678671
Global Iter: 395900 training acc: 0.65625
Global Iter: 396000 training loss: 0.702018
Global Iter: 396000 training acc: 0.46875
Global Iter: 396100 training loss: 0.681161
Global Iter: 396100 training acc: 0.625
Global Iter: 396200 training loss: 0.681423
Global Iter: 396200 training acc: 0.625
Global Iter: 396300 training loss: 0.687753
Global Iter: 396300 training acc: 0.5625
Global Iter: 396400 training loss: 0.688015
Global Iter: 396400 training acc: 0.5625
Global Iter: 396500 training loss: 0.69512
Global Iter: 396500 training acc: 0.5
Global Iter: 396600 training loss: 0.687374
Global Iter: 396600 training acc: 0.5625
Global Iter: 396700 training loss: 0.687605
Global Iter: 396700 training acc: 0.5625
Global Iter: 396800 training loss: 0.686396
Global Iter: 396800 training acc: 0.59375
Global Iter: 396900 training loss: 0.720107
Global Iter: 396900 training acc: 0.3125
Global Iter: 397000 training loss: 0.663273
Global Iter: 397000 training acc: 0.71875
Global Iter: 397100 training loss: 0.697628
Global Iter: 397100 training acc: 0.53125
Global Iter: 397200 training loss: 0.707395
Global Iter: 397200 training acc: 0.4375
Global Iter: 397300 training loss: 0.681202
Global Iter: 397300 training acc: 0.5625
Global Iter: 397400 training loss: 0.677772
Global Iter: 397400 training acc: 0.625
Global Iter: 397500 training loss: 0.706594
Global Iter: 397500 training acc: 0.4375
Global Iter: 397600 training loss: 0.67757
Global Iter: 397600 training acc: 0.625
Global Iter: 397700 training loss: 0.696375
Global Iter: 397700 training acc: 0.46875
Global Iter: 397800 training loss: 0.695351
Global Iter: 397800 training acc: 0.53125
Global Iter: 397900 training loss: 0.667053
Global Iter: 397900 training acc: 0.75
Global Iter: 398000 training loss: 0.673805
Global Iter: 398000 training acc: 0.65625
Global Iter: 398100 training loss: 0.699833
Global Iter: 398100 training acc: 0.4375
Global Iter: 398200 training loss: 0.685143
Global Iter: 398200 training acc: 0.5625
Global Iter: 398300 training loss: 0.6822
Global Iter: 398300 training acc: 0.59375
Global Iter: 398400 training loss: 0.731959
Global Iter: 398400 training acc: 0.21875
Global Iter: 398500 training loss: 0.699859
Global Iter: 398500 training acc: 0.46875
Global Iter: 398600 training loss: 0.679985
Global Iter: 398600 training acc: 0.59375
Global Iter: 398700 training loss: 0.676755
Global Iter: 398700 training acc: 0.625
Global Iter: 398800 training loss: 0.679304
Global Iter: 398800 training acc: 0.65625
Global Iter: 398900 training loss: 0.695681
Global Iter: 398900 training acc: 0.46875
Global Iter: 399000 training loss: 0.678459
Global Iter: 399000 training acc: 0.6875
Global Iter: 399100 training loss: 0.701459
Global Iter: 399100 training acc: 0.4375
Global Iter: 399200 training loss: 0.6911
Global Iter: 399200 training acc: 0.5
Global Iter: 399300 training loss: 0.689218
Global Iter: 399300 training acc: 0.5625
Global Iter: 399400 training loss: 0.698167
Global Iter: 399400 training acc: 0.46875
Global Iter: 399500 training loss: 0.699184
Global Iter: 399500 training acc: 0.5
Global Iter: 399600 training loss: 0.677897
Global Iter: 399600 training acc: 0.65625
Global Iter: 399700 training loss: 0.687746
Global Iter: 399700 training acc: 0.5625
Global Iter: 399800 training loss: 0.710573
Global Iter: 399800 training acc: 0.375
Global Iter: 399900 training loss: 0.694057
Global Iter: 399900 training acc: 0.5
Global Iter: 400000 training loss: 0.691496
Global Iter: 400000 training acc: 0.53125
Global Iter: 400100 training loss: 0.703869
Global Iter: 400100 training acc: 0.4375
Global Iter: 400200 training loss: 0.663064
Global Iter: 400200 training acc: 0.71875
Global Iter: 400300 training loss: 0.706378
Global Iter: 400300 training acc: 0.4375
Global Iter: 400400 training loss: 0.682353
Global Iter: 400400 training acc: 0.625
Global Iter: 400500 training loss: 0.681618
Global Iter: 400500 training acc: 0.625
Global Iter: 400600 training loss: 0.663545
Global Iter: 400600 training acc: 0.71875
Global Iter: 400700 training loss: 0.69665
Global Iter: 400700 training acc: 0.5
Global Iter: 400800 training loss: 0.681296
Global Iter: 400800 training acc: 0.625
Global Iter: 400900 training loss: 0.6646
Global Iter: 400900 training acc: 0.78125
Global Iter: 401000 training loss: 0.700385
Global Iter: 401000 training acc: 0.4375
Global Iter: 401100 training loss: 0.698833
Global Iter: 401100 training acc: 0.5
Global Iter: 401200 training loss: 0.6865
Global Iter: 401200 training acc: 0.53125
Global Iter: 401300 training loss: 0.734984
Global Iter: 401300 training acc: 0.21875
Global Iter: 401400 training loss: 0.688583
Global Iter: 401400 training acc: 0.53125
Global Iter: 401500 training loss: 0.698441
Global Iter: 401500 training acc: 0.46875
Global Iter: 401600 training loss: 0.691679
Global Iter: 401600 training acc: 0.5
Global Iter: 401700 training loss: 0.705892
Global Iter: 401700 training acc: 0.4375
Global Iter: 401800 training loss: 0.679334
Global Iter: 401800 training acc: 0.65625
Global Iter: 401900 training loss: 0.688653
Global Iter: 401900 training acc: 0.59375
Global Iter: 402000 training loss: 0.701418
Global Iter: 402000 training acc: 0.46875
Global Iter: 402100 training loss: 0.678307
Global Iter: 402100 training acc: 0.59375
Global Iter: 402200 training loss: 0.694332
Global Iter: 402200 training acc: 0.5
Global Iter: 402300 training loss: 0.694943
Global Iter: 402300 training acc: 0.53125
Global Iter: 402400 training loss: 0.698402
Global Iter: 402400 training acc: 0.46875
Global Iter: 402500 training loss: 0.689896
Global Iter: 402500 training acc: 0.53125
Global Iter: 402600 training loss: 0.6961
Global Iter: 402600 training acc: 0.5
Global Iter: 402700 training loss: 0.704963
Global Iter: 402700 training acc: 0.46875
Global Iter: 402800 training loss: 0.717248
Global Iter: 402800 training acc: 0.375
Global Iter: 402900 training loss: 0.681958
Global Iter: 402900 training acc: 0.5625
Global Iter: 403000 training loss: 0.685439
Global Iter: 403000 training acc: 0.625
Global Iter: 403100 training loss: 0.693146
Global Iter: 403100 training acc: 0.53125
Global Iter: 403200 training loss: 0.699322
Global Iter: 403200 training acc: 0.46875
Global Iter: 403300 training loss: 0.680007
Global Iter: 403300 training acc: 0.59375
Global Iter: 403400 training loss: 0.696056
Global Iter: 403400 training acc: 0.46875
Global Iter: 403500 training loss: 0.679977
Global Iter: 403500 training acc: 0.625
Global Iter: 403600 training loss: 0.695972
Global Iter: 403600 training acc: 0.5
Global Iter: 403700 training loss: 0.691997
Global Iter: 403700 training acc: 0.5
Global Iter: 403800 training loss: 0.663903
Global Iter: 403800 training acc: 0.75
Global Iter: 403900 training loss: 0.694528
Global Iter: 403900 training acc: 0.5
Global Iter: 404000 training loss: 0.671036
Global Iter: 404000 training acc: 0.65625
Global Iter: 404100 training loss: 0.685947
Global Iter: 404100 training acc: 0.59375
Global Iter: 404200 training loss: 0.711138
Global Iter: 404200 training acc: 0.375
Global Iter: 404300 training loss: 0.698165
Global Iter: 404300 training acc: 0.5
Global Iter: 404400 training loss: 0.680619
Global Iter: 404400 training acc: 0.5625
Global Iter: 404500 training loss: 0.688945
Global Iter: 404500 training acc: 0.53125
Global Iter: 404600 training loss: 0.694969
Global Iter: 404600 training acc: 0.53125
Global Iter: 404700 training loss: 0.695495
Global Iter: 404700 training acc: 0.53125
Global Iter: 404800 training loss: 0.705343
Global Iter: 404800 training acc: 0.4375
Global Iter: 404900 training loss: 0.664624
Global Iter: 404900 training acc: 0.71875
Global Iter: 405000 training loss: 0.696405
Global Iter: 405000 training acc: 0.5
Global Iter: 405100 training loss: 0.687769
Global Iter: 405100 training acc: 0.53125
Global Iter: 405200 training loss: 0.680828
Global Iter: 405200 training acc: 0.59375
Global Iter: 405300 training loss: 0.70093
Global Iter: 405300 training acc: 0.4375
Global Iter: 405400 training loss: 0.711583
Global Iter: 405400 training acc: 0.34375
Global Iter: 405500 training loss: 0.708502
Global Iter: 405500 training acc: 0.40625
Global Iter: 405600 training loss: 0.685483
Global Iter: 405600 training acc: 0.5625
Global Iter: 405700 training loss: 0.715775
Global Iter: 405700 training acc: 0.375
Global Iter: 405800 training loss: 0.673749
Global Iter: 405800 training acc: 0.65625
Global Iter: 405900 training loss: 0.7063
Global Iter: 405900 training acc: 0.4375
Global Iter: 406000 training loss: 0.690758
Global Iter: 406000 training acc: 0.5625
Global Iter: 406100 training loss: 0.681653
Global Iter: 406100 training acc: 0.625
Global Iter: 406200 training loss: 0.684038
Global Iter: 406200 training acc: 0.5625
Global Iter: 406300 training loss: 0.693115
Global Iter: 406300 training acc: 0.5
Global Iter: 406400 training loss: 0.70218
Global Iter: 406400 training acc: 0.46875
Global Iter: 406500 training loss: 0.686633
Global Iter: 406500 training acc: 0.5625
Global Iter: 406600 training loss: 0.686388
Global Iter: 406600 training acc: 0.5625
Global Iter: 406700 training loss: 0.683803
Global Iter: 406700 training acc: 0.59375
Global Iter: 406800 training loss: 0.708463
Global Iter: 406800 training acc: 0.375
Global Iter: 406900 training loss: 0.664724
Global Iter: 406900 training acc: 0.71875
Global Iter: 407000 training loss: 0.688785
Global Iter: 407000 training acc: 0.53125
Global Iter: 407100 training loss: 0.703926
Global Iter: 407100 training acc: 0.4375
Global Iter: 407200 training loss: 0.695022
Global Iter: 407200 training acc: 0.53125
Global Iter: 407300 training loss: 0.684561
Global Iter: 407300 training acc: 0.59375
Global Iter: 407400 training loss: 0.702735
Global Iter: 407400 training acc: 0.46875
Global Iter: 407500 training loss: 0.685825
Global Iter: 407500 training acc: 0.625
Global Iter: 407600 training loss: 0.694041
Global Iter: 407600 training acc: 0.53125
Global Iter: 407700 training loss: 0.692584
Global Iter: 407700 training acc: 0.53125
Global Iter: 407800 training loss: 0.669042
Global Iter: 407800 training acc: 0.71875
Global Iter: 407900 training loss: 0.677714
Global Iter: 407900 training acc: 0.65625
Global Iter: 408000 training loss: 0.707797
Global Iter: 408000 training acc: 0.40625
Global Iter: 408100 training loss: 0.682664
Global Iter: 408100 training acc: 0.59375
Global Iter: 408200 training loss: 0.682627
Global Iter: 408200 training acc: 0.59375
Global Iter: 408300 training loss: 0.730576
Global Iter: 408300 training acc: 0.21875
Global Iter: 408400 training loss: 0.703931
Global Iter: 408400 training acc: 0.4375
Global Iter: 408500 training loss: 0.687754
Global Iter: 408500 training acc: 0.5625
Global Iter: 408600 training loss: 0.676997
Global Iter: 408600 training acc: 0.625
Global Iter: 408700 training loss: 0.671361
Global Iter: 408700 training acc: 0.71875
Global Iter: 408800 training loss: 0.698445
Global Iter: 408800 training acc: 0.46875
Global Iter: 408900 training loss: 0.66922
Global Iter: 408900 training acc: 0.71875
Global Iter: 409000 training loss: 0.695834
Global Iter: 409000 training acc: 0.5
Global Iter: 409100 training loss: 0.689478
Global Iter: 409100 training acc: 0.53125
Global Iter: 409200 training loss: 0.681724
Global Iter: 409200 training acc: 0.625
Global Iter: 409300 training loss: 0.697263
Global Iter: 409300 training acc: 0.46875
Global Iter: 409400 training loss: 0.70393
Global Iter: 409400 training acc: 0.4375
Global Iter: 409500 training loss: 0.68175
Global Iter: 409500 training acc: 0.65625
Global Iter: 409600 training loss: 0.690692
Global Iter: 409600 training acc: 0.53125
Global Iter: 409700 training loss: 0.715152
Global Iter: 409700 training acc: 0.34375
Global Iter: 409800 training loss: 0.688645
Global Iter: 409800 training acc: 0.53125
Global Iter: 409900 training loss: 0.689398
Global Iter: 409900 training acc: 0.5625
Global Iter: 410000 training loss: 0.709534
Global Iter: 410000 training acc: 0.4375
Global Iter: 410100 training loss: 0.662891
Global Iter: 410100 training acc: 0.71875
Global Iter: 410200 training loss: 0.706318
Global Iter: 410200 training acc: 0.40625
Global Iter: 410300 training loss: 0.684726
Global Iter: 410300 training acc: 0.5625
Global Iter: 410400 training loss: 0.683169
Global Iter: 410400 training acc: 0.59375
Global Iter: 410500 training loss: 0.670843
Global Iter: 410500 training acc: 0.6875
Global Iter: 410600 training loss: 0.695804
Global Iter: 410600 training acc: 0.5
Global Iter: 410700 training loss: 0.678678
Global Iter: 410700 training acc: 0.65625
Global Iter: 410800 training loss: 0.663707
Global Iter: 410800 training acc: 0.75
Global Iter: 410900 training loss: 0.698499
Global Iter: 410900 training acc: 0.4375
Global Iter: 411000 training loss: 0.697734
Global Iter: 411000 training acc: 0.5
Global Iter: 411100 training loss: 0.693543
Global Iter: 411100 training acc: 0.5
Global Iter: 411200 training loss: 0.732977
Global Iter: 411200 training acc: 0.25
Global Iter: 411300 training loss: 0.695124
Global Iter: 411300 training acc: 0.5
Global Iter: 411400 training loss: 0.699507
Global Iter: 411400 training acc: 0.46875
Global Iter: 411500 training loss: 0.702763
Global Iter: 411500 training acc: 0.46875
Global Iter: 411600 training loss: 0.700914
Global Iter: 411600 training acc: 0.46875
Global Iter: 411700 training loss: 0.668932
Global Iter: 411700 training acc: 0.65625
Global Iter: 411800 training loss: 0.684862
Global Iter: 411800 training acc: 0.59375
Global Iter: 411900 training loss: 0.706167
Global Iter: 411900 training acc: 0.4375
Global Iter: 412000 training loss: 0.682971
Global Iter: 412000 training acc: 0.59375
Global Iter: 412100 training loss: 0.696941
Global Iter: 412100 training acc: 0.5
Global Iter: 412200 training loss: 0.691333
Global Iter: 412200 training acc: 0.5625
Global Iter: 412300 training loss: 0.699671
Global Iter: 412300 training acc: 0.46875
Global Iter: 412400 training loss: 0.688199
Global Iter: 412400 training acc: 0.53125
Global Iter: 412500 training loss: 0.692232
Global Iter: 412500 training acc: 0.5
Global Iter: 412600 training loss: 0.709421
Global Iter: 412600 training acc: 0.4375
Global Iter: 412700 training loss: 0.720279
Global Iter: 412700 training acc: 0.375
Global Iter: 412800 training loss: 0.692928
Global Iter: 412800 training acc: 0.53125
Global Iter: 412900 training loss: 0.674833
Global Iter: 412900 training acc: 0.65625
Global Iter: 413000 training loss: 0.684596
Global Iter: 413000 training acc: 0.59375
Global Iter: 413100 training loss: 0.689808
Global Iter: 413100 training acc: 0.5
Global Iter: 413200 training loss: 0.687109
Global Iter: 413200 training acc: 0.5625
Global Iter: 413300 training loss: 0.691519
Global Iter: 413300 training acc: 0.46875
Global Iter: 413400 training loss: 0.688056
Global Iter: 413400 training acc: 0.5625
Global Iter: 413500 training loss: 0.698016
Global Iter: 413500 training acc: 0.5
Global Iter: 413600 training loss: 0.691775
Global Iter: 413600 training acc: 0.53125
Global Iter: 413700 training loss: 0.666699
Global Iter: 413700 training acc: 0.75
Global Iter: 413800 training loss: 0.698784
Global Iter: 413800 training acc: 0.46875
Global Iter: 413900 training loss: 0.681211
Global Iter: 413900 training acc: 0.625
Global Iter: 414000 training loss: 0.681594
Global Iter: 414000 training acc: 0.59375
Global Iter: 414100 training loss: 0.720289
Global Iter: 414100 training acc: 0.34375
Global Iter: 414200 training loss: 0.698553
Global Iter: 414200 training acc: 0.46875
Global Iter: 414300 training loss: 0.685884
Global Iter: 414300 training acc: 0.5625
Global Iter: 414400 training loss: 0.6899
Global Iter: 414400 training acc: 0.53125
Global Iter: 414500 training loss: 0.69476
Global Iter: 414500 training acc: 0.5
Global Iter: 414600 training loss: 0.680595
Global Iter: 414600 training acc: 0.59375
Global Iter: 414700 training loss: 0.690173
Global Iter: 414700 training acc: 0.5
Global Iter: 414800 training loss: 0.671458
Global Iter: 414800 training acc: 0.65625
Global Iter: 414900 training loss: 0.692015
Global Iter: 414900 training acc: 0.5625
Global Iter: 415000 training loss: 0.688334
Global Iter: 415000 training acc: 0.5625
Global Iter: 415100 training loss: 0.687701
Global Iter: 415100 training acc: 0.59375
Global Iter: 415200 training loss: 0.707316
Global Iter: 415200 training acc: 0.40625
Global Iter: 415300 training loss: 0.704329
Global Iter: 415300 training acc: 0.375
Global Iter: 415400 training loss: 0.708852
Global Iter: 415400 training acc: 0.40625
Global Iter: 415500 training loss: 0.684084
Global Iter: 415500 training acc: 0.5625
Global Iter: 415600 training loss: 0.708995
Global Iter: 415600 training acc: 0.40625
Global Iter: 415700 training loss: 0.684159
Global Iter: 415700 training acc: 0.59375
Global Iter: 415800 training loss: 0.697255
Global Iter: 415800 training acc: 0.5
Global Iter: 415900 training loss: 0.698127
Global Iter: 415900 training acc: 0.5
Global Iter: 416000 training loss: 0.68448
Global Iter: 416000 training acc: 0.625
Global Iter: 416100 training loss: 0.681431
Global Iter: 416100 training acc: 0.59375
Global Iter: 416200 training loss: 0.690051
Global Iter: 416200 training acc: 0.53125
Global Iter: 416300 training loss: 0.69492
Global Iter: 416300 training acc: 0.5
Global Iter: 416400 training loss: 0.693627
Global Iter: 416400 training acc: 0.53125
Global Iter: 416500 training loss: 0.691999
Global Iter: 416500 training acc: 0.5
Global Iter: 416600 training loss: 0.681662
Global Iter: 416600 training acc: 0.59375
Global Iter: 416700 training loss: 0.710998
Global Iter: 416700 training acc: 0.375
Global Iter: 416800 training loss: 0.672586
Global Iter: 416800 training acc: 0.71875
Global Iter: 416900 training loss: 0.691519
Global Iter: 416900 training acc: 0.5625
Global Iter: 417000 training loss: 0.696739
Global Iter: 417000 training acc: 0.5
Global Iter: 417100 training loss: 0.687868
Global Iter: 417100 training acc: 0.5625
Global Iter: 417200 training loss: 0.691606
Global Iter: 417200 training acc: 0.53125
Global Iter: 417300 training loss: 0.702704
Global Iter: 417300 training acc: 0.46875
Global Iter: 417400 training loss: 0.678849
Global Iter: 417400 training acc: 0.625
Global Iter: 417500 training loss: 0.700831
Global Iter: 417500 training acc: 0.46875
Global Iter: 417600 training loss: 0.699867
Global Iter: 417600 training acc: 0.46875
Global Iter: 417700 training loss: 0.664916
Global Iter: 417700 training acc: 0.75
Global Iter: 417800 training loss: 0.684109
Global Iter: 417800 training acc: 0.625
Global Iter: 417900 training loss: 0.70508
Global Iter: 417900 training acc: 0.40625
Global Iter: 418000 training loss: 0.684285
Global Iter: 418000 training acc: 0.59375
Global Iter: 418100 training loss: 0.681023
Global Iter: 418100 training acc: 0.625
Global Iter: 418200 training loss: 0.732095
Global Iter: 418200 training acc: 0.21875
Global Iter: 418300 training loss: 0.703748
Global Iter: 418300 training acc: 0.40625
Global Iter: 418400 training loss: 0.690372
Global Iter: 418400 training acc: 0.5625
Global Iter: 418500 training loss: 0.681334
Global Iter: 418500 training acc: 0.59375
Global Iter: 418600 training loss: 0.672186
Global Iter: 418600 training acc: 0.6875
Global Iter: 418700 training loss: 0.697328
Global Iter: 418700 training acc: 0.46875
Global Iter: 418800 training loss: 0.668772
Global Iter: 418800 training acc: 0.71875
Global Iter: 418900 training loss: 0.690521
Global Iter: 418900 training acc: 0.53125
Global Iter: 419000 training loss: 0.700211
Global Iter: 419000 training acc: 0.46875
Global Iter: 419100 training loss: 0.685255
Global Iter: 419100 training acc: 0.59375
Global Iter: 419200 training loss: 0.705829
Global Iter: 419200 training acc: 0.4375
Global Iter: 419300 training loss: 0.710545
Global Iter: 419300 training acc: 0.40625
Global Iter: 419400 training loss: 0.67821
Global Iter: 419400 training acc: 0.65625
Global Iter: 419500 training loss: 0.692204
Global Iter: 419500 training acc: 0.53125
Global Iter: 419600 training loss: 0.720282
Global Iter: 419600 training acc: 0.3125
Global Iter: 419700 training loss: 0.694138
Global Iter: 419700 training acc: 0.53125
Global Iter: 419800 training loss: 0.68772
Global Iter: 419800 training acc: 0.5625
Global Iter: 419900 training loss: 0.711478
Global Iter: 419900 training acc: 0.4375
Global Iter: 420000 training loss: 0.670258
Global Iter: 420000 training acc: 0.6875
Global Iter: 420100 training loss: 0.701486
Global Iter: 420100 training acc: 0.4375
Global Iter: 420200 training loss: 0.691146
Global Iter: 420200 training acc: 0.53125
Global Iter: 420300 training loss: 0.684205
Global Iter: 420300 training acc: 0.59375
Global Iter: 420400 training loss: 0.678906
Global Iter: 420400 training acc: 0.625
Global Iter: 420500 training loss: 0.687256
Global Iter: 420500 training acc: 0.5625
Global Iter: 420600 training loss: 0.674564
Global Iter: 420600 training acc: 0.65625
Global Iter: 420700 training loss: 0.667723
Global Iter: 420700 training acc: 0.75
Global Iter: 420800 training loss: 0.70332
Global Iter: 420800 training acc: 0.4375
Global Iter: 420900 training loss: 0.704479
Global Iter: 420900 training acc: 0.46875
Global Iter: 421000 training loss: 0.69323
Global Iter: 421000 training acc: 0.5
Global Iter: 421100 training loss: 0.73575
Global Iter: 421100 training acc: 0.25
Global Iter: 421200 training loss: 0.69648
Global Iter: 421200 training acc: 0.5
Global Iter: 421300 training loss: 0.693981
Global Iter: 421300 training acc: 0.46875
Global Iter: 421400 training loss: 0.69343
Global Iter: 421400 training acc: 0.53125
Global Iter: 421500 training loss: 0.689365
Global Iter: 421500 training acc: 0.5
Global Iter: 421600 training loss: 0.676935
Global Iter: 421600 training acc: 0.625
Global Iter: 421700 training loss: 0.683154
Global Iter: 421700 training acc: 0.59375
Global Iter: 421800 training loss: 0.704605
Global Iter: 421800 training acc: 0.4375
Global Iter: 421900 training loss: 0.679409
Global Iter: 421900 training acc: 0.59375
Global Iter: 422000 training loss: 0.696566
Global Iter: 422000 training acc: 0.46875
Global Iter: 422100 training loss: 0.684449
Global Iter: 422100 training acc: 0.5625
Global Iter: 422200 training loss: 0.710086
Global Iter: 422200 training acc: 0.40625
Global Iter: 422300 training loss: 0.681924
Global Iter: 422300 training acc: 0.59375
Global Iter: 422400 training loss: 0.691487
Global Iter: 422400 training acc: 0.5
Global Iter: 422500 training loss: 0.698835
Global Iter: 422500 training acc: 0.4375
Global Iter: 422600 training loss: 0.716211
Global Iter: 422600 training acc: 0.375
Global Iter: 422700 training loss: 0.69783
Global Iter: 422700 training acc: 0.46875
Global Iter: 422800 training loss: 0.674582
Global Iter: 422800 training acc: 0.65625
Global Iter: 422900 training loss: 0.689399
Global Iter: 422900 training acc: 0.5625
Global Iter: 423000 training loss: 0.697138
Global Iter: 423000 training acc: 0.46875
Global Iter: 423100 training loss: 0.690784
Global Iter: 423100 training acc: 0.5625
Global Iter: 423200 training loss: 0.706009
Global Iter: 423200 training acc: 0.4375
Global Iter: 423300 training loss: 0.685287
Global Iter: 423300 training acc: 0.59375
Global Iter: 423400 training loss: 0.69696
Global Iter: 423400 training acc: 0.5
Global Iter: 423500 training loss: 0.685703
Global Iter: 423500 training acc: 0.59375
Global Iter: 423600 training loss: 0.661571
Global Iter: 423600 training acc: 0.78125
Global Iter: 423700 training loss: 0.695084
Global Iter: 423700 training acc: 0.5
Global Iter: 423800 training loss: 0.683103
Global Iter: 423800 training acc: 0.59375
Global Iter: 423900 training loss: 0.683745
Global Iter: 423900 training acc: 0.59375
Global Iter: 424000 training loss: 0.715483
Global Iter: 424000 training acc: 0.34375
Global Iter: 424100 training loss: 0.699663
Global Iter: 424100 training acc: 0.5
Global Iter: 424200 training loss: 0.703389
Global Iter: 424200 training acc: 0.5
Global Iter: 424300 training loss: 0.693548
Global Iter: 424300 training acc: 0.5
Global Iter: 424400 training loss: 0.697042
Global Iter: 424400 training acc: 0.5
Global Iter: 424500 training loss: 0.678727
Global Iter: 424500 training acc: 0.59375
Global Iter: 424600 training loss: 0.69214
Global Iter: 424600 training acc: 0.5
Global Iter: 424700 training loss: 0.677092
Global Iter: 424700 training acc: 0.625
Global Iter: 424800 training loss: 0.68672
Global Iter: 424800 training acc: 0.59375
Global Iter: 424900 training loss: 0.688776
Global Iter: 424900 training acc: 0.53125
Global Iter: 425000 training loss: 0.678012
Global Iter: 425000 training acc: 0.625
Global Iter: 425100 training loss: 0.706203
Global Iter: 425100 training acc: 0.4375
Global Iter: 425200 training loss: 0.707901
Global Iter: 425200 training acc: 0.375
Global Iter: 425300 training loss: 0.702298
Global Iter: 425300 training acc: 0.4375
Global Iter: 425400 training loss: 0.692102
Global Iter: 425400 training acc: 0.53125
Global Iter: 425500 training loss: 0.709665
Global Iter: 425500 training acc: 0.40625
Global Iter: 425600 training loss: 0.67764
Global Iter: 425600 training acc: 0.625
Global Iter: 425700 training loss: 0.689659
Global Iter: 425700 training acc: 0.53125
Global Iter: 425800 training loss: 0.703293
Global Iter: 425800 training acc: 0.46875
Global Iter: 425900 training loss: 0.682604
Global Iter: 425900 training acc: 0.59375
Global Iter: 426000 training loss: 0.689362
Global Iter: 426000 training acc: 0.5625
Global Iter: 426100 training loss: 0.695093
Global Iter: 426100 training acc: 0.5
Global Iter: 426200 training loss: 0.693293
Global Iter: 426200 training acc: 0.53125
Global Iter: 426300 training loss: 0.686944
Global Iter: 426300 training acc: 0.5625
Global Iter: 426400 training loss: 0.703912
Global Iter: 426400 training acc: 0.46875
Global Iter: 426500 training loss: 0.676649
Global Iter: 426500 training acc: 0.65625
Global Iter: 426600 training loss: 0.708884
Global Iter: 426600 training acc: 0.375
Global Iter: 426700 training loss: 0.668795
Global Iter: 426700 training acc: 0.6875
Global Iter: 426800 training loss: 0.681905
Global Iter: 426800 training acc: 0.625
Global Iter: 426900 training loss: 0.695427
Global Iter: 426900 training acc: 0.5
Global Iter: 427000 training loss: 0.688386
Global Iter: 427000 training acc: 0.5625
Global Iter: 427100 training loss: 0.690252
Global Iter: 427100 training acc: 0.5625
Global Iter: 427200 training loss: 0.69954
Global Iter: 427200 training acc: 0.46875
Global Iter: 427300 training loss: 0.6857
Global Iter: 427300 training acc: 0.625
Global Iter: 427400 training loss: 0.708033
Global Iter: 427400 training acc: 0.40625
Global Iter: 427500 training loss: 0.709387
Global Iter: 427500 training acc: 0.40625
Global Iter: 427600 training loss: 0.672281
Global Iter: 427600 training acc: 0.6875
Global Iter: 427700 training loss: 0.683334
Global Iter: 427700 training acc: 0.59375
Global Iter: 427800 training loss: 0.694525
Global Iter: 427800 training acc: 0.46875
Global Iter: 427900 training loss: 0.689755
Global Iter: 427900 training acc: 0.5625
Global Iter: 428000 training loss: 0.683514
Global Iter: 428000 training acc: 0.59375
Global Iter: 428100 training loss: 0.727611
Global Iter: 428100 training acc: 0.25
Global Iter: 428200 training loss: 0.705213
Global Iter: 428200 training acc: 0.4375
Global Iter: 428300 training loss: 0.683986
Global Iter: 428300 training acc: 0.59375
Global Iter: 428400 training loss: 0.691927
Global Iter: 428400 training acc: 0.53125
Global Iter: 428500 training loss: 0.680319
Global Iter: 428500 training acc: 0.65625
Global Iter: 428600 training loss: 0.709733
Global Iter: 428600 training acc: 0.40625
Global Iter: 428700 training loss: 0.66888
Global Iter: 428700 training acc: 0.6875
Global Iter: 428800 training loss: 0.685183
Global Iter: 428800 training acc: 0.53125
Global Iter: 428900 training loss: 0.695835
Global Iter: 428900 training acc: 0.46875
Global Iter: 429000 training loss: 0.688589
Global Iter: 429000 training acc: 0.59375
Global Iter: 429100 training loss: 0.702987
Global Iter: 429100 training acc: 0.4375
Global Iter: 429200 training loss: 0.703449
Global Iter: 429200 training acc: 0.4375
Global Iter: 429300 training loss: 0.675517
Global Iter: 429300 training acc: 0.65625
Global Iter: 429400 training loss: 0.690331
Global Iter: 429400 training acc: 0.5625
Global Iter: 429500 training loss: 0.721811
Global Iter: 429500 training acc: 0.3125
Global Iter: 429600 training loss: 0.684363
Global Iter: 429600 training acc: 0.5625
Global Iter: 429700 training loss: 0.68583
Global Iter: 429700 training acc: 0.5625
Global Iter: 429800 training loss: 0.697447
Global Iter: 429800 training acc: 0.5
Global Iter: 429900 training loss: 0.673329
Global Iter: 429900 training acc: 0.625
Global Iter: 430000 training loss: 0.703635
Global Iter: 430000 training acc: 0.4375
Global Iter: 430100 training loss: 0.696357
Global Iter: 430100 training acc: 0.53125
Global Iter: 430200 training loss: 0.690249
Global Iter: 430200 training acc: 0.5625
Global Iter: 430300 training loss: 0.680094
Global Iter: 430300 training acc: 0.625
Global Iter: 430400 training loss: 0.687223
Global Iter: 430400 training acc: 0.5625
Global Iter: 430500 training loss: 0.677895
Global Iter: 430500 training acc: 0.65625
Global Iter: 430600 training loss: 0.661887
Global Iter: 430600 training acc: 0.78125
Global Iter: 430700 training loss: 0.70542
Global Iter: 430700 training acc: 0.40625
Global Iter: 430800 training loss: 0.696909
Global Iter: 430800 training acc: 0.46875
Global Iter: 430900 training loss: 0.698951
Global Iter: 430900 training acc: 0.46875
Global Iter: 431000 training loss: 0.729762
Global Iter: 431000 training acc: 0.25
Global Iter: 431100 training loss: 0.692222
Global Iter: 431100 training acc: 0.5
Global Iter: 431200 training loss: 0.6935
Global Iter: 431200 training acc: 0.53125
Global Iter: 431300 training loss: 0.692849
Global Iter: 431300 training acc: 0.53125
Global Iter: 431400 training loss: 0.687343
Global Iter: 431400 training acc: 0.5625
Global Iter: 431500 training loss: 0.67668
Global Iter: 431500 training acc: 0.625
Global Iter: 431600 training loss: 0.686276
Global Iter: 431600 training acc: 0.5625
Global Iter: 431700 training loss: 0.695209
Global Iter: 431700 training acc: 0.5
Global Iter: 431800 training loss: 0.683061
Global Iter: 431800 training acc: 0.59375
Global Iter: 431900 training loss: 0.684489
Global Iter: 431900 training acc: 0.53125
Global Iter: 432000 training loss: 0.697416
Global Iter: 432000 training acc: 0.5
Global Iter: 432100 training loss: 0.703323
Global Iter: 432100 training acc: 0.4375
Global Iter: 432200 training loss: 0.685066
Global Iter: 432200 training acc: 0.59375
Global Iter: 432300 training loss: 0.695087
Global Iter: 432300 training acc: 0.5
Global Iter: 432400 training loss: 0.718527
Global Iter: 432400 training acc: 0.375
Global Iter: 432500 training loss: 0.71094
Global Iter: 432500 training acc: 0.375
Global Iter: 432600 training loss: 0.697856
Global Iter: 432600 training acc: 0.5
Global Iter: 432700 training loss: 0.679528
Global Iter: 432700 training acc: 0.625
Global Iter: 432800 training loss: 0.692242
Global Iter: 432800 training acc: 0.5625
Global Iter: 432900 training loss: 0.705827
Global Iter: 432900 training acc: 0.4375
Global Iter: 433000 training loss: 0.684176
Global Iter: 433000 training acc: 0.59375
Global Iter: 433100 training loss: 0.694128
Global Iter: 433100 training acc: 0.5
Global Iter: 433200 training loss: 0.687868
Global Iter: 433200 training acc: 0.5625
Global Iter: 433300 training loss: 0.695501
Global Iter: 433300 training acc: 0.5
Global Iter: 433400 training loss: 0.678556
Global Iter: 433400 training acc: 0.59375
Global Iter: 433500 training loss: 0.665685
Global Iter: 433500 training acc: 0.75
Global Iter: 433600 training loss: 0.693176
Global Iter: 433600 training acc: 0.5
Global Iter: 433700 training loss: 0.685307
Global Iter: 433700 training acc: 0.5625
Global Iter: 433800 training loss: 0.689122
Global Iter: 433800 training acc: 0.5625
Global Iter: 433900 training loss: 0.712236
Global Iter: 433900 training acc: 0.375
Global Iter: 434000 training loss: 0.696437
Global Iter: 434000 training acc: 0.5
Global Iter: 434100 training loss: 0.699376
Global Iter: 434100 training acc: 0.46875
Global Iter: 434200 training loss: 0.695556
Global Iter: 434200 training acc: 0.5
Global Iter: 434300 training loss: 0.693321
Global Iter: 434300 training acc: 0.5
Global Iter: 434400 training loss: 0.676772
Global Iter: 434400 training acc: 0.625
Global Iter: 434500 training loss: 0.689696
Global Iter: 434500 training acc: 0.5625
Global Iter: 434600 training loss: 0.677694
Global Iter: 434600 training acc: 0.625
Global Iter: 434700 training loss: 0.675955
Global Iter: 434700 training acc: 0.625
Global Iter: 434800 training loss: 0.688385
Global Iter: 434800 training acc: 0.5625
Global Iter: 434900 training loss: 0.679992
Global Iter: 434900 training acc: 0.59375
Global Iter: 435000 training loss: 0.70564
Global Iter: 435000 training acc: 0.4375
Global Iter: 435100 training loss: 0.70653
Global Iter: 435100 training acc: 0.4375
Global Iter: 435200 training loss: 0.697434
Global Iter: 435200 training acc: 0.46875
Global Iter: 435300 training loss: 0.692666
Global Iter: 435300 training acc: 0.53125
Global Iter: 435400 training loss: 0.712224
Global Iter: 435400 training acc: 0.375
Global Iter: 435500 training loss: 0.677446
Global Iter: 435500 training acc: 0.625
Global Iter: 435600 training loss: 0.694604
Global Iter: 435600 training acc: 0.53125
Global Iter: 435700 training loss: 0.697584
Global Iter: 435700 training acc: 0.4375
Global Iter: 435800 training loss: 0.682717
Global Iter: 435800 training acc: 0.59375
Global Iter: 435900 training loss: 0.687675
Global Iter: 435900 training acc: 0.5625
Global Iter: 436000 training loss: 0.690887
Global Iter: 436000 training acc: 0.53125
Global Iter: 436100 training loss: 0.694168
Global Iter: 436100 training acc: 0.5
Global Iter: 436200 training loss: 0.689732
Global Iter: 436200 training acc: 0.53125
Global Iter: 436300 training loss: 0.703059
Global Iter: 436300 training acc: 0.4375
Global Iter: 436400 training loss: 0.682275
Global Iter: 436400 training acc: 0.625
Global Iter: 436500 training loss: 0.707907
Global Iter: 436500 training acc: 0.40625
Global Iter: 436600 training loss: 0.673284
Global Iter: 436600 training acc: 0.6875
Global Iter: 436700 training loss: 0.679812
Global Iter: 436700 training acc: 0.65625
Global Iter: 436800 training loss: 0.692915
Global Iter: 436800 training acc: 0.53125
Global Iter: 436900 training loss: 0.694483
Global Iter: 436900 training acc: 0.53125
Global Iter: 437000 training loss: 0.694793
Global Iter: 437000 training acc: 0.53125
Global Iter: 437100 training loss: 0.701157
Global Iter: 437100 training acc: 0.46875
Global Iter: 437200 training loss: 0.68618
Global Iter: 437200 training acc: 0.5625
Global Iter: 437300 training loss: 0.701026
Global Iter: 437300 training acc: 0.4375
Global Iter: 437400 training loss: 0.70586
Global Iter: 437400 training acc: 0.40625
Global Iter: 437500 training loss: 0.668362
Global Iter: 437500 training acc: 0.75
Global Iter: 437600 training loss: 0.686319
Global Iter: 437600 training acc: 0.5625
Global Iter: 437700 training loss: 0.692608
Global Iter: 437700 training acc: 0.46875
Global Iter: 437800 training loss: 0.689474
Global Iter: 437800 training acc: 0.53125
Global Iter: 437900 training loss: 0.687747
Global Iter: 437900 training acc: 0.5625
Global Iter: 438000 training loss: 0.719091
Global Iter: 438000 training acc: 0.28125
Global Iter: 438100 training loss: 0.706384
Global Iter: 438100 training acc: 0.4375
Global Iter: 438200 training loss: 0.677971
Global Iter: 438200 training acc: 0.625
Global Iter: 438300 training loss: 0.691606
Global Iter: 438300 training acc: 0.5
Global Iter: 438400 training loss: 0.667256
Global Iter: 438400 training acc: 0.71875
Global Iter: 438500 training loss: 0.713877
Global Iter: 438500 training acc: 0.375
Global Iter: 438600 training loss: 0.670243
Global Iter: 438600 training acc: 0.71875
Global Iter: 438700 training loss: 0.680309
Global Iter: 438700 training acc: 0.59375
Global Iter: 438800 training loss: 0.70328
Global Iter: 438800 training acc: 0.4375
Global Iter: 438900 training loss: 0.680809
Global Iter: 438900 training acc: 0.625
Global Iter: 439000 training loss: 0.70623
Global Iter: 439000 training acc: 0.4375
Global Iter: 439100 training loss: 0.701368
Global Iter: 439100 training acc: 0.4375
Global Iter: 439200 training loss: 0.672865
Global Iter: 439200 training acc: 0.65625
Global Iter: 439300 training loss: 0.694885
Global Iter: 439300 training acc: 0.53125
Global Iter: 439400 training loss: 0.72225
Global Iter: 439400 training acc: 0.28125
Global Iter: 439500 training loss: 0.685127
Global Iter: 439500 training acc: 0.5625
Global Iter: 439600 training loss: 0.696622
Global Iter: 439600 training acc: 0.5
Global Iter: 439700 training loss: 0.683472
Global Iter: 439700 training acc: 0.5625
Global Iter: 439800 training loss: 0.67554
Global Iter: 439800 training acc: 0.65625
Global Iter: 439900 training loss: 0.69514
Global Iter: 439900 training acc: 0.5
Global Iter: 440000 training loss: 0.695752
Global Iter: 440000 training acc: 0.53125
Global Iter: 440100 training loss: 0.693108
Global Iter: 440100 training acc: 0.53125
Global Iter: 440200 training loss: 0.691488
Global Iter: 440200 training acc: 0.5625
Global Iter: 440300 training loss: 0.68423
Global Iter: 440300 training acc: 0.59375
Global Iter: 440400 training loss: 0.668228
Global Iter: 440400 training acc: 0.71875
Global Iter: 440500 training loss: 0.659544
Global Iter: 440500 training acc: 0.78125
Global Iter: 440600 training loss: 0.708081
Global Iter: 440600 training acc: 0.40625
Global Iter: 440700 training loss: 0.705775
Global Iter: 440700 training acc: 0.46875
Global Iter: 440800 training loss: 0.698563
Global Iter: 440800 training acc: 0.5
Global Iter: 440900 training loss: 0.736262
Global Iter: 440900 training acc: 0.21875
Global Iter: 441000 training loss: 0.691547
Global Iter: 441000 training acc: 0.5
Global Iter: 441100 training loss: 0.694603
Global Iter: 441100 training acc: 0.5
Global Iter: 441200 training loss: 0.688633
Global Iter: 441200 training acc: 0.53125
Global Iter: 441300 training loss: 0.685535
Global Iter: 441300 training acc: 0.5625
Global Iter: 441400 training loss: 0.686276
Global Iter: 441400 training acc: 0.5625
Global Iter: 441500 training loss: 0.682905
Global Iter: 441500 training acc: 0.59375
Global Iter: 441600 training loss: 0.696086
Global Iter: 441600 training acc: 0.4375
Global Iter: 441700 training loss: 0.68202
Global Iter: 441700 training acc: 0.59375
Global Iter: 441800 training loss: 0.693412
Global Iter: 441800 training acc: 0.5
Global Iter: 441900 training loss: 0.687218
Global Iter: 441900 training acc: 0.5625
Global Iter: 442000 training loss: 0.707947
Global Iter: 442000 training acc: 0.4375
Global Iter: 442100 training loss: 0.683975
Global Iter: 442100 training acc: 0.59375
Global Iter: 442200 training loss: 0.691679
Global Iter: 442200 training acc: 0.5
Global Iter: 442300 training loss: 0.707222
Global Iter: 442300 training acc: 0.4375
Global Iter: 442400 training loss: 0.704543
Global Iter: 442400 training acc: 0.40625
Global Iter: 442500 training loss: 0.692303
Global Iter: 442500 training acc: 0.53125
Global Iter: 442600 training loss: 0.677889
Global Iter: 442600 training acc: 0.625
Global Iter: 442700 training loss: 0.681403
Global Iter: 442700 training acc: 0.59375
Global Iter: 442800 training loss: 0.703671
Global Iter: 442800 training acc: 0.4375
Global Iter: 442900 training loss: 0.687033
Global Iter: 442900 training acc: 0.5625
Global Iter: 443000 training loss: 0.696151
Global Iter: 443000 training acc: 0.5
Global Iter: 443100 training loss: 0.674729
Global Iter: 443100 training acc: 0.625
Global Iter: 443200 training loss: 0.700938
Global Iter: 443200 training acc: 0.46875
Global Iter: 443300 training loss: 0.687526
Global Iter: 443300 training acc: 0.59375
Global Iter: 443400 training loss: 0.671455
Global Iter: 443400 training acc: 0.75
Global Iter: 443500 training loss: 0.691252
Global Iter: 443500 training acc: 0.53125
Global Iter: 443600 training loss: 0.68551
Global Iter: 443600 training acc: 0.5625
Global Iter: 443700 training loss: 0.694024
Global Iter: 443700 training acc: 0.53125
Global Iter: 443800 training loss: 0.720154
Global Iter: 443800 training acc: 0.34375
Global Iter: 443900 training loss: 0.693313
Global Iter: 443900 training acc: 0.5
Global Iter: 444000 training loss: 0.697411
Global Iter: 444000 training acc: 0.46875
Global Iter: 444100 training loss: 0.698041
Global Iter: 444100 training acc: 0.5
Global Iter: 444200 training loss: 0.69613
Global Iter: 444200 training acc: 0.46875
Global Iter: 444300 training loss: 0.675466
Global Iter: 444300 training acc: 0.625
Global Iter: 444400 training loss: 0.698351
Global Iter: 444400 training acc: 0.5
Global Iter: 444500 training loss: 0.683047
Global Iter: 444500 training acc: 0.59375
Global Iter: 444600 training loss: 0.682533
Global Iter: 444600 training acc: 0.59375
Global Iter: 444700 training loss: 0.688772
Global Iter: 444700 training acc: 0.53125
Global Iter: 444800 training loss: 0.683474
Global Iter: 444800 training acc: 0.59375
Global Iter: 444900 training loss: 0.701603
Global Iter: 444900 training acc: 0.46875
Global Iter: 445000 training loss: 0.700181
Global Iter: 445000 training acc: 0.46875
Global Iter: 445100 training loss: 0.704542
Global Iter: 445100 training acc: 0.4375
Global Iter: 445200 training loss: 0.696068
Global Iter: 445200 training acc: 0.5
Global Iter: 445300 training loss: 0.718614
Global Iter: 445300 training acc: 0.34375
Global Iter: 445400 training loss: 0.682946
Global Iter: 445400 training acc: 0.625
Global Iter: 445500 training loss: 0.681028
Global Iter: 445500 training acc: 0.59375
Global Iter: 445600 training loss: 0.695377
Global Iter: 445600 training acc: 0.4375
Global Iter: 445700 training loss: 0.681404
Global Iter: 445700 training acc: 0.59375
Global Iter: 445800 training loss: 0.686609
Global Iter: 445800 training acc: 0.5625
Global Iter: 445900 training loss: 0.694476
Global Iter: 445900 training acc: 0.5
Global Iter: 446000 training loss: 0.697394
Global Iter: 446000 training acc: 0.5
Global Iter: 446100 training loss: 0.691251
Global Iter: 446100 training acc: 0.5625
Global Iter: 446200 training loss: 0.703356
Global Iter: 446200 training acc: 0.4375
Global Iter: 446300 training loss: 0.687677
Global Iter: 446300 training acc: 0.59375
Global Iter: 446400 training loss: 0.705418
Global Iter: 446400 training acc: 0.4375
Global Iter: 446500 training loss: 0.67389
Global Iter: 446500 training acc: 0.6875
Global Iter: 446600 training loss: 0.675271
Global Iter: 446600 training acc: 0.65625
Global Iter: 446700 training loss: 0.692732
Global Iter: 446700 training acc: 0.5
Global Iter: 446800 training loss: 0.695559
Global Iter: 446800 training acc: 0.5
Global Iter: 446900 training loss: 0.691042
Global Iter: 446900 training acc: 0.53125
Global Iter: 447000 training loss: 0.689874
Global Iter: 447000 training acc: 0.53125
Global Iter: 447100 training loss: 0.68894
Global Iter: 447100 training acc: 0.5625
Global Iter: 447200 training loss: 0.70776
Global Iter: 447200 training acc: 0.40625
Global Iter: 447300 training loss: 0.704955
Global Iter: 447300 training acc: 0.4375
Global Iter: 447400 training loss: 0.664937
Global Iter: 447400 training acc: 0.75
Global Iter: 447500 training loss: 0.691693
Global Iter: 447500 training acc: 0.53125
Global Iter: 447600 training loss: 0.694196
Global Iter: 447600 training acc: 0.5
Global Iter: 447700 training loss: 0.68229
Global Iter: 447700 training acc: 0.59375
Global Iter: 447800 training loss: 0.6838
Global Iter: 447800 training acc: 0.5625
Global Iter: 447900 training loss: 0.72368
Global Iter: 447900 training acc: 0.28125
Global Iter: 448000 training loss: 0.709608
Global Iter: 448000 training acc: 0.40625
Global Iter: 448100 training loss: 0.679008
Global Iter: 448100 training acc: 0.625
Global Iter: 448200 training loss: 0.701073
Global Iter: 448200 training acc: 0.46875
Global Iter: 448300 training loss: 0.674456
Global Iter: 448300 training acc: 0.71875
Global Iter: 448400 training loss: 0.715555
Global Iter: 448400 training acc: 0.375
Global Iter: 448500 training loss: 0.677006
Global Iter: 448500 training acc: 0.65625
Global Iter: 448600 training loss: 0.679527
Global Iter: 448600 training acc: 0.59375
Global Iter: 448700 training loss: 0.693159
Global Iter: 448700 training acc: 0.5
Global Iter: 448800 training loss: 0.684231
Global Iter: 448800 training acc: 0.625
Global Iter: 448900 training loss: 0.700027
Global Iter: 448900 training acc: 0.46875
Global Iter: 449000 training loss: 0.702546
Global Iter: 449000 training acc: 0.46875
Global Iter: 449100 training loss: 0.678263
Global Iter: 449100 training acc: 0.65625
Global Iter: 449200 training loss: 0.698861
Global Iter: 449200 training acc: 0.5
Global Iter: 449300 training loss: 0.711438
Global Iter: 449300 training acc: 0.34375
Global Iter: 449400 training loss: 0.682086
Global Iter: 449400 training acc: 0.625
Global Iter: 449500 training loss: 0.690093
Global Iter: 449500 training acc: 0.53125
Global Iter: 449600 training loss: 0.697034
Global Iter: 449600 training acc: 0.53125
Global Iter: 449700 training loss: 0.680885
Global Iter: 449700 training acc: 0.625
Global Iter: 449800 training loss: 0.687666
Global Iter: 449800 training acc: 0.53125
Global Iter: 449900 training loss: 0.685876
Global Iter: 449900 training acc: 0.5625
Global Iter: 450000 training loss: 0.689443
Global Iter: 450000 training acc: 0.5625
Global Iter: 450100 training loss: 0.692037
Global Iter: 450100 training acc: 0.53125
Global Iter: 450200 training loss: 0.684801
Global Iter: 450200 training acc: 0.5625
Global Iter: 450300 training loss: 0.670706
Global Iter: 450300 training acc: 0.71875
Global Iter: 450400 training loss: 0.664541
Global Iter: 450400 training acc: 0.75
Global Iter: 450500 training loss: 0.709291
Global Iter: 450500 training acc: 0.375
Global Iter: 450600 training loss: 0.707289
Global Iter: 450600 training acc: 0.4375
Global Iter: 450700 training loss: 0.691042
Global Iter: 450700 training acc: 0.5625
Global Iter: 450800 training loss: 0.733157
Global Iter: 450800 training acc: 0.21875
Global Iter: 450900 training loss: 0.69857
Global Iter: 450900 training acc: 0.46875
Global Iter: 451000 training loss: 0.690684
Global Iter: 451000 training acc: 0.5
Global Iter: 451100 training loss: 0.692782
Global Iter: 451100 training acc: 0.53125
Global Iter: 451200 training loss: 0.684809
Global Iter: 451200 training acc: 0.59375
Global Iter: 451300 training loss: 0.682588
Global Iter: 451300 training acc: 0.59375
Global Iter: 451400 training loss: 0.677418
Global Iter: 451400 training acc: 0.65625
Global Iter: 451500 training loss: 0.699828
Global Iter: 451500 training acc: 0.4375
Global Iter: 451600 training loss: 0.681755
Global Iter: 451600 training acc: 0.5625
Global Iter: 451700 training loss: 0.69203
Global Iter: 451700 training acc: 0.5625
Global Iter: 451800 training loss: 0.692012
Global Iter: 451800 training acc: 0.53125
Global Iter: 451900 training loss: 0.700239
Global Iter: 451900 training acc: 0.46875
Global Iter: 452000 training loss: 0.685082
Global Iter: 452000 training acc: 0.5625
Global Iter: 452100 training loss: 0.692327
Global Iter: 452100 training acc: 0.5
Global Iter: 452200 training loss: 0.705756
Global Iter: 452200 training acc: 0.40625
Global Iter: 452300 training loss: 0.701389
Global Iter: 452300 training acc: 0.4375
Global Iter: 452400 training loss: 0.689664
Global Iter: 452400 training acc: 0.5
Global Iter: 452500 training loss: 0.682451
Global Iter: 452500 training acc: 0.59375
Global Iter: 452600 training loss: 0.681306
Global Iter: 452600 training acc: 0.59375
Global Iter: 452700 training loss: 0.704325
Global Iter: 452700 training acc: 0.4375
Global Iter: 452800 training loss: 0.688605
Global Iter: 452800 training acc: 0.5625
Global Iter: 452900 training loss: 0.68956
Global Iter: 452900 training acc: 0.53125
Global Iter: 453000 training loss: 0.683783
Global Iter: 453000 training acc: 0.5625
Global Iter: 453100 training loss: 0.694065
Global Iter: 453100 training acc: 0.5
Global Iter: 453200 training loss: 0.683835
Global Iter: 453200 training acc: 0.625
Global Iter: 453300 training loss: 0.665442
Global Iter: 453300 training acc: 0.75
Global Iter: 453400 training loss: 0.684401
Global Iter: 453400 training acc: 0.5625
Global Iter: 453500 training loss: 0.690343
Global Iter: 453500 training acc: 0.53125
Global Iter: 453600 training loss: 0.686589
Global Iter: 453600 training acc: 0.5625
Global Iter: 453700 training loss: 0.72112
Global Iter: 453700 training acc: 0.34375
Global Iter: 453800 training loss: 0.698878
Global Iter: 453800 training acc: 0.5
Global Iter: 453900 training loss: 0.708387
Global Iter: 453900 training acc: 0.4375
Global Iter: 454000 training loss: 0.695957
Global Iter: 454000 training acc: 0.5
Global Iter: 454100 training loss: 0.709285
Global Iter: 454100 training acc: 0.40625
Global Iter: 454200 training loss: 0.676824
Global Iter: 454200 training acc: 0.625
Global Iter: 454300 training loss: 0.698375
Global Iter: 454300 training acc: 0.5
Global Iter: 454400 training loss: 0.690328
Global Iter: 454400 training acc: 0.53125
Global Iter: 454500 training loss: 0.684581
Global Iter: 454500 training acc: 0.59375
Global Iter: 454600 training loss: 0.6949
Global Iter: 454600 training acc: 0.5
Global Iter: 454700 training loss: 0.682624
Global Iter: 454700 training acc: 0.59375
Global Iter: 454800 training loss: 0.692689
Global Iter: 454800 training acc: 0.5
Global Iter: 454900 training loss: 0.696792
Global Iter: 454900 training acc: 0.5
Global Iter: 455000 training loss: 0.697475
Global Iter: 455000 training acc: 0.46875
Global Iter: 455100 training loss: 0.697197
Global Iter: 455100 training acc: 0.46875
Global Iter: 455200 training loss: 0.719752
Global Iter: 455200 training acc: 0.34375
Global Iter: 455300 training loss: 0.684813
Global Iter: 455300 training acc: 0.59375
Global Iter: 455400 training loss: 0.681563
Global Iter: 455400 training acc: 0.59375
Global Iter: 455500 training loss: 0.704543
Global Iter: 455500 training acc: 0.40625
Global Iter: 455600 training loss: 0.676342
Global Iter: 455600 training acc: 0.625
Global Iter: 455700 training loss: 0.683203
Global Iter: 455700 training acc: 0.5625
Global Iter: 455800 training loss: 0.693104
Global Iter: 455800 training acc: 0.53125
Global Iter: 455900 training loss: 0.686814
Global Iter: 455900 training acc: 0.5625
Global Iter: 456000 training loss: 0.690216
Global Iter: 456000 training acc: 0.5625
Global Iter: 456100 training loss: 0.702639
Global Iter: 456100 training acc: 0.4375
Global Iter: 456200 training loss: 0.680082
Global Iter: 456200 training acc: 0.625
Global Iter: 456300 training loss: 0.704532
Global Iter: 456300 training acc: 0.4375
Global Iter: 456400 training loss: 0.669619
Global Iter: 456400 training acc: 0.6875
Global Iter: 456500 training loss: 0.676327
Global Iter: 456500 training acc: 0.65625
Global Iter: 456600 training loss: 0.698802
Global Iter: 456600 training acc: 0.5
Global Iter: 456700 training loss: 0.691157
Global Iter: 456700 training acc: 0.5
Global Iter: 456800 training loss: 0.683285
Global Iter: 456800 training acc: 0.59375
Global Iter: 456900 training loss: 0.69976
Global Iter: 456900 training acc: 0.5
Global Iter: 457000 training loss: 0.691238
Global Iter: 457000 training acc: 0.53125
Global Iter: 457100 training loss: 0.705914
Global Iter: 457100 training acc: 0.40625
Global Iter: 457200 training loss: 0.70324
Global Iter: 457200 training acc: 0.4375
Global Iter: 457300 training loss: 0.662535
Global Iter: 457300 training acc: 0.78125
Global Iter: 457400 training loss: 0.692396
Global Iter: 457400 training acc: 0.5
Global Iter: 457500 training loss: 0.690505
Global Iter: 457500 training acc: 0.53125
Global Iter: 457600 training loss: 0.677308
Global Iter: 457600 training acc: 0.59375
Global Iter: 457700 training loss: 0.684463
Global Iter: 457700 training acc: 0.5625
Global Iter: 457800 training loss: 0.717311
Global Iter: 457800 training acc: 0.3125
Global Iter: 457900 training loss: 0.705693
Global Iter: 457900 training acc: 0.375
Global Iter: 458000 training loss: 0.673562
Global Iter: 458000 training acc: 0.65625
Global Iter: 458100 training loss: 0.696859
Global Iter: 458100 training acc: 0.46875
Global Iter: 458200 training loss: 0.676061
Global Iter: 458200 training acc: 0.71875
Global Iter: 458300 training loss: 0.711392
Global Iter: 458300 training acc: 0.375
Global Iter: 458400 training loss: 0.685216
Global Iter: 458400 training acc: 0.59375
Global Iter: 458500 training loss: 0.688107
Global Iter: 458500 training acc: 0.5625
Global Iter: 458600 training loss: 0.685521
Global Iter: 458600 training acc: 0.5625
Global Iter: 458700 training loss: 0.68855
Global Iter: 458700 training acc: 0.5625
Global Iter: 458800 training loss: 0.699193
Global Iter: 458800 training acc: 0.5
Global Iter: 458900 training loss: 0.692955
Global Iter: 458900 training acc: 0.53125
Global Iter: 459000 training loss: 0.688098
Global Iter: 459000 training acc: 0.59375
Global Iter: 459100 training loss: 0.695219
Global Iter: 459100 training acc: 0.5
Global Iter: 459200 training loss: 0.713019
Global Iter: 459200 training acc: 0.34375
Global Iter: 459300 training loss: 0.676202
Global Iter: 459300 training acc: 0.65625
Global Iter: 459400 training loss: 0.686068
Global Iter: 459400 training acc: 0.5625
Global Iter: 459500 training loss: 0.695174
Global Iter: 459500 training acc: 0.53125
Global Iter: 459600 training loss: 0.686224
Global Iter: 459600 training acc: 0.59375
Global Iter: 459700 training loss: 0.687809
Global Iter: 459700 training acc: 0.5625
Global Iter: 459800 training loss: 0.693878
Global Iter: 459800 training acc: 0.5
Global Iter: 459900 training loss: 0.68144
Global Iter: 459900 training acc: 0.625
Global Iter: 460000 training loss: 0.695764
Global Iter: 460000 training acc: 0.5
Global Iter: 460100 training loss: 0.689386
Global Iter: 460100 training acc: 0.53125
Global Iter: 460200 training loss: 0.66721
Global Iter: 460200 training acc: 0.71875
Global Iter: 460300 training loss: 0.673304
Global Iter: 460300 training acc: 0.6875
Global Iter: 460400 training loss: 0.709466
Global Iter: 460400 training acc: 0.375
Global Iter: 460500 training loss: 0.696927
Global Iter: 460500 training acc: 0.5
Global Iter: 460600 training loss: 0.679542
Global Iter: 460600 training acc: 0.59375
Global Iter: 460700 training loss: 0.734845
Global Iter: 460700 training acc: 0.21875
Global Iter: 460800 training loss: 0.697409
Global Iter: 460800 training acc: 0.46875
Global Iter: 460900 training loss: 0.695567
Global Iter: 460900 training acc: 0.53125
Global Iter: 461000 training loss: 0.702635
Global Iter: 461000 training acc: 0.46875
Global Iter: 461100 training loss: 0.682461
Global Iter: 461100 training acc: 0.625
Global Iter: 461200 training loss: 0.68297
Global Iter: 461200 training acc: 0.59375
Global Iter: 461300 training loss: 0.676725
Global Iter: 461300 training acc: 0.65625
Global Iter: 461400 training loss: 0.700336
Global Iter: 461400 training acc: 0.4375
Global Iter: 461500 training loss: 0.692668
Global Iter: 461500 training acc: 0.53125
Global Iter: 461600 training loss: 0.684739
Global Iter: 461600 training acc: 0.5625
Global Iter: 461700 training loss: 0.693178
Global Iter: 461700 training acc: 0.53125
Global Iter: 461800 training loss: 0.691539
Global Iter: 461800 training acc: 0.53125
Global Iter: 461900 training loss: 0.684301
Global Iter: 461900 training acc: 0.625
Global Iter: 462000 training loss: 0.682585
Global Iter: 462000 training acc: 0.5625
Global Iter: 462100 training loss: 0.710612
Global Iter: 462100 training acc: 0.40625
Global Iter: 462200 training loss: 0.70333
Global Iter: 462200 training acc: 0.4375
Global Iter: 462300 training loss: 0.699116
Global Iter: 462300 training acc: 0.5
Global Iter: 462400 training loss: 0.683072
Global Iter: 462400 training acc: 0.5625
Global Iter: 462500 training loss: 0.680214
Global Iter: 462500 training acc: 0.625
Global Iter: 462600 training loss: 0.697138
Global Iter: 462600 training acc: 0.46875
Global Iter: 462700 training loss: 0.683917
Global Iter: 462700 training acc: 0.5625
Global Iter: 462800 training loss: 0.689368
Global Iter: 462800 training acc: 0.5625
Global Iter: 462900 training loss: 0.677198
Global Iter: 462900 training acc: 0.625
Global Iter: 463000 training loss: 0.689402
Global Iter: 463000 training acc: 0.53125
Global Iter: 463100 training loss: 0.6826
Global Iter: 463100 training acc: 0.59375
Global Iter: 463200 training loss: 0.656414
Global Iter: 463200 training acc: 0.78125
Global Iter: 463300 training loss: 0.689343
Global Iter: 463300 training acc: 0.53125
Global Iter: 463400 training loss: 0.686488
Global Iter: 463400 training acc: 0.5625
Global Iter: 463500 training loss: 0.690148
Global Iter: 463500 training acc: 0.5625
Global Iter: 463600 training loss: 0.718945
Global Iter: 463600 training acc: 0.34375
Global Iter: 463700 training loss: 0.697764
Global Iter: 463700 training acc: 0.5
Global Iter: 463800 training loss: 0.702562
Global Iter: 463800 training acc: 0.4375
Global Iter: 463900 training loss: 0.70264
Global Iter: 463900 training acc: 0.46875
Global Iter: 464000 training loss: 0.702432
Global Iter: 464000 training acc: 0.40625
Global Iter: 464100 training loss: 0.683015
Global Iter: 464100 training acc: 0.59375
Global Iter: 464200 training loss: 0.698174
Global Iter: 464200 training acc: 0.5
Global Iter: 464300 training loss: 0.689587
Global Iter: 464300 training acc: 0.53125
Global Iter: 464400 training loss: 0.681868
Global Iter: 464400 training acc: 0.59375
Global Iter: 464500 training loss: 0.693368
Global Iter: 464500 training acc: 0.53125
Global Iter: 464600 training loss: 0.681118
Global Iter: 464600 training acc: 0.59375
Global Iter: 464700 training loss: 0.690319
Global Iter: 464700 training acc: 0.53125
Global Iter: 464800 training loss: 0.69461
Global Iter: 464800 training acc: 0.5
Global Iter: 464900 training loss: 0.693936
Global Iter: 464900 training acc: 0.5
Global Iter: 465000 training loss: 0.690623
Global Iter: 465000 training acc: 0.53125
Global Iter: 465100 training loss: 0.715784
Global Iter: 465100 training acc: 0.375
Global Iter: 465200 training loss: 0.689772
Global Iter: 465200 training acc: 0.5625
Global Iter: 465300 training loss: 0.670866
Global Iter: 465300 training acc: 0.65625
Global Iter: 465400 training loss: 0.708892
Global Iter: 465400 training acc: 0.40625
Global Iter: 465500 training loss: 0.687444
Global Iter: 465500 training acc: 0.5625
Global Iter: 465600 training loss: 0.696224
Global Iter: 465600 training acc: 0.53125
Global Iter: 465700 training loss: 0.698665
Global Iter: 465700 training acc: 0.5
Global Iter: 465800 training loss: 0.690901
Global Iter: 465800 training acc: 0.5625
Global Iter: 465900 training loss: 0.690148
Global Iter: 465900 training acc: 0.5625
Global Iter: 466000 training loss: 0.701841
Global Iter: 466000 training acc: 0.4375
Global Iter: 466100 training loss: 0.680524
Global Iter: 466100 training acc: 0.65625
Global Iter: 466200 training loss: 0.696684
Global Iter: 466200 training acc: 0.46875
Global Iter: 466300 training loss: 0.667553
Global Iter: 466300 training acc: 0.6875
Global Iter: 466400 training loss: 0.680339
Global Iter: 466400 training acc: 0.625
Global Iter: 466500 training loss: 0.697857
Global Iter: 466500 training acc: 0.46875
Global Iter: 466600 training loss: 0.690046
Global Iter: 466600 training acc: 0.5
Global Iter: 466700 training loss: 0.678765
Global Iter: 466700 training acc: 0.625
Global Iter: 466800 training loss: 0.694476
Global Iter: 466800 training acc: 0.5
Global Iter: 466900 training loss: 0.69372
Global Iter: 466900 training acc: 0.53125
Global Iter: 467000 training loss: 0.701223
Global Iter: 467000 training acc: 0.4375
Global Iter: 467100 training loss: 0.696039
Global Iter: 467100 training acc: 0.46875
Global Iter: 467200 training loss: 0.660506
Global Iter: 467200 training acc: 0.8125
Global Iter: 467300 training loss: 0.694302
Global Iter: 467300 training acc: 0.53125
Global Iter: 467400 training loss: 0.688829
Global Iter: 467400 training acc: 0.5625
Global Iter: 467500 training loss: 0.683532
Global Iter: 467500 training acc: 0.59375
Global Iter: 467600 training loss: 0.679176
Global Iter: 467600 training acc: 0.59375
Global Iter: 467700 training loss: 0.718832
Global Iter: 467700 training acc: 0.28125
Global Iter: 467800 training loss: 0.708959
Global Iter: 467800 training acc: 0.40625
Global Iter: 467900 training loss: 0.68042
Global Iter: 467900 training acc: 0.59375
Global Iter: 468000 training loss: 0.700834
Global Iter: 468000 training acc: 0.46875
Global Iter: 468100 training loss: 0.672374
Global Iter: 468100 training acc: 0.71875
Global Iter: 468200 training loss: 0.720728
Global Iter: 468200 training acc: 0.34375
Global Iter: 468300 training loss: 0.685265
Global Iter: 468300 training acc: 0.59375
Global Iter: 468400 training loss: 0.682417
Global Iter: 468400 training acc: 0.59375
Global Iter: 468500 training loss: 0.681251
Global Iter: 468500 training acc: 0.59375
Global Iter: 468600 training loss: 0.687536
Global Iter: 468600 training acc: 0.59375
Global Iter: 468700 training loss: 0.695574
Global Iter: 468700 training acc: 0.46875
Global Iter: 468800 training loss: 0.691079
Global Iter: 468800 training acc: 0.53125
Global Iter: 468900 training loss: 0.684828
Global Iter: 468900 training acc: 0.5625
Global Iter: 469000 training loss: 0.692211
Global Iter: 469000 training acc: 0.53125
Global Iter: 469100 training loss: 0.71365
Global Iter: 469100 training acc: 0.3125
Global Iter: 469200 training loss: 0.67308
Global Iter: 469200 training acc: 0.65625
Global Iter: 469300 training loss: 0.688123
Global Iter: 469300 training acc: 0.5625
Global Iter: 469400 training loss: 0.696706
Global Iter: 469400 training acc: 0.5
Global Iter: 469500 training loss: 0.686092
Global Iter: 469500 training acc: 0.5625
Global Iter: 469600 training loss: 0.685655
Global Iter: 469600 training acc: 0.5625
Global Iter: 469700 training loss: 0.690378
Global Iter: 469700 training acc: 0.53125
Global Iter: 469800 training loss: 0.683918
Global Iter: 469800 training acc: 0.625
Global Iter: 469900 training loss: 0.687652
Global Iter: 469900 training acc: 0.5625
Global Iter: 470000 training loss: 0.69524
Global Iter: 470000 training acc: 0.5
Global Iter: 470100 training loss: 0.667995
Global Iter: 470100 training acc: 0.75
Global Iter: 470200 training loss: 0.679674
Global Iter: 470200 training acc: 0.65625
Global Iter: 470300 training loss: 0.701894
Global Iter: 470300 training acc: 0.4375
Global Iter: 470400 training loss: 0.698655
Global Iter: 470400 training acc: 0.46875
Global Iter: 470500 training loss: 0.689158
Global Iter: 470500 training acc: 0.53125
Global Iter: 470600 training loss: 0.736572
Global Iter: 470600 training acc: 0.21875
Global Iter: 470700 training loss: 0.693821
Global Iter: 470700 training acc: 0.5
Global Iter: 470800 training loss: 0.687432
Global Iter: 470800 training acc: 0.53125
Global Iter: 470900 training loss: 0.697917
Global Iter: 470900 training acc: 0.5
Global Iter: 471000 training loss: 0.685602
Global Iter: 471000 training acc: 0.59375
Global Iter: 471100 training loss: 0.690075
Global Iter: 471100 training acc: 0.5625
Global Iter: 471200 training loss: 0.681517
Global Iter: 471200 training acc: 0.625
Global Iter: 471300 training loss: 0.695816
Global Iter: 471300 training acc: 0.5
Global Iter: 471400 training loss: 0.68991
Global Iter: 471400 training acc: 0.53125
Global Iter: 471500 training loss: 0.69124
Global Iter: 471500 training acc: 0.53125
Global Iter: 471600 training loss: 0.700675
Global Iter: 471600 training acc: 0.5
Global Iter: 471700 training loss: 0.69198
Global Iter: 471700 training acc: 0.53125
Global Iter: 471800 training loss: 0.677793
Global Iter: 471800 training acc: 0.65625
Global Iter: 471900 training loss: 0.692136
Global Iter: 471900 training acc: 0.53125
Global Iter: 472000 training loss: 0.716369
Global Iter: 472000 training acc: 0.34375
Global Iter: 472100 training loss: 0.697007
Global Iter: 472100 training acc: 0.46875
Global Iter: 472200 training loss: 0.695648
Global Iter: 472200 training acc: 0.5
Global Iter: 472300 training loss: 0.689992
Global Iter: 472300 training acc: 0.53125
Global Iter: 472400 training loss: 0.675641
Global Iter: 472400 training acc: 0.65625
Global Iter: 472500 training loss: 0.707574
Global Iter: 472500 training acc: 0.4375
Global Iter: 472600 training loss: 0.684286
Global Iter: 472600 training acc: 0.5625
Global Iter: 472700 training loss: 0.68806
Global Iter: 472700 training acc: 0.5625
Global Iter: 472800 training loss: 0.673645
Global Iter: 472800 training acc: 0.65625
Global Iter: 472900 training loss: 0.685383
Global Iter: 472900 training acc: 0.5625
Global Iter: 473000 training loss: 0.67803
Global Iter: 473000 training acc: 0.65625
Global Iter: 473100 training loss: 0.666707
Global Iter: 473100 training acc: 0.75
Global Iter: 473200 training loss: 0.697748
Global Iter: 473200 training acc: 0.5
Global Iter: 473300 training loss: 0.690205
Global Iter: 473300 training acc: 0.5625
Global Iter: 473400 training loss: 0.694531
Global Iter: 473400 training acc: 0.53125
Global Iter: 473500 training loss: 0.726783
Global Iter: 473500 training acc: 0.28125
Global Iter: 473600 training loss: 0.698974
Global Iter: 473600 training acc: 0.46875
Global Iter: 473700 training loss: 0.702904
Global Iter: 473700 training acc: 0.4375
Global Iter: 473800 training loss: 0.696826
Global Iter: 473800 training acc: 0.5
Global Iter: 473900 training loss: 0.703862
Global Iter: 473900 training acc: 0.40625
Global Iter: 474000 training loss: 0.674096
Global Iter: 474000 training acc: 0.65625
Global Iter: 474100 training loss: 0.691559
Global Iter: 474100 training acc: 0.5625
Global Iter: 474200 training loss: 0.687067
Global Iter: 474200 training acc: 0.53125
Global Iter: 474300 training loss: 0.678618
Global Iter: 474300 training acc: 0.59375
Global Iter: 474400 training loss: 0.698176
Global Iter: 474400 training acc: 0.5
Global Iter: 474500 training loss: 0.688835
Global Iter: 474500 training acc: 0.53125
Global Iter: 474600 training loss: 0.696682
Global Iter: 474600 training acc: 0.5
Global Iter: 474700 training loss: 0.695385
Global Iter: 474700 training acc: 0.5
Global Iter: 474800 training loss: 0.691913
Global Iter: 474800 training acc: 0.53125
Global Iter: 474900 training loss: 0.692894
Global Iter: 474900 training acc: 0.53125
Global Iter: 475000 training loss: 0.716428
Global Iter: 475000 training acc: 0.375
Global Iter: 475100 training loss: 0.688289
Global Iter: 475100 training acc: 0.5625
Global Iter: 475200 training loss: 0.679223
Global Iter: 475200 training acc: 0.625
Global Iter: 475300 training loss: 0.704129
Global Iter: 475300 training acc: 0.4375
Global Iter: 475400 training loss: 0.689101
Global Iter: 475400 training acc: 0.5625
Global Iter: 475500 training loss: 0.690058
Global Iter: 475500 training acc: 0.5625
Global Iter: 475600 training loss: 0.689121
Global Iter: 475600 training acc: 0.53125
Global Iter: 475700 training loss: 0.684057
Global Iter: 475700 training acc: 0.5625
Global Iter: 475800 training loss: 0.689135
Global Iter: 475800 training acc: 0.5625
Global Iter: 475900 training loss: 0.700585
Global Iter: 475900 training acc: 0.46875
Global Iter: 476000 training loss: 0.679825
Global Iter: 476000 training acc: 0.65625
Global Iter: 476100 training loss: 0.692956
Global Iter: 476100 training acc: 0.5
Global Iter: 476200 training loss: 0.676898
Global Iter: 476200 training acc: 0.65625
Global Iter: 476300 training loss: 0.678344
Global Iter: 476300 training acc: 0.625
Global Iter: 476400 training loss: 0.709124
Global Iter: 476400 training acc: 0.40625
Global Iter: 476500 training loss: 0.696992
Global Iter: 476500 training acc: 0.5
Global Iter: 476600 training loss: 0.681633
Global Iter: 476600 training acc: 0.59375
Global Iter: 476700 training loss: 0.692051
Global Iter: 476700 training acc: 0.53125
Global Iter: 476800 training loss: 0.698361
Global Iter: 476800 training acc: 0.5
Global Iter: 476900 training loss: 0.702213
Global Iter: 476900 training acc: 0.46875
Global Iter: 477000 training loss: 0.704043
Global Iter: 477000 training acc: 0.4375
Global Iter: 477100 training loss: 0.66292
Global Iter: 477100 training acc: 0.78125
Global Iter: 477200 training loss: 0.694092
Global Iter: 477200 training acc: 0.5
Global Iter: 477300 training loss: 0.694624
Global Iter: 477300 training acc: 0.53125
Global Iter: 477400 training loss: 0.677897
Global Iter: 477400 training acc: 0.625
Global Iter: 477500 training loss: 0.688793
Global Iter: 477500 training acc: 0.5625
Global Iter: 477600 training loss: 0.716551
Global Iter: 477600 training acc: 0.3125
Global Iter: 477700 training loss: 0.704322
Global Iter: 477700 training acc: 0.40625
Global Iter: 477800 training loss: 0.677253
Global Iter: 477800 training acc: 0.625
Global Iter: 477900 training loss: 0.705185
Global Iter: 477900 training acc: 0.4375
Global Iter: 478000 training loss: 0.674679
Global Iter: 478000 training acc: 0.6875
Global Iter: 478100 training loss: 0.711924
Global Iter: 478100 training acc: 0.375
Global Iter: 478200 training loss: 0.683889
Global Iter: 478200 training acc: 0.59375
Global Iter: 478300 training loss: 0.684063
Global Iter: 478300 training acc: 0.59375
Global Iter: 478400 training loss: 0.686818
Global Iter: 478400 training acc: 0.5625
Global Iter: 478500 training loss: 0.686971
Global Iter: 478500 training acc: 0.5625
Global Iter: 478600 training loss: 0.697468
Global Iter: 478600 training acc: 0.5
Global Iter: 478700 training loss: 0.684487
Global Iter: 478700 training acc: 0.5625
Global Iter: 478800 training loss: 0.693287
Global Iter: 478800 training acc: 0.53125
Global Iter: 478900 training loss: 0.68737
Global Iter: 478900 training acc: 0.5625
Global Iter: 479000 training loss: 0.714443
Global Iter: 479000 training acc: 0.34375
Global Iter: 479100 training loss: 0.666842
Global Iter: 479100 training acc: 0.71875
Global Iter: 479200 training loss: 0.680395
Global Iter: 479200 training acc: 0.59375
Global Iter: 479300 training loss: 0.701045
Global Iter: 479300 training acc: 0.46875
Global Iter: 479400 training loss: 0.680003
Global Iter: 479400 training acc: 0.59375
Global Iter: 479500 training loss: 0.68379
Global Iter: 479500 training acc: 0.59375
Global Iter: 479600 training loss: 0.703241
Global Iter: 479600 training acc: 0.46875
Global Iter: 479700 training loss: 0.675579
Global Iter: 479700 training acc: 0.65625
Global Iter: 479800 training loss: 0.688092
Global Iter: 479800 training acc: 0.53125
Global Iter: 479900 training loss: 0.702636
Global Iter: 479900 training acc: 0.46875
Global Iter: 480000 training loss: 0.664963
Global Iter: 480000 training acc: 0.75
Global Iter: 480100 training loss: 0.677406
Global Iter: 480100 training acc: 0.625
Global Iter: 480200 training loss: 0.701409
Global Iter: 480200 training acc: 0.40625
Global Iter: 480300 training loss: 0.697111
Global Iter: 480300 training acc: 0.5
Global Iter: 480400 training loss: 0.687863
Global Iter: 480400 training acc: 0.5625
Global Iter: 480500 training loss: 0.727801
Global Iter: 480500 training acc: 0.21875
Global Iter: 480600 training loss: 0.704648
Global Iter: 480600 training acc: 0.4375
Global Iter: 480700 training loss: 0.685357
Global Iter: 480700 training acc: 0.5625
Global Iter: 480800 training loss: 0.694647
Global Iter: 480800 training acc: 0.53125
Global Iter: 480900 training loss: 0.681134
Global Iter: 480900 training acc: 0.625
Global Iter: 481000 training loss: 0.692126
Global Iter: 481000 training acc: 0.53125
Global Iter: 481100 training loss: 0.678833
Global Iter: 481100 training acc: 0.65625
Global Iter: 481200 training loss: 0.697472
Global Iter: 481200 training acc: 0.46875
Global Iter: 481300 training loss: 0.696226
Global Iter: 481300 training acc: 0.5
Global Iter: 481400 training loss: 0.689655
Global Iter: 481400 training acc: 0.53125
Global Iter: 481500 training loss: 0.694324
Global Iter: 481500 training acc: 0.5
Global Iter: 481600 training loss: 0.688872
Global Iter: 481600 training acc: 0.5625
Global Iter: 481700 training loss: 0.669713
Global Iter: 481700 training acc: 0.71875
Global Iter: 481800 training loss: 0.691019
Global Iter: 481800 training acc: 0.53125
Global Iter: 481900 training loss: 0.71019
Global Iter: 481900 training acc: 0.375
Global Iter: 482000 training loss: 0.698035
Global Iter: 482000 training acc: 0.5
Global Iter: 482100 training loss: 0.695152
Global Iter: 482100 training acc: 0.5
Global Iter: 482200 training loss: 0.701147
Global Iter: 482200 training acc: 0.5
Global Iter: 482300 training loss: 0.671889
Global Iter: 482300 training acc: 0.71875
Global Iter: 482400 training loss: 0.699111
Global Iter: 482400 training acc: 0.46875
Global Iter: 482500 training loss: 0.68269
Global Iter: 482500 training acc: 0.59375
Global Iter: 482600 training loss: 0.682821
Global Iter: 482600 training acc: 0.625
Global Iter: 482700 training loss: 0.68001
Global Iter: 482700 training acc: 0.625
Global Iter: 482800 training loss: 0.692009
Global Iter: 482800 training acc: 0.53125
Global Iter: 482900 training loss: 0.677784
Global Iter: 482900 training acc: 0.65625
Global Iter: 483000 training loss: 0.666105
Global Iter: 483000 training acc: 0.75
Global Iter: 483100 training loss: 0.69
Global Iter: 483100 training acc: 0.53125
Global Iter: 483200 training loss: 0.688847
Global Iter: 483200 training acc: 0.5625
Global Iter: 483300 training loss: 0.687678
Global Iter: 483300 training acc: 0.53125
Global Iter: 483400 training loss: 0.729414
Global Iter: 483400 training acc: 0.25
Global Iter: 483500 training loss: 0.700226
Global Iter: 483500 training acc: 0.46875
Global Iter: 483600 training loss: 0.701111
Global Iter: 483600 training acc: 0.4375
Global Iter: 483700 training loss: 0.693246
Global Iter: 483700 training acc: 0.53125
Global Iter: 483800 training loss: 0.707632
Global Iter: 483800 training acc: 0.375
Global Iter: 483900 training loss: 0.664451
Global Iter: 483900 training acc: 0.71875
Global Iter: 484000 training loss: 0.690822
Global Iter: 484000 training acc: 0.5625
Global Iter: 484100 training loss: 0.696993
Global Iter: 484100 training acc: 0.5
Global Iter: 484200 training loss: 0.676312
Global Iter: 484200 training acc: 0.625
Global Iter: 484300 training loss: 0.701458
Global Iter: 484300 training acc: 0.46875
Global Iter: 484400 training loss: 0.687686
Global Iter: 484400 training acc: 0.5625
Global Iter: 484500 training loss: 0.688089
Global Iter: 484500 training acc: 0.53125
Global Iter: 484600 training loss: 0.69683
Global Iter: 484600 training acc: 0.5
Global Iter: 484700 training loss: 0.694859
Global Iter: 484700 training acc: 0.5
Global Iter: 484800 training loss: 0.693685
Global Iter: 484800 training acc: 0.53125
Global Iter: 484900 training loss: 0.714924
Global Iter: 484900 training acc: 0.375
Global Iter: 485000 training loss: 0.680995
Global Iter: 485000 training acc: 0.625
Global Iter: 485100 training loss: 0.680287
Global Iter: 485100 training acc: 0.625
Global Iter: 485200 training loss: 0.702768
Global Iter: 485200 training acc: 0.4375
Global Iter: 485300 training loss: 0.690756
Global Iter: 485300 training acc: 0.53125
Global Iter: 485400 training loss: 0.687375
Global Iter: 485400 training acc: 0.5625
Global Iter: 485500 training loss: 0.687782
Global Iter: 485500 training acc: 0.5625
Global Iter: 485600 training loss: 0.681109
Global Iter: 485600 training acc: 0.625
Global Iter: 485700 training loss: 0.694549
Global Iter: 485700 training acc: 0.53125
Global Iter: 485800 training loss: 0.700598
Global Iter: 485800 training acc: 0.4375
Global Iter: 485900 training loss: 0.682309
Global Iter: 485900 training acc: 0.625
Global Iter: 486000 training loss: 0.693892
Global Iter: 486000 training acc: 0.5
Global Iter: 486100 training loss: 0.667209
Global Iter: 486100 training acc: 0.6875
Global Iter: 486200 training loss: 0.68323
Global Iter: 486200 training acc: 0.625
Global Iter: 486300 training loss: 0.711333
Global Iter: 486300 training acc: 0.40625
Global Iter: 486400 training loss: 0.692927
Global Iter: 486400 training acc: 0.53125
Global Iter: 486500 training loss: 0.680182
Global Iter: 486500 training acc: 0.59375
Global Iter: 486600 training loss: 0.689888
Global Iter: 486600 training acc: 0.53125
Global Iter: 486700 training loss: 0.69175
Global Iter: 486700 training acc: 0.53125
Global Iter: 486800 training loss: 0.700352
Global Iter: 486800 training acc: 0.5
Global Iter: 486900 training loss: 0.706323
Global Iter: 486900 training acc: 0.4375
Global Iter: 487000 training loss: 0.659349
Global Iter: 487000 training acc: 0.78125
Global Iter: 487100 training loss: 0.695482
Global Iter: 487100 training acc: 0.46875
Global Iter: 487200 training loss: 0.689761
Global Iter: 487200 training acc: 0.5625
Global Iter: 487300 training loss: 0.681088
Global Iter: 487300 training acc: 0.59375
Global Iter: 487400 training loss: 0.686662
Global Iter: 487400 training acc: 0.5625
Global Iter: 487500 training loss: 0.710595
Global Iter: 487500 training acc: 0.34375
Global Iter: 487600 training loss: 0.706282
Global Iter: 487600 training acc: 0.40625
Global Iter: 487700 training loss: 0.69015
Global Iter: 487700 training acc: 0.5625
Global Iter: 487800 training loss: 0.707166
Global Iter: 487800 training acc: 0.40625
Global Iter: 487900 training loss: 0.678812
Global Iter: 487900 training acc: 0.65625
Global Iter: 488000 training loss: 0.709604
Global Iter: 488000 training acc: 0.40625
Global Iter: 488100 training loss: 0.680432
Global Iter: 488100 training acc: 0.625
Global Iter: 488200 training loss: 0.687783
Global Iter: 488200 training acc: 0.5625
Global Iter: 488300 training loss: 0.685958
Global Iter: 488300 training acc: 0.5625
Global Iter: 488400 training loss: 0.686832
Global Iter: 488400 training acc: 0.5625
Global Iter: 488500 training loss: 0.691832
Global Iter: 488500 training acc: 0.5
Global Iter: 488600 training loss: 0.688178
Global Iter: 488600 training acc: 0.5625
Global Iter: 488700 training loss: 0.683505
Global Iter: 488700 training acc: 0.5625
Global Iter: 488800 training loss: 0.689776
Global Iter: 488800 training acc: 0.5625
Global Iter: 488900 training loss: 0.714028
Global Iter: 488900 training acc: 0.34375
Global Iter: 489000 training loss: 0.67057
Global Iter: 489000 training acc: 0.6875
Global Iter: 489100 training loss: 0.683993
Global Iter: 489100 training acc: 0.59375
Global Iter: 489200 training loss: 0.701216
Global Iter: 489200 training acc: 0.46875
Global Iter: 489300 training loss: 0.684269
Global Iter: 489300 training acc: 0.59375
Global Iter: 489400 training loss: 0.687262
Global Iter: 489400 training acc: 0.59375
Global Iter: 489500 training loss: 0.709344
Global Iter: 489500 training acc: 0.40625
Global Iter: 489600 training loss: 0.681866
Global Iter: 489600 training acc: 0.625
Global Iter: 489700 training loss: 0.69017
Global Iter: 489700 training acc: 0.53125
Global Iter: 489800 training loss: 0.691351
Global Iter: 489800 training acc: 0.5
Global Iter: 489900 training loss: 0.664867
Global Iter: 489900 training acc: 0.75
Global Iter: 490000 training loss: 0.677127
Global Iter: 490000 training acc: 0.65625
Global Iter: 490100 training loss: 0.701715
Global Iter: 490100 training acc: 0.4375
Global Iter: 490200 training loss: 0.694358
Global Iter: 490200 training acc: 0.5
Global Iter: 490300 training loss: 0.682457
Global Iter: 490300 training acc: 0.59375
Global Iter: 490400 training loss: 0.732634
Global Iter: 490400 training acc: 0.21875
Global Iter: 490500 training loss: 0.700378
Global Iter: 490500 training acc: 0.46875
Global Iter: 490600 training loss: 0.685655
Global Iter: 490600 training acc: 0.5625
Global Iter: 490700 training loss: 0.679715
Global Iter: 490700 training acc: 0.59375
Global Iter: 490800 training loss: 0.684089
Global Iter: 490800 training acc: 0.59375
Global Iter: 490900 training loss: 0.702417
Global Iter: 490900 training acc: 0.46875
Global Iter: 491000 training loss: 0.676912
Global Iter: 491000 training acc: 0.625
Global Iter: 491100 training loss: 0.706607
Global Iter: 491100 training acc: 0.4375
Global Iter: 491200 training loss: 0.699208
Global Iter: 491200 training acc: 0.46875
Global Iter: 491300 training loss: 0.690131
Global Iter: 491300 training acc: 0.5625
Global Iter: 491400 training loss: 0.695246
Global Iter: 491400 training acc: 0.5
Global Iter: 491500 training loss: 0.683748
Global Iter: 491500 training acc: 0.5625
Global Iter: 491600 training loss: 0.667985
Global Iter: 491600 training acc: 0.71875
Global Iter: 491700 training loss: 0.690088
Global Iter: 491700 training acc: 0.5625
Global Iter: 491800 training loss: 0.717151
Global Iter: 491800 training acc: 0.34375
Global Iter: 491900 training loss: 0.693692
Global Iter: 491900 training acc: 0.5
Global Iter: 492000 training loss: 0.692246
Global Iter: 492000 training acc: 0.5
Global Iter: 492100 training loss: 0.694269
Global Iter: 492100 training acc: 0.5
Global Iter: 492200 training loss: 0.672346
Global Iter: 492200 training acc: 0.71875
Global Iter: 492300 training loss: 0.695948
Global Iter: 492300 training acc: 0.46875
Global Iter: 492400 training loss: 0.679202
Global Iter: 492400 training acc: 0.59375
Global Iter: 492500 training loss: 0.682848
Global Iter: 492500 training acc: 0.59375
Global Iter: 492600 training loss: 0.6727
Global Iter: 492600 training acc: 0.6875
Global Iter: 492700 training loss: 0.688992
Global Iter: 492700 training acc: 0.53125
Global Iter: 492800 training loss: 0.67723
Global Iter: 492800 training acc: 0.65625
Global Iter: 492900 training loss: 0.667453
Global Iter: 492900 training acc: 0.75
Global Iter: 493000 training loss: 0.699613
Global Iter: 493000 training acc: 0.46875
Global Iter: 493100 training loss: 0.696856
Global Iter: 493100 training acc: 0.5
Global Iter: 493200 training loss: 0.685894
Global Iter: 493200 training acc: 0.53125
Global Iter: 493300 training loss: 0.73478
Global Iter: 493300 training acc: 0.21875
Global Iter: 493400 training loss: 0.692693
Global Iter: 493400 training acc: 0.5
Global Iter: 493500 training loss: 0.703453
Global Iter: 493500 training acc: 0.4375
Global Iter: 493600 training loss: 0.693458
Global Iter: 493600 training acc: 0.5
Global Iter: 493700 training loss: 0.700546
Global Iter: 493700 training acc: 0.4375
Global Iter: 493800 training loss: 0.667253
Global Iter: 493800 training acc: 0.6875
Global Iter: 493900 training loss: 0.686251
Global Iter: 493900 training acc: 0.59375
Global Iter: 494000 training loss: 0.7055
Global Iter: 494000 training acc: 0.4375
Global Iter: 494100 training loss: 0.685893
Global Iter: 494100 training acc: 0.59375
Global Iter: 494200 training loss: 0.697771
Global Iter: 494200 training acc: 0.46875
Global Iter: 494300 training loss: 0.686541
Global Iter: 494300 training acc: 0.5625
Global Iter: 494400 training loss: 0.690516
Global Iter: 494400 training acc: 0.53125
Global Iter: 494500 training loss: 0.695386
Global Iter: 494500 training acc: 0.5
Global Iter: 494600 training loss: 0.694976
Global Iter: 494600 training acc: 0.5
Global Iter: 494700 training loss: 0.695804
Global Iter: 494700 training acc: 0.46875
Global Iter: 494800 training loss: 0.719899
Global Iter: 494800 training acc: 0.34375
Global Iter: 494900 training loss: 0.676748
Global Iter: 494900 training acc: 0.59375
Global Iter: 495000 training loss: 0.677941
Global Iter: 495000 training acc: 0.625
Global Iter: 495100 training loss: 0.696689
Global Iter: 495100 training acc: 0.46875
Global Iter: 495200 training loss: 0.701926
Global Iter: 495200 training acc: 0.46875
Global Iter: 495300 training loss: 0.683762
Global Iter: 495300 training acc: 0.59375
Global Iter: 495400 training loss: 0.695425
Global Iter: 495400 training acc: 0.5
Global Iter: 495500 training loss: 0.682866
Global Iter: 495500 training acc: 0.625
Global Iter: 495600 training loss: 0.691269
Global Iter: 495600 training acc: 0.53125
Global Iter: 495700 training loss: 0.690663
Global Iter: 495700 training acc: 0.46875
Global Iter: 495800 training loss: 0.669863
Global Iter: 495800 training acc: 0.6875
Global Iter: 495900 training loss: 0.689823
Global Iter: 495900 training acc: 0.53125
Global Iter: 496000 training loss: 0.66773
Global Iter: 496000 training acc: 0.71875
Global Iter: 496100 training loss: 0.679925
Global Iter: 496100 training acc: 0.625
Global Iter: 496200 training loss: 0.713233
Global Iter: 496200 training acc: 0.375
Global Iter: 496300 training loss: 0.698118
Global Iter: 496300 training acc: 0.5
Global Iter: 496400 training loss: 0.690204
Global Iter: 496400 training acc: 0.5625
Global Iter: 496500 training loss: 0.690886
Global Iter: 496500 training acc: 0.53125
Global Iter: 496600 training loss: 0.68954
Global Iter: 496600 training acc: 0.53125
Global Iter: 496700 training loss: 0.694664
Global Iter: 496700 training acc: 0.53125
Global Iter: 496800 training loss: 0.714114
Global Iter: 496800 training acc: 0.375
Global Iter: 496900 training loss: 0.665372
Global Iter: 496900 training acc: 0.75
Global Iter: 497000 training loss: 0.700103
Global Iter: 497000 training acc: 0.5
Global Iter: 497100 training loss: 0.693138
Global Iter: 497100 training acc: 0.53125
Global Iter: 497200 training loss: 0.681381
Global Iter: 497200 training acc: 0.59375
Global Iter: 497300 training loss: 0.694466
Global Iter: 497300 training acc: 0.5
Global Iter: 497400 training loss: 0.709735
Global Iter: 497400 training acc: 0.375
Global Iter: 497500 training loss: 0.707635
Global Iter: 497500 training acc: 0.40625
Global Iter: 497600 training loss: 0.684536
Global Iter: 497600 training acc: 0.5625
Global Iter: 497700 training loss: 0.712993
Global Iter: 497700 training acc: 0.375
Global Iter: 497800 training loss: 0.67954
Global Iter: 497800 training acc: 0.65625
Global Iter: 497900 training loss: 0.698758
Global Iter: 497900 training acc: 0.46875
Global Iter: 498000 training loss: 0.67799
Global Iter: 498000 training acc: 0.625
Global Iter: 498100 training loss: 0.678825
Global Iter: 498100 training acc: 0.625
Global Iter: 498200 training loss: 0.686211
Global Iter: 498200 training acc: 0.5625
Global Iter: 498300 training loss: 0.686815
Global Iter: 498300 training acc: 0.5625
Global Iter: 498400 training loss: 0.694737
Global Iter: 498400 training acc: 0.5
Global Iter: 498500 training loss: 0.684874
Global Iter: 498500 training acc: 0.5625
Global Iter: 498600 training loss: 0.684935
Global Iter: 498600 training acc: 0.5625
Global Iter: 498700 training loss: 0.683963
Global Iter: 498700 training acc: 0.59375
Global Iter: 498800 training loss: 0.717415
Global Iter: 498800 training acc: 0.3125
Global Iter: 498900 training loss: 0.665471
Global Iter: 498900 training acc: 0.71875
Global Iter: 499000 training loss: 0.690959
Global Iter: 499000 training acc: 0.53125
Global Iter: 499100 training loss: 0.70724
Global Iter: 499100 training acc: 0.4375
Global Iter: 499200 training loss: 0.688451
Global Iter: 499200 training acc: 0.5625
Global Iter: 499300 training loss: 0.681216
Global Iter: 499300 training acc: 0.625
Global Iter: 499400 training loss: 0.704045
Global Iter: 499400 training acc: 0.4375
Global Iter: 499500 training loss: 0.683843
Global Iter: 499500 training acc: 0.625
Global Iter: 499600 training loss: 0.700216
Global Iter: 499600 training acc: 0.46875
Global Iter: 499700 training loss: 0.689689
Global Iter: 499700 training acc: 0.53125
Global Iter: 499800 training loss: 0.665179
Global Iter: 499800 training acc: 0.75
Global Iter: 499900 training loss: 0.676455
Global Iter: 499900 training acc: 0.65625
Global Iter: 500000 training loss: 0.705166
Global Iter: 500000 training acc: 0.4375
Global Iter: 500100 training loss: 0.688714
Global Iter: 500100 training acc: 0.5625
Global Iter: 500200 training loss: 0.687467
Global Iter: 500200 training acc: 0.59375
Global Iter: 500300 training loss: 0.729819
Global Iter: 500300 training acc: 0.21875
Global Iter: 500400 training loss: 0.700483
Global Iter: 500400 training acc: 0.46875
Global Iter: 500500 training loss: 0.67591
Global Iter: 500500 training acc: 0.59375
Global Iter: 500600 training loss: 0.675626
Global Iter: 500600 training acc: 0.625
Global Iter: 500700 training loss: 0.67924
Global Iter: 500700 training acc: 0.65625
Global Iter: 500800 training loss: 0.699271
Global Iter: 500800 training acc: 0.46875
Global Iter: 500900 training loss: 0.67374
Global Iter: 500900 training acc: 0.6875
Global Iter: 501000 training loss: 0.69818
Global Iter: 501000 training acc: 0.4375
Global Iter: 501100 training loss: 0.696431
Global Iter: 501100 training acc: 0.5
Global Iter: 501200 training loss: 0.691447
Global Iter: 501200 training acc: 0.5625
Global Iter: 501300 training loss: 0.705363
Global Iter: 501300 training acc: 0.46875
Global Iter: 501400 training loss: 0.697152
Global Iter: 501400 training acc: 0.5
Global Iter: 501500 training loss: 0.678523
Global Iter: 501500 training acc: 0.65625
Global Iter: 501600 training loss: 0.68693
Global Iter: 501600 training acc: 0.5625
Global Iter: 501700 training loss: 0.710587
Global Iter: 501700 training acc: 0.375
Global Iter: 501800 training loss: 0.695394
Global Iter: 501800 training acc: 0.5
Global Iter: 501900 training loss: 0.690801
Global Iter: 501900 training acc: 0.53125
Global Iter: 502000 training loss: 0.701559
Global Iter: 502000 training acc: 0.4375
Global Iter: 502100 training loss: 0.666797
Global Iter: 502100 training acc: 0.71875
Global Iter: 502200 training loss: 0.707046
Global Iter: 502200 training acc: 0.4375
Global Iter: 502300 training loss: 0.679478
Global Iter: 502300 training acc: 0.625
Global Iter: 502400 training loss: 0.681848
Global Iter: 502400 training acc: 0.625
Global Iter: 502500 training loss: 0.666515
Global Iter: 502500 training acc: 0.71875
Global Iter: 502600 training loss: 0.691573
Global Iter: 502600 training acc: 0.5
Global Iter: 502700 training loss: 0.682575
Global Iter: 502700 training acc: 0.625
Global Iter: 502800 training loss: 0.658849
Global Iter: 502800 training acc: 0.78125
Global Iter: 502900 training loss: 0.702348
Global Iter: 502900 training acc: 0.4375
Global Iter: 503000 training loss: 0.695246
Global Iter: 503000 training acc: 0.5
Global Iter: 503100 training loss: 0.687338
Global Iter: 503100 training acc: 0.53125
Global Iter: 503200 training loss: 0.733267
Global Iter: 503200 training acc: 0.21875
Global Iter: 503300 training loss: 0.689384
Global Iter: 503300 training acc: 0.53125
Global Iter: 503400 training loss: 0.698191
Global Iter: 503400 training acc: 0.46875
Global Iter: 503500 training loss: 0.697643
Global Iter: 503500 training acc: 0.5
Global Iter: 503600 training loss: 0.700356
Global Iter: 503600 training acc: 0.4375
Global Iter: 503700 training loss: 0.672002
Global Iter: 503700 training acc: 0.65625
Global Iter: 503800 training loss: 0.684997
Global Iter: 503800 training acc: 0.59375
Global Iter: 503900 training loss: 0.700685
Global Iter: 503900 training acc: 0.46875
Global Iter: 504000 training loss: 0.67976
Global Iter: 504000 training acc: 0.59375
Global Iter: 504100 training loss: 0.693751
Global Iter: 504100 training acc: 0.5
Global Iter: 504200 training loss: 0.694901
Global Iter: 504200 training acc: 0.53125
Global Iter: 504300 training loss: 0.696146
Global Iter: 504300 training acc: 0.46875
Global Iter: 504400 training loss: 0.695508
Global Iter: 504400 training acc: 0.53125
Global Iter: 504500 training loss: 0.694469
Global Iter: 504500 training acc: 0.5
Global Iter: 504600 training loss: 0.703507
Global Iter: 504600 training acc: 0.46875
Global Iter: 504700 training loss: 0.712185
Global Iter: 504700 training acc: 0.375
Global Iter: 504800 training loss: 0.68874
Global Iter: 504800 training acc: 0.5625
Global Iter: 504900 training loss: 0.679265
Global Iter: 504900 training acc: 0.625
Global Iter: 505000 training loss: 0.693095
Global Iter: 505000 training acc: 0.53125
Global Iter: 505100 training loss: 0.702373
Global Iter: 505100 training acc: 0.46875
Global Iter: 505200 training loss: 0.689014
Global Iter: 505200 training acc: 0.59375
Global Iter: 505300 training loss: 0.70253
Global Iter: 505300 training acc: 0.46875
Global Iter: 505400 training loss: 0.681071
Global Iter: 505400 training acc: 0.625
Global Iter: 505500 training loss: 0.693524
Global Iter: 505500 training acc: 0.5
Global Iter: 505600 training loss: 0.694215
Global Iter: 505600 training acc: 0.5
Global Iter: 505700 training loss: 0.661734
Global Iter: 505700 training acc: 0.75
Global Iter: 505800 training loss: 0.69898
Global Iter: 505800 training acc: 0.5
Global Iter: 505900 training loss: 0.675484
Global Iter: 505900 training acc: 0.65625
Global Iter: 506000 training loss: 0.684412
Global Iter: 506000 training acc: 0.59375
Global Iter: 506100 training loss: 0.719712
Global Iter: 506100 training acc: 0.375
Global Iter: 506200 training loss: 0.695945
Global Iter: 506200 training acc: 0.5
Global Iter: 506300 training loss: 0.68735
Global Iter: 506300 training acc: 0.5625
Global Iter: 506400 training loss: 0.695582
Global Iter: 506400 training acc: 0.53125
Global Iter: 506500 training loss: 0.686536
Global Iter: 506500 training acc: 0.53125
Global Iter: 506600 training loss: 0.693226
Global Iter: 506600 training acc: 0.53125
Global Iter: 506700 training loss: 0.699956
Global Iter: 506700 training acc: 0.4375
Global Iter: 506800 training loss: 0.667623
Global Iter: 506800 training acc: 0.71875
Global Iter: 506900 training loss: 0.698126
Global Iter: 506900 training acc: 0.5
Global Iter: 507000 training loss: 0.692634
Global Iter: 507000 training acc: 0.53125
Global Iter: 507100 training loss: 0.687072
Global Iter: 507100 training acc: 0.59375
Global Iter: 507200 training loss: 0.703213
Global Iter: 507200 training acc: 0.4375
Global Iter: 507300 training loss: 0.710428
Global Iter: 507300 training acc: 0.34375
Global Iter: 507400 training loss: 0.704929
Global Iter: 507400 training acc: 0.40625
Global Iter: 507500 training loss: 0.684027
Global Iter: 507500 training acc: 0.5625
Global Iter: 507600 training loss: 0.712398
Global Iter: 507600 training acc: 0.375
Global Iter: 507700 training loss: 0.675931
Global Iter: 507700 training acc: 0.65625
Global Iter: 507800 training loss: 0.703897
Global Iter: 507800 training acc: 0.4375
Global Iter: 507900 training loss: 0.686479
Global Iter: 507900 training acc: 0.5625
Global Iter: 508000 training loss: 0.677568
Global Iter: 508000 training acc: 0.625
Global Iter: 508100 training loss: 0.681884
Global Iter: 508100 training acc: 0.5625
Global Iter: 508200 training loss: 0.694659
Global Iter: 508200 training acc: 0.5
Global Iter: 508300 training loss: 0.69884
Global Iter: 508300 training acc: 0.46875
Global Iter: 508400 training loss: 0.685091
Global Iter: 508400 training acc: 0.5625
Global Iter: 508500 training loss: 0.689771
Global Iter: 508500 training acc: 0.5625
Global Iter: 508600 training loss: 0.683831
Global Iter: 508600 training acc: 0.59375
Global Iter: 508700 training loss: 0.70878
Global Iter: 508700 training acc: 0.375
Global Iter: 508800 training loss: 0.661963
Global Iter: 508800 training acc: 0.71875
Global Iter: 508900 training loss: 0.692194
Global Iter: 508900 training acc: 0.53125
Global Iter: 509000 training loss: 0.707809
Global Iter: 509000 training acc: 0.4375
Global Iter: 509100 training loss: 0.690143
Global Iter: 509100 training acc: 0.53125
Global Iter: 509200 training loss: 0.686037
Global Iter: 509200 training acc: 0.59375
Global Iter: 509300 training loss: 0.704246
Global Iter: 509300 training acc: 0.46875
Global Iter: 509400 training loss: 0.681794
Global Iter: 509400 training acc: 0.625
Global Iter: 509500 training loss: 0.694338
Global Iter: 509500 training acc: 0.53125
Global Iter: 509600 training loss: 0.691113
Global Iter: 509600 training acc: 0.53125
Global Iter: 509700 training loss: 0.667212
Global Iter: 509700 training acc: 0.71875
Global Iter: 509800 training loss: 0.674579
Global Iter: 509800 training acc: 0.65625
Global Iter: 509900 training loss: 0.712452
Global Iter: 509900 training acc: 0.40625
Global Iter: 510000 training loss: 0.67749
Global Iter: 510000 training acc: 0.59375
Global Iter: 510100 training loss: 0.681421
Global Iter: 510100 training acc: 0.59375
Global Iter: 510200 training loss: 0.735356
Global Iter: 510200 training acc: 0.21875
Global Iter: 510300 training loss: 0.701894
Global Iter: 510300 training acc: 0.4375
Global Iter: 510400 training loss: 0.6849
Global Iter: 510400 training acc: 0.5625
Global Iter: 510500 training loss: 0.675939
Global Iter: 510500 training acc: 0.625
Global Iter: 510600 training loss: 0.676642
Global Iter: 510600 training acc: 0.71875
Global Iter: 510700 training loss: 0.702866
Global Iter: 510700 training acc: 0.46875
Global Iter: 510800 training loss: 0.669513
Global Iter: 510800 training acc: 0.71875
Global Iter: 510900 training loss: 0.694559
Global Iter: 510900 training acc: 0.5
Global Iter: 511000 training loss: 0.69628
Global Iter: 511000 training acc: 0.53125
Global Iter: 511100 training loss: 0.681458
Global Iter: 511100 training acc: 0.625
Global Iter: 511200 training loss: 0.70294
Global Iter: 511200 training acc: 0.46875
Global Iter: 511300 training loss: 0.706861
Global Iter: 511300 training acc: 0.4375
Global Iter: 511400 training loss: 0.67779
Global Iter: 511400 training acc: 0.65625
Global Iter: 511500 training loss: 0.694716
Global Iter: 511500 training acc: 0.53125
Global Iter: 511600 training loss: 0.714156
Global Iter: 511600 training acc: 0.34375
Global Iter: 511700 training loss: 0.690359
Global Iter: 511700 training acc: 0.53125
Global Iter: 511800 training loss: 0.686827
Global Iter: 511800 training acc: 0.5625
Global Iter: 511900 training loss: 0.707678
Global Iter: 511900 training acc: 0.4375
Global Iter: 512000 training loss: 0.671044
Global Iter: 512000 training acc: 0.71875
Global Iter: 512100 training loss: 0.707372
Global Iter: 512100 training acc: 0.40625
Global Iter: 512200 training loss: 0.685588
Global Iter: 512200 training acc: 0.5625
Global Iter: 512300 training loss: 0.684396
Global Iter: 512300 training acc: 0.59375
Global Iter: 512400 training loss: 0.672881
Global Iter: 512400 training acc: 0.6875
Global Iter: 512500 training loss: 0.691939
Global Iter: 512500 training acc: 0.5
Global Iter: 512600 training loss: 0.673875
Global Iter: 512600 training acc: 0.65625
Global Iter: 512700 training loss: 0.663724
Global Iter: 512700 training acc: 0.75
Global Iter: 512800 training loss: 0.699581
Global Iter: 512800 training acc: 0.4375
Global Iter: 512900 training loss: 0.695936
Global Iter: 512900 training acc: 0.5
Global Iter: 513000 training loss: 0.699197
Global Iter: 513000 training acc: 0.5
Global Iter: 513100 training loss: 0.729672
Global Iter: 513100 training acc: 0.25
Global Iter: 513200 training loss: 0.69628
Global Iter: 513200 training acc: 0.5
Global Iter: 513300 training loss: 0.699407
Global Iter: 513300 training acc: 0.46875
Global Iter: 513400 training loss: 0.697025
Global Iter: 513400 training acc: 0.46875
Global Iter: 513500 training loss: 0.695499
Global Iter: 513500 training acc: 0.46875
Global Iter: 513600 training loss: 0.671235
Global Iter: 513600 training acc: 0.65625
Global Iter: 513700 training loss: 0.686542
Global Iter: 513700 training acc: 0.59375
Global Iter: 513800 training loss: 0.702969
Global Iter: 513800 training acc: 0.4375
Global Iter: 513900 training loss: 0.682515
Global Iter: 513900 training acc: 0.59375
Global Iter: 514000 training loss: 0.697085
Global Iter: 514000 training acc: 0.5
Global Iter: 514100 training loss: 0.686359
Global Iter: 514100 training acc: 0.5625
Global Iter: 514200 training loss: 0.704319
Global Iter: 514200 training acc: 0.46875
Global Iter: 514300 training loss: 0.687078
Global Iter: 514300 training acc: 0.53125
Global Iter: 514400 training loss: 0.696264
Global Iter: 514400 training acc: 0.5
Global Iter: 514500 training loss: 0.707198
Global Iter: 514500 training acc: 0.4375
Global Iter: 514600 training loss: 0.716188
Global Iter: 514600 training acc: 0.375
Global Iter: 514700 training loss: 0.692093
Global Iter: 514700 training acc: 0.53125
Global Iter: 514800 training loss: 0.67268
Global Iter: 514800 training acc: 0.65625
Global Iter: 514900 training loss: 0.68406
Global Iter: 514900 training acc: 0.59375
Global Iter: 515000 training loss: 0.699628
Global Iter: 515000 training acc: 0.5
Global Iter: 515100 training loss: 0.683709
Global Iter: 515100 training acc: 0.5625
Global Iter: 515200 training loss: 0.699882
Global Iter: 515200 training acc: 0.46875
Global Iter: 515300 training loss: 0.687556
Global Iter: 515300 training acc: 0.5625
Global Iter: 515400 training loss: 0.694326
Global Iter: 515400 training acc: 0.5
Global Iter: 515500 training loss: 0.691921
Global Iter: 515500 training acc: 0.53125
Global Iter: 515600 training loss: 0.669283
Global Iter: 515600 training acc: 0.75
Global Iter: 515700 training loss: 0.700048
Global Iter: 515700 training acc: 0.46875
Global Iter: 515800 training loss: 0.675046
Global Iter: 515800 training acc: 0.625
Global Iter: 515900 training loss: 0.684746
Global Iter: 515900 training acc: 0.59375
Global Iter: 516000 training loss: 0.717017
Global Iter: 516000 training acc: 0.34375
Global Iter: 516100 training loss: 0.700681
Global Iter: 516100 training acc: 0.46875
Global Iter: 516200 training loss: 0.687461
Global Iter: 516200 training acc: 0.5625
Global Iter: 516300 training loss: 0.693605
Global Iter: 516300 training acc: 0.53125
Global Iter: 516400 training loss: 0.698138
Global Iter: 516400 training acc: 0.5
Global Iter: 516500 training loss: 0.685517
Global Iter: 516500 training acc: 0.59375
Global Iter: 516600 training loss: 0.69976
Global Iter: 516600 training acc: 0.5
Global Iter: 516700 training loss: 0.677533
Global Iter: 516700 training acc: 0.65625
Global Iter: 516800 training loss: 0.686073
Global Iter: 516800 training acc: 0.5625
Global Iter: 516900 training loss: 0.686357
Global Iter: 516900 training acc: 0.5625
Global Iter: 517000 training loss: 0.684693
Global Iter: 517000 training acc: 0.59375
Global Iter: 517100 training loss: 0.706248
Global Iter: 517100 training acc: 0.40625
Global Iter: 517200 training loss: 0.712262
Global Iter: 517200 training acc: 0.375
Global Iter: 517300 training loss: 0.71388
Global Iter: 517300 training acc: 0.40625
Global Iter: 517400 training loss: 0.689104
Global Iter: 517400 training acc: 0.5625
Global Iter: 517500 training loss: 0.707389
Global Iter: 517500 training acc: 0.40625
Global Iter: 517600 training loss: 0.685867
Global Iter: 517600 training acc: 0.59375
Global Iter: 517700 training loss: 0.694434
Global Iter: 517700 training acc: 0.5
Global Iter: 517800 training loss: 0.692565
Global Iter: 517800 training acc: 0.5
Global Iter: 517900 training loss: 0.677899
Global Iter: 517900 training acc: 0.625
Global Iter: 518000 training loss: 0.678851
Global Iter: 518000 training acc: 0.59375
Global Iter: 518100 training loss: 0.694632
Global Iter: 518100 training acc: 0.53125
Global Iter: 518200 training loss: 0.693394
Global Iter: 518200 training acc: 0.5
Global Iter: 518300 training loss: 0.692174
Global Iter: 518300 training acc: 0.53125
Global Iter: 518400 training loss: 0.695642
Global Iter: 518400 training acc: 0.5
Global Iter: 518500 training loss: 0.687286
Global Iter: 518500 training acc: 0.59375
Global Iter: 518600 training loss: 0.708837
Global Iter: 518600 training acc: 0.375
Global Iter: 518700 training loss: 0.668394
Global Iter: 518700 training acc: 0.71875
Global Iter: 518800 training loss: 0.6869
Global Iter: 518800 training acc: 0.5625
Global Iter: 518900 training loss: 0.695332
Global Iter: 518900 training acc: 0.5
Global Iter: 519000 training loss: 0.689985
Global Iter: 519000 training acc: 0.5625
Global Iter: 519100 training loss: 0.688114
Global Iter: 519100 training acc: 0.53125
Global Iter: 519200 training loss: 0.702438
Global Iter: 519200 training acc: 0.46875
Global Iter: 519300 training loss: 0.681251
Global Iter: 519300 training acc: 0.625
Global Iter: 519400 training loss: 0.700872
Global Iter: 519400 training acc: 0.46875
Global Iter: 519500 training loss: 0.699916
Global Iter: 519500 training acc: 0.46875
Global Iter: 519600 training loss: 0.664509
Global Iter: 519600 training acc: 0.75
Global Iter: 519700 training loss: 0.682293
Global Iter: 519700 training acc: 0.625
Global Iter: 519800 training loss: 0.701461
Global Iter: 519800 training acc: 0.40625
Global Iter: 519900 training loss: 0.684628
Global Iter: 519900 training acc: 0.59375
Global Iter: 520000 training loss: 0.678522
Global Iter: 520000 training acc: 0.625
Global Iter: 520100 training loss: 0.730203
Global Iter: 520100 training acc: 0.21875
Global Iter: 520200 training loss: 0.713567
Global Iter: 520200 training acc: 0.40625
Global Iter: 520300 training loss: 0.68571
Global Iter: 520300 training acc: 0.5625
Global Iter: 520400 training loss: 0.68345
Global Iter: 520400 training acc: 0.59375
Global Iter: 520500 training loss: 0.67386
Global Iter: 520500 training acc: 0.6875
Global Iter: 520600 training loss: 0.698479
Global Iter: 520600 training acc: 0.46875
Global Iter: 520700 training loss: 0.670502
Global Iter: 520700 training acc: 0.71875
Global Iter: 520800 training loss: 0.692353
Global Iter: 520800 training acc: 0.53125
Global Iter: 520900 training loss: 0.704583
Global Iter: 520900 training acc: 0.46875
Global Iter: 521000 training loss: 0.683304
Global Iter: 521000 training acc: 0.59375
Global Iter: 521100 training loss: 0.709235
Global Iter: 521100 training acc: 0.4375
Global Iter: 521200 training loss: 0.706016
Global Iter: 521200 training acc: 0.40625
Global Iter: 521300 training loss: 0.670626
Global Iter: 521300 training acc: 0.65625
Global Iter: 521400 training loss: 0.692013
Global Iter: 521400 training acc: 0.53125
Global Iter: 521500 training loss: 0.723339
Global Iter: 521500 training acc: 0.3125
Global Iter: 521600 training loss: 0.69012
Global Iter: 521600 training acc: 0.53125
Global Iter: 521700 training loss: 0.682856
Global Iter: 521700 training acc: 0.5625
Global Iter: 521800 training loss: 0.705895
Global Iter: 521800 training acc: 0.4375
Global Iter: 521900 training loss: 0.66937
Global Iter: 521900 training acc: 0.6875
Global Iter: 522000 training loss: 0.703359
Global Iter: 522000 training acc: 0.4375
Global Iter: 522100 training loss: 0.694789
Global Iter: 522100 training acc: 0.53125
Global Iter: 522200 training loss: 0.682349
Global Iter: 522200 training acc: 0.59375
Global Iter: 522300 training loss: 0.681709
Global Iter: 522300 training acc: 0.625
Global Iter: 522400 training loss: 0.689594
Global Iter: 522400 training acc: 0.5625
Global Iter: 522500 training loss: 0.671998
Global Iter: 522500 training acc: 0.65625
Global Iter: 522600 training loss: 0.665401
Global Iter: 522600 training acc: 0.75
Global Iter: 522700 training loss: 0.704011
Global Iter: 522700 training acc: 0.4375
Global Iter: 522800 training loss: 0.698464
Global Iter: 522800 training acc: 0.46875
Global Iter: 522900 training loss: 0.69695
Global Iter: 522900 training acc: 0.5
Global Iter: 523000 training loss: 0.730001
Global Iter: 523000 training acc: 0.25
Global Iter: 523100 training loss: 0.695855
Global Iter: 523100 training acc: 0.5
Global Iter: 523200 training loss: 0.701087
Global Iter: 523200 training acc: 0.46875
Global Iter: 523300 training loss: 0.694712
Global Iter: 523300 training acc: 0.53125
Global Iter: 523400 training loss: 0.689771
Global Iter: 523400 training acc: 0.5
Global Iter: 523500 training loss: 0.674806
Global Iter: 523500 training acc: 0.625
Global Iter: 523600 training loss: 0.685312
Global Iter: 523600 training acc: 0.59375
Global Iter: 523700 training loss: 0.704092
Global Iter: 523700 training acc: 0.4375
Global Iter: 523800 training loss: 0.686751
Global Iter: 523800 training acc: 0.59375
Global Iter: 523900 training loss: 0.698748
Global Iter: 523900 training acc: 0.46875
Global Iter: 524000 training loss: 0.68653
Global Iter: 524000 training acc: 0.5625
Global Iter: 524100 training loss: 0.708896
Global Iter: 524100 training acc: 0.40625
Global Iter: 524200 training loss: 0.684176
Global Iter: 524200 training acc: 0.59375
Global Iter: 524300 training loss: 0.690012
Global Iter: 524300 training acc: 0.5
Global Iter: 524400 training loss: 0.704134
Global Iter: 524400 training acc: 0.4375
Global Iter: 524500 training loss: 0.715296
Global Iter: 524500 training acc: 0.375
Global Iter: 524600 training loss: 0.695904
Global Iter: 524600 training acc: 0.46875
Global Iter: 524700 training loss: 0.672257
Global Iter: 524700 training acc: 0.65625
Global Iter: 524800 training loss: 0.688777
Global Iter: 524800 training acc: 0.5625
Global Iter: 524900 training loss: 0.699247
Global Iter: 524900 training acc: 0.46875
Global Iter: 525000 training loss: 0.69076
Global Iter: 525000 training acc: 0.5625
Global Iter: 525100 training loss: 0.700974
Global Iter: 525100 training acc: 0.4375
Global Iter: 525200 training loss: 0.68062
Global Iter: 525200 training acc: 0.59375
Global Iter: 525300 training loss: 0.698817
Global Iter: 525300 training acc: 0.5
Global Iter: 525400 training loss: 0.683081
Global Iter: 525400 training acc: 0.59375
Global Iter: 525500 training loss: 0.66085
Global Iter: 525500 training acc: 0.78125
Global Iter: 525600 training loss: 0.693467
Global Iter: 525600 training acc: 0.5
Global Iter: 525700 training loss: 0.680774
Global Iter: 525700 training acc: 0.59375
Global Iter: 525800 training loss: 0.683037
Global Iter: 525800 training acc: 0.59375
Global Iter: 525900 training loss: 0.714316
Global Iter: 525900 training acc: 0.34375
Global Iter: 526000 training loss: 0.696649
Global Iter: 526000 training acc: 0.5
Global Iter: 526100 training loss: 0.698497
Global Iter: 526100 training acc: 0.5
Global Iter: 526200 training loss: 0.699787
Global Iter: 526200 training acc: 0.5
Global Iter: 526300 training loss: 0.698674
Global Iter: 526300 training acc: 0.5
Global Iter: 526400 training loss: 0.679171
Global Iter: 526400 training acc: 0.59375
Global Iter: 526500 training loss: 0.693209
Global Iter: 526500 training acc: 0.5
Global Iter: 526600 training loss: 0.67501
Global Iter: 526600 training acc: 0.625
Global Iter: 526700 training loss: 0.678483
Global Iter: 526700 training acc: 0.59375
Global Iter: 526800 training loss: 0.687527
Global Iter: 526800 training acc: 0.53125
Global Iter: 526900 training loss: 0.681966
Global Iter: 526900 training acc: 0.625
Global Iter: 527000 training loss: 0.69911
Global Iter: 527000 training acc: 0.4375
Global Iter: 527100 training loss: 0.708868
Global Iter: 527100 training acc: 0.375
Global Iter: 527200 training loss: 0.706187
Global Iter: 527200 training acc: 0.4375
Global Iter: 527300 training loss: 0.692029
Global Iter: 527300 training acc: 0.53125
Global Iter: 527400 training loss: 0.707189
Global Iter: 527400 training acc: 0.40625
Global Iter: 527500 training loss: 0.682869
Global Iter: 527500 training acc: 0.625
Global Iter: 527600 training loss: 0.691749
Global Iter: 527600 training acc: 0.53125
Global Iter: 527700 training loss: 0.704178
Global Iter: 527700 training acc: 0.46875
Global Iter: 527800 training loss: 0.684366
Global Iter: 527800 training acc: 0.59375
Global Iter: 527900 training loss: 0.687365
Global Iter: 527900 training acc: 0.5625
Global Iter: 528000 training loss: 0.694077
Global Iter: 528000 training acc: 0.5
Global Iter: 528100 training loss: 0.698547
Global Iter: 528100 training acc: 0.53125
Global Iter: 528200 training loss: 0.684697
Global Iter: 528200 training acc: 0.5625
Global Iter: 528300 training loss: 0.701467
Global Iter: 528300 training acc: 0.46875
Global Iter: 528400 training loss: 0.678841
Global Iter: 528400 training acc: 0.65625
Global Iter: 528500 training loss: 0.711376
Global Iter: 528500 training acc: 0.375
Global Iter: 528600 training loss: 0.672365
Global Iter: 528600 training acc: 0.6875
Global Iter: 528700 training loss: 0.682583
Global Iter: 528700 training acc: 0.625
Global Iter: 528800 training loss: 0.695891
Global Iter: 528800 training acc: 0.5
Global Iter: 528900 training loss: 0.687615
Global Iter: 528900 training acc: 0.5625
Global Iter: 529000 training loss: 0.685859
Global Iter: 529000 training acc: 0.5625
Global Iter: 529100 training loss: 0.699655
Global Iter: 529100 training acc: 0.46875
Global Iter: 529200 training loss: 0.679208
Global Iter: 529200 training acc: 0.625
Global Iter: 529300 training loss: 0.706689
Global Iter: 529300 training acc: 0.40625
Global Iter: 529400 training loss: 0.71049
Global Iter: 529400 training acc: 0.40625
Global Iter: 529500 training loss: 0.670776
Global Iter: 529500 training acc: 0.71875
Global Iter: 529600 training loss: 0.681391
Global Iter: 529600 training acc: 0.59375
Global Iter: 529700 training loss: 0.700449
Global Iter: 529700 training acc: 0.46875
Global Iter: 529800 training loss: 0.688357
Global Iter: 529800 training acc: 0.5625
Global Iter: 529900 training loss: 0.679607
Global Iter: 529900 training acc: 0.59375
Global Iter: 530000 training loss: 0.72533
Global Iter: 530000 training acc: 0.25
Global Iter: 530100 training loss: 0.70574
Global Iter: 530100 training acc: 0.4375
Global Iter: 530200 training loss: 0.679262
Global Iter: 530200 training acc: 0.59375
Global Iter: 530300 training loss: 0.690337
Global Iter: 530300 training acc: 0.53125
Global Iter: 530400 training loss: 0.678614
Global Iter: 530400 training acc: 0.65625
Global Iter: 530500 training loss: 0.709547
Global Iter: 530500 training acc: 0.40625
Global Iter: 530600 training loss: 0.666819
Global Iter: 530600 training acc: 0.6875
Global Iter: 530700 training loss: 0.692273
Global Iter: 530700 training acc: 0.53125
Global Iter: 530800 training loss: 0.698856
Global Iter: 530800 training acc: 0.46875
Global Iter: 530900 training loss: 0.68174
Global Iter: 530900 training acc: 0.59375
Global Iter: 531000 training loss: 0.70119
Global Iter: 531000 training acc: 0.4375
Global Iter: 531100 training loss: 0.701465
Global Iter: 531100 training acc: 0.4375
Global Iter: 531200 training loss: 0.680648
Global Iter: 531200 training acc: 0.65625
Global Iter: 531300 training loss: 0.686569
Global Iter: 531300 training acc: 0.5625
Global Iter: 531400 training loss: 0.719181
Global Iter: 531400 training acc: 0.3125
Global Iter: 531500 training loss: 0.687809
Global Iter: 531500 training acc: 0.5625
Global Iter: 531600 training loss: 0.687791
Global Iter: 531600 training acc: 0.5625
Global Iter: 531700 training loss: 0.695254
Global Iter: 531700 training acc: 0.5
Global Iter: 531800 training loss: 0.670755
Global Iter: 531800 training acc: 0.625
Global Iter: 531900 training loss: 0.699924
Global Iter: 531900 training acc: 0.4375
Global Iter: 532000 training loss: 0.687722
Global Iter: 532000 training acc: 0.53125
Global Iter: 532100 training loss: 0.685596
Global Iter: 532100 training acc: 0.5625
Global Iter: 532200 training loss: 0.681168
Global Iter: 532200 training acc: 0.625
Global Iter: 532300 training loss: 0.691497
Global Iter: 532300 training acc: 0.5625
Global Iter: 532400 training loss: 0.678197
Global Iter: 532400 training acc: 0.65625
Global Iter: 532500 training loss: 0.659375
Global Iter: 532500 training acc: 0.78125
Global Iter: 532600 training loss: 0.702584
Global Iter: 532600 training acc: 0.40625
Global Iter: 532700 training loss: 0.702931
Global Iter: 532700 training acc: 0.46875
Global Iter: 532800 training loss: 0.705363
Global Iter: 532800 training acc: 0.46875
Global Iter: 532900 training loss: 0.733039
Global Iter: 532900 training acc: 0.25
Global Iter: 533000 training loss: 0.695929
Global Iter: 533000 training acc: 0.5
Global Iter: 533100 training loss: 0.694087
Global Iter: 533100 training acc: 0.53125
Global Iter: 533200 training loss: 0.693773
Global Iter: 533200 training acc: 0.53125
Global Iter: 533300 training loss: 0.687099
Global Iter: 533300 training acc: 0.5625
Global Iter: 533400 training loss: 0.681761
Global Iter: 533400 training acc: 0.625
Global Iter: 533500 training loss: 0.689252
Global Iter: 533500 training acc: 0.5625
Global Iter: 533600 training loss: 0.694637
Global Iter: 533600 training acc: 0.5
Global Iter: 533700 training loss: 0.6787
Global Iter: 533700 training acc: 0.59375
Global Iter: 533800 training loss: 0.690498
Global Iter: 533800 training acc: 0.53125
Global Iter: 533900 training loss: 0.69892
Global Iter: 533900 training acc: 0.5
Global Iter: 534000 training loss: 0.702358
Global Iter: 534000 training acc: 0.4375
Global Iter: 534100 training loss: 0.680771
Global Iter: 534100 training acc: 0.59375
Global Iter: 534200 training loss: 0.695277
Global Iter: 534200 training acc: 0.5
Global Iter: 534300 training loss: 0.714641
Global Iter: 534300 training acc: 0.375
Global Iter: 534400 training loss: 0.715727
Global Iter: 534400 training acc: 0.375
Global Iter: 534500 training loss: 0.695017
Global Iter: 534500 training acc: 0.5
Global Iter: 534600 training loss: 0.675336
Global Iter: 534600 training acc: 0.625
Global Iter: 534700 training loss: 0.686222
Global Iter: 534700 training acc: 0.5625
Global Iter: 534800 training loss: 0.70112
Global Iter: 534800 training acc: 0.4375
Global Iter: 534900 training loss: 0.683398
Global Iter: 534900 training acc: 0.59375
Global Iter: 535000 training loss: 0.698564
Global Iter: 535000 training acc: 0.5
Global Iter: 535100 training loss: 0.685605
Global Iter: 535100 training acc: 0.5625
Global Iter: 535200 training loss: 0.698047
Global Iter: 535200 training acc: 0.5
Global Iter: 535300 training loss: 0.684871
Global Iter: 535300 training acc: 0.59375
Global Iter: 535400 training loss: 0.663741
Global Iter: 535400 training acc: 0.75
Global Iter: 535500 training loss: 0.698882
Global Iter: 535500 training acc: 0.5
Global Iter: 535600 training loss: 0.686314
Global Iter: 535600 training acc: 0.5625
Global Iter: 535700 training loss: 0.690953
Global Iter: 535700 training acc: 0.5625
Global Iter: 535800 training loss: 0.71209
Global Iter: 535800 training acc: 0.375
Global Iter: 535900 training loss: 0.699537
Global Iter: 535900 training acc: 0.5
Global Iter: 536000 training loss: 0.69917
Global Iter: 536000 training acc: 0.46875
Global Iter: 536100 training loss: 0.691381
Global Iter: 536100 training acc: 0.5
Global Iter: 536200 training loss: 0.690679
Global Iter: 536200 training acc: 0.5
Global Iter: 536300 training loss: 0.681127
Global Iter: 536300 training acc: 0.625
Global Iter: 536400 training loss: 0.689453
Global Iter: 536400 training acc: 0.5625
Global Iter: 536500 training loss: 0.679461
Global Iter: 536500 training acc: 0.625
Global Iter: 536600 training loss: 0.678745
Global Iter: 536600 training acc: 0.625
Global Iter: 536700 training loss: 0.68736
Global Iter: 536700 training acc: 0.5625
Global Iter: 536800 training loss: 0.681318
Global Iter: 536800 training acc: 0.59375
Global Iter: 536900 training loss: 0.703
Global Iter: 536900 training acc: 0.4375
Global Iter: 537000 training loss: 0.703615
Global Iter: 537000 training acc: 0.4375
Global Iter: 537100 training loss: 0.700361
Global Iter: 537100 training acc: 0.46875
Global Iter: 537200 training loss: 0.694773
Global Iter: 537200 training acc: 0.53125
Global Iter: 537300 training loss: 0.706091
Global Iter: 537300 training acc: 0.375
Global Iter: 537400 training loss: 0.683187
Global Iter: 537400 training acc: 0.625
Global Iter: 537500 training loss: 0.69286
Global Iter: 537500 training acc: 0.53125
Global Iter: 537600 training loss: 0.701518
Global Iter: 537600 training acc: 0.4375
Global Iter: 537700 training loss: 0.683356
Global Iter: 537700 training acc: 0.59375
Global Iter: 537800 training loss: 0.685788
Global Iter: 537800 training acc: 0.5625
Global Iter: 537900 training loss: 0.69051
Global Iter: 537900 training acc: 0.53125
Global Iter: 538000 training loss: 0.693493
Global Iter: 538000 training acc: 0.5
Global Iter: 538100 training loss: 0.692217
Global Iter: 538100 training acc: 0.53125
Global Iter: 538200 training loss: 0.70523
Global Iter: 538200 training acc: 0.4375
Global Iter: 538300 training loss: 0.676547
Global Iter: 538300 training acc: 0.625
Global Iter: 538400 training loss: 0.705085
Global Iter: 538400 training acc: 0.40625
Global Iter: 538500 training loss: 0.670867
Global Iter: 538500 training acc: 0.6875
Global Iter: 538600 training loss: 0.675963
Global Iter: 538600 training acc: 0.65625
Global Iter: 538700 training loss: 0.693168
Global Iter: 538700 training acc: 0.53125
Global Iter: 538800 training loss: 0.692745
Global Iter: 538800 training acc: 0.53125
Global Iter: 538900 training loss: 0.692138
Global Iter: 538900 training acc: 0.53125
Global Iter: 539000 training loss: 0.701797
Global Iter: 539000 training acc: 0.46875
Global Iter: 539100 training loss: 0.686895
Global Iter: 539100 training acc: 0.5625
Global Iter: 539200 training loss: 0.706168
Global Iter: 539200 training acc: 0.4375
Global Iter: 539300 training loss: 0.701987
Global Iter: 539300 training acc: 0.40625
Global Iter: 539400 training loss: 0.662542
Global Iter: 539400 training acc: 0.75
Global Iter: 539500 training loss: 0.686242
Global Iter: 539500 training acc: 0.5625
Global Iter: 539600 training loss: 0.695678
Global Iter: 539600 training acc: 0.46875
Global Iter: 539700 training loss: 0.695109
Global Iter: 539700 training acc: 0.53125
Global Iter: 539800 training loss: 0.687125
Global Iter: 539800 training acc: 0.5625
Global Iter: 539900 training loss: 0.716956
Global Iter: 539900 training acc: 0.28125
Global Iter: 540000 training loss: 0.705855
Global Iter: 540000 training acc: 0.4375
Global Iter: 540100 training loss: 0.673739
Global Iter: 540100 training acc: 0.625
Global Iter: 540200 training loss: 0.693792
Global Iter: 540200 training acc: 0.5
Global Iter: 540300 training loss: 0.670335
Global Iter: 540300 training acc: 0.71875
Global Iter: 540400 training loss: 0.711783
Global Iter: 540400 training acc: 0.375
Global Iter: 540500 training loss: 0.669274
Global Iter: 540500 training acc: 0.71875
Global Iter: 540600 training loss: 0.688395
Global Iter: 540600 training acc: 0.59375
Global Iter: 540700 training loss: 0.705222
Global Iter: 540700 training acc: 0.4375
Global Iter: 540800 training loss: 0.679453
Global Iter: 540800 training acc: 0.625
Global Iter: 540900 training loss: 0.705567
Global Iter: 540900 training acc: 0.4375
Global Iter: 541000 training loss: 0.701583
Global Iter: 541000 training acc: 0.4375
Global Iter: 541100 training loss: 0.679271
Global Iter: 541100 training acc: 0.65625
Global Iter: 541200 training loss: 0.691688
Global Iter: 541200 training acc: 0.53125
Global Iter: 541300 training loss: 0.719147
Global Iter: 541300 training acc: 0.28125
Global Iter: 541400 training loss: 0.683171
Global Iter: 541400 training acc: 0.5625
Global Iter: 541500 training loss: 0.698492
Global Iter: 541500 training acc: 0.5
Global Iter: 541600 training loss: 0.688079
Global Iter: 541600 training acc: 0.5625
Global Iter: 541700 training loss: 0.675207
Global Iter: 541700 training acc: 0.65625
Global Iter: 541800 training loss: 0.694012
Global Iter: 541800 training acc: 0.5
Global Iter: 541900 training loss: 0.693547
Global Iter: 541900 training acc: 0.53125
Global Iter: 542000 training loss: 0.695843
Global Iter: 542000 training acc: 0.53125
Global Iter: 542100 training loss: 0.685476
Global Iter: 542100 training acc: 0.5625
Global Iter: 542200 training loss: 0.685778
Global Iter: 542200 training acc: 0.59375
Global Iter: 542300 training loss: 0.670499
Global Iter: 542300 training acc: 0.71875
Global Iter: 542400 training loss: 0.657788
Global Iter: 542400 training acc: 0.78125
Global Iter: 542500 training loss: 0.701123
Global Iter: 542500 training acc: 0.40625
Global Iter: 542600 training loss: 0.698057
Global Iter: 542600 training acc: 0.46875
Global Iter: 542700 training loss: 0.696882
Global Iter: 542700 training acc: 0.5
Global Iter: 542800 training loss: 0.737207
Global Iter: 542800 training acc: 0.21875
Global Iter: 542900 training loss: 0.692676
Global Iter: 542900 training acc: 0.5
Global Iter: 543000 training loss: 0.694275
Global Iter: 543000 training acc: 0.5
Global Iter: 543100 training loss: 0.689179
Global Iter: 543100 training acc: 0.53125
Global Iter: 543200 training loss: 0.688025
Global Iter: 543200 training acc: 0.5625
Global Iter: 543300 training loss: 0.689264
Global Iter: 543300 training acc: 0.5625
Global Iter: 543400 training loss: 0.684506
Global Iter: 543400 training acc: 0.59375
Global Iter: 543500 training loss: 0.707258
Global Iter: 543500 training acc: 0.4375
Global Iter: 543600 training loss: 0.684769
Global Iter: 543600 training acc: 0.59375
Global Iter: 543700 training loss: 0.695804
Global Iter: 543700 training acc: 0.5
Global Iter: 543800 training loss: 0.687976
Global Iter: 543800 training acc: 0.5625
Global Iter: 543900 training loss: 0.699584
Global Iter: 543900 training acc: 0.4375
Global Iter: 544000 training loss: 0.682963
Global Iter: 544000 training acc: 0.59375
Global Iter: 544100 training loss: 0.695269
Global Iter: 544100 training acc: 0.5
Global Iter: 544200 training loss: 0.70443
Global Iter: 544200 training acc: 0.4375
Global Iter: 544300 training loss: 0.714342
Global Iter: 544300 training acc: 0.40625
Global Iter: 544400 training loss: 0.692538
Global Iter: 544400 training acc: 0.53125
Global Iter: 544500 training loss: 0.677497
Global Iter: 544500 training acc: 0.625
Global Iter: 544600 training loss: 0.682026
Global Iter: 544600 training acc: 0.59375
Global Iter: 544700 training loss: 0.704926
Global Iter: 544700 training acc: 0.4375
Global Iter: 544800 training loss: 0.687609
Global Iter: 544800 training acc: 0.5625
Global Iter: 544900 training loss: 0.697115
Global Iter: 544900 training acc: 0.5
Global Iter: 545000 training loss: 0.675868
Global Iter: 545000 training acc: 0.625
Global Iter: 545100 training loss: 0.7
Global Iter: 545100 training acc: 0.46875
Global Iter: 545200 training loss: 0.681809
Global Iter: 545200 training acc: 0.59375
Global Iter: 545300 training loss: 0.662531
Global Iter: 545300 training acc: 0.75
Global Iter: 545400 training loss: 0.692527
Global Iter: 545400 training acc: 0.53125
Global Iter: 545500 training loss: 0.687177
Global Iter: 545500 training acc: 0.5625
Global Iter: 545600 training loss: 0.68692
Global Iter: 545600 training acc: 0.53125
Global Iter: 545700 training loss: 0.7156
Global Iter: 545700 training acc: 0.34375
Global Iter: 545800 training loss: 0.695715
Global Iter: 545800 training acc: 0.5
Global Iter: 545900 training loss: 0.700798
Global Iter: 545900 training acc: 0.46875
Global Iter: 546000 training loss: 0.693499
Global Iter: 546000 training acc: 0.5
Global Iter: 546100 training loss: 0.696518
Global Iter: 546100 training acc: 0.46875
Global Iter: 546200 training loss: 0.673873
Global Iter: 546200 training acc: 0.625
Global Iter: 546300 training loss: 0.692877
Global Iter: 546300 training acc: 0.5
Global Iter: 546400 training loss: 0.683882
Global Iter: 546400 training acc: 0.59375
Global Iter: 546500 training loss: 0.6824
Global Iter: 546500 training acc: 0.59375
Global Iter: 546600 training loss: 0.690039
Global Iter: 546600 training acc: 0.53125
Global Iter: 546700 training loss: 0.681539
Global Iter: 546700 training acc: 0.59375
Global Iter: 546800 training loss: 0.69866
Global Iter: 546800 training acc: 0.46875
Global Iter: 546900 training loss: 0.699937
Global Iter: 546900 training acc: 0.46875
Global Iter: 547000 training loss: 0.699358
Global Iter: 547000 training acc: 0.4375
Global Iter: 547100 training loss: 0.697667
Global Iter: 547100 training acc: 0.5
Global Iter: 547200 training loss: 0.722358
Global Iter: 547200 training acc: 0.34375
Global Iter: 547300 training loss: 0.677054
Global Iter: 547300 training acc: 0.625
Global Iter: 547400 training loss: 0.679121
Global Iter: 547400 training acc: 0.59375
Global Iter: 547500 training loss: 0.705359
Global Iter: 547500 training acc: 0.4375
Global Iter: 547600 training loss: 0.684942
Global Iter: 547600 training acc: 0.59375
Global Iter: 547700 training loss: 0.686335
Global Iter: 547700 training acc: 0.5625
Global Iter: 547800 training loss: 0.694788
Global Iter: 547800 training acc: 0.5
Global Iter: 547900 training loss: 0.693164
Global Iter: 547900 training acc: 0.5
Global Iter: 548000 training loss: 0.685759
Global Iter: 548000 training acc: 0.5625
Global Iter: 548100 training loss: 0.704845
Global Iter: 548100 training acc: 0.4375
Global Iter: 548200 training loss: 0.681437
Global Iter: 548200 training acc: 0.59375
Global Iter: 548300 training loss: 0.707283
Global Iter: 548300 training acc: 0.4375
Global Iter: 548400 training loss: 0.670202
Global Iter: 548400 training acc: 0.6875
Global Iter: 548500 training loss: 0.675611
Global Iter: 548500 training acc: 0.65625
Global Iter: 548600 training loss: 0.692083
Global Iter: 548600 training acc: 0.5
Global Iter: 548700 training loss: 0.690518
Global Iter: 548700 training acc: 0.5
Global Iter: 548800 training loss: 0.687734
Global Iter: 548800 training acc: 0.53125
Global Iter: 548900 training loss: 0.690731
Global Iter: 548900 training acc: 0.53125
Global Iter: 549000 training loss: 0.68838
Global Iter: 549000 training acc: 0.5625
Global Iter: 549100 training loss: 0.708162
Global Iter: 549100 training acc: 0.40625
Global Iter: 549200 training loss: 0.703829
Global Iter: 549200 training acc: 0.4375
Global Iter: 549300 training loss: 0.664078
Global Iter: 549300 training acc: 0.75
Global Iter: 549400 training loss: 0.689459
Global Iter: 549400 training acc: 0.53125
Global Iter: 549500 training loss: 0.696178
Global Iter: 549500 training acc: 0.5
Global Iter: 549600 training loss: 0.689121
Global Iter: 549600 training acc: 0.59375
Global Iter: 549700 training loss: 0.688256
Global Iter: 549700 training acc: 0.5625
Global Iter: 549800 training loss: 0.723173
Global Iter: 549800 training acc: 0.28125
Global Iter: 549900 training loss: 0.708137
Global Iter: 549900 training acc: 0.40625
Global Iter: 550000 training loss: 0.679008
Global Iter: 550000 training acc: 0.625
Global Iter: 550100 training loss: 0.698065
Global Iter: 550100 training acc: 0.46875
Global Iter: 550200 training loss: 0.671287
Global Iter: 550200 training acc: 0.71875
Global Iter: 550300 training loss: 0.71349
Global Iter: 550300 training acc: 0.375
Global Iter: 550400 training loss: 0.676168
Global Iter: 550400 training acc: 0.65625
Global Iter: 550500 training loss: 0.682359
Global Iter: 550500 training acc: 0.59375
Global Iter: 550600 training loss: 0.695871
Global Iter: 550600 training acc: 0.5
Global Iter: 550700 training loss: 0.677177
Global Iter: 550700 training acc: 0.625
Global Iter: 550800 training loss: 0.698719
Global Iter: 550800 training acc: 0.46875
Global Iter: 550900 training loss: 0.703945
Global Iter: 550900 training acc: 0.46875
Global Iter: 551000 training loss: 0.672583
Global Iter: 551000 training acc: 0.65625
Global Iter: 551100 training loss: 0.695122
Global Iter: 551100 training acc: 0.5
Global Iter: 551200 training loss: 0.717511
Global Iter: 551200 training acc: 0.34375
Global Iter: 551300 training loss: 0.677944
Global Iter: 551300 training acc: 0.625
Global Iter: 551400 training loss: 0.693289
Global Iter: 551400 training acc: 0.53125
Global Iter: 551500 training loss: 0.690312
Global Iter: 551500 training acc: 0.53125
Global Iter: 551600 training loss: 0.683222
Global Iter: 551600 training acc: 0.625
Global Iter: 551700 training loss: 0.691205
Global Iter: 551700 training acc: 0.53125
Global Iter: 551800 training loss: 0.688554
Global Iter: 551800 training acc: 0.5625
Global Iter: 551900 training loss: 0.689775
Global Iter: 551900 training acc: 0.5625
Global Iter: 552000 training loss: 0.691837
Global Iter: 552000 training acc: 0.53125
Global Iter: 552100 training loss: 0.688402
Global Iter: 552100 training acc: 0.5625
Global Iter: 552200 training loss: 0.66809
Global Iter: 552200 training acc: 0.71875
Global Iter: 552300 training loss: 0.662436
Global Iter: 552300 training acc: 0.75
Global Iter: 552400 training loss: 0.709571
Global Iter: 552400 training acc: 0.375
Global Iter: 552500 training loss: 0.702205
Global Iter: 552500 training acc: 0.4375
Global Iter: 552600 training loss: 0.690984
Global Iter: 552600 training acc: 0.5625
Global Iter: 552700 training loss: 0.731436
Global Iter: 552700 training acc: 0.21875
Global Iter: 552800 training loss: 0.697924
Global Iter: 552800 training acc: 0.46875
Global Iter: 552900 training loss: 0.696501
Global Iter: 552900 training acc: 0.5
Global Iter: 553000 training loss: 0.689224
Global Iter: 553000 training acc: 0.53125
Global Iter: 553100 training loss: 0.688781
Global Iter: 553100 training acc: 0.59375
Global Iter: 553200 training loss: 0.686527
Global Iter: 553200 training acc: 0.59375
Global Iter: 553300 training loss: 0.676532
Global Iter: 553300 training acc: 0.65625
Global Iter: 553400 training loss: 0.702428
Global Iter: 553400 training acc: 0.4375
Global Iter: 553500 training loss: 0.687776
Global Iter: 553500 training acc: 0.5625
Global Iter: 553600 training loss: 0.688408
Global Iter: 553600 training acc: 0.5625
Global Iter: 553700 training loss: 0.691903
Global Iter: 553700 training acc: 0.53125
Global Iter: 553800 training loss: 0.697282
Global Iter: 553800 training acc: 0.46875
Global Iter: 553900 training loss: 0.683546
Global Iter: 553900 training acc: 0.5625
Global Iter: 554000 training loss: 0.696036
Global Iter: 554000 training acc: 0.5
Global Iter: 554100 training loss: 0.70624
Global Iter: 554100 training acc: 0.40625
Global Iter: 554200 training loss: 0.703858
Global Iter: 554200 training acc: 0.4375
Global Iter: 554300 training loss: 0.69477
Global Iter: 554300 training acc: 0.5
Global Iter: 554400 training loss: 0.676591
Global Iter: 554400 training acc: 0.59375
Global Iter: 554500 training loss: 0.679958
Global Iter: 554500 training acc: 0.59375
Global Iter: 554600 training loss: 0.700227
Global Iter: 554600 training acc: 0.4375
Global Iter: 554700 training loss: 0.687577
Global Iter: 554700 training acc: 0.5625
Global Iter: 554800 training loss: 0.688556
Global Iter: 554800 training acc: 0.53125
Global Iter: 554900 training loss: 0.684935
Global Iter: 554900 training acc: 0.5625
Global Iter: 555000 training loss: 0.697132
Global Iter: 555000 training acc: 0.5
Global Iter: 555100 training loss: 0.683067
Global Iter: 555100 training acc: 0.625
Global Iter: 555200 training loss: 0.666371
Global Iter: 555200 training acc: 0.75
Global Iter: 555300 training loss: 0.685831
Global Iter: 555300 training acc: 0.5625
Global Iter: 555400 training loss: 0.687954
Global Iter: 555400 training acc: 0.53125
Global Iter: 555500 training loss: 0.688506
Global Iter: 555500 training acc: 0.5625
Global Iter: 555600 training loss: 0.720342
Global Iter: 555600 training acc: 0.34375
Global Iter: 555700 training loss: 0.694133
Global Iter: 555700 training acc: 0.5
Global Iter: 555800 training loss: 0.70335
Global Iter: 555800 training acc: 0.4375
Global Iter: 555900 training loss: 0.691391
Global Iter: 555900 training acc: 0.5
Global Iter: 556000 training loss: 0.70868
Global Iter: 556000 training acc: 0.40625
Global Iter: 556100 training loss: 0.681567
Global Iter: 556100 training acc: 0.625
Global Iter: 556200 training loss: 0.696815
Global Iter: 556200 training acc: 0.5
Global Iter: 556300 training loss: 0.696529
Global Iter: 556300 training acc: 0.53125
Global Iter: 556400 training loss: 0.679096
Global Iter: 556400 training acc: 0.59375
Global Iter: 556500 training loss: 0.691533
Global Iter: 556500 training acc: 0.5
Global Iter: 556600 training loss: 0.678913
Global Iter: 556600 training acc: 0.59375
Global Iter: 556700 training loss: 0.701414
Global Iter: 556700 training acc: 0.5
Global Iter: 556800 training loss: 0.693489
Global Iter: 556800 training acc: 0.5
Global Iter: 556900 training loss: 0.695296
Global Iter: 556900 training acc: 0.46875
Global Iter: 557000 training loss: 0.697955
Global Iter: 557000 training acc: 0.46875
Global Iter: 557100 training loss: 0.716697
Global Iter: 557100 training acc: 0.34375
Global Iter: 557200 training loss: 0.691954
Global Iter: 557200 training acc: 0.59375
Global Iter: 557300 training loss: 0.684797
Global Iter: 557300 training acc: 0.59375
Global Iter: 557400 training loss: 0.70693
Global Iter: 557400 training acc: 0.40625
Global Iter: 557500 training loss: 0.680187
Global Iter: 557500 training acc: 0.625
Global Iter: 557600 training loss: 0.689244
Global Iter: 557600 training acc: 0.5625
Global Iter: 557700 training loss: 0.692369
Global Iter: 557700 training acc: 0.53125
Global Iter: 557800 training loss: 0.687948
Global Iter: 557800 training acc: 0.5625
Global Iter: 557900 training loss: 0.686345
Global Iter: 557900 training acc: 0.5625
Global Iter: 558000 training loss: 0.703445
Global Iter: 558000 training acc: 0.4375
Global Iter: 558100 training loss: 0.676193
Global Iter: 558100 training acc: 0.625
Global Iter: 558200 training loss: 0.704297
Global Iter: 558200 training acc: 0.4375
Global Iter: 558300 training loss: 0.671873
Global Iter: 558300 training acc: 0.6875
Global Iter: 558400 training loss: 0.675794
Global Iter: 558400 training acc: 0.65625
Global Iter: 558500 training loss: 0.698845
Global Iter: 558500 training acc: 0.5
Global Iter: 558600 training loss: 0.69284
Global Iter: 558600 training acc: 0.5
Global Iter: 558700 training loss: 0.683467
Global Iter: 558700 training acc: 0.59375
Global Iter: 558800 training loss: 0.698235
Global Iter: 558800 training acc: 0.5
Global Iter: 558900 training loss: 0.690606
Global Iter: 558900 training acc: 0.53125
Global Iter: 559000 training loss: 0.707665
Global Iter: 559000 training acc: 0.40625
Global Iter: 559100 training loss: 0.709254
Global Iter: 559100 training acc: 0.4375
Global Iter: 559200 training loss: 0.661597
Global Iter: 559200 training acc: 0.78125
Global Iter: 559300 training loss: 0.696193
Global Iter: 559300 training acc: 0.5
Global Iter: 559400 training loss: 0.689852
Global Iter: 559400 training acc: 0.53125
Global Iter: 559500 training loss: 0.680549
Global Iter: 559500 training acc: 0.59375
Global Iter: 559600 training loss: 0.689103
Global Iter: 559600 training acc: 0.5625
Global Iter: 559700 training loss: 0.719312
Global Iter: 559700 training acc: 0.3125
Global Iter: 559800 training loss: 0.713048
Global Iter: 559800 training acc: 0.375
Global Iter: 559900 training loss: 0.673859
Global Iter: 559900 training acc: 0.65625
Global Iter: 560000 training loss: 0.700487
Global Iter: 560000 training acc: 0.46875
Global Iter: 560100 training loss: 0.670224
Global Iter: 560100 training acc: 0.71875
Global Iter: 560200 training loss: 0.709778
Global Iter: 560200 training acc: 0.375
Global Iter: 560300 training loss: 0.677624
Global Iter: 560300 training acc: 0.59375
Global Iter: 560400 training loss: 0.689206
Global Iter: 560400 training acc: 0.5625
Global Iter: 560500 training loss: 0.684029
Global Iter: 560500 training acc: 0.5625
Global Iter: 560600 training loss: 0.692101
Global Iter: 560600 training acc: 0.5625
Global Iter: 560700 training loss: 0.697601
Global Iter: 560700 training acc: 0.5
Global Iter: 560800 training loss: 0.691022
Global Iter: 560800 training acc: 0.53125
Global Iter: 560900 training loss: 0.684205
Global Iter: 560900 training acc: 0.59375
Global Iter: 561000 training loss: 0.695978
Global Iter: 561000 training acc: 0.5
Global Iter: 561100 training loss: 0.714718
Global Iter: 561100 training acc: 0.34375
Global Iter: 561200 training loss: 0.675825
Global Iter: 561200 training acc: 0.65625
Global Iter: 561300 training loss: 0.689517
Global Iter: 561300 training acc: 0.5625
Global Iter: 561400 training loss: 0.691309
Global Iter: 561400 training acc: 0.53125
Global Iter: 561500 training loss: 0.685157
Global Iter: 561500 training acc: 0.59375
Global Iter: 561600 training loss: 0.68781
Global Iter: 561600 training acc: 0.5625
Global Iter: 561700 training loss: 0.698435
Global Iter: 561700 training acc: 0.5
Global Iter: 561800 training loss: 0.683298
Global Iter: 561800 training acc: 0.625
Global Iter: 561900 training loss: 0.694163
Global Iter: 561900 training acc: 0.5
Global Iter: 562000 training loss: 0.692078
Global Iter: 562000 training acc: 0.53125
Global Iter: 562100 training loss: 0.669573
Global Iter: 562100 training acc: 0.71875
Global Iter: 562200 training loss: 0.671823
Global Iter: 562200 training acc: 0.6875
Global Iter: 562300 training loss: 0.7069
Global Iter: 562300 training acc: 0.375
Global Iter: 562400 training loss: 0.695112
Global Iter: 562400 training acc: 0.5
Global Iter: 562500 training loss: 0.67727
Global Iter: 562500 training acc: 0.59375
Global Iter: 562600 training loss: 0.732402
Global Iter: 562600 training acc: 0.21875
Global Iter: 562700 training loss: 0.702279
Global Iter: 562700 training acc: 0.46875
Global Iter: 562800 training loss: 0.691761
Global Iter: 562800 training acc: 0.53125
Global Iter: 562900 training loss: 0.707221
Global Iter: 562900 training acc: 0.46875
Global Iter: 563000 training loss: 0.67909
Global Iter: 563000 training acc: 0.625
Global Iter: 563100 training loss: 0.683364
Global Iter: 563100 training acc: 0.59375
Global Iter: 563200 training loss: 0.679096
Global Iter: 563200 training acc: 0.65625
Global Iter: 563300 training loss: 0.70239
Global Iter: 563300 training acc: 0.4375
Global Iter: 563400 training loss: 0.692359
Global Iter: 563400 training acc: 0.53125
Global Iter: 563500 training loss: 0.687878
Global Iter: 563500 training acc: 0.5625
Global Iter: 563600 training loss: 0.689351
Global Iter: 563600 training acc: 0.53125
Global Iter: 563700 training loss: 0.691706
Global Iter: 563700 training acc: 0.53125
Global Iter: 563800 training loss: 0.680381
Global Iter: 563800 training acc: 0.625
Global Iter: 563900 training loss: 0.687986
Global Iter: 563900 training acc: 0.5625
Global Iter: 564000 training loss: 0.704519
Global Iter: 564000 training acc: 0.40625
Global Iter: 564100 training loss: 0.701844
Global Iter: 564100 training acc: 0.4375
Global Iter: 564200 training loss: 0.69898
Global Iter: 564200 training acc: 0.5
Global Iter: 564300 training loss: 0.688319
Global Iter: 564300 training acc: 0.5625
Global Iter: 564400 training loss: 0.680913
Global Iter: 564400 training acc: 0.625
Global Iter: 564500 training loss: 0.702357
Global Iter: 564500 training acc: 0.46875
Global Iter: 564600 training loss: 0.688416
Global Iter: 564600 training acc: 0.5625
Global Iter: 564700 training loss: 0.689233
Global Iter: 564700 training acc: 0.5625
Global Iter: 564800 training loss: 0.67827
Global Iter: 564800 training acc: 0.625
Global Iter: 564900 training loss: 0.692525
Global Iter: 564900 training acc: 0.53125
Global Iter: 565000 training loss: 0.688323
Global Iter: 565000 training acc: 0.59375
Global Iter: 565100 training loss: 0.660599
Global Iter: 565100 training acc: 0.78125
Global Iter: 565200 training loss: 0.693427
Global Iter: 565200 training acc: 0.53125
Global Iter: 565300 training loss: 0.689142
Global Iter: 565300 training acc: 0.5625
Global Iter: 565400 training loss: 0.685434
Global Iter: 565400 training acc: 0.5625
Global Iter: 565500 training loss: 0.71386
Global Iter: 565500 training acc: 0.34375
Global Iter: 565600 training loss: 0.698605
Global Iter: 565600 training acc: 0.5
Global Iter: 565700 training loss: 0.707726
Global Iter: 565700 training acc: 0.4375
Global Iter: 565800 training loss: 0.700161
Global Iter: 565800 training acc: 0.46875
Global Iter: 565900 training loss: 0.702812
Global Iter: 565900 training acc: 0.40625
Global Iter: 566000 training loss: 0.681557
Global Iter: 566000 training acc: 0.59375
Global Iter: 566100 training loss: 0.696018
Global Iter: 566100 training acc: 0.5
Global Iter: 566200 training loss: 0.692046
Global Iter: 566200 training acc: 0.53125
Global Iter: 566300 training loss: 0.680163
Global Iter: 566300 training acc: 0.59375
Global Iter: 566400 training loss: 0.690963
Global Iter: 566400 training acc: 0.53125
Global Iter: 566500 training loss: 0.685821
Global Iter: 566500 training acc: 0.59375
Global Iter: 566600 training loss: 0.688611
Global Iter: 566600 training acc: 0.53125
Global Iter: 566700 training loss: 0.699382
Global Iter: 566700 training acc: 0.5
Global Iter: 566800 training loss: 0.695511
Global Iter: 566800 training acc: 0.5
Global Iter: 566900 training loss: 0.693497
Global Iter: 566900 training acc: 0.53125
Global Iter: 567000 training loss: 0.710798
Global Iter: 567000 training acc: 0.375
Global Iter: 567100 training loss: 0.684953
Global Iter: 567100 training acc: 0.5625
Global Iter: 567200 training loss: 0.676309
Global Iter: 567200 training acc: 0.65625
Global Iter: 567300 training loss: 0.707158
Global Iter: 567300 training acc: 0.40625
Global Iter: 567400 training loss: 0.683132
Global Iter: 567400 training acc: 0.5625
Global Iter: 567500 training loss: 0.689615
Global Iter: 567500 training acc: 0.53125
Global Iter: 567600 training loss: 0.696255
Global Iter: 567600 training acc: 0.5
Global Iter: 567700 training loss: 0.688801
Global Iter: 567700 training acc: 0.5625
Global Iter: 567800 training loss: 0.68471
Global Iter: 567800 training acc: 0.5625
Global Iter: 567900 training loss: 0.702888
Global Iter: 567900 training acc: 0.4375
Global Iter: 568000 training loss: 0.673779
Global Iter: 568000 training acc: 0.65625
Global Iter: 568100 training loss: 0.699083
Global Iter: 568100 training acc: 0.46875
Global Iter: 568200 training loss: 0.668928
Global Iter: 568200 training acc: 0.6875
Global Iter: 568300 training loss: 0.679103
Global Iter: 568300 training acc: 0.625
Global Iter: 568400 training loss: 0.697746
Global Iter: 568400 training acc: 0.46875
Global Iter: 568500 training loss: 0.693368
Global Iter: 568500 training acc: 0.5
Global Iter: 568600 training loss: 0.675748
Global Iter: 568600 training acc: 0.625
Global Iter: 568700 training loss: 0.692097
Global Iter: 568700 training acc: 0.5
Global Iter: 568800 training loss: 0.692527
Global Iter: 568800 training acc: 0.53125
Global Iter: 568900 training loss: 0.708039
Global Iter: 568900 training acc: 0.4375
Global Iter: 569000 training loss: 0.701533
Global Iter: 569000 training acc: 0.46875
Global Iter: 569100 training loss: 0.659617
Global Iter: 569100 training acc: 0.8125
Global Iter: 569200 training loss: 0.691703
Global Iter: 569200 training acc: 0.53125
Global Iter: 569300 training loss: 0.685279
Global Iter: 569300 training acc: 0.5625
Global Iter: 569400 training loss: 0.684266
Global Iter: 569400 training acc: 0.59375
Global Iter: 569500 training loss: 0.679375
Global Iter: 569500 training acc: 0.59375
Global Iter: 569600 training loss: 0.719034
Global Iter: 569600 training acc: 0.28125
Global Iter: 569700 training loss: 0.706436
Global Iter: 569700 training acc: 0.40625
Global Iter: 569800 training loss: 0.68528
Global Iter: 569800 training acc: 0.59375
Global Iter: 569900 training loss: 0.702382
Global Iter: 569900 training acc: 0.46875
Global Iter: 570000 training loss: 0.671312
Global Iter: 570000 training acc: 0.71875
Global Iter: 570100 training loss: 0.714103
Global Iter: 570100 training acc: 0.34375
Global Iter: 570200 training loss: 0.678963
Global Iter: 570200 training acc: 0.59375
Global Iter: 570300 training loss: 0.683853
Global Iter: 570300 training acc: 0.59375
Global Iter: 570400 training loss: 0.684252
Global Iter: 570400 training acc: 0.59375
Global Iter: 570500 training loss: 0.686872
Global Iter: 570500 training acc: 0.59375
Global Iter: 570600 training loss: 0.70213
Global Iter: 570600 training acc: 0.46875
Global Iter: 570700 training loss: 0.688864
Global Iter: 570700 training acc: 0.53125
Global Iter: 570800 training loss: 0.687581
Global Iter: 570800 training acc: 0.5625
Global Iter: 570900 training loss: 0.694067
Global Iter: 570900 training acc: 0.53125
Global Iter: 571000 training loss: 0.724804
Global Iter: 571000 training acc: 0.3125
Global Iter: 571100 training loss: 0.674297
Global Iter: 571100 training acc: 0.65625
Global Iter: 571200 training loss: 0.688657
Global Iter: 571200 training acc: 0.5625
Global Iter: 571300 training loss: 0.698335
Global Iter: 571300 training acc: 0.5
Global Iter: 571400 training loss: 0.686481
Global Iter: 571400 training acc: 0.5625
Global Iter: 571500 training loss: 0.687129
Global Iter: 571500 training acc: 0.5625
Global Iter: 571600 training loss: 0.68883
Global Iter: 571600 training acc: 0.53125
Global Iter: 571700 training loss: 0.683092
Global Iter: 571700 training acc: 0.625
Global Iter: 571800 training loss: 0.690838
Global Iter: 571800 training acc: 0.5625
Global Iter: 571900 training loss: 0.695721
Global Iter: 571900 training acc: 0.5
Global Iter: 572000 training loss: 0.664983
Global Iter: 572000 training acc: 0.75
Global Iter: 572100 training loss: 0.676649
Global Iter: 572100 training acc: 0.65625
Global Iter: 572200 training loss: 0.703642
Global Iter: 572200 training acc: 0.4375
Global Iter: 572300 training loss: 0.698901
Global Iter: 572300 training acc: 0.46875
Global Iter: 572400 training loss: 0.695052
Global Iter: 572400 training acc: 0.53125
Global Iter: 572500 training loss: 0.732802
Global Iter: 572500 training acc: 0.21875
Global Iter: 572600 training loss: 0.693258
Global Iter: 572600 training acc: 0.5
Global Iter: 572700 training loss: 0.695589
Global Iter: 572700 training acc: 0.53125
Global Iter: 572800 training loss: 0.695451
Global Iter: 572800 training acc: 0.5
Global Iter: 572900 training loss: 0.684021
Global Iter: 572900 training acc: 0.59375
Global Iter: 573000 training loss: 0.685491
Global Iter: 573000 training acc: 0.5625
Global Iter: 573100 training loss: 0.677597
Global Iter: 573100 training acc: 0.625
Global Iter: 573200 training loss: 0.6953
Global Iter: 573200 training acc: 0.5
Global Iter: 573300 training loss: 0.691717
Global Iter: 573300 training acc: 0.53125
Global Iter: 573400 training loss: 0.690164
Global Iter: 573400 training acc: 0.53125
Global Iter: 573500 training loss: 0.693551
Global Iter: 573500 training acc: 0.5
Global Iter: 573600 training loss: 0.690633
Global Iter: 573600 training acc: 0.53125
Global Iter: 573700 training loss: 0.67593
Global Iter: 573700 training acc: 0.65625
Global Iter: 573800 training loss: 0.69432
Global Iter: 573800 training acc: 0.53125
Global Iter: 573900 training loss: 0.713995
Global Iter: 573900 training acc: 0.34375
Global Iter: 574000 training loss: 0.695546
Global Iter: 574000 training acc: 0.46875
Global Iter: 574100 training loss: 0.693522
Global Iter: 574100 training acc: 0.5
Global Iter: 574200 training loss: 0.693033
Global Iter: 574200 training acc: 0.53125
Global Iter: 574300 training loss: 0.679877
Global Iter: 574300 training acc: 0.65625
Global Iter: 574400 training loss: 0.706055
Global Iter: 574400 training acc: 0.4375
Global Iter: 574500 training loss: 0.687263
Global Iter: 574500 training acc: 0.5625
Global Iter: 574600 training loss: 0.691695
Global Iter: 574600 training acc: 0.5625
Global Iter: 574700 training loss: 0.672914
Global Iter: 574700 training acc: 0.65625
Global Iter: 574800 training loss: 0.685778
Global Iter: 574800 training acc: 0.5625
Global Iter: 574900 training loss: 0.678302
Global Iter: 574900 training acc: 0.65625
Global Iter: 575000 training loss: 0.665821
Global Iter: 575000 training acc: 0.75
Global Iter: 575100 training loss: 0.691927
Global Iter: 575100 training acc: 0.5
Global Iter: 575200 training loss: 0.686333
Global Iter: 575200 training acc: 0.5625
Global Iter: 575300 training loss: 0.687513
Global Iter: 575300 training acc: 0.53125
Global Iter: 575400 training loss: 0.726828
Global Iter: 575400 training acc: 0.28125
Global Iter: 575500 training loss: 0.698768
Global Iter: 575500 training acc: 0.46875
Global Iter: 575600 training loss: 0.701678
Global Iter: 575600 training acc: 0.4375
Global Iter: 575700 training loss: 0.690425
Global Iter: 575700 training acc: 0.5
Global Iter: 575800 training loss: 0.711994
Global Iter: 575800 training acc: 0.40625
Global Iter: 575900 training loss: 0.674306
Global Iter: 575900 training acc: 0.65625
Global Iter: 576000 training loss: 0.686349
Global Iter: 576000 training acc: 0.5625
Global Iter: 576100 training loss: 0.691412
Global Iter: 576100 training acc: 0.53125
Global Iter: 576200 training loss: 0.680397
Global Iter: 576200 training acc: 0.59375
Global Iter: 576300 training loss: 0.694734
Global Iter: 576300 training acc: 0.5
Global Iter: 576400 training loss: 0.69203
Global Iter: 576400 training acc: 0.53125
Global Iter: 576500 training loss: 0.695583
Global Iter: 576500 training acc: 0.5
Global Iter: 576600 training loss: 0.695081
Global Iter: 576600 training acc: 0.5
Global Iter: 576700 training loss: 0.692672
Global Iter: 576700 training acc: 0.53125
Global Iter: 576800 training loss: 0.690968
Global Iter: 576800 training acc: 0.53125
Global Iter: 576900 training loss: 0.714194
Global Iter: 576900 training acc: 0.375
Global Iter: 577000 training loss: 0.689966
Global Iter: 577000 training acc: 0.5625
Global Iter: 577100 training loss: 0.679501
Global Iter: 577100 training acc: 0.625
Global Iter: 577200 training loss: 0.699802
Global Iter: 577200 training acc: 0.4375
Global Iter: 577300 training loss: 0.689592
Global Iter: 577300 training acc: 0.5625
Global Iter: 577400 training loss: 0.692696
Global Iter: 577400 training acc: 0.5625
Global Iter: 577500 training loss: 0.693354
Global Iter: 577500 training acc: 0.53125
Global Iter: 577600 training loss: 0.684646
Global Iter: 577600 training acc: 0.5625
Global Iter: 577700 training loss: 0.686984
Global Iter: 577700 training acc: 0.5625
Global Iter: 577800 training loss: 0.698009
Global Iter: 577800 training acc: 0.46875
Global Iter: 577900 training loss: 0.673808
Global Iter: 577900 training acc: 0.65625
Global Iter: 578000 training loss: 0.690431
Global Iter: 578000 training acc: 0.5
Global Iter: 578100 training loss: 0.679164
Global Iter: 578100 training acc: 0.65625
Global Iter: 578200 training loss: 0.679498
Global Iter: 578200 training acc: 0.625
Global Iter: 578300 training loss: 0.707477
Global Iter: 578300 training acc: 0.40625
Global Iter: 578400 training loss: 0.696396
Global Iter: 578400 training acc: 0.5
Global Iter: 578500 training loss: 0.686137
Global Iter: 578500 training acc: 0.59375
Global Iter: 578600 training loss: 0.687559
Global Iter: 578600 training acc: 0.53125
Global Iter: 578700 training loss: 0.694237
Global Iter: 578700 training acc: 0.5
Global Iter: 578800 training loss: 0.700731
Global Iter: 578800 training acc: 0.46875
Global Iter: 578900 training loss: 0.700835
Global Iter: 578900 training acc: 0.4375
Global Iter: 579000 training loss: 0.661407
Global Iter: 579000 training acc: 0.78125
Global Iter: 579100 training loss: 0.6957
Global Iter: 579100 training acc: 0.5
Global Iter: 579200 training loss: 0.692603
Global Iter: 579200 training acc: 0.53125
Global Iter: 579300 training loss: 0.680153
Global Iter: 579300 training acc: 0.625
Global Iter: 579400 training loss: 0.68652
Global Iter: 579400 training acc: 0.5625
Global Iter: 579500 training loss: 0.716216
Global Iter: 579500 training acc: 0.3125
Global Iter: 579600 training loss: 0.709374
Global Iter: 579600 training acc: 0.40625
Global Iter: 579700 training loss: 0.679222
Global Iter: 579700 training acc: 0.625
Global Iter: 579800 training loss: 0.704398
Global Iter: 579800 training acc: 0.4375
Global Iter: 579900 training loss: 0.669875
Global Iter: 579900 training acc: 0.6875
Global Iter: 580000 training loss: 0.710374
Global Iter: 580000 training acc: 0.375
Global Iter: 580100 training loss: 0.684076
Global Iter: 580100 training acc: 0.59375
Global Iter: 580200 training loss: 0.68341
Global Iter: 580200 training acc: 0.59375
Global Iter: 580300 training loss: 0.691803
Global Iter: 580300 training acc: 0.5625
Global Iter: 580400 training loss: 0.686394
Global Iter: 580400 training acc: 0.5625
Global Iter: 580500 training loss: 0.696006
Global Iter: 580500 training acc: 0.5
Global Iter: 580600 training loss: 0.687683
Global Iter: 580600 training acc: 0.5625
Global Iter: 580700 training loss: 0.692199
Global Iter: 580700 training acc: 0.53125
Global Iter: 580800 training loss: 0.68503
Global Iter: 580800 training acc: 0.5625
Global Iter: 580900 training loss: 0.715205
Global Iter: 580900 training acc: 0.34375
Global Iter: 581000 training loss: 0.661614
Global Iter: 581000 training acc: 0.71875
Global Iter: 581100 training loss: 0.684099
Global Iter: 581100 training acc: 0.59375
Global Iter: 581200 training loss: 0.70245
Global Iter: 581200 training acc: 0.46875
Global Iter: 581300 training loss: 0.684793
Global Iter: 581300 training acc: 0.59375
Global Iter: 581400 training loss: 0.682348
Global Iter: 581400 training acc: 0.59375
Global Iter: 581500 training loss: 0.705343
Global Iter: 581500 training acc: 0.46875
Global Iter: 581600 training loss: 0.675971
Global Iter: 581600 training acc: 0.65625
Global Iter: 581700 training loss: 0.692292
Global Iter: 581700 training acc: 0.53125
Global Iter: 581800 training loss: 0.697097
Global Iter: 581800 training acc: 0.46875
Global Iter: 581900 training loss: 0.663611
Global Iter: 581900 training acc: 0.75
Global Iter: 582000 training loss: 0.678218
Global Iter: 582000 training acc: 0.625
Global Iter: 582100 training loss: 0.706472
Global Iter: 582100 training acc: 0.40625
Global Iter: 582200 training loss: 0.699666
Global Iter: 582200 training acc: 0.5
Global Iter: 582300 training loss: 0.68908
Global Iter: 582300 training acc: 0.5625
Global Iter: 582400 training loss: 0.736446
Global Iter: 582400 training acc: 0.21875
Global Iter: 582500 training loss: 0.701471
Global Iter: 582500 training acc: 0.4375
Global Iter: 582600 training loss: 0.684082
Global Iter: 582600 training acc: 0.5625
Global Iter: 582700 training loss: 0.689766
Global Iter: 582700 training acc: 0.53125
Global Iter: 582800 training loss: 0.681787
Global Iter: 582800 training acc: 0.625
Global Iter: 582900 training loss: 0.695536
Global Iter: 582900 training acc: 0.53125
Global Iter: 583000 training loss: 0.680675
Global Iter: 583000 training acc: 0.65625
Global Iter: 583100 training loss: 0.695501
Global Iter: 583100 training acc: 0.46875
Global Iter: 583200 training loss: 0.69574
Global Iter: 583200 training acc: 0.5
Global Iter: 583300 training loss: 0.692697
Global Iter: 583300 training acc: 0.53125
Global Iter: 583400 training loss: 0.696954
Global Iter: 583400 training acc: 0.5
Global Iter: 583500 training loss: 0.689644
Global Iter: 583500 training acc: 0.5625
Global Iter: 583600 training loss: 0.66845
Global Iter: 583600 training acc: 0.71875
Global Iter: 583700 training loss: 0.692058
Global Iter: 583700 training acc: 0.53125
Global Iter: 583800 training loss: 0.709171
Global Iter: 583800 training acc: 0.375
Global Iter: 583900 training loss: 0.695451
Global Iter: 583900 training acc: 0.5
Global Iter: 584000 training loss: 0.696981
Global Iter: 584000 training acc: 0.5
Global Iter: 584100 training loss: 0.696962
Global Iter: 584100 training acc: 0.5
Global Iter: 584200 training loss: 0.668147
Global Iter: 584200 training acc: 0.71875
Global Iter: 584300 training loss: 0.696809
Global Iter: 584300 training acc: 0.46875
Global Iter: 584400 training loss: 0.68594
Global Iter: 584400 training acc: 0.59375
Global Iter: 584500 training loss: 0.684659
Global Iter: 584500 training acc: 0.625
Global Iter: 584600 training loss: 0.680698
Global Iter: 584600 training acc: 0.625
Global Iter: 584700 training loss: 0.691476
Global Iter: 584700 training acc: 0.53125
Global Iter: 584800 training loss: 0.677392
Global Iter: 584800 training acc: 0.65625
Global Iter: 584900 training loss: 0.665408
Global Iter: 584900 training acc: 0.75
Global Iter: 585000 training loss: 0.690652
Global Iter: 585000 training acc: 0.53125
Global Iter: 585100 training loss: 0.687103
Global Iter: 585100 training acc: 0.5625
Global Iter: 585200 training loss: 0.688809
Global Iter: 585200 training acc: 0.53125
Global Iter: 585300 training loss: 0.730998
Global Iter: 585300 training acc: 0.25
Global Iter: 585400 training loss: 0.700398
Global Iter: 585400 training acc: 0.46875
Global Iter: 585500 training loss: 0.699969
Global Iter: 585500 training acc: 0.4375
Global Iter: 585600 training loss: 0.688518
Global Iter: 585600 training acc: 0.53125
Global Iter: 585700 training loss: 0.708302
Global Iter: 585700 training acc: 0.375
Global Iter: 585800 training loss: 0.665536
Global Iter: 585800 training acc: 0.71875
Global Iter: 585900 training loss: 0.690751
Global Iter: 585900 training acc: 0.5625
Global Iter: 586000 training loss: 0.694287
Global Iter: 586000 training acc: 0.5
Global Iter: 586100 training loss: 0.676667
Global Iter: 586100 training acc: 0.625
Global Iter: 586200 training loss: 0.691813
Global Iter: 586200 training acc: 0.46875
Global Iter: 586300 training loss: 0.686452
Global Iter: 586300 training acc: 0.5625
Global Iter: 586400 training loss: 0.691798
Global Iter: 586400 training acc: 0.53125
Global Iter: 586500 training loss: 0.69812
Global Iter: 586500 training acc: 0.5
Global Iter: 586600 training loss: 0.694443
Global Iter: 586600 training acc: 0.5
Global Iter: 586700 training loss: 0.692167
Global Iter: 586700 training acc: 0.53125
Global Iter: 586800 training loss: 0.712786
Global Iter: 586800 training acc: 0.375
Global Iter: 586900 training loss: 0.679837
Global Iter: 586900 training acc: 0.625
Global Iter: 587000 training loss: 0.676366
Global Iter: 587000 training acc: 0.625
Global Iter: 587100 training loss: 0.702103
Global Iter: 587100 training acc: 0.4375
Global Iter: 587200 training loss: 0.69412
Global Iter: 587200 training acc: 0.53125
Global Iter: 587300 training loss: 0.689572
Global Iter: 587300 training acc: 0.5625
Global Iter: 587400 training loss: 0.685904
Global Iter: 587400 training acc: 0.5625
Global Iter: 587500 training loss: 0.676294
Global Iter: 587500 training acc: 0.625
Global Iter: 587600 training loss: 0.695683
Global Iter: 587600 training acc: 0.53125
Global Iter: 587700 training loss: 0.703008
Global Iter: 587700 training acc: 0.4375
Global Iter: 587800 training loss: 0.684109
Global Iter: 587800 training acc: 0.625
Global Iter: 587900 training loss: 0.691321
Global Iter: 587900 training acc: 0.5
Global Iter: 588000 training loss: 0.665623
Global Iter: 588000 training acc: 0.6875
Global Iter: 588100 training loss: 0.676851
Global Iter: 588100 training acc: 0.625
Global Iter: 588200 training loss: 0.708352
Global Iter: 588200 training acc: 0.40625
Global Iter: 588300 training loss: 0.694051
Global Iter: 588300 training acc: 0.53125
Global Iter: 588400 training loss: 0.682678
Global Iter: 588400 training acc: 0.59375
Global Iter: 588500 training loss: 0.694065
Global Iter: 588500 training acc: 0.53125
Global Iter: 588600 training loss: 0.693779
Global Iter: 588600 training acc: 0.53125
Global Iter: 588700 training loss: 0.696505
Global Iter: 588700 training acc: 0.5
Global Iter: 588800 training loss: 0.703598
Global Iter: 588800 training acc: 0.4375
Global Iter: 588900 training loss: 0.656401
Global Iter: 588900 training acc: 0.78125
Global Iter: 589000 training loss: 0.703779
Global Iter: 589000 training acc: 0.46875
Global Iter: 589100 training loss: 0.685744
Global Iter: 589100 training acc: 0.5625
Global Iter: 589200 training loss: 0.686009
Global Iter: 589200 training acc: 0.59375
Global Iter: 589300 training loss: 0.68953
Global Iter: 589300 training acc: 0.5625
Global Iter: 589400 training loss: 0.714611
Global Iter: 589400 training acc: 0.34375
Global Iter: 589500 training loss: 0.702923
Global Iter: 589500 training acc: 0.40625
Global Iter: 589600 training loss: 0.687596
Global Iter: 589600 training acc: 0.5625
Global Iter: 589700 training loss: 0.713149
Global Iter: 589700 training acc: 0.40625
Global Iter: 589800 training loss: 0.679053
Global Iter: 589800 training acc: 0.65625
Global Iter: 589900 training loss: 0.708996
Global Iter: 589900 training acc: 0.40625
Global Iter: 590000 training loss: 0.681555
Global Iter: 590000 training acc: 0.625
Global Iter: 590100 training loss: 0.683425
Global Iter: 590100 training acc: 0.5625
Global Iter: 590200 training loss: 0.688417
Global Iter: 590200 training acc: 0.5625
Global Iter: 590300 training loss: 0.68497
Global Iter: 590300 training acc: 0.5625
Global Iter: 590400 training loss: 0.693497
Global Iter: 590400 training acc: 0.5
Global Iter: 590500 training loss: 0.687725
Global Iter: 590500 training acc: 0.5625
Global Iter: 590600 training loss: 0.689232
Global Iter: 590600 training acc: 0.5625
Global Iter: 590700 training loss: 0.684495
Global Iter: 590700 training acc: 0.5625
Global Iter: 590800 training loss: 0.709517
Global Iter: 590800 training acc: 0.34375
Global Iter: 590900 training loss: 0.673971
Global Iter: 590900 training acc: 0.6875
Global Iter: 591000 training loss: 0.685411
Global Iter: 591000 training acc: 0.59375
Global Iter: 591100 training loss: 0.700996
Global Iter: 591100 training acc: 0.46875
Global Iter: 591200 training loss: 0.687173
Gl